[
  {
    "id": "gitlab-ci-02-variables-secrets",
    "slug": "gitlab-ci-02-variables-secrets",
    "path": "backend/devops",
    "fullPath": "backend/devops/gitlab-ci-02-variables-secrets",
    "title": "GitLab CI/CD 시리즈 #2: Variables와 Secrets 관리",
    "excerpt": "GitLab CI/CD의 변수 유형과 우선순위, Protected/Masked 변수, 그리고 Vault 연동까지 시크릿 관리 전략을 다룹니다.",
    "content": "# GitLab CI/CD 시리즈 #2: Variables와 Secrets 관리\n\n## 시리즈 개요\n\n| # | 주제 | 핵심 내용 |\n|---|------|----------|\n| 1 | 기초 | .gitlab-ci.yml 구조, Stages, Jobs, Pipeline 흐름 |\n| **2** | **Variables & Secrets** | 변수 유형, 우선순위, 외부 Vault 연동 |\n| 3 | Runners & Executors | Docker, Kubernetes, Docker-in-Docker |\n| 4 | Pipeline 아키텍처 | Parent-Child, Multi-Project Pipeline |\n| 5 | 고급 Job 제어 | rules, needs, DAG, extends |\n| 6 | 외부 통합 | Triggers, Webhooks, API |\n\n---\n\n## Variables 개요\n\nGitLab CI/CD에서 **Variables**는 파이프라인 동작을 제어하고, 환경별 설정을 관리하며, 시크릿을 전달하는 핵심 메커니즘입니다.\n\n### 변수 유형\n\n| 유형 | 정의 위치 | 예시 |\n|-----|----------|------|\n| **Predefined** | GitLab 자동 제공 | `$CI_COMMIT_SHA`, `$CI_JOB_ID` |\n| **File-defined** | `.gitlab-ci.yml` | `variables:` 블록 |\n| **Project** | Settings > CI/CD | API 키, 배포 토큰 |\n| **Group** | 그룹 Settings | 공통 시크릿 |\n| **Instance** | Admin > CI/CD | 전역 설정 |\n\n---\n\n## Predefined Variables\n\nGitLab은 **200개 이상의 사전 정의 변수**를 제공합니다. 파이프라인 콘텍스트 정보를 담고 있습니다.\n\n### 자주 사용하는 변수\n\n```yaml\njob:\n  script:\n    # Pipeline 정보\n    - echo \"Pipeline ID: $CI_PIPELINE_ID\"\n    - echo \"Pipeline Source: $CI_PIPELINE_SOURCE\"\n    \n    # Commit 정보\n    - echo \"Branch: $CI_COMMIT_BRANCH\"\n    - echo \"Tag: $CI_COMMIT_TAG\"\n    - echo \"SHA: $CI_COMMIT_SHA\"\n    - echo \"Short SHA: $CI_COMMIT_SHORT_SHA\"\n    - echo \"Message: $CI_COMMIT_MESSAGE\"\n    \n    # Merge Request\n    - echo \"MR IID: $CI_MERGE_REQUEST_IID\"\n    - echo \"Target Branch: $CI_MERGE_REQUEST_TARGET_BRANCH_NAME\"\n    \n    # Project\n    - echo \"Project Path: $CI_PROJECT_PATH\"\n    - echo \"Project URL: $CI_PROJECT_URL\"\n    \n    # Job\n    - echo \"Job Name: $CI_JOB_NAME\"\n    - echo \"Job Stage: $CI_JOB_STAGE\"\n```\n\n### Pipeline Source 값\n\n| 값 | 트리거 |\n|---|--------|\n| `push` | 브랜치 푸시 |\n| `merge_request_event` | MR 생성/업데이트 |\n| `schedule` | 스케줄 파이프라인 |\n| `web` | UI에서 수동 실행 |\n| `api` | API 호출 |\n| `trigger` | 다른 파이프라인에서 트리거 |\n| `pipeline` | 멀티 프로젝트 파이프라인 |\n| `parent_pipeline` | Parent 파이프라인에서 |\n\n---\n\n## File-defined Variables\n\n`.gitlab-ci.yml`에서 직접 변수를 정의합니다.\n\n### 전역 변수\n\n```yaml\nvariables:\n  NODE_ENV: \"production\"\n  DOCKER_REGISTRY: \"registry.gitlab.com\"\n  \nstages:\n  - build\n  - test\n\nbuild:\n  script:\n    - echo \"Environment: $NODE_ENV\"\n    - docker build -t $DOCKER_REGISTRY/myapp .\n```\n\n### Job 레벨 변수\n\n```yaml\nvariables:\n  GLOBAL_VAR: \"global\"\n\njob1:\n  variables:\n    JOB_VAR: \"job1-specific\"\n  script:\n    - echo \"$GLOBAL_VAR\"  # global\n    - echo \"$JOB_VAR\"     # job1-specific\n\njob2:\n  script:\n    - echo \"$GLOBAL_VAR\"  # global\n    - echo \"$JOB_VAR\"     # (비어있음)\n```\n\n### 변수 확장 (Expansion)\n\n변수 안에서 다른 변수를 참조할 수 있습니다:\n\n```yaml\nvariables:\n  PROJECT: \"myapp\"\n  VERSION: \"1.0.0\"\n  IMAGE_TAG: \"$PROJECT:$VERSION\"  # myapp:1.0.0\n\njob:\n  script:\n    - docker build -t $IMAGE_TAG .\n```\n\n### 확장 비활성화\n\n```yaml\nvariables:\n  # 방법 1: 따옴표와 이스케이프\n  LITERAL: \"$$NOT_EXPANDED\"\n  \n  # 방법 2: expand: false (GitLab 15.6+)\n  PASSWORD:\n    value: \"pa$$word\"\n    expand: false\n```\n\n---\n\n## Project/Group Variables (UI 설정)\n\n민감한 정보는 `.gitlab-ci.yml`에 직접 작성하지 않고 **GitLab UI**에서 설정합니다.\n\n### 설정 경로\n\n- **Project**: `Settings > CI/CD > Variables`\n- **Group**: `Group Settings > CI/CD > Variables`\n- **Instance**: `Admin > Settings > CI/CD > Variables`\n\n### 변수 옵션\n\n| 옵션 | 설명 |\n|------|------|\n| **Protected** | Protected 브랜치/태그에서만 사용 가능 |\n| **Masked** | Job 로그에서 `[MASKED]`로 표시 |\n| **Expanded** | 다른 변수 참조 허용 (기본 true) |\n\n### Protected Variables\n\n```mermaid\nflowchart TB\n    subgraph Variables [\"Protected Variable: PROD_API_KEY\"]\n        V[\"PROD_API_KEY = secret\"]\n    end\n    \n    subgraph Branches\n        Main[\"main - Protected ✓\"]\n        Feature[\"feature/* - 일반\"]\n    end\n    \n    V -->|접근 가능| Main\n    V -->|접근 불가| Feature\n```\n\n> [!IMPORTANT]\n> **Protected 변수**는 Protected 브랜치/태그에서 실행되는 파이프라인에서만 사용 가능합니다. `feature/*` 브랜치에서 프로덕션 시크릿 접근을 방지합니다.\n\n### Masked Variables\n\n```yaml\njob:\n  script:\n    - echo \"Token: $SECRET_TOKEN\"\n    # 로그 출력: Token: [MASKED]\n```\n\n**Masking 조건**:\n\n- 최소 8자\n- Base64 유효 문자만 (`A-Za-z0-9+/=@:.~`)\n- 여러 줄 불가\n\n---\n\n## Variable 우선순위\n\n동일한 이름의 변수가 여러 위치에 정의된 경우, **우선순위**에 따라 값이 결정됩니다.\n\n### 우선순위 (높은 순)\n\n```mermaid\nflowchart TB\n    T[Trigger variables] --> M\n    M[Manual pipeline variables] --> S\n    S[Scheduled pipeline variables] --> J\n    J[Job-level variables] --> G\n    G[Global .gitlab-ci.yml] --> P\n    P[Project variables] --> GR\n    GR[Group variables] --> I\n    I[Instance variables] --> PR\n    PR[Predefined variables]\n```\n\n1. **Trigger variables** (트리거 시 전달)\n2. **Manual pipeline variables** (수동 실행 시 입력)\n3. **Scheduled pipeline variables** (스케줄 설정)\n4. **Job-level** (`.gitlab-ci.yml` Job 내)\n5. **Global** (`.gitlab-ci.yml` 전역)\n6. **Project variables** (UI 설정)\n7. **Group variables** (상위 그룹 포함)\n8. **Instance variables** (Admin 설정)\n9. **Predefined variables** (GitLab 제공)\n\n### 예시: 우선순위 확인\n\n```yaml\nvariables:\n  MY_VAR: \"from-gitlab-ci\"  # 5번 우선순위\n\njob:\n  variables:\n    MY_VAR: \"from-job\"      # 4번 우선순위 (이 값 사용)\n  script:\n    - echo \"$MY_VAR\"        # from-job\n```\n\n---\n\n## dotenv Artifacts\n\nJob 간에 **동적으로 생성된 변수**를 전달할 수 있습니다.\n\n### 변수 생성 및 전파\n\n```yaml\ngenerate-version:\n  stage: build\n  script:\n    - VERSION=$(date +%Y%m%d%H%M%S)\n    - echo \"VERSION=$VERSION\" >> build.env\n    - echo \"COMMIT_HASH=$CI_COMMIT_SHORT_SHA\" >> build.env\n  artifacts:\n    reports:\n      dotenv: build.env\n\nuse-version:\n  stage: deploy\n  needs:\n    - job: generate-version\n      artifacts: true\n  script:\n    - echo \"Deploying version: $VERSION\"\n    - echo \"Commit: $COMMIT_HASH\"\n```\n\n### 동작 원리\n\n```mermaid\nsequenceDiagram\n    participant Gen as generate-version\n    participant GitLab as GitLab\n    participant Use as use-version\n    \n    Gen->>Gen: echo \"VERSION=123\" >> build.env\n    Gen->>GitLab: artifacts upload (dotenv)\n    GitLab->>Use: artifacts download\n    Use->>Use: $VERSION 변수 사용 가능\n```\n\n> [!TIP]\n> **dotenv**는 Job 간 동적 값 전달에 유용합니다. 빌드 버전, Git 태그, 계산된 값 등을 전달할 때 사용합니다.\n\n---\n\n## File Type Variables\n\n큰 데이터나 인증서는 **File 타입 변수**로 저장합니다.\n\n### 설정 (UI)\n\n1. `Settings > CI/CD > Variables`\n2. `Type: File` 선택\n3. 파일 내용 입력\n\n### 사용\n\n```yaml\njob:\n  script:\n    # 변수 값은 파일 경로\n    - cat $KUBE_CONFIG  # 파일 내용 출력\n    - kubectl --kubeconfig=$KUBE_CONFIG get pods\n```\n\n### 일반적인 용도\n\n- Kubernetes kubeconfig\n- GCP 서비스 계정 JSON\n- SSH 프라이빗 키\n- TLS 인증서\n\n---\n\n## External Secrets 연동\n\n### HashiCorp Vault\n\nGitLab Premium 이상에서 Vault와 네이티브 통합을 지원합니다.\n\n```yaml\njob:\n  id_tokens:\n    VAULT_ID_TOKEN:\n      aud: https://vault.example.com\n  secrets:\n    DATABASE_PASSWORD:\n      vault: production/db/password@secrets\n      file: false\n  script:\n    - echo \"Password: $DATABASE_PASSWORD\"\n```\n\n### Vault JWT 인증 (무료)\n\n```yaml\nvariables:\n  VAULT_ADDR: \"https://vault.example.com\"\n\nget-secrets:\n  image: hashicorp/vault:latest\n  script:\n    - export VAULT_TOKEN=$(vault write -field=token auth/jwt/login role=myapp jwt=$CI_JOB_JWT)\n    - vault kv get -field=password secret/myapp/db > db_password.txt\n  artifacts:\n    paths:\n      - db_password.txt\n```\n\n### AWS Secrets Manager\n\n```yaml\nvariables:\n  AWS_DEFAULT_REGION: ap-northeast-2\n\nget-aws-secret:\n  image: amazon/aws-cli:latest\n  script:\n    - SECRET=$(aws secretsmanager get-secret-value --secret-id prod/db/password --query SecretString --output text)\n    - echo \"SECRET=$SECRET\" >> aws.env\n  artifacts:\n    reports:\n      dotenv: aws.env\n```\n\n---\n\n## 변수 보안 Best Practices\n\n### 1. Protected 변수 활용\n\n```yaml\n# 프로덕션 배포는 Protected 브랜치에서만\ndeploy-prod:\n  script:\n    - deploy --token $PROD_DEPLOY_TOKEN  # Protected 변수\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n```\n\n### 2. Masked 변수 필수화\n\n민감한 모든 변수에 Masked 옵션을 활성화합니다.\n\n### 3. 최소 권한 원칙\n\n```yaml\n# 잘못된 예: 모든 Job에서 프로덕션 시크릿 접근\nvariables:\n  PROD_DB_PASSWORD: $PROD_DB_PASSWORD  # 위험!\n\n# 올바른 예: 필요한 Job에서만 사용\ndeploy-prod:\n  variables:\n    DB_PASSWORD: $PROD_DB_PASSWORD\n  script:\n    - deploy.sh\n```\n\n### 4. 시크릿 로테이션\n\n정기적으로 시크릿을 교체하고, 변수 업데이트 시 기존 파이프라인에 영향 없이 새 값이 적용됩니다.\n\n---\n\n## 정리\n\n| 개념 | 설명 |\n|-----|------|\n| **Predefined** | GitLab이 자동 제공하는 200+ 변수 |\n| **File-defined** | `.gitlab-ci.yml`에 정의 |\n| **Project/Group** | UI에서 설정, 시크릿 저장 |\n| **Protected** | Protected 브랜치/태그에서만 접근 |\n| **Masked** | 로그에서 `[MASKED]` 처리 |\n| **dotenv** | Job 간 동적 변수 전달 |\n| **우선순위** | Trigger > Job > Global > Project > Group > Instance |\n\n---\n\n## 다음 편 예고\n\n**3편: Runners와 Executors**에서는 다음을 다룹니다:\n\n- Runner 유형 (Shared, Group, Project)\n- Executor 종류 (Shell, Docker, Kubernetes)\n- **Docker-in-Docker (DinD)** 심화\n- TLS 설정과 보안\n- Runner 성능 최적화\n\n---\n\n## 참고 자료\n\n- [GitLab CI/CD Variables](https://docs.gitlab.com/ee/ci/variables/)\n- [Predefined Variables Reference](https://docs.gitlab.com/ee/ci/variables/predefined_variables.html)\n- [Using External Secrets](https://docs.gitlab.com/ee/ci/secrets/)\n- [Variable Precedence](https://docs.gitlab.com/ee/ci/variables/#cicd-variable-precedence)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitLab",
      "CI/CD",
      "Pipeline",
      "Secrets"
    ],
    "readingTime": 7,
    "wordCount": 1249,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitlab-ci",
    "date": "Mon Jan 05 2026 09:00:00 GMT+0900 (Korean Standard Time)"
  },
  {
    "id": "indexer-06-idempotency",
    "slug": "indexer-06-idempotency",
    "path": "blockchain/indexer",
    "fullPath": "blockchain/indexer/indexer-06-idempotency",
    "title": "멱등성 있는 인덱서 핸들러 설계 - 재처리 안전성 확보",
    "excerpt": "시리즈 목차 블록체인 인덱서란? 인덱서 아키텍처 Deep Dive 이력 테이블 vs 스냅샷 테이블 Rust로 인덱서 SDK 만들기",
    "content": "## 시리즈 목차\n\n1. [블록체인 인덱서란?](/blog/blockchain/indexer/indexer-01-introduction)\n2. [인덱서 아키텍처 Deep Dive](/blog/blockchain/indexer/indexer-02-architecture)\n3. [이력 테이블 vs 스냅샷 테이블](/blog/blockchain/indexer/indexer-03-database-design)\n4. [Rust로 인덱서 SDK 만들기](/blog/blockchain/indexer/indexer-04-rust-sdk)\n5. [Diesel ORM 실전 활용](/blog/blockchain/indexer/indexer-05-diesel-orm)\n6. **멱등성 있는 인덱서 핸들러 설계** (현재 글)\n\n---\n\n## 멱등성(Idempotency)이란?\n\n> **같은 연산을 여러 번 수행해도 결과가 동일한 성질**\n\n```\nf(x) = f(f(x)) = f(f(f(x))) = ...\n```\n\n### 예시\n\n| 연산 | 멱등성 |\n|------|--------|\n| `x = 5` | ✅ 멱등 (몇 번 해도 x=5) |\n| `x = x + 1` | ❌ 비멱등 (할 때마다 증가) |\n| `INSERT ON CONFLICT UPDATE` | ✅ 멱등 |\n| `INSERT` (중복 시 에러) | ❌ 비멱등 |\n\n---\n\n## 왜 인덱서에서 멱등성이 중요한가?\n\n인덱서는 **여러 상황에서 동일 트랜잭션을 재처리**할 수 있습니다:\n\n### 재처리가 발생하는 상황\n\n```mermaid\nflowchart TD\n    subgraph Scenarios[\"재처리 시나리오\"]\n        S1[\"1. 프로세서 재시작\"]\n        S2[\"2. 네트워크 장애 후 재연결\"]\n        S3[\"3. 수동 복구 (Restore)\"]\n        S4[\"4. 롤백 후 재처리\"]\n    end\n    \n    subgraph Result[\"이상적인 결과\"]\n        R[\"동일한 데이터 상태\"]\n    end\n    \n    S1 --> R\n    S2 --> R\n    S3 --> R\n    S4 --> R\n```\n\n### 멱등성 없는 핸들러의 문제\n\n```sql\n-- ❌ 비멱등 쿼리: 같은 트랜잭션 재처리 시\nINSERT INTO balances (address, amount) VALUES ('0x123', 100);\nINSERT INTO balances (address, amount) VALUES ('0x123', 100);  -- 중복 에러!\n\n-- 또는 잔액 증가 방식\nUPDATE balances SET amount = amount + 100 WHERE address = '0x123';\nUPDATE balances SET amount = amount + 100 WHERE address = '0x123';  -- 200이 됨!\n```\n\n---\n\n## 멱등 쿼리 작성법\n\n### 1. Upsert + 버전 체크\n\n5편에서 다룬 패턴의 핵심:\n\n```sql\nINSERT INTO current_nft_owners (nft_id, owner_address, last_version)\nVALUES ('nft#1', 'addr_C', 300)\nON CONFLICT (nft_id) \nDO UPDATE SET \n    owner_address = EXCLUDED.owner_address,\n    last_version = EXCLUDED.last_version\nWHERE current_nft_owners.last_version < EXCLUDED.last_version;\n```\n\n**핵심**: `WHERE` 조건으로 **더 오래된 버전은 무시**\n\n### 2. 이력 테이블의 복합 PK\n\n```sql\nCREATE TABLE nft_transfers (\n    nft_id TEXT,\n    transaction_version BIGINT,\n    -- 복합 PK로 중복 방지\n    PRIMARY KEY (nft_id, transaction_version),\n    ...\n);\n\n-- 동일 트랜잭션 재처리해도 안전\nINSERT INTO nft_transfers (nft_id, transaction_version, ...)\nVALUES ('nft#1', 300, ...)\nON CONFLICT (nft_id, transaction_version) DO NOTHING;\n```\n\n### 3. 절대값 저장 vs 상대값 저장\n\n```\n❌ 상대값 (비멱등)\n\"잔액을 100 증가시킨다\" → UPDATE SET balance = balance + 100\n\n✅ 절대값 (멱등)\n\"잔액을 500으로 설정한다\" → UPDATE SET balance = 500\n```\n\n---\n\n## 핸들러 설계 패턴\n\n### 멱등 핸들러 구조\n\n```rust\n#[async_trait]\nimpl TransactionHandler for NftHandler {\n    async fn process(&self, ctx: TransactionContext) -> Result<Option<TransactionContext>> {\n        for tx in &ctx.transactions {\n            let events = self.parse_events(tx)?;\n            \n            for event in events {\n                // 멱등한 저장 로직\n                self.store_idempotent(&event).await?;\n            }\n        }\n        \n        Ok(Some(ctx))\n    }\n}\n\nimpl NftHandler {\n    async fn store_idempotent(&self, event: &TransferEvent) -> Result<()> {\n        let mut conn = self.pool.get().await?;\n        \n        // 1. 이력 테이블: INSERT ... ON CONFLICT DO NOTHING\n        diesel::insert_into(transfer_history::table)\n            .values(HistoryRow::from(event))\n            .on_conflict((transfer_history::nft_id, transfer_history::tx_version))\n            .do_nothing()\n            .execute(&mut conn)\n            .await?;\n        \n        // 2. 스냅샷 테이블: UPSERT with version check\n        diesel::insert_into(current_owners::table)\n            .values(CurrentRow::from(event))\n            .on_conflict(current_owners::nft_id)\n            .do_update()\n            .set((\n                current_owners::owner.eq(excluded(current_owners::owner)),\n                current_owners::version.eq(excluded(current_owners::version)),\n            ))\n            .filter(current_owners::version.lt(excluded(current_owners::version)))\n            .execute(&mut conn)\n            .await?;\n        \n        Ok(())\n    }\n}\n```\n\n---\n\n## 복구(Restore) API 설계\n\n### 왜 필요한가?\n\n- 인덱싱 버그 발견 후 **과거 데이터 수정**\n- 일부 트랜잭션 **누락** 발생 시 재처리\n- 새로운 테이블 추가 후 **과거 데이터 백필**\n\n### API 설계\n\n```rust\n#[derive(Deserialize)]\npub struct RestoreRequest {\n    pub start_version: u64,\n    pub end_version: u64,\n}\n\n#[derive(Serialize)]\npub struct RestoreResponse {\n    pub status: String,\n    pub processed_count: u64,\n}\n\n// POST /restore\npub async fn restore_handler(\n    State(state): State<AppState>,\n    Json(req): Json<RestoreRequest>,\n) -> Result<Json<RestoreResponse>, AppError> {\n    // 백그라운드 태스크로 처리\n    tokio::spawn(async move {\n        state.indexer\n            .process_range(req.start_version, req.end_version)\n            .await\n    });\n    \n    Ok(Json(RestoreResponse {\n        status: \"accepted\".to_string(),\n        processed_count: req.end_version - req.start_version + 1,\n    }))\n}\n```\n\n### 범위 재처리 구현\n\n```rust\nimpl Indexer {\n    pub async fn process_range(\n        &self,\n        start: u64,\n        end: u64,\n    ) -> Result<()> {\n        tracing::info!(start, end, \"Starting restore process\");\n        \n        // gRPC 스트림을 특정 범위로 요청\n        let stream = self.client\n            .get_transactions(start, Some(end))\n            .await?;\n        \n        while let Some(batch) = stream.next().await {\n            let batch = batch?;\n            \n            // 멱등 핸들러로 처리 (재처리해도 안전!)\n            self.handler.process(batch).await?;\n            \n            tracing::debug!(version = batch.last_version, \"Processed batch\");\n        }\n        \n        tracing::info!(start, end, \"Restore completed\");\n        Ok(())\n    }\n}\n```\n\n---\n\n## 버전 트래킹과 재시작\n\n### 처리 상태 테이블\n\n```sql\nCREATE TABLE processor_status (\n    processor_name TEXT PRIMARY KEY,\n    last_processed_version BIGINT NOT NULL,\n    last_updated_at TIMESTAMP DEFAULT NOW()\n);\n```\n\n### 버전 트래커 구현\n\n```rust\npub struct VersionTracker {\n    processor_name: String,\n    pool: Pool<AsyncPgConnection>,\n    update_interval: u64,  // N개 배치마다 업데이트\n}\n\nimpl VersionTracker {\n    pub async fn record(&self, version: u64) -> Result<()> {\n        diesel::insert_into(processor_status::table)\n            .values((\n                processor_status::processor_name.eq(&self.processor_name),\n                processor_status::last_processed_version.eq(version as i64),\n            ))\n            .on_conflict(processor_status::processor_name)\n            .do_update()\n            .set((\n                processor_status::last_processed_version.eq(version as i64),\n                processor_status::last_updated_at.eq(diesel::dsl::now),\n            ))\n            .execute(&mut self.pool.get().await?)\n            .await?;\n        \n        Ok(())\n    }\n    \n    pub async fn get_last_version(&self) -> Result<Option<u64>> {\n        processor_status::table\n            .filter(processor_status::processor_name.eq(&self.processor_name))\n            .select(processor_status::last_processed_version)\n            .first::<i64>(&mut self.pool.get().await?)\n            .await\n            .optional()?\n            .map(|v| v as u64)\n    }\n}\n```\n\n---\n\n## 멱등성 체크리스트\n\n인덱서 핸들러를 구현할 때 확인할 사항:\n\n| 항목 | 체크 |\n|------|------|\n| 이력 테이블에 복합 PK 사용? | ☐ |\n| `ON CONFLICT DO NOTHING` 또는 `DO UPDATE` 사용? | ☐ |\n| 스냅샷 테이블에 버전 체크 조건 추가? | ☐ |\n| 상대값(+/-) 대신 절대값 저장? | ☐ |\n| 복구 API 구현? | ☐ |\n| 버전 트래커로 진행률 기록? | ☐ |\n\n---\n\n## 시리즈 마무리\n\n6편에 걸쳐 블록체인 인덱서의 A to Z를 다뤘습니다:\n\n| 편 | 주제 | 핵심 |\n|----|------|------|\n| 1편 | 인덱서 개념 | 왜 필요한가 |\n| 2편 | ETL 아키텍처 | 어떻게 동작하는가 |\n| 3편 | DB 설계 | 이중 테이블 패턴 |\n| 4편 | SDK 설계 | Trait, 채널, 어댑터 |\n| 5편 | Diesel ORM | 배치 처리, Upsert |\n| 6편 | 멱등성 | 재처리 안전성 |\n\n이 시리즈가 블록체인 인덱서를 개발하거나 이해하는 데 도움이 되길 바랍니다.\n\n---\n\n## 참고 자료\n\n- [Aptos Indexer SDK Documentation](https://aptos.dev/build/indexer/indexer-sdk/documentation)\n- [Diesel - Getting Started](https://diesel.rs/guides/getting-started)\n- [PostgreSQL UPSERT](https://www.postgresql.org/docs/current/sql-insert.html#SQL-ON-CONFLICT)",
    "docType": "original",
    "category": "Uncategorized",
    "tags": [
      "blockchain",
      "indexer",
      "idempotency",
      "database",
      "architecture"
    ],
    "readingTime": 5,
    "wordCount": 919,
    "isFeatured": false,
    "isPublic": true,
    "series": "blockchain-indexer",
    "seriesOrder": 6,
    "date": "2026-01-05"
  },
  {
    "id": "indexer-05-diesel-orm",
    "slug": "indexer-05-diesel-orm",
    "path": "blockchain/indexer",
    "fullPath": "blockchain/indexer/indexer-05-diesel-orm",
    "title": "Diesel ORM 실전 활용 - 비동기 DB 처리와 배치 최적화",
    "excerpt": "시리즈 목차 블록체인 인덱서란? 인덱서 아키텍처 Deep Dive 이력 테이블 vs 스냅샷 테이블 Rust로 인덱서 SDK 만들기",
    "content": "## 시리즈 목차\n\n1. [블록체인 인덱서란?](/blog/blockchain/indexer/indexer-01-introduction)\n2. [인덱서 아키텍처 Deep Dive](/blog/blockchain/indexer/indexer-02-architecture)\n3. [이력 테이블 vs 스냅샷 테이블](/blog/blockchain/indexer/indexer-03-database-design)\n4. [Rust로 인덱서 SDK 만들기](/blog/blockchain/indexer/indexer-04-rust-sdk)\n5. **Diesel ORM 실전 활용** (현재 글)\n6. [멱등성 있는 인덱서 핸들러 설계](/blog/blockchain/indexer/indexer-06-idempotency)\n\n---\n\n## 왜 Diesel인가?\n\nRust 생태계에서 DB 접근 라이브러리를 선택할 때:\n\n| 라이브러리 | 특징 | 인덱서 적합성 |\n|-----------|------|--------------|\n| **Diesel** | 컴파일 타임 타입 안전, 매크로 기반 | ✅ 대량 처리 최적화 |\n| sqlx | 런타임 쿼리 검증, 매크로 옵션 | ⚠️ 복잡한 upsert 어려움 |\n| SeaORM | 비동기 우선, ActiveRecord 패턴 | ⚠️ 성능 오버헤드 |\n\nDiesel의 강점:\n\n- **컴파일 타임 검증**: SQL 오류를 런타임이 아닌 빌드 시점에 발견\n- **배치 처리 최적화**: `insert_into().values(&vec)` 한 줄로 대량 삽입\n- **Upsert 지원**: `ON CONFLICT` 구문을 타입 안전하게 작성\n\n---\n\n## diesel-async 설정\n\n### Cargo.toml\n\n```toml\n[dependencies]\ndiesel = { version = \"2.1\", features = [\n    \"postgres\",\n    \"chrono\",\n    \"serde_json\",\n    \"numeric\",\n] }\n\ndiesel-async = { \n    git = \"https://github.com/weiznich/diesel_async.git\",\n    features = [\n        \"postgres\",\n        \"bb8\",\n        \"async-connection-wrapper\",\n    ]\n}\n\nbb8 = \"0.8\"\ntokio = { version = \"1\", features = [\"full\"] }\n```\n\n### 커넥션 풀 설정\n\n```rust\nuse diesel_async::{\n    pooled_connection::{bb8::Pool, AsyncDieselConnectionManager},\n    AsyncPgConnection,\n};\n\npub async fn create_pool(\n    connection_string: &str,\n    pool_size: u32,\n) -> Result<Pool<AsyncPgConnection>> {\n    let config = AsyncDieselConnectionManager::<AsyncPgConnection>::new(\n        connection_string\n    );\n    \n    Pool::builder()\n        .max_size(pool_size)\n        .min_idle(Some(pool_size / 4))\n        .build(config)\n        .await\n        .map_err(|e| anyhow!(\"Failed to create pool: {}\", e))\n}\n```\n\n---\n\n## PostgreSQL 파라미터 수 제한\n\n### 문제 상황\n\nPostgreSQL wire protocol은 **16비트 정수**로 파라미터 개수를 표현합니다:\n\n```\n최대 파라미터 수 = 2^16 - 1 = 65,535\n```\n\n10개 컬럼 테이블에 10,000 row를 삽입하면:\n\n```\n파라미터 수 = 10 × 10,000 = 100,000 → 제한 초과!\n```\n\n### 해결책: 청크 분할\n\n```rust\n/// 대량 데이터를 청크로 나누어 병렬 처리\npub async fn execute_in_chunks<T, F, Fut>(\n    items: Vec<T>,\n    chunk_size: usize,\n    executor: F,\n) -> Result<()>\nwhere\n    T: Send + 'static,\n    F: Fn(Vec<T>) -> Fut + Send + Sync + Clone,\n    Fut: Future<Output = Result<()>> + Send,\n{\n    let chunks: Vec<Vec<T>> = items\n        .into_iter()\n        .collect::<Vec<_>>()\n        .chunks(chunk_size)\n        .map(|c| c.to_vec())\n        .collect();\n    \n    // 청크별 병렬 실행\n    let futures = chunks.into_iter().map(|chunk| {\n        let executor = executor.clone();\n        async move { executor(chunk).await }\n    });\n    \n    futures::future::try_join_all(futures).await?;\n    Ok(())\n}\n```\n\n### 청크 크기 계산\n\n```rust\n/// 안전한 청크 크기 계산\npub fn calculate_chunk_size(column_count: usize) -> usize {\n    const MAX_PARAMS: usize = 65_000;  // 여유 마진\n    MAX_PARAMS / column_count\n}\n\n// 예시: 10개 컬럼 테이블\nlet chunk_size = calculate_chunk_size(10);  // = 6,500\n```\n\n---\n\n## Upsert 패턴 (ON CONFLICT)\n\n### 기본 Upsert\n\n```rust\nuse diesel::prelude::*;\nuse diesel::upsert::excluded;\n\npub async fn upsert_nft_owners(\n    conn: &mut AsyncPgConnection,\n    items: Vec<NftOwnerRow>,\n) -> Result<()> {\n    use crate::schema::current_nft_owners::dsl::*;\n    \n    diesel::insert_into(current_nft_owners)\n        .values(&items)\n        .on_conflict(nft_id)\n        .do_update()\n        .set((\n            owner_address.eq(excluded(owner_address)),\n            last_transaction_version.eq(excluded(last_transaction_version)),\n            updated_at.eq(excluded(updated_at)),\n        ))\n        .execute(conn)\n        .await?;\n    \n    Ok(())\n}\n```\n\n### 조건부 Upsert (버전 체크)\n\n**더 오래된 버전이 덮어쓰지 않도록** 보호:\n\n```rust\npub async fn upsert_with_version_check(\n    conn: &mut AsyncPgConnection,\n    items: Vec<NftOwnerRow>,\n) -> Result<()> {\n    use crate::schema::current_nft_owners::dsl::*;\n    \n    diesel::insert_into(current_nft_owners)\n        .values(&items)\n        .on_conflict(nft_id)\n        .do_update()\n        .set((\n            owner_address.eq(excluded(owner_address)),\n            last_transaction_version.eq(excluded(last_transaction_version)),\n            updated_at.eq(excluded(updated_at)),\n        ))\n        // 핵심: 새 버전이 더 클 때만 업데이트\n        .filter(last_transaction_version.lt(excluded(last_transaction_version)))\n        .execute(conn)\n        .await?;\n    \n    Ok(())\n}\n```\n\n---\n\n## 이력 + 스냅샷 동시 저장\n\n3편에서 설명한 이중 테이블 패턴을 Diesel로 구현:\n\n```rust\npub async fn store_transfers(\n    conn: &mut AsyncPgConnection,\n    transfers: Vec<TransferEvent>,\n) -> Result<()> {\n    // 도메인 이벤트 → DB 모델 변환\n    let history_rows: Vec<TransferHistoryRow> = transfers\n        .iter()\n        .map(TransferHistoryRow::from)\n        .collect();\n    \n    let current_rows: Vec<CurrentOwnerRow> = transfers\n        .iter()\n        .map(CurrentOwnerRow::from)\n        .collect();\n    \n    // 트랜잭션 내에서 두 테이블 동시 갱신\n    conn.transaction::<_, diesel::result::Error, _>(|conn| async move {\n        // 1. 이력 테이블: INSERT\n        diesel::insert_into(transfer_history::table)\n            .values(&history_rows)\n            .execute(conn)\n            .await?;\n        \n        // 2. 스냅샷 테이블: UPSERT\n        diesel::insert_into(current_owners::table)\n            .values(&current_rows)\n            .on_conflict(current_owners::nft_id)\n            .do_update()\n            .set((\n                current_owners::owner_address\n                    .eq(excluded(current_owners::owner_address)),\n                current_owners::last_version\n                    .eq(excluded(current_owners::last_version)),\n            ))\n            .filter(current_owners::last_version\n                .lt(excluded(current_owners::last_version)))\n            .execute(conn)\n            .await?;\n        \n        Ok(())\n    }.scope_boxed()).await?;\n    \n    Ok(())\n}\n```\n\n---\n\n## 배치 저장 통합 예시\n\n모든 패턴을 통합한 실제 저장 로직:\n\n```rust\nimpl NftTransferHandler {\n    pub async fn store(&self, changes: Vec<TransferEvent>) -> Result<()> {\n        if changes.is_empty() {\n            return Ok(());\n        }\n        \n        // 청크 크기 계산 (TransferHistoryRow가 8컬럼이라고 가정)\n        let chunk_size = calculate_chunk_size(8);\n        \n        execute_in_chunks(changes, chunk_size, |chunk| {\n            let pool = self.db_pool.clone();\n            async move {\n                let mut conn = pool.get().await?;\n                store_transfers(&mut conn, chunk).await\n            }\n        }).await\n    }\n}\n```\n\n---\n\n## 커넥션 풀 최적화\n\n### 권장 설정\n\n```rust\nlet pool = Pool::builder()\n    // 최대 연결 수: CPU 코어 × 2~4\n    .max_size(num_cpus::get() as u32 * 2)\n    \n    // 최소 유휴 연결: 워밍업 시간 단축\n    .min_idle(Some(4))\n    \n    // 연결 획득 타임아웃\n    .connection_timeout(Duration::from_secs(30))\n    \n    // 유휴 연결 타임아웃\n    .idle_timeout(Some(Duration::from_secs(600)))\n    \n    .build(config)\n    .await?;\n```\n\n### 연결 풀 사이징 공식\n\n```\n최적 풀 크기 ≈ (코어 수 × 2) + 디스크 스핀들 수\n\n예시: 8코어 + NVMe SSD\n→ (8 × 2) + 1 = 17 connections\n```\n\n---\n\n## 정리\n\n- **diesel-async**: Diesel + Tokio 비동기 조합\n- **청크 분할**: 65,535 파라미터 제한 우회\n- **조건부 Upsert**: 버전 역전 방지\n- **이중 테이블 저장**: 트랜잭션 내 원자적 갱신\n- **커넥션 풀**: 적절한 사이징으로 성능 최적화\n\n---\n\n## 다음 편 예고\n\n**[6편: 멱등성 있는 인덱서 핸들러 설계 - 재처리 안전성 확보](/blog/blockchain/indexer/indexer-06-idempotency)**\n\n- 멱등성(Idempotency)이란?\n- 왜 인덱서에서 중요한가?\n- 멱등 쿼리 작성법\n- 복구(Restore) API 설계",
    "docType": "original",
    "category": "Uncategorized",
    "tags": [
      "blockchain",
      "indexer",
      "rust",
      "diesel",
      "postgresql",
      "async"
    ],
    "readingTime": 4,
    "wordCount": 800,
    "isFeatured": false,
    "isPublic": true,
    "series": "blockchain-indexer",
    "seriesOrder": 5,
    "date": "2026-01-05"
  },
  {
    "id": "indexer-04-rust-sdk",
    "slug": "indexer-04-rust-sdk",
    "path": "blockchain/indexer",
    "fullPath": "blockchain/indexer/indexer-04-rust-sdk",
    "title": "Rust로 인덱서 SDK 만들기 - 파이프라인 오케스트레이터 설계",
    "excerpt": "시리즈 목차 블록체인 인덱서란? 인덱서 아키텍처 Deep Dive 이력 테이블 vs 스냅샷 테이블 **Rust로 인덱서 SDK 만들기** (현재 글) [Diesel ORM 실전 활용](/blog/blockchain/i",
    "content": "## 시리즈 목차\n\n1. [블록체인 인덱서란?](/blog/blockchain/indexer/indexer-01-introduction)\n2. [인덱서 아키텍처 Deep Dive](/blog/blockchain/indexer/indexer-02-architecture)\n3. [이력 테이블 vs 스냅샷 테이블](/blog/blockchain/indexer/indexer-03-database-design)\n4. **Rust로 인덱서 SDK 만들기** (현재 글)\n5. [Diesel ORM 실전 활용](/blog/blockchain/indexer/indexer-05-diesel-orm)\n6. [멱등성 있는 인덱서 핸들러 설계](/blog/blockchain/indexer/indexer-06-idempotency)\n\n---\n\n## 왜 SDK가 필요한가?\n\n여러 도메인의 인덱서를 개발하다 보면 **반복되는 코드**가 있습니다:\n\n- gRPC 스트림 연결 및 재연결\n- 트랜잭션 필터링\n- 버전 트래킹\n- 배치 저장 로직\n- 헬스체크 엔드포인트\n\n이런 보일러플레이트를 **SDK로 추상화**하면:\n\n| 개별 구현 | SDK 사용 |\n|----------|---------|\n| 각 인덱서마다 500줄+ | 비즈니스 로직 50줄 |\n| 버그 수정 시 N곳 패치 | 한 곳만 수정 |\n| 일관성 없는 에러 처리 | 표준화된 패턴 |\n\n---\n\n## SDK 핵심 컴포넌트\n\n```mermaid\nflowchart TB\n    subgraph SDK[\"인덱서 SDK\"]\n        Orchestrator[\"파이프라인 오케스트레이터\"]\n        \n        subgraph Steps[\"처리 단계 (Steps)\"]\n            Extractor[\"이벤트 추출기\"]\n            Filter[\"트랜잭션 필터\"]\n            Handler[\"비즈니스 핸들러\"]\n            Tracker[\"버전 트래커\"]\n        end\n        \n        subgraph Infra[\"인프라\"]\n            Config[\"설정 로더\"]\n            Server[\"REST 서버\"]\n            Pool[\"DB 커넥션 풀\"]\n        end\n    end\n    \n    Orchestrator --> Steps\n    Orchestrator --> Infra\n```\n\n---\n\n## 1. 파이프라인 오케스트레이터\n\n오케스트레이터는 **전체 파이프라인의 생명주기를 관리**합니다:\n\n```rust\npub struct PipelineOrchestrator<H: TransactionHandler> {\n    config: IndexerConfig,\n    handler: H,\n    db_pool: Pool<AsyncPgConnection>,\n}\n\nimpl<H: TransactionHandler> PipelineOrchestrator<H> {\n    pub async fn run(&mut self) -> Result<()> {\n        // 1. DB 마이그레이션 실행\n        self.run_migrations().await?;\n        \n        // 2. 시작 버전 결정\n        let start_version = self.determine_starting_version().await?;\n        \n        // 3. 파이프라인 구성\n        let pipeline = self.build_pipeline(start_version);\n        \n        // 4. REST 서버 + 파이프라인 동시 실행\n        tokio::select! {\n            result = pipeline.run() => result,\n            result = self.run_server() => result,\n        }\n    }\n}\n```\n\n### 구성요소 연결\n\n```mermaid\nflowchart LR\n    subgraph Pipeline[\"파이프라인\"]\n        direction LR\n        Stream[\"gRPC\\n스트림\"]\n        Filter[\"필터\"]\n        Handler[\"핸들러\"]\n        Tracker[\"버전\\n트래커\"]\n    end\n    \n    Stream -->|Channel| Filter -->|Channel| Handler -->|Channel| Tracker\n```\n\n각 단계는 **채널(Channel)**로 연결되어 비동기적으로 데이터를 전달합니다.\n\n---\n\n## 2. 핸들러 Trait 설계\n\nSDK의 핵심은 **확장 포인트를 잘 정의**하는 것입니다.\n\n### Trait 정의\n\n```rust\n#[async_trait]\npub trait TransactionHandler: Clone + Send + Sync + 'static {\n    /// 트랜잭션 배치를 처리하고 결과를 반환\n    async fn process(\n        &self,\n        context: TransactionContext,\n    ) -> Result<Option<TransactionContext>, ProcessorError>;\n}\n```\n\n### 사용자 구현 예시\n\n```rust\n#[derive(Clone)]\npub struct NftTransferHandler {\n    db_pool: Pool<AsyncPgConnection>,\n}\n\n#[async_trait]\nimpl TransactionHandler for NftTransferHandler {\n    async fn process(\n        &self,\n        context: TransactionContext,\n    ) -> Result<Option<TransactionContext>, ProcessorError> {\n        // 1. 이벤트 파싱\n        let transfers = self.parse_transfer_events(&context.transactions)?;\n        \n        // 2. DB 저장\n        self.store_transfers(&transfers).await?;\n        \n        // 3. 다음 단계로 전달 (None이면 중단)\n        Ok(Some(context))\n    }\n}\n```\n\n### Trait 설계 원칙\n\n| 원칙 | 적용 |\n|------|------|\n| **Clone** | 멀티스레드 환경에서 핸들러 복제 |\n| **Send + Sync** | 스레드 간 안전한 공유 |\n| **'static** | 비동기 태스크에서 수명 관리 |\n| **Option 반환** | 파이프라인 중단 제어 |\n\n---\n\n## 3. Producer-Consumer 채널 패턴\n\n### 왜 채널인가?\n\n단계 간 직접 호출 대신 **채널을 사용하는 이유**:\n\n```\n❌ 직접 호출\n   Step1.process() → Step2.process() → Step3.process()\n   → 모든 단계가 동기적으로 블로킹\n\n✅ 채널 기반\n   Step1 → [Channel] → Step2 → [Channel] → Step3\n   → 각 단계가 독립적으로 병렬 실행\n```\n\n### 구현 패턴\n\n```rust\nuse tokio::sync::mpsc;\n\npub struct Step<T, U> {\n    receiver: mpsc::Receiver<T>,\n    sender: mpsc::Sender<U>,\n    processor: Box<dyn Processor<T, U>>,\n}\n\nimpl<T, U> Step<T, U> {\n    pub async fn run(mut self) {\n        while let Some(input) = self.receiver.recv().await {\n            match self.processor.process(input).await {\n                Ok(Some(output)) => {\n                    let _ = self.sender.send(output).await;\n                }\n                Ok(None) => continue,  // 필터링됨\n                Err(e) => {\n                    tracing::error!(\"Processing error: {}\", e);\n                    // 에러 처리 정책에 따라 계속/중단\n                }\n            }\n        }\n    }\n}\n```\n\n### 백프레셔(Backpressure)\n\n채널 버퍼가 가득 차면 **자동으로 생산 속도가 조절**됩니다:\n\n```rust\n// 버퍼 크기 10인 채널\nlet (tx, rx) = mpsc::channel::<TransactionBatch>(10);\n\n// 버퍼가 가득 차면 send가 대기\ntx.send(batch).await?;  // 10개 초과 시 여기서 블록\n```\n\n---\n\n## 4. 어댑터 패턴 적용\n\n사용자의 핸들러를 SDK 파이프라인에 연결하는 **어댑터**:\n\n```mermaid\nflowchart LR\n    subgraph SDK[\"SDK 내부\"]\n        Step[\"파이프라인 Step\"]\n    end\n    \n    subgraph Adapter[\"어댑터\"]\n        Wrapper[\"HandlerWrapper\"]\n    end\n    \n    subgraph User[\"사용자 코드\"]\n        Handler[\"TransactionHandler\"]\n    end\n    \n    Step -->|인터페이스| Wrapper -->|위임| Handler\n```\n\n### 구현\n\n```rust\npub struct HandlerWrapper<H: TransactionHandler> {\n    handler: H,\n}\n\nimpl<H: TransactionHandler> HandlerWrapper<H> {\n    pub fn new(handler: H) -> Self {\n        Self { handler }\n    }\n}\n\n#[async_trait]\nimpl<H: TransactionHandler> PipelineStep for HandlerWrapper<H> {\n    type Input = TransactionContext;\n    type Output = TransactionContext;\n    \n    async fn process(&self, input: Self::Input) -> Result<Option<Self::Output>> {\n        self.handler.process(input).await\n    }\n}\n```\n\n---\n\n## 5. 설정과 시작 버전\n\n### 설정 구조\n\n```rust\n#[derive(Debug, Clone, Deserialize)]\npub struct IndexerConfig {\n    pub processor_name: String,\n    \n    #[serde(flatten)]\n    pub db_config: DatabaseConfig,\n    \n    #[serde(flatten)]\n    pub stream_config: StreamConfig,\n    \n    #[serde(default)]\n    pub filter_config: FilterConfig,\n    \n    pub server_port: u16,\n}\n\n#[derive(Debug, Clone, Deserialize)]\npub struct DatabaseConfig {\n    pub connection_string: String,\n    pub pool_size: u32,\n    pub schema_name: String,\n    pub run_migrations: bool,\n}\n```\n\n### 시작 버전 결정 로직\n\n```rust\nimpl<H: TransactionHandler> PipelineOrchestrator<H> {\n    async fn determine_starting_version(&self) -> Result<u64> {\n        // 1. 설정에서 명시된 버전\n        let config_version = self.config.stream_config.starting_version;\n        \n        // 2. DB에서 마지막 처리 버전 조회\n        let db_version = self.get_last_processed_version().await?;\n        \n        // 3. 둘 중 큰 값 선택 (안전한 시작점)\n        Ok(config_version.max(db_version.map(|v| v + 1).unwrap_or(0)))\n    }\n}\n```\n\n---\n\n## 전체 아키텍처 다이어그램\n\n```mermaid\nflowchart TB\n    subgraph User[\"사용자 코드\"]\n        Main[\"main.rs\"]\n        Handler[\"MyHandler\"]\n    end\n    \n    subgraph SDK[\"인덱서 SDK\"]\n        Orchestrator[\"PipelineOrchestrator\"]\n        \n        subgraph Pipeline[\"파이프라인\"]\n            Stream[\"StreamSubscriber\"]\n            Filter[\"TransactionFilter\"]\n            Wrapper[\"HandlerWrapper\"]\n            Tracker[\"VersionTracker\"]\n        end\n        \n        Server[\"REST Server\"]\n        Pool[\"DB Pool\"]\n    end\n    \n    Main -->|설정 전달| Orchestrator\n    Handler -->|Trait 구현| Wrapper\n    Orchestrator --> Pipeline\n    Orchestrator --> Server\n    Pipeline --> Pool\n```\n\n---\n\n## 정리\n\n- **오케스트레이터**: 파이프라인 생명주기 관리\n- **핸들러 Trait**: 비즈니스 로직 주입점\n- **채널 패턴**: 비동기 단계 간 데이터 전달\n- **어댑터**: 사용자 코드와 SDK 연결\n- **설정 기반**: 환경별 유연한 구성\n\n---\n\n## 다음 편 예고\n\n**[5편: Diesel ORM 실전 활용 - 비동기 DB 처리와 배치 최적화](/blog/blockchain/indexer/indexer-05-diesel-orm)**\n\n- `diesel-async` 소개\n- 파라미터 수 제한과 청크 처리\n- Upsert 패턴 구현\n- 커넥션 풀 최적화",
    "docType": "original",
    "category": "Uncategorized",
    "tags": [
      "blockchain",
      "indexer",
      "rust",
      "sdk",
      "architecture",
      "design-patterns"
    ],
    "readingTime": 5,
    "wordCount": 883,
    "isFeatured": false,
    "isPublic": true,
    "series": "blockchain-indexer",
    "seriesOrder": 4,
    "date": "2026-01-05"
  },
  {
    "id": "indexer-03-database-design",
    "slug": "indexer-03-database-design",
    "path": "blockchain/indexer",
    "fullPath": "blockchain/indexer/indexer-03-database-design",
    "title": "이력 테이블 vs 스냅샷 테이블 - 인덱서 DB 설계 전략",
    "excerpt": "시리즈 목차 블록체인 인덱서란? 인덱서 아키텍처 Deep Dive **이력 테이블 vs 스냅샷 테이블** (현재 글) Rust로 인덱서 SDK 만들기 [Diesel ORM 실전 활용](/blog/blockchain/indexer/",
    "content": "## 시리즈 목차\n\n1. [블록체인 인덱서란?](/blog/blockchain/indexer/indexer-01-introduction)\n2. [인덱서 아키텍처 Deep Dive](/blog/blockchain/indexer/indexer-02-architecture)\n3. **이력 테이블 vs 스냅샷 테이블** (현재 글)\n4. [Rust로 인덱서 SDK 만들기](/blog/blockchain/indexer/indexer-04-rust-sdk)\n5. [Diesel ORM 실전 활용](/blog/blockchain/indexer/indexer-05-diesel-orm)\n6. [멱등성 있는 인덱서 핸들러 설계](/blog/blockchain/indexer/indexer-06-idempotency)\n\n---\n\n## 인덱서 DB 설계의 핵심 질문\n\n인덱서 DB를 설계할 때 가장 먼저 마주치는 질문이 있습니다:\n\n> **\"변경 이력을 모두 저장할 것인가, 현재 상태만 저장할 것인가?\"**\n\n정답은 **둘 다**입니다. 이것이 **이중 테이블 패턴**입니다.\n\n---\n\n## 이력 테이블 (History Table)\n\n### 개념\n\n이력 테이블은 **모든 변경 사항을 시간 순서대로 누적** 저장합니다.\n\n```mermaid\nflowchart LR\n    subgraph History[\"이력 테이블\"]\n        R1[\"v=100: A→B\"]\n        R2[\"v=200: B→C\"]\n        R3[\"v=300: C→A\"]\n    end\n    \n    R1 --> R2 --> R3\n```\n\n### 스키마 예시\n\n```sql\nCREATE TABLE nft_transfers (\n    id SERIAL,\n    nft_id TEXT NOT NULL,\n    from_address TEXT NOT NULL,\n    to_address TEXT NOT NULL,\n    transaction_version BIGINT NOT NULL,\n    transaction_hash TEXT NOT NULL,\n    transferred_at TIMESTAMP NOT NULL,\n    \n    -- 복합 PK: 같은 NFT가 여러 번 전송될 수 있음\n    PRIMARY KEY (nft_id, transaction_version)\n);\n\n-- 조회 최적화 인덱스\nCREATE INDEX idx_transfers_to ON nft_transfers(to_address);\nCREATE INDEX idx_transfers_from ON nft_transfers(from_address);\nCREATE INDEX idx_transfers_time ON nft_transfers(transferred_at DESC);\n```\n\n### 사용 사례\n\n| 쿼리 | 이력 테이블 필요 |\n|------|-----------------|\n| \"이 NFT의 거래 이력\" | ✅ 필수 |\n| \"지난 24시간 거래량\" | ✅ 필수 |\n| \"시간대별 활성 사용자\" | ✅ 필수 |\n| \"특정 시점의 소유자\" | ✅ 필수 |\n\n---\n\n## 스냅샷 테이블 (Current State Table)\n\n### 개념\n\n스냅샷 테이블은 **현재 상태만 유지**합니다. 새 이벤트가 오면 기존 row를 **덮어씁니다**.\n\n```mermaid\nflowchart TB\n    subgraph Before[\"트랜잭션 처리 전\"]\n        S1[\"NFT#1: Owner=B\"]\n    end\n    \n    subgraph Event[\"새 이벤트: B→C\"]\n        E1[\"Transfer Event\"]\n    end\n    \n    subgraph After[\"트랜잭션 처리 후\"]\n        S2[\"NFT#1: Owner=C\"]\n    end\n    \n    Before --> Event --> After\n```\n\n### 스키마 예시\n\n```sql\nCREATE TABLE current_nft_owners (\n    nft_id TEXT PRIMARY KEY,  -- 단일 PK: NFT당 하나의 row\n    owner_address TEXT NOT NULL,\n    last_transaction_version BIGINT NOT NULL,\n    last_updated_at TIMESTAMP NOT NULL\n);\n\n-- 소유자 기준 조회 인덱스\nCREATE INDEX idx_owners_address ON current_nft_owners(owner_address);\n```\n\n### 사용 사례\n\n| 쿼리 | 스냅샷 테이블 적합 |\n|------|-------------------|\n| \"내가 보유한 NFT 목록\" | ✅ 최적 |\n| \"현재 토큰 잔액\" | ✅ 최적 |\n| \"활성 구독 목록\" | ✅ 최적 |\n| \"현재 설정값\" | ✅ 최적 |\n\n---\n\n## 이중 테이블 패턴\n\n### 왜 둘 다 필요한가?\n\n| 요구사항 | 이력 테이블 | 스냅샷 테이블 |\n|----------|:----------:|:------------:|\n| 변경 이력 추적 | ✅ | ❌ |\n| 현재 상태 빠른 조회 | ❌ (비효율) | ✅ |\n| 특정 시점 상태 조회 | ✅ | ❌ |\n| 저장 공간 효율 | ❌ (누적) | ✅ (고정) |\n| 분석/집계 쿼리 | ✅ | ❌ |\n\n### 아키텍처\n\n```mermaid\nflowchart TB\n    Event[\"새 이벤트\"]\n    \n    subgraph Writer[\"저장 로직\"]\n        Insert[\"이력 테이블 INSERT\"]\n        Upsert[\"스냅샷 테이블 UPSERT\"]\n    end\n    \n    subgraph Tables[\"테이블\"]\n        History[(이력 테이블)]\n        Current[(스냅샷 테이블)]\n    end\n    \n    Event --> Insert\n    Event --> Upsert\n    Insert --> History\n    Upsert --> Current\n```\n\n### 저장 로직 예시\n\n하나의 트랜잭션에서 **두 테이블을 동시에 갱신**합니다:\n\n```sql\n-- 1. 이력 테이블: 항상 INSERT\nINSERT INTO nft_transfers (nft_id, from_address, to_address, transaction_version, ...)\nVALUES ('nft#1', 'addr_B', 'addr_C', 300, ...);\n\n-- 2. 스냅샷 테이블: UPSERT (있으면 UPDATE, 없으면 INSERT)\nINSERT INTO current_nft_owners (nft_id, owner_address, last_transaction_version, ...)\nVALUES ('nft#1', 'addr_C', 300, ...)\nON CONFLICT (nft_id) \nDO UPDATE SET \n    owner_address = EXCLUDED.owner_address,\n    last_transaction_version = EXCLUDED.last_transaction_version,\n    last_updated_at = EXCLUDED.last_updated_at\nWHERE current_nft_owners.last_transaction_version < EXCLUDED.last_transaction_version;\n```\n\n> **핵심**: `WHERE` 조건으로 **더 오래된 버전이 덮어쓰지 않도록** 보호\n\n---\n\n## 실제 PKI 인덱서 스키마 예시\n\n인증서 관리 시스템의 인덱서를 예로 들어보겠습니다:\n\n### 이력 테이블\n\n```sql\nCREATE TABLE certificates (\n    id SERIAL,\n    serial_number TEXT NOT NULL,\n    status TEXT NOT NULL,  -- 'ACTIVE', 'REVOKED', 'EXPIRED'\n    issuer_id TEXT NOT NULL,\n    valid_from TIMESTAMP NOT NULL,\n    valid_until TIMESTAMP NOT NULL,\n    transaction_version BIGINT NOT NULL,\n    event_type TEXT NOT NULL,  -- 'ISSUED', 'REVOKED', 'RENEWED'\n    created_at TIMESTAMP DEFAULT NOW(),\n    \n    PRIMARY KEY (serial_number, transaction_version)\n);\n```\n\n### 스냅샷 테이블\n\n```sql\nCREATE TABLE current_certificates (\n    serial_number TEXT PRIMARY KEY,\n    status TEXT NOT NULL,\n    issuer_id TEXT NOT NULL,\n    valid_from TIMESTAMP NOT NULL,\n    valid_until TIMESTAMP NOT NULL,\n    last_transaction_version BIGINT NOT NULL,\n    last_updated_at TIMESTAMP NOT NULL\n);\n```\n\n---\n\n## 성능 비교\n\n### 쿼리별 성능\n\n| 쿼리 유형 | 이력 테이블 | 스냅샷 테이블 |\n|----------|------------|--------------|\n| 현재 소유자 조회 | O(log N) ~ O(N)* | **O(1)** |\n| 거래 이력 조회 | **O(K)** | 불가능 |\n| 시간대별 집계 | **O(M)** | 불가능 |\n| 전체 현재 상태 | O(N log N)* | **O(N)** |\n\n*: 그룹핑/정렬 필요\n\n### 저장 공간\n\n```\n예시: NFT 컬렉션 10,000개, 평균 50회 거래\n\n이력 테이블: 10,000 × 50 = 500,000 rows\n스냅샷 테이블: 10,000 rows (고정)\n```\n\n---\n\n## 인덱스 설계 전략\n\n### 이력 테이블 인덱스\n\n```sql\n-- 시간 범위 쿼리 (최근 거래, 일별 집계)\nCREATE INDEX idx_history_time ON transfers(transferred_at DESC);\n\n-- 주체별 조회 (내 거래 이력)\nCREATE INDEX idx_history_address ON transfers(from_address, transferred_at DESC);\nCREATE INDEX idx_history_to ON transfers(to_address, transferred_at DESC);\n\n-- 버전 기반 조회 (동기화, 복구)\nCREATE INDEX idx_history_version ON transfers(transaction_version);\n```\n\n### 스냅샷 테이블 인덱스\n\n```sql\n-- 소유자별 목록 조회 (내 NFT 목록)\nCREATE INDEX idx_current_owner ON current_nft_owners(owner_address);\n\n-- 상태별 필터링 (활성/비활성)\nCREATE INDEX idx_current_status ON current_nft_owners(status) \nWHERE status = 'ACTIVE';  -- 부분 인덱스\n```\n\n---\n\n## 정리\n\n- **이력 테이블**: 모든 변경 누적, 분석/추적용\n- **스냅샷 테이블**: 현재 상태만, 빠른 조회용\n- **이중 테이블 패턴**: 두 테이블을 동시에 갱신\n- **UPSERT + WHERE 조건**: 버전 역전 방지\n\n---\n\n## 다음 편 예고\n\n**[4편: Rust로 인덱서 SDK 만들기 - 파이프라인 오케스트레이터 설계](/blog/blockchain/indexer/indexer-04-rust-sdk)**\n\n- SDK가 필요한 이유\n- 핵심 컴포넌트 설계\n- Trait 기반 확장성\n- Producer-Consumer 패턴 적용",
    "docType": "original",
    "category": "Uncategorized",
    "tags": [
      "blockchain",
      "indexer",
      "database",
      "postgresql",
      "schema-design"
    ],
    "readingTime": 5,
    "wordCount": 889,
    "isFeatured": false,
    "isPublic": true,
    "series": "blockchain-indexer",
    "seriesOrder": 3,
    "date": "2026-01-05"
  },
  {
    "id": "indexer-02-architecture",
    "slug": "indexer-02-architecture",
    "path": "blockchain/indexer",
    "fullPath": "blockchain/indexer/indexer-02-architecture",
    "title": "인덱서 아키텍처 Deep Dive - ETL 파이프라인으로 이해하기",
    "excerpt": "시리즈 목차 블록체인 인덱서란? **인덱서 아키텍처 Deep Dive** (현재 글) 이력 테이블 vs 스냅샷 테이블 Rust로 인덱서 SDK 만들기 [Diesel ORM 실전 활용](/blog/blockchain/index",
    "content": "## 시리즈 목차\n\n1. [블록체인 인덱서란?](/blog/blockchain/indexer/indexer-01-introduction)\n2. **인덱서 아키텍처 Deep Dive** (현재 글)\n3. [이력 테이블 vs 스냅샷 테이블](/blog/blockchain/indexer/indexer-03-database-design)\n4. [Rust로 인덱서 SDK 만들기](/blog/blockchain/indexer/indexer-04-rust-sdk)\n5. [Diesel ORM 실전 활용](/blog/blockchain/indexer/indexer-05-diesel-orm)\n6. [멱등성 있는 인덱서 핸들러 설계](/blog/blockchain/indexer/indexer-06-idempotency)\n\n---\n\n## 인덱서는 ETL 파이프라인이다\n\n인덱서의 핵심은 **ETL(Extract-Transform-Load)** 파이프라인입니다. 데이터 엔지니어링에서 친숙한 이 패턴이 블록체인 인덱서에도 그대로 적용됩니다.\n\n```mermaid\nflowchart LR\n    subgraph Extract[\"Extract\"]\n        Node[\"블록체인 노드\"]\n        Stream[\"gRPC 스트림\"]\n    end\n    \n    subgraph Transform[\"Transform\"]\n        Parser[\"이벤트 파서\"]\n        Model[\"도메인 모델\"]\n    end\n    \n    subgraph Load[\"Load\"]\n        Query[\"SQL 생성\"]\n        DB[(PostgreSQL)]\n    end\n    \n    Node -->|트랜잭션| Stream -->|배치| Parser -->|파싱| Model -->|변환| Query -->|저장| DB\n```\n\n각 단계를 자세히 살펴보겠습니다.\n\n---\n\n## 1. Extract: gRPC 트랜잭션 스트림\n\n### 왜 gRPC인가?\n\n블록체인 노드와 인덱서 간 통신에는 **gRPC 스트리밍**이 표준으로 사용됩니다:\n\n| 방식 | 장점 | 단점 |\n|------|------|------|\n| **REST Polling** | 구현 단순 | 지연, 불필요한 요청 |\n| **WebSocket** | 실시간, 양방향 | 프로토콜 오버헤드 |\n| **gRPC Stream** | 고성능, 타입 안전, 배치 지원 | 초기 설정 복잡 |\n\ngRPC의 **Server-side streaming**을 사용하면:\n\n- 연결 유지하며 **새 트랜잭션을 푸시**\n- Protocol Buffers로 **직렬화 효율 극대화**\n- 자동 재연결 및 **백프레셔(backpressure)** 지원\n\n### 스트림 구독 흐름\n\n```mermaid\nsequenceDiagram\n    participant Indexer as 인덱서\n    participant Node as 블록체인 노드\n    participant DB as PostgreSQL\n    \n    Indexer->>DB: 마지막 처리 버전 조회 (v=1000)\n    Indexer->>Node: GetTransactions(start=1001)\n    \n    loop 스트리밍\n        Node-->>Indexer: 트랜잭션 배치 (v=1001~1100)\n        Indexer->>Indexer: 파싱 & 저장\n        Indexer->>DB: 버전 업데이트 (v=1100)\n        Node-->>Indexer: 트랜잭션 배치 (v=1101~1200)\n        Indexer->>Indexer: 파싱 & 저장\n    end\n```\n\n### 시작 버전(Starting Version) 결정\n\n인덱서가 시작할 때 **어디서부터 처리할지** 결정해야 합니다:\n\n```\n시작 버전 = MAX(설정값, DB에 저장된 마지막 버전 + 1)\n```\n\n이를 통해:\n\n- **신규 배포 시**: 설정된 버전부터 시작\n- **재시작 시**: 마지막 처리 지점부터 재개\n- **장애 복구 시**: 중복 처리 없이 이어서 진행\n\n---\n\n## 2. Transform: 이벤트 파싱\n\n### 원시 트랜잭션 구조\n\n블록체인 노드에서 받는 원시 데이터는 복잡한 구조를 가집니다:\n\n```json\n{\n  \"version\": 12345,\n  \"hash\": \"0xabc...\",\n  \"events\": [\n    {\n      \"type\": \"0x1::nft::TransferEvent\",\n      \"data\": \"0x0102030405...\"  // BCS 인코딩된 바이트\n    }\n  ]\n}\n```\n\n### 파싱 파이프라인\n\n파싱은 여러 단계로 나뉩니다:\n\n```mermaid\nflowchart TB\n    Raw[\"원시 트랜잭션\"]\n    Filter[\"필터링\"]\n    Decode[\"이벤트 디코딩\"]\n    Map[\"도메인 모델 매핑\"]\n    Output[\"파싱된 변경사항\"]\n    \n    Raw --> Filter\n    Filter -->|관심 있는 이벤트만| Decode\n    Decode -->|BCS 역직렬화| Map\n    Map -->|NFT Transfer, Token Mint 등| Output\n```\n\n### 필터링 전략\n\n모든 트랜잭션을 처리할 필요는 없습니다:\n\n| 필터 유형 | 예시 | 목적 |\n|----------|------|------|\n| **발신자 필터** | 시스템 계정 제외 | 노이즈 감소 |\n| **이벤트 타입 필터** | `TransferEvent`만 | 관심 이벤트 집중 |\n| **모듈 필터** | 특정 컨트랙트만 | 범위 한정 |\n\n---\n\n## 3. Load: DB 적재\n\n### 배치 저장의 중요성\n\n트랜잭션을 하나씩 저장하면 성능이 급격히 저하됩니다:\n\n```\n❌ 트랜잭션별 개별 INSERT\n   → 1000 TPS × 1ms/쿼리 = 1초에 1000 쿼리 = 병목\n\n✅ 배치 INSERT\n   → 1000 트랜잭션 × 1 배치 = 10ms = 100배 빠름\n```\n\n### 청크 단위 처리\n\nPostgreSQL에는 **쿼리당 파라미터 수 제한(~65,535)**이 있습니다.\n\n10개 컬럼 테이블에 10,000 row를 삽입하면:\n\n- 파라미터 수 = 10 × 10,000 = **100,000 → 제한 초과!**\n\n해결책: **청크 단위 분할**\n\n```\n전체 10,000 rows\n    ↓\n청크 1: 1~6,000 rows (60,000 params)\n청크 2: 6,001~10,000 rows (40,000 params)\n    ↓\n각 청크를 병렬 실행\n```\n\n5편에서 Diesel ORM으로 이를 구현하는 방법을 다룹니다.\n\n---\n\n## 전체 아키텍처 다이어그램\n\n```mermaid\nflowchart TB\n    subgraph Blockchain[\"블록체인 레이어\"]\n        Node[\"Full Node\"]\n        GRPC[\"gRPC Data Service\"]\n    end\n    \n    subgraph Indexer[\"인덱서 프로세서\"]\n        direction TB\n        Config[\"설정 로더\"]\n        Stream[\"스트림 구독자\"]\n        Filter[\"트랜잭션 필터\"]\n        Parser[\"이벤트 파서\"]\n        Storer[\"배치 저장기\"]\n        Tracker[\"버전 트래커\"]\n    end\n    \n    subgraph Storage[\"스토리지 레이어\"]\n        Postgres[(PostgreSQL)]\n        Tables[\"이력 + 스냅샷 테이블\"]\n    end\n    \n    subgraph API[\"API 레이어\"]\n        Hasura[\"Hasura GraphQL\"]\n        REST[\"REST API\"]\n    end\n    \n    Node --> GRPC\n    GRPC -->|gRPC Stream| Stream\n    Config --> Stream\n    Stream --> Filter\n    Filter --> Parser\n    Parser --> Storer\n    Storer --> Postgres\n    Postgres --> Tables\n    Storer --> Tracker\n    Tracker -->|처리 버전 기록| Postgres\n    Tables --> Hasura\n    Tables --> REST\n```\n\n---\n\n## 파이프라인 단계별 책임\n\n| 단계 | 책임 | 실패 시 동작 |\n|------|------|-------------|\n| **스트림 구독자** | gRPC 연결 관리, 재연결 | 지수 백오프 재시도 |\n| **트랜잭션 필터** | 불필요 트랜잭션 제외 | 패스스루(다음 단계로) |\n| **이벤트 파서** | 도메인 모델 변환 | 에러 로깅 후 스킵 |\n| **배치 저장기** | 청크 단위 DB 쓰기 | 트랜잭션 롤백 후 재시도 |\n| **버전 트래커** | 처리 진행률 기록 | 다음 배치에서 재기록 |\n\n---\n\n## 정리\n\n- 인덱서는 **Extract-Transform-Load** 패턴으로 동작\n- **gRPC 스트리밍**으로 실시간 트랜잭션 수신\n- **이벤트 파싱**으로 원시 데이터를 도메인 모델로 변환\n- **배치 저장**으로 DB 쓰기 성능 최적화\n- **버전 트래킹**으로 장애 복구 및 재시작 지원\n\n---\n\n## 다음 편 예고\n\n**[3편: 이력 테이블 vs 스냅샷 테이블 - 인덱서 DB 설계 전략](/blog/blockchain/indexer/indexer-03-database-design)**\n\n- 변경 이력 추적의 필요성\n- 현재 상태 조회 최적화\n- 이중 테이블 패턴 상세 설명\n- SQL 스키마 예시",
    "docType": "original",
    "category": "Uncategorized",
    "tags": [
      "blockchain",
      "indexer",
      "etl",
      "architecture",
      "grpc"
    ],
    "readingTime": 4,
    "wordCount": 757,
    "isFeatured": false,
    "isPublic": true,
    "series": "blockchain-indexer",
    "seriesOrder": 2,
    "date": "2026-01-05"
  },
  {
    "id": "indexer-01-introduction",
    "slug": "indexer-01-introduction",
    "path": "blockchain/indexer",
    "fullPath": "blockchain/indexer/indexer-01-introduction",
    "title": "블록체인 인덱서란? - 왜 필요하고 어떻게 동작하는가",
    "excerpt": "시리즈 목차 **블록체인 인덱서란?** (현재 글) 인덱서 아키텍처 Deep Dive 이력 테이블 vs 스냅샷 테이블 Rust로 인덱서 SDK 만들기 [Diesel ORM 실전 활용](/blog/blockchain/index",
    "content": "## 시리즈 목차\n\n1. **블록체인 인덱서란?** (현재 글)\n2. [인덱서 아키텍처 Deep Dive](/blog/blockchain/indexer/indexer-02-architecture)\n3. [이력 테이블 vs 스냅샷 테이블](/blog/blockchain/indexer/indexer-03-database-design)\n4. [Rust로 인덱서 SDK 만들기](/blog/blockchain/indexer/indexer-04-rust-sdk)\n5. [Diesel ORM 실전 활용](/blog/blockchain/indexer/indexer-05-diesel-orm)\n6. [멱등성 있는 인덱서 핸들러 설계](/blog/blockchain/indexer/indexer-06-idempotency)\n\n---\n\n## 블록체인의 데이터 조회, 왜 어려운가?\n\n블록체인은 분산 원장이라는 특성상 **모든 트랜잭션이 시간 순서대로 누적**됩니다. 이는 데이터 무결성 측면에서는 훌륭하지만, 데이터 조회 측면에서는 심각한 한계가 있습니다.\n\n### 실제 사례: NFT 마켓플레이스\n\nNFT 마켓플레이스에서 \"내가 보유한 NFT 목록\"을 조회한다고 가정해봅시다.\n\n```\n❓ 질문: 사용자 0x1234...가 보유한 NFT는?\n```\n\n블록체인 네이티브 방식으로 이 질문에 답하려면:\n\n1. **제네시스 블록부터 최신 블록까지** 모든 트랜잭션을 스캔\n2. NFT 전송 이벤트 중 **수신자가 0x1234인 것**을 필터링\n3. 동시에 **발신자가 0x1234인 것**도 찾아서 차감\n4. 최종적으로 현재 보유 목록 계산\n\n이 과정이 얼마나 비효율적인지 숫자로 보겠습니다:\n\n| 블록체인 | 일일 트랜잭션 수 | 1년간 누적 |\n|---------|-----------------|-----------|\n| Ethereum | ~1.2M | ~438M |\n| Aptos | ~2.5M | ~912M |\n\n**수억 건의 트랜잭션을 매번 풀스캔?** 현실적으로 불가능합니다.\n\n---\n\n## 인덱서란 무엇인가?\n\n**인덱서(Indexer)**는 블록체인의 원시 트랜잭션 데이터를 **개발자 친화적인 형태로 변환**하여 별도의 데이터베이스에 저장하는 오프체인(off-chain) 시스템입니다.\n\n```mermaid\nflowchart LR\n    subgraph Blockchain[\"블록체인 노드\"]\n        TX[트랜잭션 스트림]\n    end\n    \n    subgraph Indexer[\"인덱서\"]\n        Parser[이벤트 파싱]\n        Store[DB 저장]\n    end\n    \n    subgraph DB[\"PostgreSQL\"]\n        Tables[(정규화된 테이블)]\n    end\n    \n    subgraph API[\"API 레이어\"]\n        GraphQL[GraphQL/REST]\n    end\n    \n    TX --> Parser --> Store --> Tables --> GraphQL\n```\n\n### 인덱서의 핵심 역할\n\n1. **실시간 구독**: gRPC 스트림으로 새 트랜잭션을 즉시 수신\n2. **이벤트 파싱**: 인코딩된 원시 데이터를 도메인 모델로 변환\n3. **정규화 저장**: PostgreSQL 등 쿼리 최적화 DB에 적재\n4. **빠른 조회**: 인덱스 기반 밀리초 단위 응답\n\n---\n\n## 인덱서 vs RPC 노드\n\n| 구분 | RPC 노드 | 인덱서 |\n|------|---------|--------|\n| **목적** | 트랜잭션 실행, 상태 조회 | 데이터 분석, 집계 쿼리 |\n| **조회 방식** | 현재 상태만 (포인트 쿼리) | 이력 + 집계 (범위 쿼리) |\n| **응답 속도** | 단순 쿼리는 빠름 | 복잡한 쿼리도 빠름 |\n| **사용 사례** | 잔액 조회, 트랜잭션 전송 | NFT 목록, 거래 이력, 통계 |\n\n### 구체적 비교 예시\n\n```\n질문: \"지난 30일간 가장 많이 거래된 NFT 컬렉션 Top 10\"\n```\n\n**RPC 노드 방식**:\n\n- ❌ 불가능하거나 수 분~수 시간 소요\n- 30일치 모든 블록을 순차 스캔해야 함\n\n**인덱서 방식**:\n\n- ✅ 수 밀리초 내 응답\n- 이미 정규화된 테이블에 `GROUP BY` + `ORDER BY` 쿼리\n\n```sql\nSELECT collection_id, COUNT(*) as trade_count\nFROM nft_trades\nWHERE traded_at > NOW() - INTERVAL '30 days'\nGROUP BY collection_id\nORDER BY trade_count DESC\nLIMIT 10;\n```\n\n---\n\n## 인덱서가 필요한 서비스들\n\n현대 블록체인 서비스 대부분은 인덱서 없이는 작동할 수 없습니다:\n\n| 서비스 유형 | 필요한 쿼리 | 인덱서 필수 여부 |\n|------------|------------|-----------------|\n| NFT 마켓플레이스 | 소유권 이력, 가격 추이 | ✅ 필수 |\n| DeFi 대시보드 | TVL, APY 계산, 포지션 조회 | ✅ 필수 |\n| 블록 익스플로러 | 트랜잭션 검색, 계정 활동 | ✅ 필수 |\n| 지갑 앱 | 토큰 잔액, 거래 이력 | ✅ 필수 |\n| 온체인 게임 | 아이템 소유, 게임 상태 | ✅ 필수 |\n\n---\n\n## 인덱서 동작 원리 미리보기\n\n인덱서의 핵심 동작은 **ETL(Extract-Transform-Load)** 파이프라인과 유사합니다:\n\n```mermaid\nflowchart TB\n    subgraph Extract[\"1. Extract\"]\n        Stream[\"gRPC 스트림 구독\"]\n        Batch[\"트랜잭션 배치 수신\"]\n    end\n    \n    subgraph Transform[\"2. Transform\"]\n        Parse[\"이벤트 파싱\"]\n        Model[\"도메인 모델 변환\"]\n    end\n    \n    subgraph Load[\"3. Load\"]\n        Query[\"SQL 쿼리 생성\"]\n        Write[\"DB 배치 쓰기\"]\n    end\n    \n    Stream --> Batch --> Parse --> Model --> Query --> Write\n```\n\n각 단계의 자세한 구현은 다음 편에서 다루겠습니다.\n\n---\n\n## 정리\n\n- 블록체인은 **누적 구조**라 복잡한 쿼리에 부적합\n- 인덱서는 **오프체인 ETL 시스템**으로 이 문제를 해결\n- 현대 블록체인 서비스는 **인덱서 없이는 불가능**\n- 다음 편에서 인덱서의 **아키텍처를 상세히** 살펴봅니다\n\n---\n\n## 다음 편 예고\n\n**[2편: 인덱서 아키텍처 Deep Dive - ETL 파이프라인으로 이해하기](/blog/blockchain/indexer/indexer-02-architecture)**\n\n- gRPC 트랜잭션 스트림 상세 분석\n- 파싱 파이프라인 설계\n- 배치 저장 최적화 전략",
    "docType": "original",
    "category": "Uncategorized",
    "tags": [
      "blockchain",
      "indexer",
      "database",
      "architecture"
    ],
    "readingTime": 4,
    "wordCount": 610,
    "isFeatured": false,
    "isPublic": true,
    "series": "blockchain-indexer",
    "seriesOrder": 1,
    "date": "2026-01-05"
  },
  {
    "id": "gitops-06-ci-integration",
    "slug": "gitops-06-ci-integration",
    "path": "backend/gitops",
    "fullPath": "backend/gitops/gitops-06-ci-integration",
    "title": "GitOps 심화 시리즈 #6: CI/CD 파이프라인 통합과 Progressive Delivery",
    "excerpt": "GitOps에서 CI와 CD의 분리, Image Updater, Progressive Delivery(Argo Rollouts, Flagger)를 다루는 시리즈 완결편입니다.",
    "content": "# GitOps 심화 시리즈 #6: CI/CD 파이프라인 통합과 Progressive Delivery\n\n## 시리즈 개요\n\n| # | 주제 | 핵심 내용 |\n|---|------|----------|\n| 1 | GitOps 개요 | 철학과 원칙, Push vs Pull 배포, Reconciliation |\n| 2 | ArgoCD Deep Dive | 아키텍처, Application CRD, Sync 전략 |\n| 3 | Flux CD & GitOps Toolkit | 컨트롤러 아키텍처, GitRepository, Kustomization |\n| 4 | 환경별 설정 관리 | Kustomize vs Helm, 전략 선택 기준 |\n| 5 | Secrets Management | Sealed Secrets, External Secrets, SOPS |\n| **6** | **CI/CD 파이프라인 통합** | Image Updater, Progressive Delivery |\n\n---\n\n## GitOps에서 CI와 CD의 분리\n\n전통적인 CI/CD 파이프라인에서는 CI와 CD가 하나의 파이프라인에서 연속적으로 실행됩니다. GitOps에서는 **명확하게 분리**됩니다.\n\n```mermaid\nflowchart TB\n    subgraph CI [CI Pipeline - 소스 레포]\n        Code[코드 변경] --> Build[빌드]\n        Build --> Test[테스트]\n        Test --> Push[이미지 Push]\n        Push --> UpdateManifest[매니페스트 업데이트]\n    end\n    \n    subgraph GitOps [GitOps 레포]\n        Manifest[K8s Manifests]\n    end\n    \n    subgraph CD [CD - GitOps Agent]\n        Agent[ArgoCD / Flux]\n        Cluster[Kubernetes Cluster]\n    end\n    \n    UpdateManifest -->|PR 생성 또는 직접 커밋| Manifest\n    Manifest -->|Watch| Agent\n    Agent -->|Sync| Cluster\n```\n\n### 왜 분리하는가?\n\n| 관점 | 통합 CI/CD | 분리된 CI + GitOps |\n|-----|-----------|------------------|\n| **배포 권한** | CI가 클러스터 접근 | 클러스터 내부 Agent만 접근 |\n| **감사 추적** | CI 로그에만 기록 | Git 커밋 히스토리 |\n| **롤백** | 재빌드 필요 | `git revert` |\n| **환경 일관성** | CI 파이프라인에 의존 | Git이 SSOT |\n\n> [!IMPORTANT]\n> **핵심 원칙**: CI는 **아티팩트(이미지)를 생성**하고, CD는 **GitOps Agent가 담당**합니다. CI가 클러스터에 직접 접근하지 않습니다.\n\n---\n\n## CI 파이프라인에서 매니페스트 업데이트\n\n이미지 빌드 후 GitOps 레포의 매니페스트를 업데이트하는 여러 방법이 있습니다.\n\n### 방법 1: CI에서 직접 커밋\n\n```yaml\n# .github/workflows/build.yaml\nname: Build and Update Manifest\n\non:\n  push:\n    branches: [main]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout source\n        uses: actions/checkout@v4\n      \n      - name: Build and push image\n        run: |\n          docker build -t ghcr.io/myorg/myapp:${{ github.sha }} .\n          docker push ghcr.io/myorg/myapp:${{ github.sha }}\n      \n      - name: Update GitOps repo\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITOPS_PAT }}\n        run: |\n          git clone https://$GITHUB_TOKEN@github.com/myorg/gitops-repo.git\n          cd gitops-repo\n          \n          # Kustomize 사용 시\n          cd apps/myapp/overlays/prod\n          kustomize edit set image ghcr.io/myorg/myapp:${{ github.sha }}\n          \n          git config user.name \"GitHub Actions\"\n          git config user.email \"actions@github.com\"\n          git add .\n          git commit -m \"Update myapp to ${{ github.sha }}\"\n          git push\n```\n\n### 방법 2: PR 생성 (권장)\n\n```yaml\n      - name: Create PR to GitOps repo\n        uses: peter-evans/create-pull-request@v6\n        with:\n          token: ${{ secrets.GITOPS_PAT }}\n          repository: myorg/gitops-repo\n          branch: update-myapp-${{ github.sha }}\n          title: \"Update myapp to ${{ github.sha }}\"\n          body: |\n            Automated image update\n            - Commit: ${{ github.sha }}\n            - Build: ${{ github.run_id }}\n          commit-message: \"chore: update myapp image to ${{ github.sha }}\"\n```\n\n> [!TIP]\n> **PR 방식의 장점**: 코드 리뷰 프로세스 활용, 자동화된 테스트 실행, 승인 후 배포\n\n### 방법 3: 이미지 태그 자동 업데이트 (권장)\n\nCI는 이미지만 푸시하고, **GitOps Agent가 자동으로 매니페스트를 업데이트**합니다. 다음 섹션에서 자세히 다룹니다.\n\n---\n\n## ArgoCD Image Updater\n\n**ArgoCD Image Updater**는 컨테이너 레지스트리를 모니터링하고, 새 이미지 태그가 발견되면 자동으로 Application을 업데이트합니다.\n\n### 동작 원리\n\n```mermaid\nsequenceDiagram\n    participant Registry as Container Registry\n    participant Updater as Image Updater\n    participant Git as GitOps Repo\n    participant ArgoCD as ArgoCD\n    participant K8s as Kubernetes\n    \n    loop 주기적 스캔\n        Updater->>Registry: 새 이미지 태그 확인\n        Registry-->>Updater: v1.2.4 발견 (현재: v1.2.3)\n    end\n    \n    Updater->>Git: 매니페스트 업데이트 커밋\n    Git-->>ArgoCD: 변경 감지\n    ArgoCD->>K8s: Sync\n```\n\n### 설치\n\n```bash\nkubectl apply -n argocd \\\n  -f https://raw.githubusercontent.com/argoproj-labs/argocd-image-updater/stable/manifests/install.yaml\n```\n\n### Application 설정\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: myapp\n  namespace: argocd\n  annotations:\n    # Image Updater 활성화\n    argocd-image-updater.argoproj.io/image-list: myapp=ghcr.io/myorg/myapp\n    \n    # 업데이트 전략: semver\n    argocd-image-updater.argoproj.io/myapp.update-strategy: semver\n    \n    # SemVer 제약\n    argocd-image-updater.argoproj.io/myapp.allow-tags: regexp:^v[0-9]+\\.[0-9]+\\.[0-9]+$\n    \n    # Git에 커밋 (write-back)\n    argocd-image-updater.argoproj.io/write-back-method: git\n    argocd-image-updater.argoproj.io/git-branch: main\nspec:\n  source:\n    repoURL: https://github.com/myorg/gitops.git\n    path: apps/myapp\n```\n\n### Update Strategies\n\n| 전략 | 설명 | 예시 |\n|-----|------|-----|\n| `semver` | SemVer 최신 | v1.2.3 < v1.2.4 < v1.3.0 |\n| `latest` | 최신 푸시된 태그 | 날짜/시간 기준 |\n| `name` | 알파벳순 최신 | a < b < c |\n| `digest` | 특정 태그의 digest 변경 | :latest의 실제 이미지 변경 |\n\n### Write-back Methods\n\n```yaml\n# 1. Git 직접 커밋 (권장)\nargocd-image-updater.argoproj.io/write-back-method: git\n\n# 2. ArgoCD에 오버라이드 (Git에 기록 안 됨)\nargocd-image-updater.argoproj.io/write-back-method: argocd\n```\n\n> [!WARNING]\n> `argocd` 방식은 Git에 기록되지 않아 **GitOps 원칙에 위배**됩니다. 프로덕션에서는 `git` 방식을 권장합니다.\n\n---\n\n## Flux Image Automation\n\nFlux는 Image Automation을 **핵심 기능으로 내장**하고 있습니다 (3편에서 간략히 소개했습니다).\n\n### 전체 구성\n\n```yaml\n# 1. ImageRepository: 레지스트리 스캔\napiVersion: image.toolkit.fluxcd.io/v1beta2\nkind: ImageRepository\nmetadata:\n  name: myapp\n  namespace: flux-system\nspec:\n  image: ghcr.io/myorg/myapp\n  interval: 5m\n  secretRef:\n    name: ghcr-auth\n\n---\n# 2. ImagePolicy: 태그 선택 정책\napiVersion: image.toolkit.fluxcd.io/v1beta2\nkind: ImagePolicy\nmetadata:\n  name: myapp\n  namespace: flux-system\nspec:\n  imageRepositoryRef:\n    name: myapp\n  policy:\n    semver:\n      range: \">=1.0.0\"\n\n---\n# 3. ImageUpdateAutomation: Git 업데이트\napiVersion: image.toolkit.fluxcd.io/v1beta2\nkind: ImageUpdateAutomation\nmetadata:\n  name: myapp\n  namespace: flux-system\nspec:\n  interval: 5m\n  sourceRef:\n    kind: GitRepository\n    name: myapp\n  git:\n    checkout:\n      ref:\n        branch: main\n    commit:\n      author:\n        name: Flux\n        email: flux@myorg.com\n      messageTemplate: |\n        Auto-update images\n        \n        {{range .Changed.Changes}}\n        - {{.OldValue}} -> {{.NewValue}}\n        {{end}}\n    push:\n      branch: main\n  update:\n    path: ./deploy\n    strategy: Setters\n```\n\n### 마커 기반 업데이트\n\n```yaml\n# deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  template:\n    spec:\n      containers:\n      - name: myapp\n        image: ghcr.io/myorg/myapp:v1.2.3  # {\"$imagepolicy\": \"flux-system:myapp\"}\n```\n\nFlux가 새 버전을 발견하면:\n\n```yaml\n        image: ghcr.io/myorg/myapp:v1.2.4  # {\"$imagepolicy\": \"flux-system:myapp\"}\n```\n\n---\n\n## Progressive Delivery\n\n새 버전을 **점진적으로 배포**하여 위험을 최소화하는 전략입니다.\n\n### 배포 전략 비교\n\n```mermaid\nflowchart TB\n    subgraph Recreate [Recreate]\n        R1[v1 삭제] --> R2[v2 생성]\n        R_Note[다운타임 발생]\n    end\n    \n    subgraph Rolling [Rolling Update]\n        RU1[v1: 3개] --> RU2[v1: 2개, v2: 1개]\n        RU2 --> RU3[v1: 1개, v2: 2개]\n        RU3 --> RU4[v2: 3개]\n    end\n    \n    subgraph BlueGreen [Blue-Green]\n        BG1[Blue: v1 운영] --> BG2[Green: v2 대기]\n        BG2 --> BG3[트래픽 전환]\n        BG3 --> BG4[Green: v2 운영]\n    end\n    \n    subgraph Canary [Canary]\n        C1[v1: 100%] --> C2[v1: 90%, v2: 10%]\n        C2 --> C3[v1: 50%, v2: 50%]\n        C3 --> C4[v2: 100%]\n    end\n```\n\n| 전략 | 다운타임 | 리소스 | 롤백 속도 | 위험 노출 |\n|-----|---------|-------|---------|---------|\n| Recreate | ✅ 있음 | 낮음 | 느림 | 높음 |\n| Rolling | ❌ 없음 | 일시 증가 | 중간 | 중간 |\n| Blue-Green | ❌ 없음 | 2배 | 빠름 | 낮음 |\n| Canary | ❌ 없음 | 약간 증가 | 빠름 | 낮음 |\n\n---\n\n## Argo Rollouts\n\n**Argo Rollouts**는 Kubernetes Deployment를 대체하여 Blue-Green, Canary 배포를 제공합니다.\n\n### 설치\n\n```bash\nkubectl create namespace argo-rollouts\nkubectl apply -n argo-rollouts \\\n  -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml\n```\n\n### Canary Rollout\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: myapp\nspec:\n  replicas: 5\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: myapp\n        image: ghcr.io/myorg/myapp:v1.2.3\n        ports:\n        - containerPort: 8080\n  strategy:\n    canary:\n      # 단계별 배포\n      steps:\n      - setWeight: 10      # 10% 트래픽\n      - pause: {duration: 5m}  # 5분 대기\n      - setWeight: 30\n      - pause: {duration: 5m}\n      - setWeight: 50\n      - pause: {}          # 수동 승인 대기\n      - setWeight: 100\n      \n      # 분석 실행 (자동 롤백)\n      analysis:\n        templates:\n        - templateName: success-rate\n        startingStep: 1\n        args:\n        - name: service-name\n          value: myapp\n```\n\n### AnalysisTemplate\n\n메트릭 기반 자동 롤백:\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: AnalysisTemplate\nmetadata:\n  name: success-rate\nspec:\n  args:\n  - name: service-name\n  metrics:\n  - name: success-rate\n    interval: 1m\n    failureLimit: 3\n    successCondition: result[0] >= 0.95\n    provider:\n      prometheus:\n        address: http://prometheus.monitoring:9090\n        query: |\n          sum(rate(http_requests_total{\n            service=\"{{args.service-name}}\",\n            status=~\"2..\"\n          }[5m])) /\n          sum(rate(http_requests_total{\n            service=\"{{args.service-name}}\"\n          }[5m]))\n```\n\n### Blue-Green Rollout\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: myapp\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    # ...\n  strategy:\n    blueGreen:\n      activeService: myapp-active\n      previewService: myapp-preview\n      autoPromotionEnabled: false  # 수동 승인\n      prePromotionAnalysis:\n        templates:\n        - templateName: smoke-test\n```\n\n### ArgoCD와 통합\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: myapp\nspec:\n  source:\n    repoURL: https://github.com/myorg/gitops.git\n    path: apps/myapp\n  # Rollout 리소스를 인식하도록 설정\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n```\n\n---\n\n## Flagger\n\n**Flagger**는 Flux와 함께 사용되는 Progressive Delivery 도구입니다. Istio, Linkerd, NGINX Ingress 등과 통합됩니다.\n\n### 설치 (with Flux)\n\n```yaml\napiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: flagger\n  namespace: flux-system\nspec:\n  interval: 1h\n  url: https://flagger.app\n\n---\napiVersion: helm.toolkit.fluxcd.io/v2\nkind: HelmRelease\nmetadata:\n  name: flagger\n  namespace: flagger-system\nspec:\n  interval: 1h\n  chart:\n    spec:\n      chart: flagger\n      sourceRef:\n        kind: HelmRepository\n        name: flagger\n        namespace: flux-system\n  values:\n    meshProvider: istio\n    metricsServer: http://prometheus.monitoring:9090\n```\n\n### Canary CRD\n\n```yaml\napiVersion: flagger.app/v1beta1\nkind: Canary\nmetadata:\n  name: myapp\n  namespace: production\nspec:\n  targetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: myapp\n  \n  # Istio VirtualService 자동 생성\n  service:\n    port: 8080\n    targetPort: 8080\n  \n  # 분석 설정\n  analysis:\n    interval: 1m           # 분석 주기\n    threshold: 5           # 최대 실패 횟수\n    maxWeight: 50          # 최대 Canary 트래픽\n    stepWeight: 10         # 단계별 증가량\n    \n    metrics:\n    - name: request-success-rate\n      thresholdRange:\n        min: 99\n      interval: 1m\n    \n    - name: request-duration\n      thresholdRange:\n        max: 500\n      interval: 1m\n    \n    # Webhook 테스트\n    webhooks:\n    - name: smoke-test\n      type: pre-rollout\n      url: http://flagger-loadtester/\n      timeout: 30s\n      metadata:\n        type: bash\n        cmd: \"curl -s http://myapp-canary:8080/health | grep ok\"\n```\n\n### 동작 흐름\n\n```mermaid\nsequenceDiagram\n    participant Git as GitOps Repo\n    participant Flux as Flux\n    participant Flagger as Flagger\n    participant Istio as Istio\n    participant Prom as Prometheus\n    \n    Git->>Flux: 새 이미지 감지\n    Flux->>Flux: Deployment 업데이트\n    \n    Flagger->>Flagger: Canary Deployment 생성\n    Flagger->>Istio: VirtualService 업데이트 (10% canary)\n    \n    loop 분석\n        Flagger->>Prom: 성공률 확인\n        alt 성공률 >= 99%\n            Flagger->>Istio: 트래픽 증가 (20%, 30%, ...)\n        else 실패\n            Flagger->>Istio: 롤백 (100% primary)\n            Flagger->>Flagger: Canary 삭제\n        end\n    end\n    \n    Flagger->>Flux: Primary 업데이트\n    Flagger->>Istio: 100% primary\n```\n\n---\n\n## 프로덕션 GitOps 워크플로우\n\n모든 것을 종합한 프로덕션 워크플로우입니다.\n\n```mermaid\nflowchart TB\n    subgraph Dev [개발]\n        Code[코드 작성]\n        PR[Pull Request]\n        Review[코드 리뷰]\n    end\n    \n    subgraph CI [CI Pipeline]\n        Build[빌드]\n        Test[테스트]\n        Push[이미지 Push]\n    end\n    \n    subgraph GitOps [GitOps Repo]\n        ImageUpdate[Image Updater\\n또는 Flux Image Automation]\n        Manifest[K8s Manifests]\n    end\n    \n    subgraph CD [CD - Progressive Delivery]\n        Agent[ArgoCD / Flux]\n        Rollout[Argo Rollouts / Flagger]\n        Canary[Canary 분석]\n    end\n    \n    subgraph Prod [Production]\n        Cluster[Kubernetes Cluster]\n        Monitor[Prometheus/Grafana]\n    end\n    \n    Code --> PR --> Review\n    Review -->|Merge| Build --> Test --> Push\n    Push -->|새 이미지 감지| ImageUpdate\n    ImageUpdate --> Manifest\n    Manifest --> Agent\n    Agent --> Rollout --> Canary\n    Canary -->|성공| Cluster\n    Canary <-->|메트릭 수집| Monitor\n    \n    Canary -->|실패| Rollback[자동 롤백]\n```\n\n### 레포지토리 구조 (최종)\n\n```\ngitops-repo/\n├── clusters/\n│   ├── dev/\n│   │   └── kustomization.yaml\n│   ├── staging/\n│   │   └── kustomization.yaml\n│   └── prod/\n│       └── kustomization.yaml\n│\n├── infrastructure/\n│   ├── base/\n│   │   ├── ingress-nginx/\n│   │   ├── cert-manager/\n│   │   ├── monitoring/\n│   │   └── flagger/\n│   └── overlays/\n│       ├── dev/\n│       └── prod/\n│\n├── apps/\n│   ├── frontend/\n│   │   ├── base/\n│   │   │   ├── deployment.yaml\n│   │   │   ├── service.yaml\n│   │   │   ├── canary.yaml        # Flagger Canary\n│   │   │   └── kustomization.yaml\n│   │   └── overlays/\n│   │       ├── dev/\n│   │       ├── staging/\n│   │       └── prod/\n│   └── backend/\n│       └── ...\n│\n└── image-automation/              # Flux Image Automation\n    ├── image-repositories.yaml\n    ├── image-policies.yaml\n    └── image-update-automation.yaml\n```\n\n---\n\n## 정리: GitOps 시리즈 총정리\n\n6편의 시리즈를 통해 다룬 내용을 정리합니다.\n\n| 편 | 주제 | 핵심 메시지 |\n|---|------|-----------|\n| 1 | GitOps 개요 | Git = SSOT, Pull 모델, Reconciliation |\n| 2 | ArgoCD | 모놀리식 아키텍처, 강력한 UI, ApplicationSet |\n| 3 | Flux CD | 마이크로서비스 컨트롤러, Image Automation 내장 |\n| 4 | 설정 관리 | Kustomize(순수 YAML) vs Helm(템플릿) |\n| 5 | Secrets | Sealed Secrets, ESO, SOPS |\n| 6 | CI/CD 통합 | CI와 CD 분리, Progressive Delivery |\n\n### GitOps 성숙도 모델\n\n```mermaid\nflowchart LR\n    L1[Level 1\\nGit에 매니페스트 저장]\n    L2[Level 2\\nGitOps Agent 도입]\n    L3[Level 3\\n환경별 설정 분리]\n    L4[Level 4\\nSecrets 자동화]\n    L5[Level 5\\nProgressive Delivery]\n    \n    L1 --> L2 --> L3 --> L4 --> L5\n```\n\n### 시작하기 권장 순서\n\n1. **ArgoCD 또는 Flux 설치**\n2. **간단한 앱으로 GitOps 워크플로우 경험**\n3. **Kustomize로 환경별 설정 분리**\n4. **Sealed Secrets 또는 ESO 도입**\n5. **Image Updater로 자동화**\n6. **Argo Rollouts 또는 Flagger로 Progressive Delivery**\n\n---\n\n## 마치며\n\nGitOps는 단순한 도구가 아닌 **운영 철학**입니다. Git을 중심으로 선언적 인프라를 관리하고, 자동화된 Reconciliation으로 일관성을 유지합니다.\n\n이 시리즈가 여러분의 GitOps 여정에 도움이 되길 바랍니다. 🚀\n\n---\n\n## 참고 자료\n\n- [ArgoCD Image Updater](https://argocd-image-updater.readthedocs.io/)\n- [Flux Image Automation](https://fluxcd.io/flux/guides/image-update/)\n- [Argo Rollouts](https://argoproj.github.io/argo-rollouts/)\n- [Flagger](https://flagger.app/)\n- [Progressive Delivery with Flux](https://fluxcd.io/flagger/)\n- [GitOps Working Group - CNCF](https://github.com/cncf/tag-app-delivery/tree/main/gitops-wg)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitOps",
      "Kubernetes",
      "CI/CD",
      "ArgoCD"
    ],
    "readingTime": 10,
    "wordCount": 1933,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitops",
    "date": "2026-01-05"
  },
  {
    "id": "gitops-05-secrets-management",
    "slug": "gitops-05-secrets-management",
    "path": "backend/gitops",
    "fullPath": "backend/gitops/gitops-05-secrets-management",
    "title": "GitOps 심화 시리즈 #5: Secrets Management - Git에 비밀을 안전하게 저장하기",
    "excerpt": "GitOps에서 Secrets를 안전하게 관리하는 방법. Sealed Secrets, External Secrets Operator, SOPS의 동작 원리와 선택 기준을 다룹니다.",
    "content": "# GitOps 심화 시리즈 #5: Secrets Management - Git에 비밀을 안전하게 저장하기\n\n## 시리즈 개요\n\n| # | 주제 | 핵심 내용 |\n|---|------|----------|\n| 1 | GitOps 개요 | 철학과 원칙, Push vs Pull 배포, Reconciliation |\n| 2 | ArgoCD Deep Dive | 아키텍처, Application CRD, Sync 전략 |\n| 3 | Flux CD & GitOps Toolkit | 컨트롤러 아키텍처, GitRepository, Kustomization |\n| 4 | 환경별 설정 관리 | Kustomize vs Helm, 전략 선택 기준 |\n| **5** | **Secrets Management** | Sealed Secrets, External Secrets, SOPS |\n| 6 | CI/CD 파이프라인 통합 | Image Updater, Progressive Delivery |\n\n---\n\n## GitOps에서 Secrets의 딜레마\n\nGitOps의 핵심 원칙은 **Git을 Single Source of Truth**로 삼는 것입니다. 하지만 Kubernetes Secret을 Git에 저장하면?\n\n```yaml\n# ⚠️ 절대 이렇게 하면 안 됩니다!\napiVersion: v1\nkind: Secret\nmetadata:\n  name: db-credentials\ntype: Opaque\ndata:\n  username: YWRtaW4=        # base64는 암호화가 아님!\n  password: c3VwZXJzZWNyZXQ=  # 누구나 디코딩 가능\n```\n\n```bash\n# 즉시 평문 노출\necho \"c3VwZXJzZWNyZXQ=\" | base64 -d\n# 출력: supersecret\n```\n\n> [!CAUTION]\n> **Base64는 인코딩이지 암호화가 아닙니다**. Git 히스토리에 한 번 들어가면 영구히 노출됩니다.\n\n### 딜레마의 본질\n\n```mermaid\nflowchart TB\n    subgraph Problem [GitOps Secrets 딜레마]\n        SSOT[Git = Single Source of Truth]\n        Secret[Secrets도 Git에 있어야?]\n        Risk[평문 노출 위험]\n    end\n    \n    SSOT --> Secret\n    Secret --> Risk\n    \n    subgraph Solutions [해결 방법]\n        Encrypted[암호화해서 Git에 저장]\n        External[외부 저장소 참조]\n    end\n    \n    Risk --> Encrypted\n    Risk --> External\n    \n    Encrypted --> SealedSecrets[Sealed Secrets]\n    Encrypted --> SOPS[SOPS]\n    External --> ESO[External Secrets Operator]\n    External --> CSI[Secrets Store CSI Driver]\n```\n\n---\n\n## 해결책 1: Sealed Secrets\n\n**Sealed Secrets**는 Bitnami에서 개발한 Kubernetes 컨트롤러로, 클러스터 내에서만 복호화 가능한 암호화된 Secret을 Git에 저장합니다.\n\n### 동작 원리\n\n```mermaid\nsequenceDiagram\n    participant Dev as 개발자\n    participant CLI as kubeseal CLI\n    participant Controller as Sealed Secrets Controller\n    participant K8s as Kubernetes API\n    \n    Note over Controller: RSA 키 쌍 보유 (private/public)\n    \n    Dev->>CLI: 원본 Secret YAML 제공\n    CLI->>Controller: 공개키 요청\n    Controller-->>CLI: 공개키 반환\n    CLI->>CLI: 공개키로 암호화\n    CLI->>Dev: SealedSecret YAML 생성\n    \n    Dev->>K8s: git push → GitOps → SealedSecret 적용\n    K8s->>Controller: SealedSecret 감지\n    Controller->>Controller: 비밀키로 복호화\n    Controller->>K8s: 일반 Secret 생성\n```\n\n### 설치\n\n```bash\n# 컨트롤러 설치\nhelm repo add sealed-secrets https://bitnami-labs.github.io/sealed-secrets\nhelm install sealed-secrets sealed-secrets/sealed-secrets \\\n  -n kube-system\n\n# CLI 설치 (macOS)\nbrew install kubeseal\n```\n\n### 사용법\n\n```bash\n# 1. 원본 Secret 생성 (적용하지 않음!)\nkubectl create secret generic db-credentials \\\n  --from-literal=username=admin \\\n  --from-literal=password=supersecret \\\n  --dry-run=client -o yaml > secret.yaml\n\n# 2. Sealed Secret으로 암호화\nkubeseal --format yaml < secret.yaml > sealed-secret.yaml\n\n# 3. 원본 삭제, Sealed Secret만 Git에 커밋\nrm secret.yaml\ngit add sealed-secret.yaml\ngit commit -m \"Add encrypted database credentials\"\n```\n\n### 생성된 SealedSecret\n\n```yaml\napiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  name: db-credentials\n  namespace: default\nspec:\n  encryptedData:\n    username: AgBy8BQ...암호화된_데이터...==\n    password: AgCtr2I...암호화된_데이터...==\n  template:\n    metadata:\n      name: db-credentials\n      namespace: default\n    type: Opaque\n```\n\n### Scope 설정\n\nSealedSecret은 **어떤 조건에서 복호화를 허용할지** 범위(scope)를 지정할 수 있습니다:\n\n```bash\n# strict (기본): 동일한 namespace + name만 허용\nkubeseal --scope strict\n\n# namespace-wide: 동일 namespace 내 다른 이름 허용\nkubeseal --scope namespace-wide\n\n# cluster-wide: 모든 namespace에서 사용 가능\nkubeseal --scope cluster-wide\n```\n\n| Scope | 이름 변경 | 네임스페이스 변경 | 보안 수준 |\n|-------|---------|----------------|----------|\n| `strict` | ❌ | ❌ | 높음 |\n| `namespace-wide` | ✅ | ❌ | 중간 |\n| `cluster-wide` | ✅ | ✅ | 낮음 |\n\n### 키 관리\n\n> [!WARNING]\n> Sealed Secrets 컨트롤러의 **비밀키가 유출되면 모든 SealedSecret이 복호화**됩니다. 키 백업이 필수입니다.\n\n```bash\n# 키 백업\nkubectl get secret -n kube-system \\\n  -l sealedsecrets.bitnami.com/sealed-secrets-key \\\n  -o yaml > sealed-secrets-key-backup.yaml\n\n# 키 복원 (재해 복구 시)\nkubectl apply -f sealed-secrets-key-backup.yaml\nkubectl delete pod -n kube-system -l app.kubernetes.io/name=sealed-secrets\n```\n\n### 한계\n\n1. **클러스터 종속**: 다른 클러스터에서는 복호화 불가\n2. **키 로테이션 복잡**: 키 변경 시 모든 SealedSecret 재암호화 필요\n3. **단일 실패점**: 컨트롤러 장애 시 Secret 생성 불가\n\n---\n\n## 해결책 2: External Secrets Operator (ESO)\n\n**External Secrets Operator**는 외부 비밀 관리 시스템(AWS Secrets Manager, HashiCorp Vault 등)에서 Secret을 가져와 Kubernetes Secret으로 동기화합니다.\n\n### 동작 원리\n\n```mermaid\nflowchart LR\n    subgraph External [외부 Secret 저장소]\n        AWS[AWS Secrets Manager]\n        Vault[HashiCorp Vault]\n        GCP[GCP Secret Manager]\n        Azure[Azure Key Vault]\n    end\n    \n    subgraph Cluster [Kubernetes Cluster]\n        ESO[External Secrets\\nOperator]\n        ExtSecret[ExternalSecret CR]\n        K8sSecret[Kubernetes Secret]\n    end\n    \n    ExtSecret -->|참조| ESO\n    ESO -->|API 호출| AWS & Vault & GCP & Azure\n    ESO -->|생성/동기화| K8sSecret\n```\n\n**핵심 포인트**: Git에는 **ExternalSecret (참조 정보)**만 저장하고, 실제 비밀 값은 외부 저장소에 있습니다.\n\n### 설치\n\n```bash\nhelm repo add external-secrets https://charts.external-secrets.io\nhelm install external-secrets external-secrets/external-secrets \\\n  -n external-secrets --create-namespace\n```\n\n### SecretStore & ClusterSecretStore\n\n먼저 외부 저장소와의 연결을 설정합니다:\n\n```yaml\n# ClusterSecretStore (클러스터 전역)\napiVersion: external-secrets.io/v1beta1\nkind: ClusterSecretStore\nmetadata:\n  name: aws-secrets-manager\nspec:\n  provider:\n    aws:\n      service: SecretsManager\n      region: ap-northeast-2\n      auth:\n        jwt:\n          serviceAccountRef:\n            name: external-secrets-sa\n            namespace: external-secrets\n```\n\n```yaml\n# AWS IAM Role for Service Account (IRSA) 설정 필요\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-secrets-sa\n  namespace: external-secrets\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::123456789:role/external-secrets-role\n```\n\n### ExternalSecret\n\n```yaml\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: db-credentials\n  namespace: production\nspec:\n  # 동기화 주기\n  refreshInterval: 1h\n  \n  # SecretStore 참조\n  secretStoreRef:\n    name: aws-secrets-manager\n    kind: ClusterSecretStore\n  \n  # 생성할 Secret 설정\n  target:\n    name: db-credentials\n    creationPolicy: Owner\n  \n  # 데이터 매핑\n  data:\n    - secretKey: username      # K8s Secret의 키\n      remoteRef:\n        key: prod/database     # AWS Secrets Manager의 경로\n        property: username     # JSON 내 속성\n    \n    - secretKey: password\n      remoteRef:\n        key: prod/database\n        property: password\n```\n\n### 전체 Secret 가져오기\n\n```yaml\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: all-db-credentials\nspec:\n  # ...\n  dataFrom:\n    - extract:\n        key: prod/database  # JSON 전체를 가져와서 각 키를 Secret 데이터로\n```\n\n### 지원하는 Provider\n\n| Provider | 설명 |\n|----------|------|\n| AWS Secrets Manager | AWS 네이티브 |\n| AWS Parameter Store | SSM 파라미터 |\n| HashiCorp Vault | 온프레미스/클라우드 |\n| GCP Secret Manager | GCP 네이티브 |\n| Azure Key Vault | Azure 네이티브 |\n| 1Password | 팀 비밀 공유 |\n| Doppler | SaaS 비밀 관리 |\n\n### 장점과 단점\n\n**장점**:\n\n- Git에 비밀 값이 전혀 없음\n- 자동 로테이션 지원\n- 중앙 집중식 비밀 관리\n\n**단점**:\n\n- 외부 서비스 의존성\n- 네트워크 지연\n- 추가 비용 (AWS Secrets Manager 등)\n\n---\n\n## 해결책 3: SOPS (Secrets OPerationS)\n\n**SOPS**는 Mozilla에서 개발한 파일 레벨 암호화 도구입니다. YAML, JSON, ENV 파일의 **값만 선택적으로 암호화**합니다.\n\n### 동작 원리\n\n```mermaid\nflowchart LR\n    subgraph Encryption [암호화 키]\n        AGE[age key]\n        AWS_KMS[AWS KMS]\n        GCP_KMS[GCP KMS]\n        Azure_KMS[Azure Key Vault]\n        PGP[PGP]\n    end\n    \n    subgraph Files [파일]\n        Plain[평문 YAML]\n        Encrypted[암호화된 YAML\\n키는 평문, 값만 암호화]\n    end\n    \n    Plain -->|sops -e| Encrypted\n    Encrypted -->|sops -d| Plain\n    \n    Encryption --> Plain\n```\n\n### 특징: 키는 평문, 값만 암호화\n\n```yaml\n# 암호화 후 (읽기 쉬움!)\napiVersion: v1\nkind: Secret\nmetadata:\n  name: db-credentials\nstringData:\n  username: ENC[AES256_GCM,data:6FLiRc8=,iv:...,tag:...,type:str]\n  password: ENC[AES256_GCM,data:HdNqmY3WUr8=,iv:...,tag:...,type:str]\nsops:\n  age:\n    - recipient: age1...\n      enc: |\n        -----BEGIN AGE ENCRYPTED FILE-----\n        ...\n        -----END AGE ENCRYPTED FILE-----\n  lastmodified: \"2024-01-15T10:00:00Z\"\n  mac: ENC[AES256_GCM,...]\n  version: 3.8.1\n```\n\n> [!TIP]\n> **Git diff가 의미있습니다**. 어떤 키가 변경되었는지 바로 알 수 있습니다.\n\n### Age 키로 사용하기 (권장)\n\n```bash\n# age 설치\nbrew install age\n\n# 키 쌍 생성\nage-keygen -o key.txt\n# Public key: age1abc...\n# Private key 저장됨\n\n# SOPS 설치\nbrew install sops\n\n# .sops.yaml 설정 (레포지토리 루트)\ncat > .sops.yaml << EOF\ncreation_rules:\n  - path_regex: .*secrets.*\\.yaml$\n    age: age1abc...  # 공개키\nEOF\n```\n\n### 암호화/복호화\n\n```bash\n# 암호화\nsops -e secrets.yaml > secrets.enc.yaml\n\n# 복호화\nsops -d secrets.enc.yaml > secrets.yaml\n\n# 제자리 편집 (복호화 → 편집 → 저장 시 재암호화)\nsops secrets.enc.yaml\n```\n\n### Flux와 SOPS 통합\n\nFlux는 SOPS를 **네이티브로 지원**합니다:\n\n```yaml\n# 복호화 키를 Secret으로 저장\napiVersion: v1\nkind: Secret\nmetadata:\n  name: sops-age\n  namespace: flux-system\nstringData:\n  age.agekey: |\n    # created: 2024-01-15\n    # public key: age1abc...\n    AGE-SECRET-KEY-1...\n\n---\n# Kustomization에서 복호화 활성화\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: my-app\n  namespace: flux-system\nspec:\n  # ...\n  decryption:\n    provider: sops\n    secretRef:\n      name: sops-age\n```\n\n### ArgoCD와 SOPS\n\nArgoCD는 플러그인을 통해 SOPS를 지원합니다:\n\n```yaml\n# argocd-cm ConfigMap\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\n  namespace: argocd\ndata:\n  configManagementPlugins: |\n    - name: kustomize-sops\n      generate:\n        command: [\"bash\", \"-c\"]\n        args: [\"kustomize build . | sops -d /dev/stdin\"]\n```\n\n### AWS KMS와 함께 사용\n\n```yaml\n# .sops.yaml\ncreation_rules:\n  - path_regex: .*prod.*secrets.*\\.yaml$\n    kms: arn:aws:kms:ap-northeast-2:123456789:key/abc-def\n    \n  - path_regex: .*dev.*secrets.*\\.yaml$\n    kms: arn:aws:kms:ap-northeast-2:123456789:key/xyz-123\n```\n\n```bash\n# 환경별 다른 KMS 키 사용\nsops -e --kms arn:aws:kms:... secrets.yaml\n```\n\n---\n\n## 전략 선택 가이드\n\n```mermaid\nflowchart TB\n    Start[Secrets 관리 전략 선택] --> Q1{외부 비밀 관리 시스템\\n이미 사용 중?}\n    \n    Q1 -->|Yes| ESO[External Secrets Operator]\n    Q1 -->|No| Q2{멀티 클러스터?}\n    \n    Q2 -->|Yes| Q3{중앙 비밀 저장소\\n도입 가능?}\n    Q3 -->|Yes| ESO\n    Q3 -->|No| SOPS[SOPS + age/KMS]\n    \n    Q2 -->|No| Q4{단순함 우선?}\n    Q4 -->|Yes| SealedSecrets[Sealed Secrets]\n    Q4 -->|No| SOPS\n```\n\n### 비교 표\n\n| 관점 | Sealed Secrets | ESO | SOPS |\n|-----|----------------|-----|------|\n| **Git에 저장되는 것** | 암호화된 SealedSecret | 참조(ExternalSecret) | 암호화된 파일 |\n| **복호화 위치** | 클러스터 내 | 외부 저장소 | 클러스터/로컬 |\n| **멀티 클러스터** | ❌ (각 클러스터 키 다름) | ✅ (중앙 저장소) | ✅ (같은 키 공유) |\n| **비밀 로테이션** | 수동 | 자동 (refreshInterval) | 수동 |\n| **외부 의존성** | 없음 | 있음 (Vault, AWS 등) | 선택적 (age vs KMS) |\n| **Flux 지원** | ✅ | ✅ | ✅ (네이티브) |\n| **ArgoCD 지원** | ✅ | ✅ | ✅ (플러그인) |\n| **학습 곡선** | 낮음 | 중간 | 중간 |\n\n### 권장 시나리오\n\n| 상황 | 권장 솔루션 |\n|-----|-----------|\n| 단일 클러스터, 빠른 시작 | Sealed Secrets |\n| AWS/GCP/Azure 사용, 중앙 관리 | External Secrets Operator |\n| 멀티 클러스터, Flux 사용 | SOPS + age |\n| 기존 Vault 인프라 | ESO + Vault |\n| 규제 요구사항 (감사 로그 필요) | ESO + AWS Secrets Manager |\n\n---\n\n## 보안 모범 사례\n\n### 1. 최소 권한 원칙\n\n```yaml\n# ESO ServiceAccount에 필요한 권한만 부여\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"secretsmanager:GetSecretValue\"\n      ],\n      \"Resource\": [\n        \"arn:aws:secretsmanager:*:*:secret:prod/*\"\n      ]\n    }\n  ]\n}\n```\n\n### 2. 네임스페이스 격리\n\n```yaml\n# SecretStore를 네임스페이스별로 분리\napiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: team-a-secrets\n  namespace: team-a\nspec:\n  provider:\n    aws:\n      service: SecretsManager\n      region: ap-northeast-2\n      # team-a 전용 IAM Role\n      auth:\n        jwt:\n          serviceAccountRef:\n            name: team-a-eso-sa\n```\n\n### 3. 자동 로테이션\n\n```yaml\n# ESO: 1시간마다 동기화\nspec:\n  refreshInterval: 1h\n\n# AWS Secrets Manager: 자동 로테이션 활성화\n# Lambda 함수가 주기적으로 비밀 값 변경\n```\n\n### 4. 감사 로깅\n\n```bash\n# AWS CloudTrail에서 Secret 접근 로그 확인\naws cloudtrail lookup-events \\\n  --lookup-attributes AttributeKey=EventName,AttributeValue=GetSecretValue\n```\n\n---\n\n## 정리\n\n| 솔루션 | 핵심 개념 | 장점 | 단점 |\n|-------|----------|------|------|\n| **Sealed Secrets** | 클러스터 키로 암호화 | 단순, 외부 의존성 없음 | 클러스터 종속, 키 관리 |\n| **ESO** | 외부 저장소 참조 | 중앙 관리, 자동 로테이션 | 외부 의존성, 비용 |\n| **SOPS** | 파일 레벨 암호화 | Git diff 가능, 유연함 | 수동 로테이션 |\n\n---\n\n## 다음 편 예고\n\n**6편: CI/CD 파이프라인 통합**에서는 다음을 다룹니다:\n\n- GitOps에서 CI와 CD의 분리\n- ArgoCD Image Updater\n- Flux Image Automation\n- Progressive Delivery (Argo Rollouts, Flagger)\n- 프로덕션 GitOps 워크플로우\n\n---\n\n## 참고 자료\n\n- [Sealed Secrets](https://github.com/bitnami-labs/sealed-secrets)\n- [External Secrets Operator](https://external-secrets.io/)\n- [SOPS](https://github.com/getsops/sops)\n- [Flux SOPS Integration](https://fluxcd.io/flux/guides/mozilla-sops/)\n- [ArgoCD Secrets Management](https://argo-cd.readthedocs.io/en/stable/operator-manual/secret-management/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitOps",
      "Kubernetes",
      "Security",
      "Secrets"
    ],
    "readingTime": 9,
    "wordCount": 1785,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitops",
    "date": "2026-01-05"
  },
  {
    "id": "gitops-04-config-management",
    "slug": "gitops-04-config-management",
    "path": "backend/gitops",
    "fullPath": "backend/gitops/gitops-04-config-management",
    "title": "GitOps 심화 시리즈 #4: 환경별 설정 관리 - Kustomize vs Helm",
    "excerpt": "Kustomize의 Base/Overlays 패턴, Helm의 values 관리 전략, 그리고 언제 무엇을 선택해야 하는지 실무 관점에서 다룹니다.",
    "content": "# GitOps 심화 시리즈 #4: 환경별 설정 관리 - Kustomize vs Helm\n\n## 시리즈 개요\n\n| # | 주제 | 핵심 내용 |\n|---|------|----------|\n| 1 | GitOps 개요 | 철학과 원칙, Push vs Pull 배포, Reconciliation |\n| 2 | ArgoCD Deep Dive | 아키텍처, Application CRD, Sync 전략 |\n| 3 | Flux CD & GitOps Toolkit | 컨트롤러 아키텍처, GitRepository, Kustomization |\n| **4** | **환경별 설정 관리** | Kustomize vs Helm, 전략 선택 기준 |\n| 5 | Secrets Management | Sealed Secrets, External Secrets, SOPS |\n| 6 | CI/CD 파이프라인 통합 | Image Updater, Progressive Delivery |\n\n---\n\n## 왜 환경별 설정 관리가 중요한가?\n\n실제 프로덕션 환경에서는 동일한 애플리케이션을 **여러 환경(dev, staging, prod)**에 배포합니다. 환경마다 다른 것들:\n\n| 항목 | dev | staging | prod |\n|-----|-----|---------|------|\n| replicas | 1 | 2 | 5 |\n| CPU request | 100m | 200m | 500m |\n| DB host | dev-db.local | staging-db.local | prod-db.aws |\n| Log level | debug | info | warn |\n| Ingress domain | dev.example.com | staging.example.com | example.com |\n\n**문제**: 환경마다 완전히 별개의 YAML 파일을 관리하면?\n\n- 중복 코드가 대량 발생\n- 한 곳 수정 시 모든 환경에 동기화 필요\n- 실수로 누락되는 변경 발생\n\n**해결책**: Kustomize 또는 Helm으로 **공통 부분(Base)과 환경별 차이(Overlays/Values)를 분리**\n\n---\n\n## Kustomize: Template-free Configuration\n\nKustomize는 **템플릿 없이** YAML을 패치하는 방식입니다. Kubernetes 1.14부터 `kubectl` 에 내장되었습니다.\n\n### 핵심 철학\n\n- **Base**: 공통 리소스 정의\n- **Overlays**: 환경별 패치\n- **No templating**: Go template 같은 문법 없음\n- **Pure YAML**: 결과물도 순수 YAML\n\n### 디렉토리 구조\n\n```\nmyapp/\n├── base/\n│   ├── kustomization.yaml\n│   ├── deployment.yaml\n│   ├── service.yaml\n│   └── configmap.yaml\n└── overlays/\n    ├── dev/\n    │   ├── kustomization.yaml\n    │   └── patch-replicas.yaml\n    ├── staging/\n    │   ├── kustomization.yaml\n    │   └── patch-resources.yaml\n    └── prod/\n        ├── kustomization.yaml\n        ├── patch-replicas.yaml\n        ├── patch-resources.yaml\n        └── patch-ingress.yaml\n```\n\n### Base 정의\n\n```yaml\n# base/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n  - deployment.yaml\n  - service.yaml\n  - configmap.yaml\n\ncommonLabels:\n  app: myapp\n```\n\n```yaml\n# base/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: myapp\n        image: myapp:latest\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 200m\n            memory: 256Mi\n```\n\n### Overlay 정의\n\n```yaml\n# overlays/prod/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n  - ../../base\n\n# 네임스페이스 설정\nnamespace: production\n\n# 공통 레이블 추가\ncommonLabels:\n  environment: production\n\n# 이미지 태그 변경\nimages:\n  - name: myapp\n    newTag: v1.2.3\n\n# 패치 적용\npatches:\n  - path: patch-replicas.yaml\n  - path: patch-resources.yaml\n```\n\n```yaml\n# overlays/prod/patch-replicas.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  replicas: 5\n```\n\n```yaml\n# overlays/prod/patch-resources.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  template:\n    spec:\n      containers:\n      - name: myapp\n        resources:\n          requests:\n            cpu: 500m\n            memory: 512Mi\n          limits:\n            cpu: 1000m\n            memory: 1024Mi\n```\n\n### 빌드 및 확인\n\n```bash\n# 결과 미리보기\nkubectl kustomize overlays/prod\n\n# 직접 적용\nkubectl apply -k overlays/prod\n\n# 또는 ArgoCD에서\nargocd app create myapp-prod \\\n  --repo https://github.com/myorg/myapp.git \\\n  --path overlays/prod \\\n  --dest-server https://kubernetes.default.svc\n```\n\n### 패치 전략\n\n#### 1. Strategic Merge Patch (기본)\n\n```yaml\n# 배열의 특정 요소만 수정 (name으로 매칭)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  template:\n    spec:\n      containers:\n      - name: myapp  # 이름으로 매칭\n        env:\n        - name: LOG_LEVEL\n          value: warn\n```\n\n#### 2. JSON Patch\n\n더 정밀한 제어가 필요할 때:\n\n```yaml\n# overlays/prod/kustomization.yaml\npatches:\n  - target:\n      kind: Deployment\n      name: myapp\n    patch: |-\n      - op: replace\n        path: /spec/replicas\n        value: 5\n      - op: add\n        path: /spec/template/spec/containers/0/env/-\n        value:\n          name: NEW_VAR\n          value: \"some-value\"\n```\n\n#### 3. Replace\n\n기존 값을 완전히 대체:\n\n```yaml\npatches:\n  - target:\n      kind: ConfigMap\n      name: myconfig\n    patch: |-\n      - op: replace\n        path: /data\n        value:\n          key1: newvalue1\n          key2: newvalue2\n```\n\n### Components (재사용 가능한 패치 모음)\n\nKustomize 3.7+에서 지원하는 **Components**는 여러 Overlay에서 공유할 수 있는 패치 모음입니다:\n\n```\nmyapp/\n├── base/\n├── components/\n│   ├── high-availability/\n│   │   └── kustomization.yaml\n│   ├── monitoring/\n│   │   └── kustomization.yaml\n│   └── security/\n│       └── kustomization.yaml\n└── overlays/\n    ├── dev/\n    │   └── kustomization.yaml\n    └── prod/\n        └── kustomization.yaml\n```\n\n```yaml\n# components/high-availability/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1alpha1\nkind: Component\n\npatches:\n  - patch: |-\n      apiVersion: apps/v1\n      kind: Deployment\n      metadata:\n        name: myapp\n      spec:\n        replicas: 3\n        strategy:\n          type: RollingUpdate\n          rollingUpdate:\n            maxSurge: 1\n            maxUnavailable: 0\n```\n\n```yaml\n# overlays/prod/kustomization.yaml\nresources:\n  - ../../base\n\ncomponents:\n  - ../../components/high-availability\n  - ../../components/monitoring\n```\n\n---\n\n## Helm: The Package Manager for Kubernetes\n\nHelm은 **템플릿 기반** 패키지 매니저입니다. Chart라는 패키지 형태로 애플리케이션을 배포합니다.\n\n### 핵심 개념\n\n- **Chart**: 패키지 (템플릿 + 기본값)\n- **Values**: 설정 값\n- **Release**: Chart의 설치된 인스턴스\n- **Repository**: Chart 저장소\n\n### Chart 구조\n\n```\nmychart/\n├── Chart.yaml           # 차트 메타데이터\n├── values.yaml          # 기본 values\n├── values-dev.yaml      # 환경별 values\n├── values-prod.yaml\n├── templates/\n│   ├── _helpers.tpl     # 템플릿 헬퍼\n│   ├── deployment.yaml\n│   ├── service.yaml\n│   ├── configmap.yaml\n│   ├── ingress.yaml\n│   └── NOTES.txt        # 설치 후 출력 메시지\n└── charts/              # 의존 차트\n```\n\n### Chart.yaml\n\n```yaml\napiVersion: v2\nname: mychart\nversion: 1.0.0\nappVersion: \"1.2.3\"\ndescription: My application chart\ntype: application\n\ndependencies:\n  - name: postgresql\n    version: \"12.x.x\"\n    repository: https://charts.bitnami.com/bitnami\n    condition: postgresql.enabled\n```\n\n### 템플릿 작성\n\n```yaml\n# templates/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  replicas: {{ .Values.replicaCount }}\n  selector:\n    matchLabels:\n      {{- include \"mychart.selectorLabels\" . | nindent 6 }}\n  template:\n    metadata:\n      labels:\n        {{- include \"mychart.selectorLabels\" . | nindent 8 }}\n    spec:\n      containers:\n      - name: {{ .Chart.Name }}\n        image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n        ports:\n        - containerPort: {{ .Values.service.port }}\n        resources:\n          {{- toYaml .Values.resources | nindent 12 }}\n        {{- if .Values.env }}\n        env:\n          {{- range $key, $value := .Values.env }}\n          - name: {{ $key }}\n            value: {{ $value | quote }}\n          {{- end }}\n        {{- end }}\n```\n\n### values.yaml\n\n```yaml\n# values.yaml (기본값)\nreplicaCount: 1\n\nimage:\n  repository: myapp\n  tag: \"\"\n  pullPolicy: IfNotPresent\n\nservice:\n  type: ClusterIP\n  port: 8080\n\nresources:\n  requests:\n    cpu: 100m\n    memory: 128Mi\n  limits:\n    cpu: 200m\n    memory: 256Mi\n\ningress:\n  enabled: false\n  host: \"\"\n\nenv: {}\n```\n\n```yaml\n# values-prod.yaml\nreplicaCount: 5\n\nimage:\n  tag: v1.2.3\n\nresources:\n  requests:\n    cpu: 500m\n    memory: 512Mi\n  limits:\n    cpu: 1000m\n    memory: 1024Mi\n\ningress:\n  enabled: true\n  host: myapp.example.com\n  tls:\n    - secretName: myapp-tls\n      hosts:\n        - myapp.example.com\n\nenv:\n  LOG_LEVEL: warn\n  DB_HOST: prod-db.example.com\n```\n\n### 배포\n\n```bash\n# 로컬 차트 설치\nhelm install myapp-prod ./mychart -f values-prod.yaml -n production\n\n# 리포지토리에서 설치\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install nginx bitnami/nginx -f my-values.yaml\n\n# 업그레이드\nhelm upgrade myapp-prod ./mychart -f values-prod.yaml\n\n# 롤백\nhelm rollback myapp-prod 1\n```\n\n### Helm Hooks\n\n특정 시점에 작업 실행:\n\n```yaml\n# templates/pre-upgrade-job.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}-db-migrate\n  annotations:\n    \"helm.sh/hook\": pre-upgrade\n    \"helm.sh/hook-weight\": \"0\"\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    spec:\n      containers:\n      - name: migrate\n        image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\"\n        command: [\"./migrate.sh\"]\n      restartPolicy: Never\n```\n\n| Hook | 시점 |\n|------|-----|\n| `pre-install` | 설치 전 |\n| `post-install` | 설치 후 |\n| `pre-upgrade` | 업그레이드 전 |\n| `post-upgrade` | 업그레이드 후 |\n| `pre-delete` | 삭제 전 |\n| `post-delete` | 삭제 후 |\n| `pre-rollback` | 롤백 전 |\n| `post-rollback` | 롤백 후 |\n\n---\n\n## Kustomize vs Helm: 선택 기준\n\n```mermaid\nflowchart TB\n    Start[환경별 설정 관리 필요] --> Q1{써드파티 소프트웨어?}\n    \n    Q1 -->|Yes| Helm1[Helm 사용\\n이미 차트가 있음]\n    Q1 -->|No| Q2{복잡한 조건부 로직?}\n    \n    Q2 -->|Yes| Helm2[Helm 사용\\nif/range 템플릿]\n    Q2 -->|No| Q3{팀이 YAML에 익숙?}\n    \n    Q3 -->|Yes| Kustomize1[Kustomize 사용\\n순수 YAML 유지]\n    Q3 -->|No| Q4{배포 대상이 많음?}\n    \n    Q4 -->|Yes| Helm3[Helm 사용\\n재사용성 중요]\n    Q4 -->|No| Kustomize2[Kustomize 사용\\n단순함 우선]\n```\n\n### 비교 표\n\n| 관점 | Kustomize | Helm |\n|-----|-----------|------|\n| **학습 곡선** | 낮음 (순수 YAML) | 중간 (Go template) |\n| **디버깅** | 쉬움 (YAML 그대로) | 어려움 (템플릿 렌더링 필요) |\n| **조건부 로직** | 제한적 | 강력함 (if/range) |\n| **재사용성** | Component로 가능 | Chart로 패키징 |\n| **의존성 관리** | 없음 | Chart dependencies |\n| **버전 관리** | Git으로 직접 | Chart 버전 + values |\n| **에코시스템** | kubectl 내장 | ArtifactHub, 수많은 차트 |\n| **GitOps 친화성** | 높음 | 중간 (렌더링 필요) |\n\n### Kustomize를 선택해야 할 때\n\n1. **내부 애플리케이션**을 배포할 때\n2. **단순한 환경별 차이**만 있을 때 (replicas, resources, env)\n3. **템플릿 없이 순수 YAML**을 유지하고 싶을 때\n4. **Git diff가 명확**해야 할 때\n\n### Helm을 선택해야 할 때\n\n1. **써드파티 소프트웨어** 설치 (nginx, postgresql, prometheus)\n2. **복잡한 조건부 로직**이 필요할 때\n3. **재사용 가능한 패키지**로 배포할 때\n4. **의존성 관리**가 필요할 때\n\n> [!TIP]\n> **실무 조언**: 내부 앱은 Kustomize, 인프라 컴포넌트는 Helm으로 관리하는 하이브리드 접근이 일반적입니다.\n\n---\n\n## 하이브리드 접근: Helm + Kustomize\n\nArgoCD와 Flux 모두 Helm 차트에 Kustomize를 적용하는 **post-rendering**을 지원합니다.\n\n### ArgoCD에서 Helm + Kustomize\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: nginx-prod\nspec:\n  source:\n    repoURL: https://charts.bitnami.com/bitnami\n    chart: nginx\n    targetRevision: 15.0.0\n    helm:\n      values: |\n        replicaCount: 3\n    # Kustomize post-rendering\n    kustomize:\n      patches:\n        - target:\n            kind: Deployment\n            name: nginx\n          patch: |-\n            - op: add\n              path: /spec/template/metadata/annotations\n              value:\n                custom-annotation: \"added-by-kustomize\"\n```\n\n### Flux에서 Helm + Kustomize\n\n```yaml\napiVersion: helm.toolkit.fluxcd.io/v2\nkind: HelmRelease\nmetadata:\n  name: nginx\nspec:\n  chart:\n    spec:\n      chart: nginx\n      sourceRef:\n        kind: HelmRepository\n        name: bitnami\n  # Kustomize post-rendering\n  postRenderers:\n    - kustomize:\n        patches:\n          - target:\n              kind: Deployment\n              name: nginx\n            patch: |\n              - op: add\n                path: /metadata/annotations/custom\n                value: \"post-rendered\"\n```\n\n### 사용 사례\n\n1. **Helm 차트에 커스텀 레이블/어노테이션 추가**\n2. **Helm 차트가 지원하지 않는 설정 패치**\n3. **조직 표준 정책 강제 적용** (예: 모든 Pod에 sidecar 추가)\n\n---\n\n## 실전 레포지토리 구조\n\n### Kustomize 기반 구조\n\n```\ngitops-repo/\n├── apps/\n│   ├── frontend/\n│   │   ├── base/\n│   │   └── overlays/\n│   │       ├── dev/\n│   │       ├── staging/\n│   │       └── prod/\n│   └── backend/\n│       ├── base/\n│       └── overlays/\n│           ├── dev/\n│           ├── staging/\n│           └── prod/\n├── infrastructure/\n│   ├── base/\n│   │   ├── cert-manager/\n│   │   ├── ingress-nginx/\n│   │   └── monitoring/\n│   └── overlays/\n│       ├── dev/\n│       └── prod/\n└── clusters/\n    ├── dev/\n    │   ├── apps.yaml      # ArgoCD Application\n    │   └── infra.yaml\n    └── prod/\n        ├── apps.yaml\n        └── infra.yaml\n```\n\n### Helm 기반 구조\n\n```\ngitops-repo/\n├── charts/\n│   ├── frontend/\n│   │   ├── Chart.yaml\n│   │   ├── values.yaml\n│   │   └── templates/\n│   └── backend/\n│       ├── Chart.yaml\n│       ├── values.yaml\n│       └── templates/\n├── releases/\n│   ├── dev/\n│   │   ├── frontend.yaml  # HelmRelease\n│   │   └── backend.yaml\n│   └── prod/\n│       ├── frontend.yaml\n│       └── backend.yaml\n└── infrastructure/\n    ├── cert-manager/\n    │   ├── helmrepository.yaml\n    │   └── helmrelease.yaml\n    └── ingress-nginx/\n        ├── helmrepository.yaml\n        └── helmrelease.yaml\n```\n\n---\n\n## 정리\n\n| 도구 | 장점 | 단점 | 사용 시기 |\n|-----|------|------|----------|\n| **Kustomize** | 순수 YAML, 학습 쉬움, kubectl 내장 | 조건부 로직 제한 | 내부 앱, 단순 환경 차이 |\n| **Helm** | 강력한 템플릿, 패키지화, 에코시스템 | 디버깅 어려움 | 써드파티, 복잡한 로직 |\n| **하이브리드** | 양쪽 장점 활용 | 복잡도 증가 | Helm 차트 커스터마이징 |\n\n---\n\n## 다음 편 예고\n\n**5편: Secrets Management**에서는 다음을 다룹니다:\n\n- GitOps에서 Secrets의 딜레마\n- Sealed Secrets 동작 원리와 한계\n- External Secrets Operator\n- SOPS (Secrets OPerationS)\n- 전략 선택 가이드\n\n---\n\n## 참고 자료\n\n- [Kustomize Official](https://kustomize.io/)\n- [Kustomize Components](https://kubectl.docs.kubernetes.io/guides/config_management/components/)\n- [Helm Documentation](https://helm.sh/docs/)\n- [Helm Best Practices](https://helm.sh/docs/chart_best_practices/)\n- [ArgoCD + Kustomize](https://argo-cd.readthedocs.io/en/stable/user-guide/kustomize/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitOps",
      "Kubernetes",
      "Kustomize",
      "Helm"
    ],
    "readingTime": 10,
    "wordCount": 1820,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitops",
    "date": "2026-01-05"
  },
  {
    "id": "gitops-03-flux-cd",
    "slug": "gitops-03-flux-cd",
    "path": "backend/gitops",
    "fullPath": "backend/gitops/gitops-03-flux-cd",
    "title": "GitOps 심화 시리즈 #3: Flux CD - GitOps Toolkit과 컨트롤러 아키텍처",
    "excerpt": "Flux의 GitOps Toolkit 아키텍처, Source/Kustomize/Helm Controller의 동작 원리, 그리고 ArgoCD와의 상세 비교를 다룹니다.",
    "content": "# GitOps 심화 시리즈 #3: Flux CD - GitOps Toolkit과 컨트롤러 아키텍처\n\n## 시리즈 개요\n\n| # | 주제 | 핵심 내용 |\n|---|------|----------|\n| 1 | GitOps 개요 | 철학과 원칙, Push vs Pull 배포, Reconciliation |\n| 2 | ArgoCD Deep Dive | 아키텍처, Application CRD, Sync 전략 |\n| **3** | **Flux CD & GitOps Toolkit** | 컨트롤러 아키텍처, GitRepository, Kustomization |\n| 4 | 환경별 설정 관리 | Kustomize vs Helm, 전략 선택 기준 |\n| 5 | Secrets Management | Sealed Secrets, External Secrets, SOPS |\n| 6 | CI/CD 파이프라인 통합 | Image Updater, Progressive Delivery |\n\n---\n\n## Flux란?\n\n**Flux**는 Kubernetes를 위한 GitOps 도구 집합입니다. 2019년 Weaveworks에서 시작하여 2022년 CNCF Graduated 프로젝트가 되었습니다.\n\nArgoCD가 단일 애플리케이션이라면, Flux는 **GitOps Toolkit**이라는 독립적인 컨트롤러들의 집합입니다.\n\n### 핵심 특징\n\n- **CNCF Graduated 프로젝트**: 프로덕션 검증 완료\n- **모듈러 아키텍처**: 필요한 컨트롤러만 선택적으로 설치 가능\n- **Kubernetes Native**: 모든 설정이 CRD로 관리됨\n- **Image Automation 내장**: 컨테이너 이미지 자동 업데이트 지원\n- **Multi-tenancy 지원**: 네임스페이스 기반의 팀 격리\n\n> [!NOTE]\n> Weaveworks가 2024년 폐업했지만, Flux는 CNCF Graduated 프로젝트로서 커뮤니티와 CNCF의 지원 하에 활발히 개발되고 있습니다.\n\n---\n\n## Flux vs ArgoCD: 철학의 차이\n\n본격적으로 들어가기 전에, 두 도구의 근본적인 설계 철학 차이를 이해해야 합니다.\n\n```mermaid\nflowchart TB\n    subgraph ArgoCD [ArgoCD - 모놀리식]\n        A_ALL[단일 애플리케이션\\nAPI Server + Repo Server + Controller]\n        A_UI[강력한 Web UI]\n        A_APP[Application CRD]\n    end\n    \n    subgraph Flux [Flux - 마이크로서비스]\n        F_SOURCE[Source Controller]\n        F_KUSTOMIZE[Kustomize Controller]\n        F_HELM[Helm Controller]\n        F_NOTIFY[Notification Controller]\n        F_IMAGE[Image Automation]\n    end\n    \n    A_ALL --> A_UI\n    A_ALL --> A_APP\n    \n    F_SOURCE --> F_KUSTOMIZE\n    F_SOURCE --> F_HELM\n    F_KUSTOMIZE --> F_NOTIFY\n    F_HELM --> F_NOTIFY\n```\n\n| 관점 | ArgoCD | Flux |\n|-----|--------|------|\n| **아키텍처** | 모놀리식 (단일 바이너리) | 마이크로서비스 (독립 컨트롤러) |\n| **UI** | 내장 Web UI | CLI 중심, UI는 별도 (Weave GitOps) |\n| **설치** | 한 번에 전체 설치 | 필요한 컨트롤러만 선택 설치 |\n| **멀티 클러스터** | 중앙 집중 (Hub-Spoke) | 분산 (각 클러스터에 Flux 설치) |\n| **학습 곡선** | 낮음 (UI 친화적) | 중간 (CRD 이해 필요) |\n| **리소스 사용** | 높음 | 낮음 |\n\n> [!NOTE]\n> **선택 기준**: 화려한 UI와 중앙 집중 관리가 필요하면 ArgoCD, 경량화와 유연성이 중요하면 Flux가 적합합니다.\n\n---\n\n## GitOps Toolkit 아키텍처\n\nFlux v2는 **GitOps Toolkit**이라는 컨트롤러 집합으로 구성됩니다.\n\n```mermaid\nflowchart TB\n    subgraph Sources [Source Controllers]\n        Git[GitRepository]\n        Helm[HelmRepository]\n        OCI[OCIRepository]\n        Bucket[Bucket]\n    end\n    \n    subgraph Reconcilers [Reconciler Controllers]\n        Kustomize[Kustomization]\n        HelmRelease[HelmRelease]\n    end\n    \n    subgraph Automation [Automation Controllers]\n        ImageRepo[ImageRepository]\n        ImagePolicy[ImagePolicy]\n        ImageUpdate[ImageUpdateAutomation]\n    end\n    \n    subgraph Notifications [Notification Controller]\n        Alert[Alert]\n        Provider[Provider]\n        Receiver[Receiver]\n    end\n    \n    Git --> Kustomize\n    Git --> HelmRelease\n    Helm --> HelmRelease\n    OCI --> Kustomize\n    OCI --> HelmRelease\n    \n    ImageRepo --> ImagePolicy\n    ImagePolicy --> ImageUpdate\n    ImageUpdate --> Git\n    \n    Kustomize --> Alert\n    HelmRelease --> Alert\n    Alert --> Provider\n```\n\n### 컨트롤러 역할\n\n| 컨트롤러 | 역할 | CRD |\n|---------|-----|-----|\n| **Source Controller** | Git, Helm, OCI에서 아티팩트 가져오기 | GitRepository, HelmRepository, OCIRepository, Bucket |\n| **Kustomize Controller** | Kustomize 매니페스트 적용 | Kustomization |\n| **Helm Controller** | Helm 릴리스 관리 | HelmRelease |\n| **Notification Controller** | 이벤트 알림 | Alert, Provider, Receiver |\n| **Image Automation** | 컨테이너 이미지 업데이트 자동화 | ImageRepository, ImagePolicy, ImageUpdateAutomation |\n\n---\n\n## Source Controller\n\n**Source Controller**는 외부 소스에서 아티팩트를 가져와 클러스터 내에서 사용 가능하게 만듭니다.\n\n### GitRepository\n\n가장 기본적인 소스. Git 저장소에서 매니페스트를 가져옵니다:\n\n```yaml\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: my-app\n  namespace: flux-system\nspec:\n  # 동기화 주기\n  interval: 1m\n  \n  # Git 저장소 URL\n  url: https://github.com/myorg/my-app.git\n  \n  # 브랜치/태그/커밋\n  ref:\n    branch: main\n    # 또는\n    # tag: v1.0.0\n    # commit: abc123\n  \n  # 인증 (필요시)\n  secretRef:\n    name: git-credentials\n  \n  # 특정 경로만 가져오기\n  ignore: |\n    # 전체 제외\n    /*\n    # 특정 디렉토리만 포함\n    !/deploy/\n```\n\n**인증 설정**:\n\n```yaml\n# HTTPS 인증\napiVersion: v1\nkind: Secret\nmetadata:\n  name: git-credentials\n  namespace: flux-system\ntype: Opaque\nstringData:\n  username: git\n  password: ghp_xxxxxxxxxxxx  # GitHub PAT\n\n---\n# SSH 인증\napiVersion: v1\nkind: Secret\nmetadata:\n  name: git-ssh-key\n  namespace: flux-system\ntype: Opaque\nstringData:\n  identity: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    ...\n    -----END OPENSSH PRIVATE KEY-----\n  known_hosts: |\n    github.com ssh-rsa AAAA...\n```\n\n### HelmRepository\n\nHelm Chart Repository에서 차트를 가져옵니다:\n\n```yaml\napiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: bitnami\n  namespace: flux-system\nspec:\n  interval: 1h\n  url: https://charts.bitnami.com/bitnami\n  \n  # OCI Registry도 지원\n  # type: oci\n  # url: oci://ghcr.io/myorg/charts\n```\n\n### OCIRepository\n\nOCI Registry에서 아티팩트를 가져옵니다 (Helm 차트 뿐 아니라 Kustomize 매니페스트도 가능):\n\n```yaml\napiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: OCIRepository\nmetadata:\n  name: podinfo\n  namespace: flux-system\nspec:\n  interval: 5m\n  url: oci://ghcr.io/stefanprodan/manifests/podinfo\n  ref:\n    tag: latest\n```\n\n> [!TIP]\n> **OCI 아티팩트**는 컨테이너 레지스트리에 Kubernetes 매니페스트를 저장하는 방식입니다. Git 없이도 GitOps가 가능해집니다.\n\n---\n\n## Kustomize Controller\n\n**Kustomize Controller**는 GitRepository에서 가져온 Kustomize 매니페스트를 클러스터에 적용합니다.\n\n### Kustomization CRD\n\n```yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: my-app\n  namespace: flux-system\nspec:\n  # 동기화 주기\n  interval: 10m\n  \n  # 재시도 주기 (실패 시)\n  retryInterval: 2m\n  \n  # 소스 참조\n  sourceRef:\n    kind: GitRepository\n    name: my-app\n  \n  # 매니페스트 경로\n  path: ./deploy/production\n  \n  # 타겟 네임스페이스 (모든 리소스에 적용)\n  targetNamespace: production\n  \n  # Git에서 삭제된 리소스 정리\n  prune: true\n  \n  # 타임아웃\n  timeout: 5m\n  \n  # Health Check 대기\n  wait: true\n  healthChecks:\n    - apiVersion: apps/v1\n      kind: Deployment\n      name: my-app\n      namespace: production\n```\n\n### Kustomization 간 의존성\n\nFlux의 강력한 기능 중 하나는 **의존성 관리**입니다:\n\n```yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: infrastructure\n  namespace: flux-system\nspec:\n  interval: 10m\n  sourceRef:\n    kind: GitRepository\n    name: infra\n  path: ./infrastructure\n  prune: true\n\n---\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: apps\n  namespace: flux-system\nspec:\n  interval: 10m\n  # infrastructure가 Ready 상태가 된 후에만 적용\n  dependsOn:\n    - name: infrastructure\n  sourceRef:\n    kind: GitRepository\n    name: apps\n  path: ./apps\n  prune: true\n```\n\n```mermaid\nflowchart LR\n    subgraph Dependencies [의존성 체인]\n        CRDs[CRDs] --> Controllers[Controllers]\n        Controllers --> Infrastructure[Infrastructure]\n        Infrastructure --> Apps[Applications]\n    end\n    \n    CRDs --> |\"dependsOn\"| Controllers\n    Controllers --> |\"dependsOn\"| Infrastructure\n    Infrastructure --> |\"dependsOn\"| Apps\n```\n\n### 변수 치환 (Post-build Substitution)\n\nKustomize 적용 후 변수를 치환할 수 있습니다:\n\n```yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: my-app\n  namespace: flux-system\nspec:\n  # ...\n  postBuild:\n    substitute:\n      ENVIRONMENT: production\n      REPLICAS: \"3\"\n    substituteFrom:\n      - kind: ConfigMap\n        name: cluster-config\n      - kind: Secret\n        name: cluster-secrets\n```\n\n```yaml\n# deployment.yaml (치환 대상)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  replicas: ${REPLICAS}\n  template:\n    spec:\n      containers:\n      - name: app\n        env:\n        - name: ENVIRONMENT\n          value: ${ENVIRONMENT}\n```\n\n---\n\n## Helm Controller\n\n**Helm Controller**는 Helm 릴리스를 선언적으로 관리합니다.\n\n### HelmRelease CRD\n\n```yaml\napiVersion: helm.toolkit.fluxcd.io/v2\nkind: HelmRelease\nmetadata:\n  name: nginx\n  namespace: default\nspec:\n  # 동기화 주기\n  interval: 10m\n  \n  # Helm Chart 소스\n  chart:\n    spec:\n      chart: nginx\n      version: \"15.x\"  # SemVer 범위 지원\n      sourceRef:\n        kind: HelmRepository\n        name: bitnami\n        namespace: flux-system\n  \n  # Values 설정\n  values:\n    replicaCount: 3\n    service:\n      type: ClusterIP\n  \n  # Values 파일 참조\n  valuesFrom:\n    - kind: ConfigMap\n      name: nginx-values\n      valuesKey: values.yaml\n    - kind: Secret\n      name: nginx-secrets\n      valuesKey: credentials\n  \n  # Upgrade 설정\n  upgrade:\n    remediation:\n      retries: 3\n      remediateLastFailure: true\n  \n  # Rollback 설정\n  rollback:\n    recreate: true\n    cleanupOnFail: true\n```\n\n### Git에서 Helm Chart 사용\n\nHelmRepository 대신 GitRepository에서 직접 차트를 가져올 수도 있습니다:\n\n```yaml\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: my-charts\n  namespace: flux-system\nspec:\n  interval: 5m\n  url: https://github.com/myorg/helm-charts.git\n  ref:\n    branch: main\n\n---\napiVersion: helm.toolkit.fluxcd.io/v2\nkind: HelmRelease\nmetadata:\n  name: my-app\n  namespace: default\nspec:\n  interval: 10m\n  chart:\n    spec:\n      chart: ./charts/my-app  # Git 내 경로\n      sourceRef:\n        kind: GitRepository\n        name: my-charts\n        namespace: flux-system\n```\n\n---\n\n## Image Automation\n\nFlux의 차별화된 기능: **컨테이너 이미지 버전 자동 업데이트**\n\n```mermaid\nsequenceDiagram\n    participant Registry as Container Registry\n    participant Image as ImageRepository\n    participant Policy as ImagePolicy\n    participant Update as ImageUpdateAutomation\n    participant Git as Git Repository\n    participant Flux as Flux Controllers\n    participant K8s as Kubernetes\n    \n    Image->>Registry: 새 이미지 태그 스캔\n    Registry-->>Image: v1.2.3 발견\n    Image->>Policy: 정책과 매칭\n    Policy->>Policy: SemVer 필터링\n    Policy->>Update: 업데이트 트리거\n    Update->>Git: manifest 수정 & commit\n    Git->>Flux: 변경 감지\n    Flux->>K8s: 배포\n```\n\n### ImageRepository\n\n컨테이너 레지스트리에서 이미지 태그를 스캔합니다:\n\n```yaml\napiVersion: image.toolkit.fluxcd.io/v1beta2\nkind: ImageRepository\nmetadata:\n  name: my-app\n  namespace: flux-system\nspec:\n  # 스캔할 이미지\n  image: ghcr.io/myorg/my-app\n  \n  # 스캔 주기\n  interval: 5m\n  \n  # 인증 (필요시)\n  secretRef:\n    name: ghcr-auth\n```\n\n### ImagePolicy\n\n어떤 태그를 선택할지 정책을 정의합니다:\n\n```yaml\napiVersion: image.toolkit.fluxcd.io/v1beta2\nkind: ImagePolicy\nmetadata:\n  name: my-app\n  namespace: flux-system\nspec:\n  imageRepositoryRef:\n    name: my-app\n  \n  # 정책: SemVer 범위\n  policy:\n    semver:\n      range: \">=1.0.0\"\n  \n  # 또는: 알파벳순 최신\n  # policy:\n  #   alphabetical:\n  #     order: asc\n  \n  # 또는: 숫자순 최신\n  # policy:\n  #   numerical:\n  #     order: asc\n```\n\n### ImageUpdateAutomation\n\nGit 저장소의 매니페스트를 자동 업데이트합니다:\n\n```yaml\napiVersion: image.toolkit.fluxcd.io/v1beta2\nkind: ImageUpdateAutomation\nmetadata:\n  name: my-app\n  namespace: flux-system\nspec:\n  interval: 5m\n  \n  # 업데이트할 Git 저장소\n  sourceRef:\n    kind: GitRepository\n    name: my-app\n  \n  # Git 설정\n  git:\n    checkout:\n      ref:\n        branch: main\n    commit:\n      author:\n        name: Flux\n        email: flux@myorg.com\n      messageTemplate: |\n        Update images\n        \n        {{range .Changed.Changes -}}\n        - {{.OldValue}} -> {{.NewValue}}\n        {{end}}\n    push:\n      branch: main\n  \n  # 업데이트 대상 파일\n  update:\n    path: ./deploy\n    strategy: Setters  # 마커 기반 업데이트\n```\n\n**마커 기반 업데이트**:\n\n```yaml\n# deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  template:\n    spec:\n      containers:\n      - name: app\n        image: ghcr.io/myorg/my-app:v1.0.0  # {\"$imagepolicy\": \"flux-system:my-app\"}\n```\n\n> [!IMPORTANT]\n> ArgoCD는 Image Updater가 별도 프로젝트인 반면, Flux는 **Image Automation이 핵심 기능**으로 내장되어 있습니다.\n\n---\n\n## Notification Controller\n\n배포 이벤트를 외부 시스템으로 알립니다.\n\n### Alert\n\n어떤 이벤트를 알릴지 정의:\n\n```yaml\napiVersion: notification.toolkit.fluxcd.io/v1beta3\nkind: Alert\nmetadata:\n  name: on-call-alerts\n  namespace: flux-system\nspec:\n  # 심각도 필터\n  eventSeverity: error\n  \n  # 모니터링 대상\n  eventSources:\n    - kind: Kustomization\n      name: '*'\n    - kind: HelmRelease\n      name: '*'\n  \n  # 알림 대상\n  providerRef:\n    name: slack\n```\n\n### Provider\n\n알림을 보낼 채널:\n\n```yaml\napiVersion: notification.toolkit.fluxcd.io/v1beta3\nkind: Provider\nmetadata:\n  name: slack\n  namespace: flux-system\nspec:\n  type: slack\n  channel: devops-alerts\n  secretRef:\n    name: slack-webhook\n\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: slack-webhook\n  namespace: flux-system\nstringData:\n  address: https://hooks.slack.com/services/T00/B00/XXX\n```\n\n**지원 Provider**: Slack, Discord, Microsoft Teams, GitHub, GitLab, PagerDuty, Datadog, Sentry 등\n\n### Receiver\n\n외부에서 Flux로 Webhook 수신:\n\n```yaml\napiVersion: notification.toolkit.fluxcd.io/v1\nkind: Receiver\nmetadata:\n  name: github-webhook\n  namespace: flux-system\nspec:\n  type: github\n  events:\n    - push\n  secretRef:\n    name: github-webhook-secret\n  resources:\n    - kind: GitRepository\n      name: my-app\n```\n\n---\n\n## Flux Bootstrap\n\nFlux 설치의 권장 방법은 **Bootstrap**입니다. Flux 자체를 GitOps로 관리합니다:\n\n```bash\n# GitHub 저장소로 Bootstrap\nflux bootstrap github \\\n  --owner=myorg \\\n  --repository=fleet-infra \\\n  --branch=main \\\n  --path=clusters/production \\\n  --personal\n\n# 생성되는 구조\nfleet-infra/\n└── clusters/\n    └── production/\n        └── flux-system/\n            ├── gotk-components.yaml  # Flux 컴포넌트\n            ├── gotk-sync.yaml        # 자기 자신을 관리하는 Kustomization\n            └── kustomization.yaml\n```\n\n```mermaid\nflowchart TB\n    subgraph GitRepo [fleet-infra Repository]\n        FluxSys[flux-system/]\n        Infra[infrastructure/]\n        Apps[apps/]\n    end\n    \n    subgraph Cluster [Kubernetes Cluster]\n        Flux[Flux Controllers]\n        InfraRes[Infrastructure Resources]\n        AppRes[Applications]\n    end\n    \n    Flux -->|Sync| FluxSys\n    FluxSys -->|Update| Flux\n    Flux -->|Sync| Infra\n    Infra --> InfraRes\n    Flux -->|Sync| Apps\n    Apps --> AppRes\n    \n    Note[Flux가 자기 자신을 관리!]\n```\n\n---\n\n## Multi-tenancy 패턴\n\nFlux는 네임스페이스 기반 격리를 지원합니다:\n\n```yaml\n# 팀 A 전용 소스와 Kustomization\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: team-a-apps\n  namespace: team-a  # 팀 네임스페이스\nspec:\n  url: https://github.com/myorg/team-a-apps.git\n  # ...\n\n---\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: team-a-apps\n  namespace: team-a\nspec:\n  # 해당 팀 네임스페이스로 제한\n  targetNamespace: team-a\n  serviceAccountName: team-a-reconciler  # 제한된 권한의 SA\n  # ...\n```\n\n```mermaid\nflowchart TB\n    subgraph FluxSystem [flux-system namespace]\n        FluxCtrl[Flux Controllers]\n    end\n    \n    subgraph TeamA [team-a namespace]\n        GitA[GitRepository]\n        KustomA[Kustomization]\n        SAa[ServiceAccount]\n        Apps_A[Team A Apps]\n    end\n    \n    subgraph TeamB [team-b namespace]\n        GitB[GitRepository]\n        KustomB[Kustomization]\n        SAb[ServiceAccount]\n        Apps_B[Team B Apps]\n    end\n    \n    FluxCtrl --> GitA & GitB\n    GitA --> KustomA --> Apps_A\n    GitB --> KustomB --> Apps_B\n    \n    KustomA -.->|사용| SAa\n    KustomB -.->|사용| SAb\n```\n\n---\n\n## ArgoCD vs Flux 상세 비교\n\n| 기능 | ArgoCD | Flux |\n|-----|--------|------|\n| **설치 복잡도** | helm install 한 줄 | flux bootstrap |\n| **리소스 사용** | ~400MB 메모리 | ~100MB 메모리 |\n| **Web UI** | 강력한 내장 UI | Weave GitOps (별도) |\n| **멀티 클러스터** | 중앙 ArgoCD가 관리 | 각 클러스터에 Flux |\n| **이미지 자동화** | Image Updater (별도) | 내장 |\n| **Helm 지원** | Application에서 처리 | HelmRelease CRD |\n| **OCI 지원** | 지원 | 네이티브 지원 |\n| **Health Check** | 내장 (강력) | wait/healthChecks |\n| **Webhook** | GitHub, GitLab 등 | Receiver로 다양한 소스 |\n| **SOPS 지원** | 제한적 | 네이티브 |\n\n### 언제 ArgoCD를 선택할까?\n\n- **UI가 중요**할 때 (운영팀에서 시각적 모니터링 선호)\n- **중앙 집중 관리**가 필요할 때 (한 곳에서 모든 클러스터 관리)\n- **복잡한 Sync 전략**이 필요할 때 (Sync Waves, Hooks)\n- **학습 곡선을 줄이고 싶을 때**\n\n### 언제 Flux를 선택할까?\n\n- **경량화**가 중요할 때 (Edge, 리소스 제한 환경)\n- **이미지 자동 업데이트**가 핵심 요구사항일 때\n- **SOPS로 Secrets 관리**할 때\n- **각 클러스터의 독립성**이 중요할 때\n- **Kubernetes Native 접근**을 선호할 때\n\n---\n\n## 정리\n\n| 컴포넌트 | 역할 |\n|---------|------|\n| **Source Controller** | Git, Helm, OCI에서 아티팩트 가져오기 |\n| **Kustomize Controller** | Kustomize 매니페스트 적용, 의존성 관리 |\n| **Helm Controller** | Helm 릴리스 생명주기 관리 |\n| **Image Automation** | 컨테이너 이미지 자동 업데이트 |\n| **Notification Controller** | 이벤트 알림 송수신 |\n\n---\n\n## 다음 편 예고\n\n**4편: 환경별 설정 관리**에서는 다음을 다룹니다:\n\n- Kustomize Base/Overlays 패턴 심화\n- Helm values 관리 전략\n- Kustomize vs Helm 선택 기준\n- 하이브리드 접근: Helm + Kustomize post-rendering\n\n---\n\n## 참고 자료\n\n- [Flux Official Documentation](https://fluxcd.io/flux/)\n- [Flux Components](https://fluxcd.io/flux/components/)\n- [GitOps Toolkit](https://fluxcd.io/flux/components/)\n- [Flux Image Automation](https://fluxcd.io/flux/guides/image-update/)\n- [Multi-tenancy with Flux](https://fluxcd.io/flux/components/kustomize/kustomizations/#multi-tenancy)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitOps",
      "Kubernetes",
      "Flux",
      "CNCF"
    ],
    "readingTime": 11,
    "wordCount": 2067,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitops",
    "date": "2026-01-05"
  },
  {
    "id": "gitops-02-argocd-deep-dive",
    "slug": "gitops-02-argocd-deep-dive",
    "path": "backend/gitops",
    "fullPath": "backend/gitops/gitops-02-argocd-deep-dive",
    "title": "GitOps 심화 시리즈 #2: ArgoCD Deep Dive - 아키텍처와 동작 원리",
    "excerpt": "ArgoCD의 내부 아키텍처, Application CRD 상세 분석, Sync 전략, 그리고 ApplicationSet으로 멀티 클러스터를 관리하는 방법을 깊이 있게 다룹니다.",
    "content": "# GitOps 심화 시리즈 #2: ArgoCD Deep Dive - 아키텍처와 동작 원리\n\n## 시리즈 개요\n\n| # | 주제 | 핵심 내용 |\n|---|------|----------|\n| 1 | GitOps 개요 | 철학과 원칙, Push vs Pull 배포, Reconciliation |\n| **2** | **ArgoCD Deep Dive** | 아키텍처, Application CRD, Sync 전략 |\n| 3 | Flux CD & GitOps Toolkit | 컨트롤러 아키텍처, GitRepository, Kustomization |\n| 4 | 환경별 설정 관리 | Kustomize vs Helm, 전략 선택 기준 |\n| 5 | Secrets Management | Sealed Secrets, External Secrets, SOPS |\n| 6 | CI/CD 파이프라인 통합 | Image Updater, Progressive Delivery |\n\n---\n\n## ArgoCD란?\n\nArgoCD는 Kubernetes를 위한 **선언적(Declarative) GitOps 지속 배포(Continuous Delivery) 도구**입니다. Git 저장소에 정의된 애플리케이션 상태를 지속적으로 모니터링하고, Kubernetes 클러스터의 실제 상태와 동기화합니다.\n\n### 핵심 특징\n\n- **CNCF Graduated 프로젝트**: 2022년 CNCF 졸업, 프로덕션 검증 완료\n- **다양한 매니페스트 지원**: Plain YAML, Helm, Kustomize, Jsonnet\n- **강력한 Web UI**: 실시간 애플리케이션 상태 시각화\n- **멀티 클러스터 관리**: 단일 ArgoCD 인스턴스로 여러 클러스터 관리 가능\n- **SSO 통합**: OIDC, SAML, LDAP, GitHub, GitLab 등 다양한 인증 지원\n\n---\n\n## ArgoCD 아키텍처\n\nArgoCD는 3개의 핵심 컴포넌트로 구성됩니다:\n\n```mermaid\nflowchart TB\n    subgraph External [외부]\n        User[사용자]\n        Git[Git Repository]\n        Webhook[Webhook]\n    end\n    \n    subgraph ArgoCD [ArgoCD Namespace]\n        subgraph API [API Server]\n            REST[REST API]\n            GRPC[gRPC API]\n            WebUI[Web UI]\n        end\n        \n        subgraph Repo [Repo Server]\n            Clone[Git Clone]\n            Render[Manifest Rendering]\n            Cache[Cache]\n        end\n        \n        subgraph Controller [Application Controller]\n            Reconciler[Reconciliation Loop]\n            Diff[Diff Engine]\n            Sync[Sync Engine]\n        end\n        \n        Redis[(Redis)]\n    end\n    \n    subgraph Cluster [Kubernetes Cluster]\n        APIServer[K8s API Server]\n        Resources[Workloads]\n    end\n    \n    User --> WebUI & REST\n    Git --> Clone\n    Webhook --> API\n    \n    API <--> Repo\n    API <--> Controller\n    API <--> Redis\n    \n    Controller <--> APIServer\n    APIServer --> Resources\n    \n    Repo --> Clone\n    Clone --> Render\n    Render --> Cache\n```\n\n### 1. API Server\n\nArgoCD의 **프론트엔드** 역할을 합니다.\n\n| 기능 | 설명 |\n|-----|------|\n| **REST/gRPC API** | CLI, Web UI, 외부 시스템과 통신 |\n| **Web UI** | React 기반 실시간 대시보드 |\n| **인증/인가** | RBAC, SSO 통합 |\n| **Webhook 처리** | Git 변경 시 즉시 Sync 트리거 |\n\n```bash\n# ArgoCD CLI는 gRPC API 사용\nargocd app list\nargocd app sync my-app\n```\n\n### 2. Repo Server\n\nGit 저장소와 **매니페스트 렌더링**을 담당합니다.\n\n```mermaid\nsequenceDiagram\n    participant AC as Application Controller\n    participant RS as Repo Server\n    participant Git as Git Repository\n    participant Cache as Cache\n    \n    AC->>RS: GetManifests(app, revision)\n    RS->>Cache: 캐시 확인\n    alt 캐시 Hit\n        Cache-->>RS: 캐시된 매니페스트\n    else 캐시 Miss\n        RS->>Git: git clone/fetch\n        Git-->>RS: Repository\n        RS->>RS: Helm/Kustomize 렌더링\n        RS->>Cache: 캐시 저장\n    end\n    RS-->>AC: Rendered Manifests\n```\n\n**지원하는 매니페스트 도구**:\n\n- Plain YAML/JSON\n- Helm Charts\n- Kustomize\n- Jsonnet\n- Custom Config Management Plugin (CMP)\n\n> [!TIP]\n> Repo Server는 **stateless**입니다. 렌더링 결과를 Redis에 캐싱하므로, 스케일 아웃이 용이합니다.\n\n### 3. Application Controller\n\nArgoCD의 **핵심 엔진**입니다. Kubernetes Controller 패턴으로 동작합니다.\n\n```go\n// 간략화된 Application Controller 로직\nfunc (c *ApplicationController) Reconcile(app *Application) error {\n    // 1. Git에서 Desired State 조회 (Repo Server 통해)\n    desired, err := c.repoServer.GetManifests(app)\n    if err != nil {\n        return err\n    }\n    \n    // 2. 클러스터에서 Current State 조회\n    current, err := c.kubectl.GetResources(app.Destination)\n    if err != nil {\n        return err\n    }\n    \n    // 3. Diff 계산\n    diff := c.diffEngine.Compare(desired, current)\n    \n    // 4. 상태 업데이트\n    app.Status.Sync.Status = calculateSyncStatus(diff)\n    app.Status.Health = calculateHealth(current)\n    \n    // 5. Auto Sync 활성화 시 동기화\n    if app.Spec.SyncPolicy.Automated != nil && diff.HasChanges() {\n        return c.syncEngine.Sync(app, desired)\n    }\n    \n    return nil\n}\n```\n\n**주요 책임**:\n\n- Application 리소스 감시\n- Git 상태와 클러스터 상태 비교\n- Sync Status, Health Status 계산\n- Automated Sync 실행\n\n---\n\n## Application CRD\n\nArgoCD에서 관리하는 모든 애플리케이션은 **Application Custom Resource**로 정의됩니다.\n\n### 기본 구조\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: my-app\n  namespace: argocd  # ArgoCD가 설치된 네임스페이스\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io  # 삭제 시 리소스 정리\nspec:\n  # 프로젝트 (RBAC 단위)\n  project: default\n  \n  # 소스: 어디서 가져올 것인가\n  source:\n    repoURL: https://github.com/myorg/myrepo.git\n    targetRevision: HEAD  # 브랜치, 태그, 커밋 SHA\n    path: k8s/overlays/prod  # 매니페스트 경로\n  \n  # 목적지: 어디에 배포할 것인가\n  destination:\n    server: https://kubernetes.default.svc  # 클러스터 API Server\n    namespace: production  # 타겟 네임스페이스\n  \n  # 동기화 정책\n  syncPolicy:\n    automated:\n      prune: true      # Git에서 삭제된 리소스 클러스터에서도 삭제\n      selfHeal: true   # 수동 변경 시 Git 상태로 복구\n    syncOptions:\n      - CreateNamespace=true  # 네임스페이스 자동 생성\n\nstatus:\n  sync:\n    status: Synced  # Synced, OutOfSync, Unknown\n    revision: abc123\n  health:\n    status: Healthy  # Healthy, Progressing, Degraded, Missing\n  resources:\n    - kind: Deployment\n      name: my-app\n      status: Synced\n      health: Healthy\n```\n\n### Source 설정: Helm\n\n```yaml\nspec:\n  source:\n    repoURL: https://charts.bitnami.com/bitnami\n    chart: nginx\n    targetRevision: 15.0.0\n    helm:\n      releaseName: my-nginx\n      values: |\n        replicaCount: 3\n        service:\n          type: ClusterIP\n      parameters:\n        - name: image.tag\n          value: \"1.25\"\n      valueFiles:\n        - values-prod.yaml\n```\n\n### Source 설정: Kustomize\n\n```yaml\nspec:\n  source:\n    repoURL: https://github.com/myorg/myrepo.git\n    path: k8s/overlays/prod\n    kustomize:\n      namePrefix: prod-\n      nameSuffix: -v1\n      images:\n        - name: my-app\n          newTag: v1.2.3\n      commonLabels:\n        environment: production\n```\n\n---\n\n## Sync 상태와 Health 상태\n\nArgoCD는 두 가지 상태를 추적합니다:\n\n### Sync Status\n\nGit의 Desired State와 클러스터의 Current State 비교 결과\n\n```mermaid\nflowchart LR\n    subgraph SyncStatus [Sync Status]\n        Synced[Synced ✓]\n        OutOfSync[OutOfSync ⚠]\n        Unknown[Unknown ?]\n    end\n    \n    Git[Git Repository] --> |비교| Cluster[Kubernetes Cluster]\n    \n    Cluster --> |일치| Synced\n    Cluster --> |불일치| OutOfSync\n    Cluster --> |확인 불가| Unknown\n```\n\n| 상태 | 의미 |\n|-----|------|\n| **Synced** | Git과 클러스터 상태 일치 |\n| **OutOfSync** | 차이 존재 (Git 변경 or 수동 수정) |\n| **Unknown** | 상태 확인 불가 |\n\n### Health Status\n\n클러스터에 배포된 리소스의 실제 상태\n\n| 상태 | 의미 | 예시 |\n|-----|------|-----|\n| **Healthy** | 정상 동작 중 | Deployment replicas 충족 |\n| **Progressing** | 진행 중 | 롤아웃 중 |\n| **Degraded** | 문제 발생 | Pod CrashLoopBackOff |\n| **Suspended** | 일시 중지 | HPA 일시 중지 |\n| **Missing** | 리소스 없음 | 아직 생성 안됨 |\n\n```mermaid\nflowchart TB\n    subgraph HealthCheck [Health Check]\n        Deploy[Deployment] --> |replicas 확인| Health{상태}\n        Health --> |Available >= Desired| Healthy[Healthy]\n        Health --> |롤아웃 중| Progressing[Progressing]\n        Health --> |Available < Desired| Degraded[Degraded]\n    end\n```\n\n---\n\n## Sync 전략\n\n### Manual Sync vs Automated Sync\n\n```yaml\n# Manual Sync: 명시적 트리거 필요\nspec:\n  syncPolicy: {}  # 또는 생략\n\n# Automated Sync: Git 변경 시 자동 동기화\nspec:\n  syncPolicy:\n    automated: {}\n```\n\n**Automated Sync 옵션**:\n\n```yaml\nspec:\n  syncPolicy:\n    automated:\n      prune: true       # Git에서 삭제된 리소스 정리\n      selfHeal: true    # Drift 발생 시 자동 복구\n      allowEmpty: false # 빈 매니페스트 허용 여부\n```\n\n> [!WARNING]\n> `prune: true`는 Git에서 리소스 정의를 삭제하면 클러스터에서도 **즉시 삭제**됩니다. 실수로 파일을 삭제하면 프로덕션 리소스가 사라질 수 있습니다.\n\n### Self-Heal 동작\n\n```mermaid\nsequenceDiagram\n    participant Ops as 운영자\n    participant K8s as Kubernetes\n    participant ArgoCD as ArgoCD Controller\n    participant Git as Git Repository\n    \n    Ops->>K8s: kubectl scale deployment --replicas=5\n    Note over K8s: 수동 변경 (Drift)\n    \n    ArgoCD->>K8s: 현재 상태 확인 (replicas: 5)\n    ArgoCD->>Git: Git 상태 확인 (replicas: 3)\n    ArgoCD->>ArgoCD: Drift 감지!\n    \n    alt selfHeal: true\n        ArgoCD->>K8s: kubectl apply (replicas: 3)\n        Note over K8s: Git 상태로 복구\n    else selfHeal: false\n        Note over ArgoCD: OutOfSync 상태 유지\n        Note over K8s: 변경 유지\n    end\n```\n\n### Sync Options\n\n세밀한 동기화 제어를 위한 옵션들:\n\n```yaml\nspec:\n  syncPolicy:\n    syncOptions:\n      - CreateNamespace=true       # 네임스페이스 자동 생성\n      - PrunePropagationPolicy=foreground  # 삭제 전파 정책\n      - PruneLast=true             # 다른 리소스 먼저 적용 후 Prune\n      - Replace=true               # apply 대신 replace 사용\n      - ServerSideApply=true       # Server-Side Apply 사용\n      - FailOnSharedResource=true  # 다른 앱과 리소스 공유 금지\n      - RespectIgnoreDifferences=true  # ignoreDifferences 존중\n```\n\n---\n\n## Sync Waves와 Hooks\n\n복잡한 배포 시나리오를 위해 **순서 제어**가 필요할 때 사용합니다.\n\n### Sync Waves\n\n리소스에 `argocd.argoproj.io/sync-wave` 어노테이션으로 순서 지정:\n\n```yaml\n# Wave 0: 먼저 실행 (기본값)\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-app\n  annotations:\n    argocd.argoproj.io/sync-wave: \"0\"\n\n---\n# Wave 1: Namespace 생성 후 실행\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\n  annotations:\n    argocd.argoproj.io/sync-wave: \"1\"\n\n---\n# Wave 2: ConfigMap 이후 실행\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\n  annotations:\n    argocd.argoproj.io/sync-wave: \"2\"\n```\n\n```mermaid\ngantt\n    title Sync Waves 실행 순서\n    dateFormat X\n    axisFormat %s\n    \n    section Wave 0\n    Namespace    :0, 1\n    \n    section Wave 1\n    ConfigMap    :1, 2\n    Secret       :1, 2\n    \n    section Wave 2\n    Deployment   :2, 3\n    Service      :2, 3\n```\n\n### Sync Hooks\n\n특정 시점에 Job을 실행합니다:\n\n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migration\n  annotations:\n    argocd.argoproj.io/hook: PreSync           # 언제 실행?\n    argocd.argoproj.io/hook-delete-policy: HookSucceeded  # 언제 삭제?\nspec:\n  template:\n    spec:\n      containers:\n      - name: migrate\n        image: my-app:latest\n        command: [\"./migrate.sh\"]\n      restartPolicy: Never\n```\n\n**Hook Phases**:\n\n| Phase | 시점 | 사용 사례 |\n|-------|-----|----------|\n| `PreSync` | Sync 이전 | DB 마이그레이션, 백업 |\n| `Sync` | 일반 리소스와 함께 | 특수 처리 필요 리소스 |\n| `PostSync` | Sync 완료 후 | 알림 전송, 테스트 실행 |\n| `SyncFail` | Sync 실패 시 | 롤백, 알림 |\n\n**Delete Policy**:\n\n| 정책 | 설명 |\n|-----|------|\n| `HookSucceeded` | 성공 시 삭제 |\n| `HookFailed` | 실패 시 삭제 |\n| `BeforeHookCreation` | 다음 Hook 생성 전 삭제 |\n\n### 실전 예시: Blue-Green 배포\n\n```yaml\n# PreSync: 새 버전 배포 전 준비\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pre-deploy-check\n  annotations:\n    argocd.argoproj.io/hook: PreSync\n    argocd.argoproj.io/sync-wave: \"-1\"\nspec:\n  template:\n    spec:\n      containers:\n      - name: check\n        image: curlimages/curl\n        command: [\"curl\", \"-f\", \"http://health-check-endpoint\"]\n      restartPolicy: Never\n\n---\n# Sync: 새 버전 Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app-green\n  annotations:\n    argocd.argoproj.io/sync-wave: \"0\"\nspec:\n  replicas: 3\n  # ...\n\n---\n# PostSync: 트래픽 전환\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: switch-traffic\n  annotations:\n    argocd.argoproj.io/hook: PostSync\n    argocd.argoproj.io/sync-wave: \"1\"\nspec:\n  template:\n    spec:\n      containers:\n      - name: switch\n        image: my-tool:latest\n        command: [\"./switch-service.sh\", \"my-app-green\"]\n      restartPolicy: Never\n```\n\n---\n\n## ApplicationSet\n\n**ApplicationSet**은 단일 템플릿에서 여러 Application을 자동 생성합니다. 멀티 클러스터, 멀티 테넌트 환경에서 필수입니다.\n\n### Generator 종류\n\n```mermaid\nflowchart TB\n    subgraph Generators [ApplicationSet Generators]\n        List[List Generator]\n        Cluster[Cluster Generator]\n        Git[Git Generator]\n        Matrix[Matrix Generator]\n        Merge[Merge Generator]\n    end\n    \n    List --> |정적 목록| Apps1[Applications]\n    Cluster --> |등록된 클러스터| Apps2[Applications]\n    Git --> |디렉토리/파일 구조| Apps3[Applications]\n    Matrix --> |Generator 조합| Apps4[Applications]\n```\n\n### List Generator\n\n정적 목록에서 Application 생성:\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: my-apps\n  namespace: argocd\nspec:\n  goTemplate: true\n  generators:\n  - list:\n      elements:\n      - cluster: dev\n        url: https://dev.k8s.example.com\n        namespace: dev\n      - cluster: staging\n        url: https://staging.k8s.example.com\n        namespace: staging\n      - cluster: prod\n        url: https://prod.k8s.example.com\n        namespace: prod\n  template:\n    metadata:\n      name: 'my-app-{{.cluster}}'\n    spec:\n      project: default\n      source:\n        repoURL: https://github.com/myorg/myrepo.git\n        targetRevision: HEAD\n        path: 'k8s/overlays/{{.cluster}}'\n      destination:\n        server: '{{.url}}'\n        namespace: '{{.namespace}}'\n```\n\n### Cluster Generator\n\nArgoCD에 등록된 클러스터에서 자동 생성:\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: cluster-addons\n  namespace: argocd\nspec:\n  goTemplate: true\n  generators:\n  - clusters:\n      selector:\n        matchLabels:\n          environment: production\n  template:\n    metadata:\n      name: '{{.name}}-monitoring'\n    spec:\n      project: default\n      source:\n        repoURL: https://github.com/myorg/cluster-addons.git\n        path: monitoring\n      destination:\n        server: '{{.server}}'\n        namespace: monitoring\n      syncPolicy:\n        automated:\n          prune: true\n          selfHeal: true\n```\n\n### Git Directory Generator\n\nGit 디렉토리 구조에서 자동 생성:\n\n```\napps/\n├── frontend/\n│   └── kustomization.yaml\n├── backend/\n│   └── kustomization.yaml\n└── database/\n    └── kustomization.yaml\n```\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: apps\n  namespace: argocd\nspec:\n  goTemplate: true\n  generators:\n  - git:\n      repoURL: https://github.com/myorg/gitops.git\n      revision: HEAD\n      directories:\n      - path: apps/*\n  template:\n    metadata:\n      name: '{{.path.basename}}'\n    spec:\n      project: default\n      source:\n        repoURL: https://github.com/myorg/gitops.git\n        path: '{{.path.path}}'\n      destination:\n        server: https://kubernetes.default.svc\n        namespace: '{{.path.basename}}'\n```\n\n> [!TIP]\n> Git Directory Generator를 사용하면 디렉토리만 추가하면 **자동으로 Application이 생성**됩니다. 코드 변경 없이 새 앱을 추가할 수 있습니다.\n\n---\n\n## 프로젝트(Project)와 RBAC\n\n### AppProject\n\nAppProject는 Application을 그룹화하고 접근 제어를 적용합니다:\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: AppProject\nmetadata:\n  name: production\n  namespace: argocd\nspec:\n  description: Production applications\n  \n  # 허용된 소스 저장소\n  sourceRepos:\n  - https://github.com/myorg/*\n  \n  # 허용된 목적지\n  destinations:\n  - namespace: 'prod-*'\n    server: https://prod.k8s.example.com\n  \n  # 허용된 클러스터 리소스\n  clusterResourceWhitelist:\n  - group: ''\n    kind: Namespace\n  \n  # 거부된 네임스페이스 리소스  \n  namespaceResourceBlacklist:\n  - group: ''\n    kind: ResourceQuota\n  \n  # RBAC 역할\n  roles:\n  - name: developer\n    description: Developer access\n    policies:\n    - p, proj:production:developer, applications, get, production/*, allow\n    - p, proj:production:developer, applications, sync, production/*, allow\n    groups:\n    - developers\n```\n\n### RBAC 정책\n\n```csv\n# p, <role>, <resource>, <action>, <object>, <effect>\n\n# 개발자: 읽기 + sync만 가능\np, role:developer, applications, get, */*, allow\np, role:developer, applications, sync, */*, allow\n\n# 운영자: 모든 권한\np, role:operator, applications, *, */*, allow\np, role:operator, clusters, *, *, allow\n\n# 특정 프로젝트만 접근\np, role:team-a, applications, *, team-a/*, allow\n```\n\n---\n\n## 트러블슈팅\n\n### OutOfSync 상태가 해결되지 않음\n\n```bash\n# 상세 diff 확인\nargocd app diff my-app --local ./manifests\n\n# 특정 리소스 무시 설정\n```\n\n```yaml\nspec:\n  ignoreDifferences:\n  - group: apps\n    kind: Deployment\n    jsonPointers:\n    - /spec/replicas  # HPA가 관리하는 필드 무시\n  - group: \"\"\n    kind: Service\n    jqPathExpressions:\n    - .spec.clusterIP  # 자동 할당 필드 무시\n```\n\n### Sync 실패\n\n```bash\n# 동기화 상태 확인\nargocd app get my-app\n\n# 이벤트 확인\nkubectl describe application my-app -n argocd\n\n# 강제 재동기화\nargocd app sync my-app --force\n```\n\n### Application 삭제 시 리소스가 남음\n\n```yaml\n# Finalizer 확인\nmetadata:\n  finalizers:\n  - resources-finalizer.argocd.argoproj.io  # 이게 있어야 함\n```\n\n```bash\n# 수동으로 리소스 삭제 후 Application 삭제\nargocd app delete my-app --cascade=false\nkubectl delete all -l app.kubernetes.io/instance=my-app\n```\n\n---\n\n## 정리\n\n| 개념 | 설명 |\n|-----|------|\n| **API Server** | Web UI, CLI, Webhook 처리 |\n| **Repo Server** | Git clone, 매니페스트 렌더링 |\n| **Application Controller** | Reconciliation Loop 실행 |\n| **Application CRD** | 배포 단위 정의 (source, destination, syncPolicy) |\n| **Sync Waves** | 리소스 적용 순서 제어 |\n| **Hooks** | PreSync, PostSync 등 특정 시점 Job 실행 |\n| **ApplicationSet** | 템플릿 기반 다중 Application 생성 |\n| **AppProject** | Application 그룹화 및 RBAC |\n\n---\n\n## 다음 편 예고\n\n**3편: Flux CD & GitOps Toolkit**에서는 다음을 다룹니다:\n\n- Flux의 마이크로서비스 아키텍처\n- Source Controller, Kustomize Controller, Helm Controller\n- GitRepository, Kustomization, HelmRelease CRD\n- ArgoCD vs Flux 상세 비교\n\n---\n\n## 참고 자료\n\n- [ArgoCD Official Documentation](https://argo-cd.readthedocs.io/)\n- [ArgoCD Architecture](https://argo-cd.readthedocs.io/en/stable/operator-manual/architecture/)\n- [ApplicationSet Controller](https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/)\n- [Sync Waves and Hooks](https://argo-cd.readthedocs.io/en/stable/user-guide/sync-waves/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitOps",
      "Kubernetes",
      "ArgoCD"
    ],
    "readingTime": 11,
    "wordCount": 2179,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitops",
    "date": "2026-01-05"
  },
  {
    "id": "gitops-01-introduction",
    "slug": "gitops-01-introduction",
    "path": "backend/gitops",
    "fullPath": "backend/gitops/gitops-01-introduction",
    "title": "GitOps 심화 시리즈 #1: GitOps란 무엇인가 - 철학과 원칙",
    "excerpt": "GitOps의 4가지 핵심 원칙, Push vs Pull 배포 모델의 차이, 그리고 Kubernetes Controller 패턴과의 연결고리를 깊이 있게 이해합니다.",
    "content": "# GitOps 심화 시리즈 #1: GitOps란 무엇인가 - 철학과 원칙\n\n## 시리즈 개요\n\n| # | 주제 | 핵심 내용 |\n|---|------|----------|\n| **1** | **GitOps 개요** | 철학과 원칙, Push vs Pull 배포, Reconciliation |\n| 2 | ArgoCD Deep Dive | 아키텍처, Application CRD, Sync 전략 |\n| 3 | Flux CD & GitOps Toolkit | 컨트롤러 아키텍처, GitRepository, Kustomization |\n| 4 | 환경별 설정 관리 | Kustomize vs Helm, 전략 선택 기준 |\n| 5 | Secrets Management | Sealed Secrets, External Secrets, SOPS |\n| 6 | CI/CD 파이프라인 통합 | Image Updater, Progressive Delivery |\n\n---\n\n## GitOps 이전의 세계\n\n전통적인 CI/CD 파이프라인을 떠올려봅시다. Jenkins나 GitHub Actions가 코드를 빌드하고, 테스트를 통과하면 `kubectl apply`나 `helm upgrade` 명령으로 클러스터에 직접 배포합니다.\n\n```mermaid\nsequenceDiagram\n    participant Dev as 개발자\n    participant Git as Git Repository\n    participant CI as CI Server\n    participant K8s as Kubernetes\n\n    Dev->>Git: git push\n    Git->>CI: Webhook 트리거\n    CI->>CI: Build & Test\n    CI->>K8s: kubectl apply / helm upgrade\n    Note over K8s: 배포 완료\n```\n\n이 방식은 동작하지만, 몇 가지 근본적인 문제가 있습니다:\n\n| 문제 | 설명 |\n|-----|------|\n| **Credential 분산** | CI 서버가 프로덕션 클러스터에 접근하는 강력한 권한 보유 |\n| **Drift 감지 불가** | 누군가 `kubectl edit`으로 직접 수정하면 Git과 실제 상태 불일치 |\n| **Audit Trail 부재** | 누가, 언제, 왜 변경했는지 추적 어려움 |\n| **롤백의 복잡성** | 이전 상태로 돌아가려면 \"어떤 버전이 배포되어 있었는지\" 찾아야 함 |\n\n> [!WARNING]\n> **실제 사고 사례**: CI 서버가 해킹당하면 공격자가 프로덕션 클러스터에 임의 코드를 배포할 수 있습니다. 2021년 Codecov 사태에서 CI 파이프라인이 공격 벡터가 되어 수천 개 기업의 credential이 유출되었습니다.\n\n---\n\n## GitOps의 핵심 정의\n\n**GitOps**는 Git을 **Single Source of Truth (SSOT)**로 삼아, 원하는 시스템 상태를 선언적으로 정의하고, 자동화된 프로세스가 실제 상태를 Git에 정의된 상태와 지속적으로 일치시키는 운영 방식입니다.\n\n2021년 CNCF 산하 [OpenGitOps 프로젝트](https://opengitops.dev/)에서 공식화한 **4가지 원칙**을 살펴봅시다:\n\n### 1. Declarative (선언적)\n\n시스템의 **원하는 상태(Desired State)**를 선언적으로 기술합니다.\n\n```yaml\n# \"3개의 nginx Pod가 실행되어야 한다\"는 선언\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.25\n```\n\n- **명령형(Imperative)** 접근: \"nginx Pod를 3개 만들어라\" (`kubectl scale`)\n- **선언형(Declarative)** 접근: \"nginx Pod가 3개인 상태가 되어야 한다\" (YAML manifest)\n\n> [!TIP]\n> Kubernetes 자체가 선언적 시스템입니다. GitOps는 이 철학을 Git과 결합한 것입니다.\n\n### 2. Versioned and Immutable (버전 관리 및 불변)\n\nGit은 본질적으로 모든 변경사항을 버전 관리합니다. 각 커밋은 불변(Immutable)하며, 전체 히스토리가 보존됩니다.\n\n```bash\n# 모든 변경 이력 추적 가능\ngit log --oneline\n# a1b2c3d (HEAD) feat: scale nginx to 5 replicas\n# d4e5f6g fix: update nginx image to 1.25.3\n# g7h8i9j initial: deploy nginx with 3 replicas\n\n# 특정 시점으로 롤백\ngit revert a1b2c3d\n```\n\n이것이 왜 중요한가?\n\n- **Audit Trail**: 누가, 언제, 왜 변경했는지 모든 기록이 남음\n- **Rollback**: `git revert`로 이전 상태로 즉시 복구 가능\n- **Compliance**: 금융, 헬스케어 등 규제 산업의 감사 요구사항 충족\n\n### 3. Pulled Automatically (자동으로 Pull)\n\n원하는 상태의 변경사항이 Git에 푸시되면, **에이전트가 자동으로 감지하고 적용**합니다.\n\n```mermaid\nflowchart LR\n    subgraph GitRepo [Git Repository]\n        Manifest[Kubernetes Manifests]\n    end\n    \n    subgraph Cluster [Kubernetes Cluster]\n        Agent[GitOps Agent\\nArgoCD / Flux]\n        Workload[Running Workloads]\n    end\n    \n    Agent -->|1. Pull & Watch| GitRepo\n    Agent -->|2. Compare| Workload\n    Agent -->|3. Apply changes| Workload\n    \n    style Agent fill:#4CAF50,color:#fff\n```\n\n여기서 핵심은 **Pull 모델**입니다. CI 서버가 클러스터에 접근하는 것이 아니라, 클러스터 내부의 에이전트가 Git을 주기적으로 확인합니다.\n\n### 4. Continuously Reconciled (지속적 조정)\n\n에이전트는 단순히 변경을 한 번 적용하고 끝나는 것이 아닙니다. **Reconciliation Loop**를 통해 실제 상태와 원하는 상태를 지속적으로 비교하고, 차이가 발생하면 자동으로 수정합니다.\n\n```mermaid\nflowchart TB\n    subgraph ReconciliationLoop [Reconciliation Loop]\n        direction TB\n        Observe[1. 현재 상태 관찰\\nCurrent State]\n        Compare[2. 원하는 상태와 비교\\nDesired State in Git]\n        Drift{Drift\\n발견?}\n        Apply[3. 차이 조정\\nReconcile]\n        Wait[4. 대기\\nInterval]\n    end\n    \n    Observe --> Compare --> Drift\n    Drift -->|Yes| Apply --> Wait\n    Drift -->|No| Wait\n    Wait --> Observe\n```\n\n> [!IMPORTANT]\n> **Drift Detection**: 누군가 `kubectl edit`으로 replicas를 5개로 변경하면? GitOps 에이전트가 감지하고 Git에 정의된 3개로 자동 복구합니다. 이것이 **Self-Healing**입니다.\n\n---\n\n## Push vs Pull 배포 모델\n\nGitOps를 이해하는 핵심은 **Push 모델과 Pull 모델의 차이**입니다.\n\n### Push 모델 (전통적 CI/CD)\n\n```mermaid\nflowchart LR\n    subgraph External [클러스터 외부]\n        CI[CI Server]\n    end\n    \n    subgraph Cluster [Kubernetes Cluster]\n        API[API Server]\n        Pod[Pods]\n    end\n    \n    CI -->|1. kubectl apply\\n강력한 권한 필요| API\n    API --> Pod\n    \n    style CI fill:#f44336,color:#fff\n```\n\n**특징**:\n\n- CI 서버가 클러스터에 **직접 접근**\n- CI 서버에 admin 수준의 kubeconfig 필요\n- CI 서버가 침해되면 클러스터도 위험\n\n### Pull 모델 (GitOps)\n\n```mermaid\nflowchart LR\n    subgraph External [클러스터 외부]\n        Git[Git Repository]\n    end\n    \n    subgraph Cluster [Kubernetes Cluster]\n        Agent[GitOps Agent]\n        API[API Server]\n        Pod[Pods]\n    end\n    \n    Agent -->|1. Git clone/pull\\n읽기 권한만| Git\n    Agent -->|2. kubectl apply\\n내부 통신| API\n    API --> Pod\n    \n    style Agent fill:#4CAF50,color:#fff\n```\n\n**특징**:\n\n- 클러스터 내부의 에이전트가 Git을 **Pull**\n- Git에 대한 읽기 전용 권한만 필요\n- 외부에서 클러스터로의 인바운드 연결 없음\n\n### 보안 관점에서의 비교\n\n| 관점 | Push 모델 | Pull 모델 |\n|-----|----------|----------|\n| **네트워크 방향** | 외부 → 클러스터 | 클러스터 → 외부 (Git) |\n| **필요 권한** | CI에 cluster-admin | Agent에 내부 권한 |\n| **공격 표면** | CI 서버 침해 시 클러스터 위험 | Git 저장소 보호에 집중 |\n| **방화벽** | 인바운드 허용 필요 | 아웃바운드만 허용 |\n\n> [!NOTE]\n> 실제로 ArgoCD나 Flux도 클러스터 내에서 강력한 권한을 가집니다. 하지만 **공격 벡터가 Git 저장소로 단일화**되어 보안 관리가 단순해집니다.\n\n---\n\n## Kubernetes Controller Pattern과의 연결\n\nGitOps가 Kubernetes에서 특히 잘 작동하는 이유는 **Controller Pattern** 때문입니다. 이미 Kubernetes 시리즈에서 다뤘듯이, 모든 Kubernetes 컨트롤러는 Reconciliation Loop로 동작합니다.\n\n```go\n// Kubernetes Controller의 핵심 로직\nfunc (c *Controller) Reconcile(ctx context.Context, req Request) (Result, error) {\n    // 1. 현재 상태 조회\n    current, err := c.Get(ctx, req.NamespacedName)\n    if err != nil {\n        return Result{}, err\n    }\n    \n    // 2. 원하는 상태와 비교\n    desired := c.calculateDesiredState(current)\n    \n    // 3. 차이가 있으면 조정\n    if !reflect.DeepEqual(current.Spec, desired) {\n        current.Spec = desired\n        return Result{}, c.Update(ctx, current)\n    }\n    \n    // 4. Requeue for next reconciliation\n    return Result{RequeueAfter: 30 * time.Second}, nil\n}\n```\n\nGitOps 에이전트(ArgoCD, Flux)는 이 패턴의 **상위 레벨 구현**입니다:\n\n| 개념 | Kubernetes Controller | GitOps Agent |\n|-----|----------------------|--------------|\n| Desired State | Spec in YAML | Git Repository |\n| Current State | Status in API Server | Live Kubernetes Objects |\n| Reconciliation | Controller Manager | ArgoCD/Flux Controller |\n| Watch mechanism | Informer (etcd watch) | Git polling / Webhook |\n\n```mermaid\nflowchart TB\n    subgraph GitOpsLevel [GitOps Level]\n        Git[Git Repository\\nDesired State]\n        ArgoCD[ArgoCD Controller]\n    end\n    \n    subgraph K8sLevel [Kubernetes Level]\n        API[API Server\\nDesired State]\n        DC[Deployment Controller]\n        RSC[ReplicaSet Controller]\n        Kubelet[kubelet]\n    end\n    \n    subgraph NodeLevel [Node Level]\n        Pod[Running Pods\\nCurrent State]\n    end\n    \n    Git --> ArgoCD\n    ArgoCD --> API\n    API --> DC --> RSC --> Kubelet --> Pod\n    \n    ArgoCD -.->|Reconcile| Git\n    DC -.->|Reconcile| API\n    RSC -.->|Reconcile| API\n    Kubelet -.->|Reconcile| API\n```\n\n> [!TIP]\n> GitOps는 Kubernetes의 선언적 모델을 **Git까지 확장**한 것입니다. \"Infrastructure as Code\"의 자연스러운 진화입니다.\n\n---\n\n## GitOps의 이점\n\n### 1. 완전한 감사 추적 (Audit Trail)\n\n모든 변경은 Git 커밋으로 기록됩니다. `git log`, `git blame`으로 완벽한 히스토리 추적이 가능합니다.\n\n```bash\n# 누가 replicas를 변경했는가?\ngit log -p -- k8s/deployment.yaml | grep -A5 \"replicas:\"\n\n# 특정 시점의 클러스터 상태는?\ngit show abc123:k8s/deployment.yaml\n```\n\n### 2. 간편한 롤백\n\n문제가 발생하면 `git revert`로 즉시 이전 상태로 복구합니다.\n\n```bash\n# 마지막 변경 롤백\ngit revert HEAD\ngit push\n\n# GitOps 에이전트가 자동으로 이전 상태 적용\n```\n\n### 3. Drift Detection과 Self-Healing\n\n```mermaid\nsequenceDiagram\n    participant Dev as 운영자\n    participant K8s as Kubernetes\n    participant Agent as GitOps Agent\n    participant Git as Git Repository\n\n    Dev->>K8s: kubectl edit (수동 변경)\n    Note over K8s: Drift 발생!\n    \n    Agent->>K8s: 현재 상태 확인\n    Agent->>Git: Git 상태 확인\n    Agent->>Agent: 차이 감지\n    Agent->>K8s: Git 상태로 복구\n    Note over K8s: Self-Heal 완료\n```\n\n### 4. Pull Request 기반 변경 관리\n\n인프라 변경도 코드 리뷰와 동일한 워크플로우를 따릅니다.\n\n```mermaid\nflowchart LR\n    subgraph PR [Pull Request Workflow]\n        Create[PR 생성] --> Review[코드 리뷰]\n        Review --> Approve[승인]\n        Approve --> Merge[Merge to main]\n    end\n    \n    subgraph Deploy [자동 배포]\n        Merge --> Agent[GitOps Agent]\n        Agent --> Apply[클러스터 적용]\n    end\n```\n\n---\n\n## GitOps 도구: ArgoCD vs Flux\n\nGitOps를 구현하는 대표적인 두 도구가 있습니다. 다음 편에서 각각을 깊이 다루겠지만, 간략히 비교합니다.\n\n| 특성 | ArgoCD | Flux |\n|-----|--------|------|\n| **CNCF 단계** | Graduated | Graduated |\n| **UI** | 강력한 Web UI 제공 | CLI 중심 (Web UI는 별도 프로젝트) |\n| **아키텍처** | 단일 애플리케이션 | 마이크로서비스 (GitOps Toolkit) |\n| **멀티 클러스터** | ApplicationSet | Kustomization + 클러스터별 디렉토리 |\n| **이미지 자동 업데이트** | Image Updater (별도) | Image Automation Controller (내장) |\n| **주요 사용자** | Intuit, Red Hat | Weaveworks, AWS |\n\n```mermaid\nflowchart TB\n    subgraph ArgoCD [ArgoCD 아키텍처]\n        A_API[API Server]\n        A_Repo[Repo Server]\n        A_Controller[Application Controller]\n        A_API --> A_Repo\n        A_API --> A_Controller\n    end\n    \n    subgraph Flux [Flux 아키텍처]\n        F_Source[Source Controller]\n        F_Kustomize[Kustomize Controller]\n        F_Helm[Helm Controller]\n        F_Notification[Notification Controller]\n    end\n```\n\n> [!NOTE]\n> **선택 기준**: UI가 중요하고 중앙 집중식 관리를 원하면 ArgoCD, 경량화와 모듈화를 원하면 Flux가 적합합니다. 둘 다 성숙한 프로젝트입니다.\n\n---\n\n## 실무 도입 시 고려사항\n\n### 1. 레포지토리 구조\n\n**Monorepo vs Polyrepo**\n\n```\n# Monorepo 방식\ngitops-repo/\n├── apps/\n│   ├── frontend/\n│   ├── backend/\n│   └── database/\n├── infrastructure/\n│   ├── monitoring/\n│   └── ingress/\n└── clusters/\n    ├── dev/\n    ├── staging/\n    └── prod/\n```\n\n```\n# Polyrepo 방식\nfrontend-gitops/      # 프론트엔드 팀 관리\nbackend-gitops/       # 백엔드 팀 관리\nplatform-gitops/      # 플랫폼 팀 관리\n```\n\n### 2. 환경별 설정 관리\n\nKustomize나 Helm으로 환경별 설정을 분리합니다:\n\n```\nbase/\n├── deployment.yaml\n├── service.yaml\n└── kustomization.yaml\n\noverlays/\n├── dev/\n│   ├── kustomization.yaml\n│   └── replica-patch.yaml\n├── staging/\n│   └── kustomization.yaml\n└── prod/\n    ├── kustomization.yaml\n    └── resource-patch.yaml\n```\n\n### 3. Secrets 처리\n\nGit에 평문 Secret을 저장하면 안 됩니다. 다음 중 하나를 선택합니다:\n\n- **Sealed Secrets**: 암호화된 형태로 Git에 저장\n- **External Secrets Operator**: AWS Secrets Manager 등 외부 참조\n- **SOPS**: 파일 레벨 암호화\n\n(5편에서 자세히 다룹니다)\n\n---\n\n## 정리\n\n| 개념 | 설명 |\n|-----|------|\n| **GitOps** | Git을 Single Source of Truth로 삼는 운영 방식 |\n| **4가지 원칙** | Declarative, Versioned, Pulled, Reconciled |\n| **Pull 모델** | 클러스터 내부 에이전트가 Git을 감시 |\n| **Reconciliation** | 지속적으로 현재 상태를 원하는 상태로 조정 |\n| **Self-Healing** | 수동 변경(Drift)을 자동으로 Git 상태로 복구 |\n\n---\n\n## 다음 편 예고\n\n**2편: ArgoCD Deep Dive**에서는 다음을 다룹니다:\n\n- ArgoCD 아키텍처 상세 (API Server, Repo Server, Controller)\n- Application CRD 완전 분석\n- Sync 전략: Manual vs Automated, Self-Heal, Prune\n- Sync Waves와 Hooks\n- ApplicationSet으로 멀티 클러스터 관리\n\n---\n\n## 참고 자료\n\n- [OpenGitOps Principles](https://opengitops.dev/principles)\n- [ArgoCD - Declarative GitOps CD for Kubernetes](https://argo-cd.readthedocs.io/)\n- [Flux - The GitOps family of projects](https://fluxcd.io/)\n- [CNCF GitOps Working Group](https://github.com/cncf/tag-app-delivery/tree/main/gitops-wg)\n- [Guide to GitOps - Weaveworks](https://www.weave.works/technologies/gitops/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitOps",
      "Kubernetes",
      "ArgoCD",
      "Flux"
    ],
    "readingTime": 9,
    "wordCount": 1718,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitops",
    "date": "2026-01-05"
  },
  {
    "id": "gitlab-ci-06-external-integrations",
    "slug": "gitlab-ci-06-external-integrations",
    "path": "backend/devops",
    "fullPath": "backend/devops/gitlab-ci-06-external-integrations",
    "title": "GitLab CI/CD 시리즈 #6: 외부 통합 - Triggers, Webhooks, API",
    "excerpt": "Pipeline Triggers, Webhooks, API를 통한 외부 시스템 연동, ChatOps, 그리고 GitOps 도구와의 통합을 다룹니다.",
    "content": "# GitLab CI/CD 시리즈 #6: 외부 통합 - Triggers, Webhooks, API\n\n## 시리즈 개요\n\n| # | 주제 | 핵심 내용 |\n|---|------|----------|\n| 1 | 기초 | .gitlab-ci.yml 구조, Stages, Jobs, Pipeline 흐름 |\n| 2 | Variables & Secrets | 변수 유형, 우선순위, 외부 Vault 연동 |\n| 3 | Runners & Executors | Docker, Kubernetes, Docker-in-Docker |\n| 4 | Pipeline 아키텍처 | Parent-Child, Multi-Project Pipeline |\n| 5 | 고급 Job 제어 | rules, needs, DAG, extends |\n| **6** | **외부 통합** | Triggers, Webhooks, API |\n\n---\n\n## Pipeline Triggers\n\n외부 시스템에서 **토큰 기반으로 파이프라인을 트리거**합니다.\n\n### Trigger Token 생성\n\n1. `Settings > CI/CD > Pipeline trigger tokens`\n2. `Add trigger` 클릭\n3. 설명 입력 후 생성\n4. 생성된 토큰 복사\n\n### 트리거 실행\n\n```bash\n# 기본 트리거\ncurl --request POST \\\n  --form \"token=YOUR_TRIGGER_TOKEN\" \\\n  --form \"ref=main\" \\\n  \"https://gitlab.com/api/v4/projects/PROJECT_ID/trigger/pipeline\"\n\n# 변수와 함께\ncurl --request POST \\\n  --form \"token=YOUR_TRIGGER_TOKEN\" \\\n  --form \"ref=main\" \\\n  --form \"variables[DEPLOY_ENV]=production\" \\\n  --form \"variables[VERSION]=1.2.3\" \\\n  \"https://gitlab.com/api/v4/projects/PROJECT_ID/trigger/pipeline\"\n```\n\n### 파이프라인에서 트리거 감지\n\n```yaml\ndeploy:\n  script:\n    - ./deploy.sh\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"trigger\"\n      variables:\n        DEPLOY_TOKEN: $DEPLOY_TOKEN  # 트리거로 전달된 변수\n```\n\n### Trigger 전용 Job\n\n```yaml\ntriggered-deploy:\n  script:\n    - echo \"Deploying version $VERSION to $DEPLOY_ENV\"\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"trigger\"\n  needs: []  # 다른 Job 대기 없이 즉시 실행\n```\n\n---\n\n## Webhooks\n\n**외부 이벤트를 수신**하여 파이프라인을 트리거합니다.\n\n### Webhook URL 형식\n\n```\nhttps://gitlab.com/api/v4/projects/PROJECT_ID/ref/REF_NAME/trigger/pipeline?token=TOKEN\n```\n\n### 실제 Webhook 설정\n\n```bash\n# GitHub → GitLab 트리거\n# GitHub 저장소의 Webhooks에 등록\nhttps://gitlab.com/api/v4/projects/12345/ref/main/trigger/pipeline?token=abc123\n\n# AWS SNS → GitLab 트리거\n# Lambda를 통해 변환 후 트리거\n```\n\n### Webhook Payload 접근\n\n```yaml\nprocess-webhook:\n  script:\n    - echo \"$TRIGGER_PAYLOAD\" | jq .\n    - export EVENT_TYPE=$(echo \"$TRIGGER_PAYLOAD\" | jq -r '.event_type')\n    - |\n      if [ \"$EVENT_TYPE\" = \"release\" ]; then\n        ./deploy-release.sh\n      fi\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"trigger\"\n```\n\n---\n\n## GitLab API로 파이프라인 제어\n\n### 파이프라인 생성\n\n```bash\n# Personal Access Token 사용\ncurl --request POST \\\n  --header \"PRIVATE-TOKEN: $GITLAB_TOKEN\" \\\n  --header \"Content-Type: application/json\" \\\n  --data '{\n    \"ref\": \"main\",\n    \"variables\": [\n      {\"key\": \"DEPLOY_ENV\", \"value\": \"staging\"},\n      {\"key\": \"DEBUG\", \"value\": \"true\"}\n    ]\n  }' \\\n  \"https://gitlab.com/api/v4/projects/PROJECT_ID/pipeline\"\n```\n\n### 파이프라인 상태 조회\n\n```bash\n# 특정 파이프라인\ncurl --header \"PRIVATE-TOKEN: $GITLAB_TOKEN\" \\\n  \"https://gitlab.com/api/v4/projects/PROJECT_ID/pipelines/PIPELINE_ID\"\n\n# 최근 파이프라인\ncurl --header \"PRIVATE-TOKEN: $GITLAB_TOKEN\" \\\n  \"https://gitlab.com/api/v4/projects/PROJECT_ID/pipelines?per_page=5\"\n```\n\n### Job 재시도\n\n```bash\ncurl --request POST \\\n  --header \"PRIVATE-TOKEN: $GITLAB_TOKEN\" \\\n  \"https://gitlab.com/api/v4/projects/PROJECT_ID/jobs/JOB_ID/retry\"\n```\n\n### 파이프라인 취소\n\n```bash\ncurl --request POST \\\n  --header \"PRIVATE-TOKEN: $GITLAB_TOKEN\" \\\n  \"https://gitlab.com/api/v4/projects/PROJECT_ID/pipelines/PIPELINE_ID/cancel\"\n```\n\n---\n\n## CI Job Token\n\n파이프라인 내에서 **GitLab API를 호출**할 때 사용하는 임시 토큰입니다.\n\n### CI_JOB_TOKEN 사용\n\n```yaml\nnotify-other-project:\n  script:\n    # 다른 프로젝트 파이프라인 트리거\n    - |\n      curl --request POST \\\n        --form \"token=$CI_JOB_TOKEN\" \\\n        --form \"ref=main\" \\\n        \"https://gitlab.com/api/v4/projects/OTHER_PROJECT_ID/trigger/pipeline\"\n```\n\n### 아티팩트 다운로드\n\n```yaml\ndownload-artifacts:\n  script:\n    - |\n      curl --header \"JOB-TOKEN: $CI_JOB_TOKEN\" \\\n        --output artifacts.zip \\\n        \"https://gitlab.com/api/v4/projects/PROJECT_ID/jobs/JOB_ID/artifacts\"\n    - unzip artifacts.zip\n```\n\n### 권한 설정\n\n`Settings > CI/CD > Token Access`에서 허용할 프로젝트를 설정합니다.\n\n```mermaid\nflowchart LR\n    subgraph ProjectA [Project A]\n        PA_Job[Job]\n    end\n    \n    subgraph ProjectB [Project B]\n        PB_API[GitLab API]\n        PB_Token[Token Access 설정]\n    end\n    \n    PA_Job -->|CI_JOB_TOKEN| PB_API\n    PB_Token -->|허용| PA_Job\n```\n\n---\n\n## ChatOps 연동\n\n### Slack 알림\n\n```yaml\nnotify-slack:\n  stage: .post\n  script:\n    - |\n      curl -X POST -H 'Content-type: application/json' \\\n        --data '{\n          \"channel\": \"#deployments\",\n          \"username\": \"GitLab CI\",\n          \"text\": \"✅ Pipeline succeeded for $CI_PROJECT_NAME\",\n          \"attachments\": [{\n            \"color\": \"good\",\n            \"fields\": [\n              {\"title\": \"Branch\", \"value\": \"'$CI_COMMIT_BRANCH'\", \"short\": true},\n              {\"title\": \"Commit\", \"value\": \"'$CI_COMMIT_SHORT_SHA'\", \"short\": true}\n            ]\n          }]\n        }' \\\n        $SLACK_WEBHOOK_URL\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      when: on_success\n```\n\n### 배포 승인 (Slack → GitLab)\n\n```yaml\n# Slack 버튼 클릭 → Lambda → GitLab API\nrequest-approval:\n  stage: deploy\n  script:\n    - |\n      curl -X POST -H 'Content-type: application/json' \\\n        --data '{\n          \"text\": \"🚀 Production deployment pending\",\n          \"attachments\": [{\n            \"text\": \"Approve deployment?\",\n            \"callback_id\": \"deploy_'$CI_PIPELINE_ID'\",\n            \"actions\": [\n              {\"name\": \"approve\", \"text\": \"Approve\", \"type\": \"button\", \"style\": \"primary\"},\n              {\"name\": \"reject\", \"text\": \"Reject\", \"type\": \"button\", \"style\": \"danger\"}\n            ]\n          }]\n        }' \\\n        $SLACK_WEBHOOK_URL\n  environment:\n    name: production\n    action: prepare\n```\n\n---\n\n## 외부 CI/CD 시스템 연동\n\n### Jenkins → GitLab\n\n```groovy\n// Jenkinsfile\npipeline {\n    agent any\n    stages {\n        stage('Build') {\n            steps {\n                sh 'make build'\n            }\n        }\n        stage('Trigger GitLab') {\n            steps {\n                sh '''\n                    curl --request POST \\\n                      --form \"token=${GITLAB_TRIGGER_TOKEN}\" \\\n                      --form \"ref=main\" \\\n                      --form \"variables[JENKINS_BUILD]=${BUILD_NUMBER}\" \\\n                      \"https://gitlab.com/api/v4/projects/${GITLAB_PROJECT_ID}/trigger/pipeline\"\n                '''\n            }\n        }\n    }\n}\n```\n\n### GitHub Actions → GitLab\n\n```yaml\n# .github/workflows/trigger-gitlab.yml\nname: Trigger GitLab Pipeline\n\non:\n  push:\n    branches: [main]\n\njobs:\n  trigger:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Trigger GitLab\n        run: |\n          curl --request POST \\\n            --form \"token=${{ secrets.GITLAB_TRIGGER_TOKEN }}\" \\\n            --form \"ref=main\" \\\n            --form \"variables[GITHUB_SHA]=${{ github.sha }}\" \\\n            \"https://gitlab.com/api/v4/projects/${{ secrets.GITLAB_PROJECT_ID }}/trigger/pipeline\"\n```\n\n---\n\n## GitOps 도구 연동\n\n### GitLab CI → ArgoCD\n\n```yaml\ndeploy-argocd:\n  stage: deploy\n  image: argoproj/argocd:latest\n  script:\n    # ArgoCD 로그인\n    - argocd login $ARGOCD_SERVER --username admin --password $ARGOCD_PASSWORD --insecure\n    \n    # 이미지 태그 업데이트\n    - argocd app set $APP_NAME --helm-set image.tag=$CI_COMMIT_SHA\n    \n    # Sync 트리거\n    - argocd app sync $APP_NAME --prune\n    \n    # 배포 완료 대기\n    - argocd app wait $APP_NAME --timeout 300\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n```\n\n### Image Updater 패턴\n\n```yaml\n# CI는 이미지만 빌드 & 푸시\n# ArgoCD Image Updater가 자동으로 감지하여 배포\n\nbuild-and-push:\n  stage: build\n  script:\n    - docker build -t $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA .\n    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n    \n    # SemVer 태그도 푸시 (Image Updater가 감지)\n    - docker tag $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA $CI_REGISTRY_IMAGE:v$VERSION\n    - docker push $CI_REGISTRY_IMAGE:v$VERSION\n```\n\n```mermaid\nflowchart LR\n    subgraph CI [GitLab CI]\n        Build[Build & Push Image]\n    end\n    \n    subgraph Registry [Container Registry]\n        Images[(Images)]\n    end\n    \n    subgraph GitOps [GitOps]\n        Updater[ArgoCD Image Updater]\n        ArgoCD[ArgoCD]\n        K8s[Kubernetes]\n    end\n    \n    Build --> Images\n    Updater -->|Scan| Images\n    Updater -->|Update| ArgoCD\n    ArgoCD -->|Sync| K8s\n```\n\n> [!TIP]\n> CI/CD 분리 원칙: **CI는 아티팩트 생성**, **CD는 GitOps Agent**가 담당합니다. GitOps 시리즈 6편에서 자세히 다뤘습니다.\n\n---\n\n## Scheduled Pipelines (cron)\n\n정기적으로 파이프라인을 실행합니다.\n\n### 설정\n\n`Build > Pipeline schedules > New schedule`\n\n```yaml\nnightly-test:\n  script:\n    - npm run test:full\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"schedule\"\n      variables:\n        FULL_TEST: \"true\"\n\ndaily-backup:\n  script:\n    - ./backup.sh\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"schedule\" && $SCHEDULE_TYPE == \"backup\"\n```\n\n### Schedule 변수\n\n```yaml\n# Schedule 설정에서 SCHEDULE_TYPE=security 지정\n\nsecurity-scan:\n  script:\n    - trivy image $CI_REGISTRY_IMAGE:latest\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"schedule\" && $SCHEDULE_TYPE == \"security\"\n```\n\n---\n\n## 실전 예제: 완전한 외부 연동\n\n```yaml\nstages:\n  - build\n  - deploy\n  - notify\n\nvariables:\n  DOCKER_IMAGE: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n\nbuild:\n  stage: build\n  script:\n    - docker build -t $DOCKER_IMAGE .\n    - docker push $DOCKER_IMAGE\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n\n# ArgoCD 배포 트리거\ntrigger-argocd:\n  stage: deploy\n  image: curlimages/curl:latest\n  script:\n    - |\n      # GitOps 레포에 이미지 태그 업데이트 PR 생성\n      curl --request POST \\\n        --header \"PRIVATE-TOKEN: $GITOPS_TOKEN\" \\\n        --header \"Content-Type: application/json\" \\\n        --data '{\n          \"branch\": \"update-'$CI_COMMIT_SHORT_SHA'\",\n          \"commit_message\": \"Update image to '$DOCKER_IMAGE'\",\n          \"actions\": [{\n            \"action\": \"update\",\n            \"file_path\": \"apps/myapp/values.yaml\",\n            \"content\": \"image:\\n  tag: '$CI_COMMIT_SHA'\"\n          }]\n        }' \\\n        \"https://gitlab.com/api/v4/projects/GITOPS_PROJECT_ID/repository/commits\"\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n\n# Slack 알림\nnotify-success:\n  stage: notify\n  script:\n    - |\n      curl -X POST -H 'Content-type: application/json' \\\n        --data '{\n          \"channel\": \"#deployments\",\n          \"text\": \"✅ '$CI_PROJECT_NAME' deployed successfully\",\n          \"attachments\": [{\n            \"color\": \"good\",\n            \"fields\": [\n              {\"title\": \"Version\", \"value\": \"'$CI_COMMIT_SHORT_SHA'\"},\n              {\"title\": \"Pipeline\", \"value\": \"'$CI_PIPELINE_URL'\"}\n            ]\n          }]\n        }' \\\n        $SLACK_WEBHOOK_URL\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      when: on_success\n\nnotify-failure:\n  stage: notify\n  script:\n    - |\n      curl -X POST -H 'Content-type: application/json' \\\n        --data '{\n          \"channel\": \"#deployments\",\n          \"text\": \"❌ '$CI_PROJECT_NAME' deployment failed!\",\n          \"attachments\": [{\n            \"color\": \"danger\",\n            \"fields\": [\n              {\"title\": \"Pipeline\", \"value\": \"'$CI_PIPELINE_URL'\"}\n            ]\n          }]\n        }' \\\n        $SLACK_WEBHOOK_URL\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      when: on_failure\n```\n\n---\n\n## 정리: 시리즈 완결\n\n6편의 시리즈를 통해 GitLab CI/CD의 핵심을 다뤘습니다.\n\n| 편 | 주제 | 핵심 메시지 |\n|---|------|-----------|\n| 1 | 기초 | .gitlab-ci.yml, Stages, Jobs, Pipeline |\n| 2 | Variables | Predefined, Protected, Masked, dotenv |\n| 3 | Runners | Executors, DinD, Kubernetes |\n| 4 | 아키텍처 | Parent-Child, Multi-Project, 동적 생성 |\n| 5 | Job 제어 | rules, needs, DAG, extends |\n| 6 | 외부 통합 | Triggers, Webhooks, API, GitOps |\n\n### 다음 단계\n\n- **GitOps 시리즈**: ArgoCD/Flux CD를 활용한 CD 자동화\n- **Kubernetes 시리즈**: 클러스터 운영 심화\n\n---\n\n## 참고 자료\n\n- [Pipeline Triggers](https://docs.gitlab.com/ee/ci/triggers/)\n- [CI/CD API](https://docs.gitlab.com/ee/api/pipelines.html)\n- [CI Job Token](https://docs.gitlab.com/ee/ci/jobs/ci_job_token.html)\n- [Scheduled Pipelines](https://docs.gitlab.com/ee/ci/pipelines/schedules.html)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitLab",
      "CI/CD",
      "Pipeline",
      "API",
      "Webhook"
    ],
    "readingTime": 7,
    "wordCount": 1302,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitlab-ci",
    "date": "2026-01-05"
  },
  {
    "id": "gitlab-ci-05-advanced-job-control",
    "slug": "gitlab-ci-05-advanced-job-control",
    "path": "backend/devops",
    "fullPath": "backend/devops/gitlab-ci-05-advanced-job-control",
    "title": "GitLab CI/CD 시리즈 #5: 고급 Job 제어 - rules, needs, DAG",
    "excerpt": "rules 조건 분기, needs를 활용한 DAG 실행, Job 템플릿과 extends, !reference 태그까지 고급 Job 제어 기법을 다룹니다.",
    "content": "# GitLab CI/CD 시리즈 #5: 고급 Job 제어 - rules, needs, DAG\n\n## 시리즈 개요\n\n| # | 주제 | 핵심 내용 |\n|---|------|----------|\n| 1 | 기초 | .gitlab-ci.yml 구조, Stages, Jobs, Pipeline 흐름 |\n| 2 | Variables & Secrets | 변수 유형, 우선순위, 외부 Vault 연동 |\n| 3 | Runners & Executors | Docker, Kubernetes, Docker-in-Docker |\n| 4 | Pipeline 아키텍처 | Parent-Child, Multi-Project Pipeline |\n| **5** | **고급 Job 제어** | rules, needs, DAG, extends |\n| 6 | 외부 통합 | Triggers, Webhooks, API |\n\n---\n\n## rules: 조건부 Job 실행\n\n`rules`는 `only/except`를 대체하는 **강력한 조건부 실행 키워드**입니다.\n\n### 기본 문법\n\n```yaml\njob:\n  script:\n    - echo \"Hello\"\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      when: always\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n      when: manual\n    - when: never  # 기본값\n```\n\n### rules 평가 순서\n\n```mermaid\nflowchart TB\n    Start[rules 평가 시작] --> R1{Rule 1 조건?}\n    R1 -->|매칭| Apply1[Rule 1 적용, 평가 종료]\n    R1 -->|불일치| R2{Rule 2 조건?}\n    R2 -->|매칭| Apply2[Rule 2 적용, 평가 종료]\n    R2 -->|불일치| RN{... Rule N 조건?}\n    RN -->|매칭| ApplyN[Rule N 적용]\n    RN -->|불일치| Never[Job 실행 안 함]\n```\n\n> [!IMPORTANT]\n> rules는 **위에서 아래로 순차 평가**하며, 첫 번째 매칭된 rule이 적용됩니다. 매칭되는 rule이 없으면 Job이 실행되지 않습니다.\n\n### rules 조건 유형\n\n#### if: 표현식 평가\n\n```yaml\nrules:\n  # 브랜치 조건\n  - if: $CI_COMMIT_BRANCH == \"main\"\n  \n  # Pipeline Source\n  - if: $CI_PIPELINE_SOURCE == \"push\"\n  - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n  \n  # 변수 존재 여부\n  - if: $DEPLOY_TOKEN\n  \n  # 정규식\n  - if: $CI_COMMIT_TAG =~ /^v\\d+\\.\\d+\\.\\d+$/\n  \n  # 복합 조건\n  - if: $CI_COMMIT_BRANCH == \"main\" && $CI_PIPELINE_SOURCE == \"push\"\n```\n\n#### changes: 파일 변경 감지\n\n```yaml\nbuild-frontend:\n  rules:\n    - changes:\n        - frontend/**/*\n        - shared/**/*\n\nbuild-backend:\n  rules:\n    - changes:\n        paths:\n          - backend/**/*\n        compare_to: main  # main 브랜치와 비교\n```\n\n#### exists: 파일 존재 확인\n\n```yaml\ndocker-build:\n  rules:\n    - exists:\n        - Dockerfile\n        - docker-compose.yml\n\nnpm-build:\n  rules:\n    - exists:\n        - package.json\n```\n\n### when 옵션\n\n| 값 | 동작 |\n|---|------|\n| `on_success` | 이전 Stage 성공 시 (기본값) |\n| `always` | 항상 실행 |\n| `never` | 실행 안 함 |\n| `manual` | 수동 승인 필요 |\n| `delayed` | 지연 후 실행 |\n\n```yaml\ndeploy-prod:\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      when: manual\n      allow_failure: false  # 블로킹 수동 Job\n```\n\n### only/except에서 마이그레이션\n\n```yaml\n# 이전 (only/except) - 더 이상 권장하지 않음\njob:\n  only:\n    - main\n  except:\n    - tags\n\n# 현재 (rules) - 권장\njob:\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\" && $CI_COMMIT_TAG == null\n```\n\n---\n\n## workflow:rules: 전역 파이프라인 제어\n\n**파이프라인 자체의 생성 여부**를 제어합니다.\n\n```yaml\nworkflow:\n  rules:\n    # MR 파이프라인: MR 이벤트에서만\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n    \n    # 브랜치 파이프라인: main, develop만\n    - if: $CI_COMMIT_BRANCH == \"main\"\n    - if: $CI_COMMIT_BRANCH == \"develop\"\n    \n    # 태그 파이프라인\n    - if: $CI_COMMIT_TAG\n    \n    # 그 외: 파이프라인 생성 안 함\n\nstages:\n  - build\n  - test\n```\n\n### 중복 파이프라인 방지\n\n```yaml\nworkflow:\n  rules:\n    # MR 이벤트 시 브랜치 파이프라인 방지 (MR 파이프라인만 실행)\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n    - if: $CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS\n      when: never  # MR이 열려있으면 브랜치 파이프라인 스킵\n    - if: $CI_COMMIT_BRANCH\n```\n\n---\n\n## needs: DAG (Directed Acyclic Graph)\n\n`needs`는 **Stage를 무시하고 Job 간 직접 의존성**을 정의합니다.\n\n### Stage 기반 vs DAG\n\n```mermaid\nflowchart LR\n    subgraph Stage-based [Stage 기반]\n        direction TB\n        S1[Stage 1] --> S2[Stage 2] --> S3[Stage 3]\n    end\n    \n    subgraph DAG [DAG 기반]\n        direction LR\n        A[Job A] --> C[Job C]\n        B[Job B] --> D[Job D]\n        C --> E[Job E]\n        D --> E\n    end\n```\n\n### 기본 사용법\n\n```yaml\nstages:\n  - build\n  - test\n  - deploy\n\nbuild-frontend:\n  stage: build\n  script: make build-frontend\n\nbuild-backend:\n  stage: build\n  script: make build-backend\n\ntest-frontend:\n  stage: test\n  needs: [build-frontend]  # build-frontend 완료 즉시 시작\n  script: make test-frontend\n\ntest-backend:\n  stage: test\n  needs: [build-backend]\n  script: make test-backend\n\ndeploy:\n  stage: deploy\n  needs: [test-frontend, test-backend]\n  script: make deploy\n```\n\n### DAG 실행 흐름\n\n```mermaid\nflowchart LR\n    BF[build-frontend] --> TF[test-frontend]\n    BB[build-backend] --> TB[test-backend]\n    TF --> D[deploy]\n    TB --> D\n```\n\n`build-frontend`가 완료되면 `build-backend`를 기다리지 않고 **즉시** `test-frontend`가 시작됩니다.\n\n### needs 옵션\n\n```yaml\ntest:\n  needs:\n    - job: build\n      artifacts: true   # 아티팩트 다운로드 (기본값)\n      optional: false   # 필수 의존성 (기본값)\n\ndeploy:\n  needs:\n    - job: test\n      artifacts: false  # 아티팩트 불필요\n    - job: security-scan\n      optional: true    # 실패해도 진행\n```\n\n### parallel과 needs\n\n```yaml\ntest:\n  parallel: 3\n  script: run-tests.sh\n\nreport:\n  needs:\n    - job: test\n      parallel:\n        matrix:\n          - RUNNER: [1, 2, 3]  # parallel 모든 인스턴스 대기\n```\n\n---\n\n## dependencies: 아티팩트 제어\n\n`dependencies`는 **아티팩트 다운로드**를 제어합니다.\n\n```yaml\nbuild:\n  stage: build\n  script: make build\n  artifacts:\n    paths:\n      - dist/\n\ntest:\n  stage: test\n  dependencies:\n    - build  # build의 아티팩트만 다운로드\n  script: make test\n\ndeploy:\n  stage: deploy\n  dependencies: []  # 아티팩트 다운로드 안 함\n  script: make deploy\n```\n\n### needs vs dependencies\n\n| 특성 | needs | dependencies |\n|-----|-------|--------------|\n| **실행 순서** | 제어함 (DAG) | 제어 안 함 |\n| **아티팩트** | 기본 포함 | 전용 제어 |\n| **Stage 무시** | 가능 | 불가능 |\n\n> [!TIP]\n> `needs`를 사용하면 `dependencies`가 필요 없는 경우가 많습니다. `needs: [job]`은 해당 Job의 아티팩트를 자동으로 가져옵니다.\n\n---\n\n## extends: Job 템플릿\n\n`extends`로 **Job 설정을 상속**합니다.\n\n### 기본 사용법\n\n```yaml\n.test-template:\n  stage: test\n  image: node:20\n  before_script:\n    - npm ci\n  cache:\n    paths:\n      - node_modules/\n\nunit-test:\n  extends: .test-template\n  script:\n    - npm run test:unit\n\nintegration-test:\n  extends: .test-template\n  script:\n    - npm run test:integration\n  services:\n    - postgres:15\n```\n\n### 다중 상속\n\n```yaml\n.base:\n  tags:\n    - docker\n\n.node:\n  image: node:20\n\n.cache:\n  cache:\n    paths:\n      - node_modules/\n\nbuild:\n  extends:\n    - .base\n    - .node\n    - .cache\n  script:\n    - npm run build\n```\n\n### 상속 순서\n\n```mermaid\nflowchart TB\n    Base[.base] --> Node[.node]\n    Node --> Cache[.cache]\n    Cache --> Job[실제 Job]\n    \n    subgraph Merge [병합 결과]\n        M[tags + image + cache + script]\n    end\n    \n    Job --> M\n```\n\n나중에 정의된 값이 **이전 값을 덮어씁니다**.\n\n---\n\n## !reference: 세밀한 재사용\n\n`!reference`는 **특정 키만 선택적으로 재사용**합니다.\n\n```yaml\n.setup:\n  before_script:\n    - echo \"Setting up...\"\n  after_script:\n    - echo \"Cleaning up...\"\n  script:\n    - echo \"Default script\"\n\n.test-vars:\n  variables:\n    TEST_ENV: \"test\"\n\nbuild:\n  # .setup의 before_script만 가져옴\n  before_script:\n    - !reference [.setup, before_script]\n    - echo \"Additional setup\"\n  script:\n    - npm run build\n  variables:\n    # .test-vars의 variables 병합\n    !reference [.test-vars, variables]\n```\n\n### extends vs !reference\n\n```yaml\n# extends: 전체 병합\njob1:\n  extends: .template  # 모든 키 상속\n\n# !reference: 선택적 재사용\njob2:\n  script:\n    - !reference [.template, script]  # script만 가져옴\n```\n\n---\n\n## 실전 예제: 완전한 파이프라인\n\n```yaml\n# 전역 설정\ndefault:\n  image: node:20-alpine\n  interruptible: true\n\nvariables:\n  npm_config_cache: \"$CI_PROJECT_DIR/.npm\"\n\nworkflow:\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n    - if: $CI_COMMIT_BRANCH == \"main\"\n    - if: $CI_COMMIT_TAG\n\nstages:\n  - prepare\n  - build\n  - test\n  - deploy\n\n# 템플릿\n.node-cache:\n  cache:\n    key:\n      files:\n        - package-lock.json\n    paths:\n      - .npm/\n      - node_modules/\n\n.deploy-template:\n  image: bitnami/kubectl:latest\n  before_script:\n    - kubectl config use-context $KUBE_CONTEXT\n\n# Jobs\ninstall:\n  stage: prepare\n  extends: .node-cache\n  script:\n    - npm ci\n  artifacts:\n    paths:\n      - node_modules/\n    expire_in: 1 hour\n\nlint:\n  stage: build\n  needs: [install]\n  script:\n    - npm run lint\n  allow_failure: true\n\nbuild:\n  stage: build\n  needs: [install]\n  script:\n    - npm run build\n  artifacts:\n    paths:\n      - dist/\n\nunit-test:\n  stage: test\n  needs:\n    - job: build\n      artifacts: true\n  script:\n    - npm run test:unit\n  coverage: '/Coverage: (\\d+\\.?\\d*)%/'\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n    - if: $CI_COMMIT_BRANCH == \"main\"\n\ne2e-test:\n  stage: test\n  needs: [build]\n  image: mcr.microsoft.com/playwright:v1.40.0\n  script:\n    - npm run test:e2e\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n    - when: manual\n      allow_failure: true\n\ndeploy-staging:\n  stage: deploy\n  extends: .deploy-template\n  needs: [unit-test]\n  environment:\n    name: staging\n    url: https://staging.example.com\n  script:\n    - kubectl apply -f k8s/staging/\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n\ndeploy-production:\n  stage: deploy\n  extends: .deploy-template\n  needs: [unit-test, e2e-test]\n  environment:\n    name: production\n    url: https://example.com\n  script:\n    - kubectl apply -f k8s/production/\n  rules:\n    - if: $CI_COMMIT_TAG =~ /^v\\d+\\.\\d+\\.\\d+$/\n      when: manual\n```\n\n---\n\n## 정리\n\n| 키워드 | 용도 |\n|-------|------|\n| `rules` | 조건부 Job 실행 (if, changes, exists) |\n| `workflow:rules` | 전역 파이프라인 생성 제어 |\n| `needs` | DAG 의존성, Stage 무시 |\n| `dependencies` | 아티팩트 다운로드 제어 |\n| `extends` | Job 템플릿 상속 |\n| `!reference` | 선택적 키 재사용 |\n\n---\n\n## 다음 편 예고\n\n**6편: 외부 통합**에서는 다음을 다룹니다:\n\n- Pipeline Triggers (토큰 기반)\n- Webhooks로 파이프라인 트리거\n- API를 통한 파이프라인 제어\n- ChatOps 연동\n- GitOps 시리즈와의 연결\n\n---\n\n## 참고 자료\n\n- [rules Reference](https://docs.gitlab.com/ee/ci/yaml/#rules)\n- [needs - DAG](https://docs.gitlab.com/ee/ci/yaml/#needs)\n- [!reference Tag](https://docs.gitlab.com/ee/ci/yaml/yaml_optimization.html#reference-tags)\n- [Job Keywords Reference](https://docs.gitlab.com/ee/ci/yaml/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitLab",
      "CI/CD",
      "Pipeline",
      "DAG"
    ],
    "readingTime": 7,
    "wordCount": 1345,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitlab-ci",
    "date": "2026-01-05"
  },
  {
    "id": "gitlab-ci-04-pipeline-architectures",
    "slug": "gitlab-ci-04-pipeline-architectures",
    "path": "backend/devops",
    "fullPath": "backend/devops/gitlab-ci-04-pipeline-architectures",
    "title": "GitLab CI/CD 시리즈 #4: Pipeline 아키텍처 - Parent-Child와 Multi-Project",
    "excerpt": "복잡한 파이프라인을 효율적으로 관리하기 위한 Parent-Child Pipeline, Multi-Project Pipeline, 동적 파이프라인 생성을 다룹니다.",
    "content": "# GitLab CI/CD 시리즈 #4: Pipeline 아키텍처 - Parent-Child와 Multi-Project\n\n## 시리즈 개요\n\n| # | 주제 | 핵심 내용 |\n|---|------|----------|\n| 1 | 기초 | .gitlab-ci.yml 구조, Stages, Jobs, Pipeline 흐름 |\n| 2 | Variables & Secrets | 변수 유형, 우선순위, 외부 Vault 연동 |\n| 3 | Runners & Executors | Docker, Kubernetes, Docker-in-Docker |\n| **4** | **Pipeline 아키텍처** | Parent-Child, Multi-Project Pipeline |\n| 5 | 고급 Job 제어 | rules, needs, DAG, extends |\n| 6 | 외부 통합 | Triggers, Webhooks, API |\n\n---\n\n## 왜 Pipeline 아키텍처가 필요한가?\n\n단일 `.gitlab-ci.yml`이 수백 줄로 커지면 관리가 어려워집니다.\n\n### 문제점\n\n```yaml\n# 거대한 단일 파일의 문제점\nstages:\n  - build\n  - test\n  - deploy\n\n# 50개 이상의 Jobs...\nbuild-frontend:\n  # ...\nbuild-backend:\n  # ...\nbuild-mobile-ios:\n  # ...\nbuild-mobile-android:\n  # ...\n# ... 수백 줄\n```\n\n### 해결책\n\n| 패턴 | 용도 |\n|------|------|\n| **include** | 설정 파일 분리 (동일 프로젝트 내) |\n| **Parent-Child** | 동적 파이프라인, 조건부 실행 |\n| **Multi-Project** | 프로젝트 간 트리거 |\n| **DAG** | Stage 무시하고 Job 간 의존성 직접 정의 |\n\n---\n\n## include: 설정 파일 분리\n\n가장 기본적인 모듈화 방법입니다.\n\n### include 유형\n\n```yaml\ninclude:\n  # 1. 로컬 파일\n  - local: '/templates/docker.yml'\n  \n  # 2. 다른 프로젝트의 파일\n  - project: 'my-group/ci-templates'\n    ref: main\n    file: '/templates/node.yml'\n  \n  # 3. 원격 URL\n  - remote: 'https://example.com/ci/template.yml'\n  \n  # 4. GitLab 제공 템플릿\n  - template: 'Auto-DevOps.gitlab-ci.yml'\n```\n\n### 실전 구조\n\n```\nproject/\n├── .gitlab-ci.yml              # 메인 파일\n├── .gitlab/\n│   ├── ci/\n│   │   ├── build.yml           # 빌드 Jobs\n│   │   ├── test.yml            # 테스트 Jobs\n│   │   └── deploy.yml          # 배포 Jobs\n│   └── templates/\n│       └── docker.yml          # 공통 템플릿\n```\n\n```yaml\n# .gitlab-ci.yml\nstages:\n  - build\n  - test\n  - deploy\n\ninclude:\n  - local: '.gitlab/ci/build.yml'\n  - local: '.gitlab/ci/test.yml'\n  - local: '.gitlab/ci/deploy.yml'\n```\n\n```yaml\n# .gitlab/ci/build.yml\nbuild-app:\n  stage: build\n  script:\n    - npm run build\n```\n\n---\n\n## Parent-Child Pipelines\n\n**Parent Pipeline**이 **Child Pipeline**을 트리거하는 구조입니다.\n\n### 기본 구조\n\n```mermaid\nflowchart TB\n    subgraph Parent [Parent Pipeline]\n        P1[build]\n        P2[trigger-child]\n    end\n    \n    subgraph Child [Child Pipeline]\n        C1[test-unit]\n        C2[test-e2e]\n        C3[deploy]\n    end\n    \n    P1 --> P2\n    P2 -->|trigger| C1 & C2\n    C1 & C2 --> C3\n```\n\n### 정적 Child Pipeline\n\n```yaml\n# .gitlab-ci.yml (Parent)\nstages:\n  - build\n  - trigger\n\nbuild:\n  stage: build\n  script:\n    - npm run build\n  artifacts:\n    paths:\n      - dist/\n\ntrigger-child:\n  stage: trigger\n  trigger:\n    include: .gitlab/child-pipeline.yml\n    strategy: depend  # Child 완료까지 대기\n```\n\n```yaml\n# .gitlab/child-pipeline.yml (Child)\nstages:\n  - test\n  - deploy\n\nunit-test:\n  stage: test\n  script:\n    - npm run test:unit\n\ndeploy:\n  stage: deploy\n  script:\n    - ./deploy.sh\n```\n\n### strategy 옵션\n\n| 옵션 | 동작 |\n|------|------|\n| `depend` | Child 완료까지 Parent Job 대기 |\n| (없음) | Parent Job 즉시 완료, Child 비동기 실행 |\n\n---\n\n## 동적 Child Pipeline\n\n**런타임에 Child Pipeline YAML을 생성**합니다.\n\n### 동적 생성 예제\n\n```yaml\n# .gitlab-ci.yml\nstages:\n  - generate\n  - trigger\n\ngenerate-pipeline:\n  stage: generate\n  script:\n    - |\n      # 변경된 디렉토리에 따라 동적으로 파이프라인 생성\n      cat > child-pipeline.yml <<EOF\n      stages:\n        - test\n      \n      $(for dir in $(git diff --name-only HEAD~1 | cut -d/ -f1 | sort -u); do\n        echo \"${dir}-test:\"\n        echo \"  stage: test\"\n        echo \"  script:\"\n        echo \"    - echo 'Testing $dir'\"\n        echo \"\"\n      done)\n      EOF\n  artifacts:\n    paths:\n      - child-pipeline.yml\n\ntrigger-tests:\n  stage: trigger\n  trigger:\n    include:\n      - artifact: child-pipeline.yml\n        job: generate-pipeline\n    strategy: depend\n```\n\n### Monorepo 패턴\n\n```yaml\n# .gitlab-ci.yml\nstages:\n  - detect\n  - build\n\ndetect-changes:\n  stage: detect\n  script:\n    - |\n      # 변경된 서비스 감지\n      for service in frontend backend api; do\n        if git diff --name-only HEAD~1 | grep -q \"^$service/\"; then\n          echo \"$service\" >> changed_services.txt\n        fi\n      done\n      \n      # 동적 파이프라인 생성\n      echo \"stages:\" > child.yml\n      echo \"  - build\" >> child.yml\n      echo \"\" >> child.yml\n      \n      while read service; do\n        cat >> child.yml <<EOF\n      build-${service}:\n        stage: build\n        script:\n          - cd ${service} && make build\n      EOF\n      done < changed_services.txt\n  artifacts:\n    paths:\n      - child.yml\n\ntrigger-builds:\n  stage: build\n  trigger:\n    include:\n      - artifact: child.yml\n        job: detect-changes\n  rules:\n    - exists:\n        - changed_services.txt\n```\n\n---\n\n## 변경된 파일 기반 트리거\n\n`rules:changes`를 사용하여 특정 파일 변경 시에만 Child를 트리거합니다.\n\n```yaml\n# .gitlab-ci.yml\nstages:\n  - triggers\n\ntrigger-frontend:\n  stage: triggers\n  trigger:\n    include: frontend/.gitlab-ci.yml\n  rules:\n    - changes:\n        - frontend/**/*\n\ntrigger-backend:\n  stage: triggers\n  trigger:\n    include: backend/.gitlab-ci.yml\n  rules:\n    - changes:\n        - backend/**/*\n\ntrigger-shared:\n  stage: triggers\n  trigger:\n    include: shared/.gitlab-ci.yml\n  rules:\n    - changes:\n        - shared/**/*\n```\n\n```mermaid\nflowchart TB\n    subgraph Parent [Parent Pipeline]\n        Check[변경 감지]\n    end\n    \n    subgraph Children [Child Pipelines]\n        F[Frontend Pipeline]\n        B[Backend Pipeline]\n        S[Shared Pipeline]\n    end\n    \n    Check -->|frontend/* 변경| F\n    Check -->|backend/* 변경| B\n    Check -->|shared/* 변경| S\n```\n\n---\n\n## Multi-Project Pipelines\n\n**다른 프로젝트의 파이프라인을 트리거**합니다.\n\n### 기본 사용법\n\n```yaml\n# Project A의 .gitlab-ci.yml\nstages:\n  - build\n  - trigger\n\nbuild:\n  stage: build\n  script:\n    - npm run build\n\ntrigger-project-b:\n  stage: trigger\n  trigger:\n    project: my-group/project-b\n    branch: main\n    strategy: depend\n```\n\n### 변수 전달\n\n```yaml\ntrigger-deploy:\n  stage: trigger\n  trigger:\n    project: my-group/deployment\n    branch: main\n  variables:\n    DEPLOY_ENV: production\n    IMAGE_TAG: $CI_COMMIT_SHA\n```\n\n### 양방향 트리거 (Upstream → Downstream)\n\n```mermaid\nflowchart LR\n    subgraph Upstream [Project A - 라이브러리]\n        U1[build]\n        U2[publish]\n        U3[trigger]\n    end\n    \n    subgraph Downstream [Project B - 앱]\n        D1[install]\n        D2[build]\n        D3[deploy]\n    end\n    \n    U1 --> U2 --> U3\n    U3 -->|trigger| D1\n    D1 --> D2 --> D3\n```\n\n---\n\n## 실전 예제: 마이크로서비스 배포\n\n```yaml\n# infrastructure/.gitlab-ci.yml (Parent)\nstages:\n  - build\n  - trigger-services\n  - deploy-infra\n\nbuild-base-images:\n  stage: build\n  script:\n    - docker build -t base-node:$CI_COMMIT_SHA ./base-images/node\n    - docker push base-node:$CI_COMMIT_SHA\n\ntrigger-user-service:\n  stage: trigger-services\n  trigger:\n    project: my-org/user-service\n    branch: main\n  variables:\n    BASE_IMAGE_TAG: $CI_COMMIT_SHA\n\ntrigger-order-service:\n  stage: trigger-services\n  trigger:\n    project: my-org/order-service\n    branch: main\n  variables:\n    BASE_IMAGE_TAG: $CI_COMMIT_SHA\n\ntrigger-payment-service:\n  stage: trigger-services\n  trigger:\n    project: my-org/payment-service\n    branch: main\n  variables:\n    BASE_IMAGE_TAG: $CI_COMMIT_SHA\n\ndeploy-kubernetes:\n  stage: deploy-infra\n  trigger:\n    project: my-org/k8s-manifests\n    branch: main\n  needs:\n    - trigger-user-service\n    - trigger-order-service\n    - trigger-payment-service\n  variables:\n    SERVICES_VERSION: $CI_COMMIT_SHA\n```\n\n---\n\n## Pipeline 간 아티팩트 공유\n\n### Parent → Child\n\n```yaml\n# Parent\nparent-job:\n  script:\n    - echo \"data\" > file.txt\n  artifacts:\n    paths:\n      - file.txt\n\ntrigger-child:\n  trigger:\n    include: child.yml\n  needs:\n    - parent-job\n```\n\n```yaml\n# Child (child.yml)\nchild-job:\n  script:\n    - cat file.txt  # Parent의 아티팩트 사용 가능\n```\n\n### Multi-Project 간 아티팩트\n\n```yaml\n# Project B\ndownstream-job:\n  script:\n    - |\n      # Project A의 아티팩트 다운로드\n      curl --header \"PRIVATE-TOKEN: $API_TOKEN\" \\\n        \"$CI_API_V4_URL/projects/123/jobs/$UPSTREAM_JOB_ID/artifacts\" \\\n        --output artifacts.zip\n      unzip artifacts.zip\n```\n\n---\n\n## 파이프라인 시각화\n\n### Pipeline Graph\n\n```mermaid\nflowchart TB\n    subgraph MainPipeline [Main Pipeline]\n        direction LR\n        Build[Build Stage]\n        Test[Test Stage]\n        Deploy[Deploy Stage]\n    end\n    \n    subgraph ChildPipelines [Child Pipelines]\n        direction TB\n        Frontend[Frontend Pipeline]\n        Backend[Backend Pipeline]\n    end\n    \n    Build --> Frontend & Backend\n    Frontend & Backend --> Deploy\n```\n\nGitLab UI에서는 이러한 관계가 **시각적으로 표시**됩니다.\n\n---\n\n## 정리\n\n| 패턴 | 용도 | 키워드 |\n|-----|------|--------|\n| **include** | 설정 파일 분리 | `include: local/project/remote` |\n| **Parent-Child** | 동적 파이프라인 | `trigger: include:` |\n| **Multi-Project** | 프로젝트 간 트리거 | `trigger: project:` |\n| **동적 생성** | 런타임 파이프라인 | `artifact: + trigger` |\n\n---\n\n## 다음 편 예고\n\n**5편: 고급 Job 제어**에서는 다음을 다룹니다:\n\n- `rules` vs `only/except` 마이그레이션\n- `needs`와 DAG 실행\n- `dependencies`와 아티팩트 제어\n- `workflow:rules` 전역 제어\n- Job 템플릿과 `extends`, `!reference`\n\n---\n\n## 참고 자료\n\n- [Parent-Child Pipelines](https://docs.gitlab.com/ee/ci/pipelines/downstream_pipelines.html)\n- [Multi-Project Pipelines](https://docs.gitlab.com/ee/ci/pipelines/multi_project_pipelines.html)\n- [Pipeline Architectures](https://docs.gitlab.com/ee/ci/pipelines/pipeline_architectures.html)\n- [Dynamic Child Pipelines](https://docs.gitlab.com/ee/ci/pipelines/downstream_pipelines.html#dynamic-child-pipelines)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitLab",
      "CI/CD",
      "Pipeline"
    ],
    "readingTime": 6,
    "wordCount": 1148,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitlab-ci",
    "date": "2026-01-05"
  },
  {
    "id": "gitlab-ci-03-runners-executors",
    "slug": "gitlab-ci-03-runners-executors",
    "path": "backend/devops",
    "fullPath": "backend/devops/gitlab-ci-03-runners-executors",
    "title": "GitLab CI/CD 시리즈 #3: Runners와 Executors - Docker-in-Docker 심화",
    "excerpt": "GitLab Runner의 아키텍처와 Executor 유형, Docker-in-Docker(DinD) 설정, Kubernetes Executor 구성을 다룹니다.",
    "content": "# GitLab CI/CD 시리즈 #3: Runners와 Executors - Docker-in-Docker 심화\n\n## 시리즈 개요\n\n| # | 주제 | 핵심 내용 |\n|---|------|----------|\n| 1 | 기초 | .gitlab-ci.yml 구조, Stages, Jobs, Pipeline 흐름 |\n| 2 | Variables & Secrets | 변수 유형, 우선순위, 외부 Vault 연동 |\n| **3** | **Runners & Executors** | Docker, Kubernetes, Docker-in-Docker |\n| 4 | Pipeline 아키텍처 | Parent-Child, Multi-Project Pipeline |\n| 5 | 고급 Job 제어 | rules, needs, DAG, extends |\n| 6 | 외부 통합 | Triggers, Webhooks, API |\n\n---\n\n## GitLab Runner란?\n\n**GitLab Runner**는 CI/CD Jobs를 실제로 실행하는 에이전트입니다. GitLab 서버와 분리되어 동작하며, 다양한 환경에서 실행할 수 있습니다.\n\n```mermaid\nflowchart TB\n    subgraph GitLab\n        Server[GitLab Server]\n        Queue[Job Queue]\n    end\n    \n    subgraph Runners\n        R1[Runner 1 - Docker]\n        R2[Runner 2 - Kubernetes]\n        R3[Runner 3 - Shell]\n    end\n    \n    Server --> Queue\n    Queue <--> R1 & R2 & R3\n```\n\n### Runner 유형\n\n| 유형 | 범위 | 설정 위치 |\n|-----|------|----------|\n| **Shared** | 인스턴스 전체 | Admin Area |\n| **Group** | 특정 그룹과 하위 프로젝트 | Group Settings |\n| **Project** | 특정 프로젝트만 | Project Settings |\n\n### Runner 등록\n\n```bash\n# Runner 설치 (Linux)\ncurl -L \"https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh\" | sudo bash\nsudo apt-get install gitlab-runner\n\n# Runner 등록\nsudo gitlab-runner register \\\n  --url \"https://gitlab.com/\" \\\n  --registration-token \"PROJECT_REGISTRATION_TOKEN\" \\\n  --description \"My Docker Runner\" \\\n  --executor \"docker\" \\\n  --docker-image \"alpine:latest\"\n```\n\n---\n\n## Executor 유형\n\nExecutor는 Jobs가 **어떤 환경에서 실행되는지** 결정합니다.\n\n### 주요 Executors\n\n| Executor | 격리 수준 | 용도 |\n|----------|----------|------|\n| **Shell** | 없음 | 간단한 스크립트, 호스트 직접 접근 |\n| **Docker** | 컨테이너 | 일반적인 CI/CD |\n| **Docker Machine** | VM + 컨테이너 | Auto-scaling |\n| **Kubernetes** | Pod | 클라우드 네이티브 |\n| **VirtualBox** | VM | 완전 격리 필요 시 |\n\n### Shell Executor\n\n호스트에서 직접 명령을 실행합니다.\n\n```toml\n# /etc/gitlab-runner/config.toml\n[[runners]]\n  name = \"shell-runner\"\n  executor = \"shell\"\n  shell = \"bash\"\n```\n\n```yaml\njob:\n  script:\n    - whoami  # gitlab-runner 사용자\n    - ls /home\n```\n\n> [!WARNING]\n> Shell Executor는 **격리가 없어** 보안에 취약합니다. 신뢰할 수 있는 코드만 실행하세요.\n\n### Docker Executor\n\n가장 일반적인 Executor입니다. 각 Job은 독립된 컨테이너에서 실행됩니다.\n\n```toml\n[[runners]]\n  name = \"docker-runner\"\n  executor = \"docker\"\n  [runners.docker]\n    image = \"alpine:latest\"\n    privileged = false\n    volumes = [\"/cache\"]\n    shm_size = 0\n```\n\n```yaml\njob:\n  image: node:20-alpine\n  script:\n    - npm ci\n    - npm run build\n```\n\n---\n\n## Docker-in-Docker (DinD)\n\nCI/CD 파이프라인에서 **Docker 이미지를 빌드**해야 할 때 DinD를 사용합니다.\n\n### 왜 DinD가 필요한가?\n\n```mermaid\nflowchart TB\n    subgraph Host [호스트]\n        Docker[Docker Daemon]\n        subgraph Runner [GitLab Runner Container]\n            Job[CI Job]\n            Job -->|docker build| Docker\n        end\n    end\n```\n\n일반 Docker Executor에서는 `docker` 명령을 사용할 수 없습니다. 컨테이너 안에 Docker 데몬이 없기 때문입니다.\n\n### DinD 설정\n\n```yaml\ndefault:\n  image: docker:24.0.5\n  services:\n    - name: docker:24.0.5-dind\n      alias: docker\n\nvariables:\n  DOCKER_HOST: tcp://docker:2376\n  DOCKER_TLS_CERTDIR: \"/certs\"\n  DOCKER_TLS_VERIFY: 1\n  DOCKER_CERT_PATH: \"$DOCKER_TLS_CERTDIR/client\"\n\nbuild:\n  stage: build\n  script:\n    - docker info\n    - docker build -t myapp:$CI_COMMIT_SHA .\n    - docker push myapp:$CI_COMMIT_SHA\n```\n\n### DinD 아키텍처\n\n```mermaid\nflowchart TB\n    subgraph Pod/Container [Runner 실행 환경]\n        subgraph JobContainer [Job Container]\n            CLI[docker CLI]\n        end\n        \n        subgraph DinD [DinD Service Container]\n            Daemon[Docker Daemon]\n            Images[(Images)]\n        end\n        \n        CLI -->|TCP :2376| Daemon\n        Daemon --> Images\n    end\n```\n\n### TLS 활성화 vs 비활성화\n\n#### TLS 활성화 (기본, 권장)\n\n```yaml\nvariables:\n  DOCKER_HOST: tcp://docker:2376\n  DOCKER_TLS_CERTDIR: \"/certs\"\n  DOCKER_TLS_VERIFY: 1\n  DOCKER_CERT_PATH: \"$DOCKER_TLS_CERTDIR/client\"\n```\n\n#### TLS 비활성화 (테스트용)\n\n```yaml\nvariables:\n  DOCKER_HOST: tcp://docker:2375\n  DOCKER_TLS_CERTDIR: \"\"\n```\n\n> [!CAUTION]\n> TLS 비활성화는 **암호화되지 않은 통신**을 사용합니다. 프로덕션에서는 사용하지 마세요.\n\n### Runner 설정 (config.toml)\n\n```toml\n[[runners]]\n  name = \"docker-runner\"\n  executor = \"docker\"\n  [runners.docker]\n    tls_verify = false\n    image = \"docker:24.0.5\"\n    privileged = true  # DinD 필수\n    disable_entrypoint_overwrite = false\n    oom_kill_disable = false\n    disable_cache = false\n    volumes = [\"/certs/client\", \"/cache\"]\n    shm_size = 0\n```\n\n---\n\n## Docker Socket Binding (대안)\n\nDinD 대신 **호스트 Docker 소켓을 마운트**하는 방식도 있습니다.\n\n```toml\n[[runners]]\n  [runners.docker]\n    volumes = [\"/var/run/docker.sock:/var/run/docker.sock\"]\n```\n\n```yaml\nbuild:\n  image: docker:24.0.5\n  script:\n    - docker build -t myapp .\n```\n\n### DinD vs Socket Binding\n\n| 특성 | DinD | Socket Binding |\n|-----|------|----------------|\n| **격리** | 완전 격리 | 호스트와 공유 |\n| **보안** | 안전 | 호스트 접근 가능 |\n| **성능** | 약간 느림 | 빠름 |\n| **캐시** | Job마다 초기화 | 호스트 캐시 공유 |\n| **빌드 레이어** | 매번 다운로드 | 캐시 활용 |\n\n> [!IMPORTANT]\n> **Socket Binding**은 호스트 Docker에 직접 접근하므로 **신뢰할 수 있는 코드만** 실행해야 합니다. DinD가 더 안전합니다.\n\n---\n\n## Kubernetes Executor\n\nKubernetes 클러스터에서 Jobs를 Pod로 실행합니다.\n\n### 설정\n\n```toml\n[[runners]]\n  name = \"k8s-runner\"\n  executor = \"kubernetes\"\n  [runners.kubernetes]\n    namespace = \"gitlab-runner\"\n    image = \"alpine:latest\"\n    privileged = false\n    \n    cpu_request = \"100m\"\n    cpu_limit = \"1\"\n    memory_request = \"128Mi\"\n    memory_limit = \"1Gi\"\n    \n    service_cpu_request = \"100m\"\n    service_memory_request = \"128Mi\"\n    \n    poll_interval = 5\n    poll_timeout = 3600\n```\n\n### Kubernetes DinD\n\n```toml\n[[runners]]\n  [runners.kubernetes]\n    privileged = true\n    \n    [[runners.kubernetes.services]]\n      name = \"docker:24.0.5-dind\"\n      alias = \"docker\"\n      command = [\"--storage-driver=overlay2\"]\n```\n\n```yaml\nbuild:\n  image: docker:24.0.5\n  services:\n    - docker:24.0.5-dind\n  variables:\n    DOCKER_HOST: tcp://docker:2376\n    DOCKER_TLS_CERTDIR: \"/certs\"\n  script:\n    - docker build -t myapp .\n```\n\n### Pod 생성 흐름\n\n```mermaid\nsequenceDiagram\n    participant GitLab as GitLab Server\n    participant Runner as GitLab Runner\n    participant K8s as Kubernetes API\n    participant Pod as Job Pod\n    \n    GitLab->>Runner: Job 할당\n    Runner->>K8s: Pod 생성 요청\n    K8s->>Pod: Pod 스케줄링\n    Pod->>Pod: Job 실행\n    Pod->>Runner: 결과 전송\n    Runner->>GitLab: Job 완료 보고\n    Runner->>K8s: Pod 삭제\n```\n\n---\n\n## Runner 태그와 Job 매칭\n\nRunner에 태그를 지정하여 특정 Jobs만 실행하도록 제한합니다.\n\n### Runner 태그\n\n```toml\n[[runners]]\n  name = \"gpu-runner\"\n  tags = [\"gpu\", \"cuda\", \"ml\"]\n```\n\n### Job 태그\n\n```yaml\ntrain-model:\n  tags:\n    - gpu\n    - ml\n  script:\n    - python train.py\n\ndeploy:\n  tags:\n    - docker\n  script:\n    - ./deploy.sh\n```\n\n### 태그 없는 Job 처리\n\n```toml\n[[runners]]\n  run_untagged = true  # 태그 없는 Job도 실행\n```\n\n---\n\n## Runner 성능 최적화\n\n### 동시 실행\n\n```toml\nconcurrent = 10  # 전체 Runner가 동시 실행할 수 있는 최대 Job 수\n\n[[runners]]\n  limit = 5  # 이 Runner의 최대 동시 Job 수\n```\n\n### 캐시 설정\n\n```yaml\ndefault:\n  cache:\n    key:\n      files:\n        - package-lock.json\n    paths:\n      - node_modules/\n    policy: pull-push\n\nbuild:\n  cache:\n    policy: pull  # 읽기만\n```\n\n### 분산 캐시 (S3)\n\n```toml\n[[runners]]\n  [runners.cache]\n    Type = \"s3\"\n    Shared = true\n    [runners.cache.s3]\n      ServerAddress = \"s3.amazonaws.com\"\n      BucketName = \"gitlab-runner-cache\"\n      BucketLocation = \"ap-northeast-2\"\n```\n\n---\n\n## 실전 예제: 멀티 아키텍처 빌드\n\n```yaml\nstages:\n  - build\n  - manifest\n\nvariables:\n  DOCKER_HOST: tcp://docker:2376\n  DOCKER_TLS_CERTDIR: \"/certs\"\n\n.docker-build:\n  image: docker:24.0.5\n  services:\n    - docker:24.0.5-dind\n  before_script:\n    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n\nbuild-amd64:\n  extends: .docker-build\n  tags:\n    - amd64\n  script:\n    - docker build --platform linux/amd64 -t $CI_REGISTRY_IMAGE:amd64 .\n    - docker push $CI_REGISTRY_IMAGE:amd64\n\nbuild-arm64:\n  extends: .docker-build\n  tags:\n    - arm64\n  script:\n    - docker build --platform linux/arm64 -t $CI_REGISTRY_IMAGE:arm64 .\n    - docker push $CI_REGISTRY_IMAGE:arm64\n\ncreate-manifest:\n  extends: .docker-build\n  stage: manifest\n  script:\n    - docker manifest create $CI_REGISTRY_IMAGE:latest\n        $CI_REGISTRY_IMAGE:amd64\n        $CI_REGISTRY_IMAGE:arm64\n    - docker manifest push $CI_REGISTRY_IMAGE:latest\n```\n\n---\n\n## 정리\n\n| 개념 | 설명 |\n|-----|------|\n| **Runner** | Jobs를 실행하는 에이전트 |\n| **Executor** | 실행 환경 결정 (Shell, Docker, K8s) |\n| **DinD** | 컨테이너 안에서 Docker 빌드 |\n| **TLS** | DinD 통신 암호화 (권장) |\n| **Socket Binding** | 호스트 Docker 공유 (보안 주의) |\n| **Tags** | Runner와 Job 매칭 |\n\n---\n\n## 다음 편 예고\n\n**4편: Pipeline 아키텍처**에서는 다음을 다룹니다:\n\n- 기본 Pipeline vs DAG Pipeline\n- **Parent-Child Pipelines** (동적 파이프라인)\n- **Multi-Project Pipelines** (크로스 프로젝트)\n- `trigger` 키워드 심화\n- 동적 Child Pipeline 생성\n\n---\n\n## 참고 자료\n\n- [GitLab Runner Documentation](https://docs.gitlab.com/runner/)\n- [Docker Executor](https://docs.gitlab.com/runner/executors/docker.html)\n- [Using Docker to Build Docker Images](https://docs.gitlab.com/ee/ci/docker/using_docker_build.html)\n- [Kubernetes Executor](https://docs.gitlab.com/runner/executors/kubernetes.html)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitLab",
      "CI/CD",
      "Docker",
      "Kubernetes",
      "Runner"
    ],
    "readingTime": 7,
    "wordCount": 1214,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitlab-ci",
    "date": "2026-01-05"
  },
  {
    "id": "gitlab-ci-01-fundamentals",
    "slug": "gitlab-ci-01-fundamentals",
    "path": "backend/devops",
    "fullPath": "backend/devops/gitlab-ci-01-fundamentals",
    "title": "GitLab CI/CD 시리즈 #1: 기초 - .gitlab-ci.yml의 구조와 Pipeline 이해",
    "excerpt": "GitLab CI/CD의 핵심인 .gitlab-ci.yml 파일 구조, Stages와 Jobs의 관계, Pipeline 실행 흐름을 체계적으로 이해합니다.",
    "content": "# GitLab CI/CD 시리즈 #1: 기초 - .gitlab-ci.yml의 구조와 Pipeline 이해\n\n## 시리즈 개요\n\n| # | 주제 | 핵심 내용 |\n|---|------|----------|\n| **1** | **기초** | .gitlab-ci.yml 구조, Stages, Jobs, Pipeline 흐름 |\n| 2 | Variables & Secrets | 변수 유형, 우선순위, 외부 Vault 연동 |\n| 3 | Runners & Executors | Docker, Kubernetes, Docker-in-Docker |\n| 4 | Pipeline 아키텍처 | Parent-Child, Multi-Project Pipeline |\n| 5 | 고급 Job 제어 | rules, needs, DAG, extends |\n| 6 | 외부 통합 | Triggers, Webhooks, API |\n\n---\n\n## GitLab CI/CD란?\n\nGitLab CI/CD는 GitLab에 내장된 **지속적 통합(CI)** 및 **지속적 배포(CD)** 도구입니다. 코드가 저장소에 푸시될 때마다 자동으로 빌드, 테스트, 배포를 실행합니다.\n\n```mermaid\nflowchart LR\n    subgraph Developer\n        Code[코드 작성]\n    end\n    \n    subgraph GitLab\n        Push[git push]\n        CI[CI/CD Pipeline]\n        Runner[GitLab Runner]\n    end\n    \n    subgraph Artifacts\n        Build[빌드 결과물]\n        Report[테스트 리포트]\n    end\n    \n    Code --> Push --> CI\n    CI --> Runner\n    Runner --> Build\n    Runner --> Report\n```\n\n### 핵심 구성 요소\n\n| 구성 요소 | 역할 |\n|----------|------|\n| **`.gitlab-ci.yml`** | 파이프라인 정의 파일 (프로젝트 루트) |\n| **Pipeline** | Jobs의 집합, 코드 변경 시 트리거 |\n| **Stage** | Jobs를 그룹화하는 단계 |\n| **Job** | 실제 작업을 수행하는 단위 |\n| **Runner** | Jobs를 실행하는 에이전트 |\n\n---\n\n## .gitlab-ci.yml 기본 구조\n\n프로젝트 루트에 `.gitlab-ci.yml` 파일을 생성하면 GitLab이 자동으로 인식합니다.\n\n### 최소 예제\n\n```yaml\n# 가장 간단한 .gitlab-ci.yml\nbuild-job:\n  script:\n    - echo \"Hello, GitLab CI!\"\n```\n\n이 한 줄만으로도 파이프라인이 생성됩니다. `script`는 필수 키워드입니다.\n\n### 완전한 기본 구조\n\n```yaml\n# 1. 전역 기본값 설정\ndefault:\n  image: node:20-alpine\n  before_script:\n    - npm ci\n\n# 2. Stages 정의 (실행 순서)\nstages:\n  - build\n  - test\n  - deploy\n\n# 3. Jobs 정의\nbuild-job:\n  stage: build\n  script:\n    - npm run build\n  artifacts:\n    paths:\n      - dist/\n\ntest-job:\n  stage: test\n  script:\n    - npm run test\n\ndeploy-job:\n  stage: deploy\n  script:\n    - echo \"Deploying to production...\"\n  when: manual  # 수동 승인 필요\n```\n\n---\n\n## Stages: 실행 순서 정의\n\n**Stages**는 Jobs를 그룹화하고 **실행 순서**를 정의합니다.\n\n```yaml\nstages:\n  - build      # 1단계: 모든 build 스테이지 Jobs 병렬 실행\n  - test       # 2단계: build 완료 후 test Jobs 병렬 실행\n  - deploy     # 3단계: test 완료 후 deploy Jobs 실행\n```\n\n### Stage 실행 흐름\n\n```mermaid\nflowchart LR\n    subgraph Build [Stage: build]\n        B1[build-frontend]\n        B2[build-backend]\n    end\n    \n    subgraph Test [Stage: test]\n        T1[unit-test]\n        T2[integration-test]\n        T3[lint]\n    end\n    \n    subgraph Deploy [Stage: deploy]\n        D1[deploy-staging]\n        D2[deploy-prod]\n    end\n    \n    B1 --> T1 & T2 & T3\n    B2 --> T1 & T2 & T3\n    T1 & T2 & T3 --> D1\n    T1 & T2 & T3 --> D2\n```\n\n> [!IMPORTANT]\n> 같은 Stage 내의 Jobs는 **병렬로 실행**됩니다. 다음 Stage는 이전 Stage의 모든 Jobs가 **성공해야** 시작됩니다.\n\n### 기본 Stages\n\n`stages`를 명시하지 않으면 다음 기본값이 적용됩니다:\n\n```yaml\nstages:\n  - .pre      # 항상 첫 번째\n  - build\n  - test\n  - deploy\n  - .post     # 항상 마지막\n```\n\n---\n\n## Jobs: 실제 작업 단위\n\n**Job**은 파이프라인의 기본 실행 단위입니다. 각 Job은 독립적인 환경에서 실행됩니다.\n\n### Job 기본 문법\n\n```yaml\njob-name:                    # Job 이름 (자유롭게 지정)\n  stage: test                # 소속 Stage\n  image: python:3.12         # 실행 환경 (Docker 이미지)\n  script:                    # 실행할 명령어 (필수)\n    - pip install -r requirements.txt\n    - pytest\n  tags:                      # Runner 선택 태그\n    - docker\n```\n\n### 예약된 Job 이름\n\n일부 이름은 특별한 의미를 가지므로 **사용할 수 없습니다**:\n\n| 예약어 | 용도 |\n|--------|------|\n| `image` | Docker 이미지 지정 |\n| `services` | 서비스 컨테이너 |\n| `stages` | Stage 정의 |\n| `include` | 외부 파일 포함 |\n| `variables` | 변수 정의 |\n| `default` | 기본값 설정 |\n\n### 숨겨진 Job (템플릿)\n\n점(`.`)으로 시작하는 Job은 실행되지 않고 **템플릿**으로 사용됩니다:\n\n```yaml\n.test-template:      # 실행되지 않음, 템플릿\n  stage: test\n  before_script:\n    - setup-test-env.sh\n\nunit-test:\n  extends: .test-template  # 템플릿 상속\n  script:\n    - pytest unit/\n\nintegration-test:\n  extends: .test-template\n  script:\n    - pytest integration/\n```\n\n---\n\n## Script: 명령어 실행\n\n### script, before_script, after_script\n\n```yaml\njob:\n  before_script:     # script 이전에 실행\n    - echo \"Setting up...\"\n    - apt-get update\n    \n  script:            # 메인 명령어 (필수)\n    - echo \"Running main task...\"\n    - npm run build\n    \n  after_script:      # script 이후에 항상 실행 (실패해도)\n    - echo \"Cleaning up...\"\n    - rm -rf temp/\n```\n\n### 실행 순서\n\n```mermaid\nflowchart TB\n    subgraph Job [Job 실행]\n        BS[before_script] --> S[script]\n        S --> AS[after_script]\n    end\n    \n    S -->|실패해도| AS\n```\n\n> [!TIP]\n> `after_script`는 Job의 성공/실패와 관계없이 **항상 실행**됩니다. 리소스 정리에 유용합니다.\n\n### 여러 줄 스크립트\n\n```yaml\njob:\n  script:\n    # 방법 1: 배열로 나열\n    - echo \"First command\"\n    - echo \"Second command\"\n    \n    # 방법 2: 리터럴 블록\n    - |\n      echo \"Multi-line\"\n      echo \"commands\"\n      if [ \"$DEBUG\" = \"true\" ]; then\n        echo \"Debug mode\"\n      fi\n    \n    # 방법 3: 폴딩 블록 (한 줄로 연결)\n    - >\n      curl -X POST\n      -H \"Content-Type: application/json\"\n      -d '{\"key\": \"value\"}'\n      https://api.example.com\n```\n\n---\n\n## Pipeline 트리거 방식\n\n파이프라인은 다양한 이벤트로 트리거됩니다.\n\n### 기본 트리거\n\n| 트리거 | 설명 |\n|--------|------|\n| `push` | 브랜치에 커밋 푸시 |\n| `merge_request_event` | MR 생성/업데이트 |\n| `schedule` | 스케줄 (cron) |\n| `web` | GitLab UI에서 수동 실행 |\n| `api` | API 호출 |\n| `trigger` | 다른 파이프라인에서 트리거 |\n\n### 트리거별 조건 분기\n\n```yaml\nbuild:\n  stage: build\n  script:\n    - npm run build\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"push\"\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n\ndeploy:\n  stage: deploy\n  script:\n    - deploy.sh\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      when: manual  # main 브랜치는 수동 배포\n```\n\n### Predefined Variables\n\nGitLab은 파이프라인 실행 시 [다양한 변수](https://docs.gitlab.com/ee/ci/variables/predefined_variables.html)를 자동으로 제공합니다:\n\n```yaml\njob:\n  script:\n    - echo \"Branch: $CI_COMMIT_BRANCH\"\n    - echo \"Commit SHA: $CI_COMMIT_SHA\"\n    - echo \"Project: $CI_PROJECT_NAME\"\n    - echo \"Pipeline Source: $CI_PIPELINE_SOURCE\"\n    - echo \"MR IID: $CI_MERGE_REQUEST_IID\"\n```\n\n---\n\n## 실전 예제: Node.js 풀스택 프로젝트\n\n```yaml\ndefault:\n  image: node:20-alpine\n\nstages:\n  - install\n  - build\n  - test\n  - deploy\n\n# 캐시 설정 (의존성 재사용)\n.node-cache:\n  cache:\n    key:\n      files:\n        - package-lock.json\n    paths:\n      - node_modules/\n    policy: pull\n\ninstall-deps:\n  stage: install\n  extends: .node-cache\n  cache:\n    policy: pull-push  # 캐시 저장\n  script:\n    - npm ci\n  artifacts:\n    paths:\n      - node_modules/\n    expire_in: 1 hour\n\nbuild-app:\n  stage: build\n  extends: .node-cache\n  script:\n    - npm run build\n  artifacts:\n    paths:\n      - dist/\n    expire_in: 1 week\n\nlint:\n  stage: test\n  extends: .node-cache\n  script:\n    - npm run lint\n  allow_failure: true  # 실패해도 파이프라인 계속\n\nunit-test:\n  stage: test\n  extends: .node-cache\n  script:\n    - npm run test:unit -- --coverage\n  coverage: '/All files\\s+\\|\\s+(\\d+\\.?\\d*)\\s*\\|/'\n  artifacts:\n    reports:\n      junit: junit.xml\n      coverage_report:\n        coverage_format: cobertura\n        path: coverage/cobertura-coverage.xml\n\ne2e-test:\n  stage: test\n  image: mcr.microsoft.com/playwright:v1.40.0\n  extends: .node-cache\n  script:\n    - npm run test:e2e\n  artifacts:\n    when: on_failure\n    paths:\n      - test-results/\n\ndeploy-staging:\n  stage: deploy\n  script:\n    - echo \"Deploying to staging...\"\n  environment:\n    name: staging\n    url: https://staging.example.com\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"develop\"\n\ndeploy-production:\n  stage: deploy\n  script:\n    - echo \"Deploying to production...\"\n  environment:\n    name: production\n    url: https://example.com\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      when: manual\n```\n\n---\n\n## Pipeline 시각화와 디버깅\n\n### GitLab UI에서 Pipeline 확인\n\n```mermaid\nflowchart TB\n    subgraph Pipeline [Pipeline 뷰]\n        direction LR\n        S1[install] --> S2[build] --> S3[test] --> S4[deploy]\n    end\n    \n    subgraph Jobs [Jobs 뷰]\n        J1[install-deps ✓]\n        J2[build-app ✓]\n        J3[lint ⚠]\n        J4[unit-test ✓]\n        J5[e2e-test ✓]\n        J6[deploy-staging 🔄]\n    end\n```\n\n### CI Lint\n\n`.gitlab-ci.yml` 문법을 검증하려면:\n\n1. **GitLab UI**: `CI/CD > Pipelines > CI lint`\n2. **API**:\n\n   ```bash\n   curl --header \"PRIVATE-TOKEN: $GITLAB_TOKEN\" \\\n     --data \"content=$(cat .gitlab-ci.yml)\" \\\n     \"https://gitlab.com/api/v4/ci/lint\"\n   ```\n\n### 디버그 로깅\n\n```yaml\nvariables:\n  CI_DEBUG_TRACE: \"true\"  # 상세 로그 출력\n\njob:\n  script:\n    - set -x  # 명령어 출력\n    - echo \"Debug info\"\n```\n\n> [!WARNING]\n> `CI_DEBUG_TRACE`는 **시크릿을 포함한 모든 변수**를 로그에 출력합니다. 프로덕션에서 사용 시 주의하세요.\n\n---\n\n## 정리\n\n| 개념 | 설명 |\n|-----|------|\n| `.gitlab-ci.yml` | 프로젝트 루트에 위치한 파이프라인 정의 파일 |\n| **Stage** | Jobs를 그룹화하고 실행 순서 정의 |\n| **Job** | 실제 작업 단위, 독립 환경에서 실행 |\n| **script** | Job에서 실행할 명령어 (필수) |\n| **Pipeline** | 트리거에 의해 생성되는 Stages/Jobs의 집합 |\n\n---\n\n## 다음 편 예고\n\n**2편: Variables와 Secrets 관리**에서는 다음을 다룹니다:\n\n- CI/CD Variables의 종류 (Predefined, Custom, Protected, Masked)\n- 프로젝트/그룹/인스턴스 레벨 변수\n- 변수 우선순위 (Precedence)\n- `.env` 파일과 dotenv artifacts\n- Vault, AWS Secrets Manager 연동\n\n---\n\n## 참고 자료\n\n- [GitLab CI/CD Documentation](https://docs.gitlab.com/ee/ci/)\n- [.gitlab-ci.yml Reference](https://docs.gitlab.com/ee/ci/yaml/)\n- [Predefined Variables](https://docs.gitlab.com/ee/ci/variables/predefined_variables.html)\n- [CI/CD Pipeline Configuration](https://docs.gitlab.com/ee/ci/pipelines/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitLab",
      "CI/CD",
      "Pipeline"
    ],
    "readingTime": 7,
    "wordCount": 1308,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitlab-ci",
    "date": "2026-01-05"
  },
  {
    "id": "k8s-06-security",
    "slug": "k8s-06-security",
    "path": "backend/kubernetes",
    "fullPath": "backend/kubernetes/k8s-06-security",
    "title": "Kubernetes 심화 시리즈 #6: 보안 완전 가이드",
    "excerpt": "RBAC, NetworkPolicy, Pod Security Standards(PSS)까지 Kubernetes 보안의 모든 것을 깊이 있게 다룹니다.",
    "content": "# Kubernetes 심화 시리즈 #6: 보안 완전 가이드\n\n## 시리즈 개요\n\n| # | 주제 | 핵심 내용 |\n|---|------|----------|\n| 1 | 워크로드 컨트롤러 심화 | Deployment, StatefulSet, DaemonSet, CronJob |\n| 2 | 서비스 네트워킹 심화 | Service 타입, kube-proxy, AWS ALB/NLB |\n| 3 | 설정 및 시크릿 관리 | ConfigMap, Secrets, AWS Secrets Manager CSI Driver |\n| 4 | Istio 서비스 메시 | VirtualService, DestinationRule, 와일드카드 서브도메인 |\n| 5 | 오토스케일링 심화 | HPA, VPA, Cluster Autoscaler, Karpenter, KEDA |\n| **6** | **보안 심화** | RBAC, NetworkPolicy, Pod Security Standards |\n\n---\n\n## Kubernetes 보안의 4가지 레이어\n\n```mermaid\nflowchart TB\n    subgraph Layer1 [\"1. 클러스터 보안\"]\n        API[\"API Server 인증/인가\"]\n        ETCD[\"etcd 암호화\"]\n    end\n    \n    subgraph Layer2 [\"2. 워크로드 보안\"]\n        PSS[\"Pod Security Standards\"]\n        SA[\"ServiceAccount\"]\n    end\n    \n    subgraph Layer3 [\"3. 네트워크 보안\"]\n        NP[\"NetworkPolicy\"]\n        mTLS[\"mTLS (Istio)\"]\n    end\n    \n    subgraph Layer4 [\"4. 런타임 보안\"]\n        Falco[\"Falco\"]\n        Seccomp[\"Seccomp\"]\n    end\n    \n    Layer1 --> Layer2 --> Layer3 --> Layer4\n```\n\n---\n\n## RBAC (Role-Based Access Control)\n\n### RBAC의 4가지 핵심 리소스\n\n```mermaid\nflowchart LR\n    subgraph Namespace [\"네임스페이스 범위\"]\n        Role[\"Role\"]\n        RB[\"RoleBinding\"]\n    end\n    \n    subgraph Cluster [\"클러스터 범위\"]\n        CR[\"ClusterRole\"]\n        CRB[\"ClusterRoleBinding\"]\n    end\n    \n    Role --> RB\n    CR --> CRB\n    CR --> RB\n```\n\n| 리소스 | 범위 | 용도 |\n|-------|-----|-----|\n| `Role` | 네임스페이스 | 특정 네임스페이스 내 권한 정의 |\n| `ClusterRole` | 클러스터 | 클러스터 전체 권한 정의 |\n| `RoleBinding` | 네임스페이스 | Role/ClusterRole을 주체에 바인딩 |\n| `ClusterRoleBinding` | 클러스터 | ClusterRole을 클러스터 전체에 바인딩 |\n\n### Role 정의\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: prod\n  name: pod-reader\nrules:\n- apiGroups: [\"\"]           # \"\" = core API group\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n  \n- apiGroups: [\"\"]\n  resources: [\"pods/log\"]   # 하위 리소스\n  verbs: [\"get\"]\n```\n\n### Verbs 완전 정리\n\n| Verb | 설명 | HTTP 메서드 |\n|------|-----|------------|\n| `get` | 단일 리소스 조회 | GET |\n| `list` | 리소스 목록 조회 | GET |\n| `watch` | 변경 감시 | GET (watch=true) |\n| `create` | 리소스 생성 | POST |\n| `update` | 리소스 전체 업데이트 | PUT |\n| `patch` | 리소스 부분 업데이트 | PATCH |\n| `delete` | 단일 리소스 삭제 | DELETE |\n| `deletecollection` | 여러 리소스 삭제 | DELETE |\n\n### ClusterRole 정의\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: secret-reader\nrules:\n# 모든 네임스페이스의 secrets 읽기\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n  \n# 특정 이름의 secrets만 (resourceNames)\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  resourceNames: [\"db-credentials\", \"api-key\"]\n  verbs: [\"get\"]\n  \n# 클러스터 범위 리소스\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"get\", \"list\"]\n  \n# 비-리소스 URL\n- nonResourceURLs: [\"/healthz\", \"/healthz/*\"]\n  verbs: [\"get\"]\n```\n\n### RoleBinding\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: prod\nsubjects:\n# 사용자\n- kind: User\n  name: jane\n  apiGroup: rbac.authorization.k8s.io\n  \n# 그룹\n- kind: Group\n  name: developers\n  apiGroup: rbac.authorization.k8s.io\n  \n# ServiceAccount\n- kind: ServiceAccount\n  name: app-sa\n  namespace: prod\n  \nroleRef:\n  kind: Role  # 또는 ClusterRole\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### 기본 ClusterRole 활용\n\nKubernetes는 기본 ClusterRole을 제공합니다:\n\n| ClusterRole | 설명 |\n|-------------|-----|\n| `cluster-admin` | 모든 권한 (슈퍼유저) |\n| `admin` | 네임스페이스 내 모든 권한 (RBAC 포함) |\n| `edit` | 대부분의 리소스 CRUD (RBAC 제외) |\n| `view` | 읽기 전용 (Secrets 제외) |\n\n```yaml\n# 개발자에게 특정 네임스페이스의 edit 권한 부여\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: developer-edit\n  namespace: dev\nsubjects:\n- kind: Group\n  name: developers\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: edit\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### 최소 권한 원칙 설계 패턴\n\n```mermaid\nflowchart TB\n    subgraph Prod [\"prod 네임스페이스\"]\n        ProdRole[\"Role: pod-viewer\\n(read-only)\"]\n        DevRB[\"RoleBinding\\n→ developers\"]\n    end\n    \n    subgraph Dev [\"dev 네임스페이스\"]\n        DevRole[\"ClusterRole: edit\\n(CRUD)\"]\n        DevRB2[\"RoleBinding\\n→ developers\"]\n    end\n    \n    subgraph Staging [\"staging 네임스페이스\"]\n        StagingRole[\"ClusterRole: edit\\n(CRUD)\"]\n        DevRB3[\"RoleBinding\\n→ developers\"]\n    end\n```\n\n```yaml\n# 1. 프로덕션: 읽기 전용\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: developers-view\n  namespace: prod\nsubjects:\n- kind: Group\n  name: developers\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: view\n  apiGroup: rbac.authorization.k8s.io\n---\n# 2. 개발/스테이징: 편집 권한\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: developers-edit\n  namespace: dev\nsubjects:\n- kind: Group\n  name: developers\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: edit\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### 권한 확인 명령어\n\n```bash\n# 현재 사용자 권한 확인\nkubectl auth can-i create pods --namespace=prod\n# yes 또는 no\n\n# 특정 사용자/SA 권한 확인\nkubectl auth can-i create pods --namespace=prod --as=system:serviceaccount:prod:app-sa\n\n# 모든 권한 나열\nkubectl auth can-i --list --namespace=prod\n```\n\n---\n\n## NetworkPolicy\n\n### NetworkPolicy 기본 개념\n\n> [!IMPORTANT]\n> NetworkPolicy는 **CNI 플러그인 지원이 필요**합니다. Calico, Cilium, Weave Net 등이 지원하며, AWS VPC CNI 단독으로는 지원하지 않습니다.\n\n```mermaid\nflowchart LR\n    subgraph Default [\"기본 상태 (정책 없음)\"]\n        A1[\"Pod A\"] <--> B1[\"Pod B\"]\n        A1 <--> C1[\"Pod C\"]\n        B1 <--> C1\n    end\n    \n    subgraph WithPolicy [\"NetworkPolicy 적용 후\"]\n        A2[\"Pod A\\n(app: frontend)\"]\n        B2[\"Pod B\\n(app: backend)\"]\n        C2[\"Pod C\\n(app: db)\"]\n        A2 --> B2\n        B2 --> C2\n        A2 -.-x C2\n    end\n```\n\n### 기본 NetworkPolicy 구조\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: backend-policy\n  namespace: prod\nspec:\n  # 이 정책이 적용될 Pod\n  podSelector:\n    matchLabels:\n      app: backend\n  \n  # 정책 유형 (Ingress, Egress, 또는 둘 다)\n  policyTypes:\n  - Ingress\n  - Egress\n  \n  # 인바운드 트래픽 규칙\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n  \n  # 아웃바운드 트래픽 규칙\n  egress:\n  - to:\n    - podSelector:\n        matchLabels:\n          app: database\n    ports:\n    - protocol: TCP\n      port: 5432\n```\n\n### from/to 셀렉터 유형\n\n```mermaid\nflowchart TB\n    subgraph Selectors [\"트래픽 소스/대상\"]\n        PS[\"podSelector\\n(같은 네임스페이스)\"]\n        NS[\"namespaceSelector\\n(다른 네임스페이스)\"]\n        IP[\"ipBlock\\n(CIDR 블록)\"]\n        COMBO[\"podSelector +\\nnamespaceSelector\\n(조합)\"]\n    end\n```\n\n| 셀렉터 | 설명 |\n|-------|-----|\n| `podSelector` | 같은 네임스페이스의 Pod |\n| `namespaceSelector` | 다른 네임스페이스의 모든 Pod |\n| `podSelector` + `namespaceSelector` | 특정 네임스페이스의 특정 Pod |\n| `ipBlock` | CIDR 기반 IP 범위 |\n\n### 셀렉터 조합 예시\n\n```yaml\ningress:\n# OR 관계 (배열의 각 항목)\n- from:\n  # 1. 같은 네임스페이스의 frontend Pod\n  - podSelector:\n      matchLabels:\n        app: frontend\n  \n  # 2. monitoring 네임스페이스의 모든 Pod\n  - namespaceSelector:\n      matchLabels:\n        name: monitoring\n  \n  # 3. 특정 IP 대역\n  - ipBlock:\n      cidr: 10.0.0.0/8\n      except:\n      - 10.1.0.0/16\n\n# AND 관계 (같은 항목 내)\n- from:\n  - namespaceSelector:\n      matchLabels:\n        env: prod\n    podSelector:\n      matchLabels:\n        role: client\n```\n\n> [!WARNING]\n> `podSelector`와 `namespaceSelector`가 **같은 `-` 아래**에 있으면 **AND** 조건입니다. **별도의 `-`**로 분리하면 **OR** 조건입니다.\n\n### Default Deny 정책 (Zero Trust 기반)\n\n```yaml\n# 1. 모든 인바운드 트래픽 차단 (기본)\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-ingress\n  namespace: prod\nspec:\n  podSelector: {}  # 모든 Pod에 적용\n  policyTypes:\n  - Ingress\n  # ingress 규칙 없음 = 모든 인바운드 차단\n---\n# 2. 모든 아웃바운드 트래픽 차단\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-egress\n  namespace: prod\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n  # DNS는 허용 (필수)\n  egress:\n  - to:\n    - namespaceSelector: {}\n      podSelector:\n        matchLabels:\n          k8s-app: kube-dns\n    ports:\n    - protocol: UDP\n      port: 53\n```\n\n### 실전 예시: 3-tier 앱 NetworkPolicy\n\n```mermaid\nflowchart LR\n    Internet[\"인터넷\"] --> FE[\"Frontend\\n(app: frontend)\"]\n    FE --> BE[\"Backend\\n(app: backend)\"]\n    BE --> DB[\"Database\\n(app: database)\"]\n    \n    FE -.-x DB\n    Internet -.-x BE\n    Internet -.-x DB\n```\n\n```yaml\n# Frontend: 외부에서만 접근 가능\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: frontend-policy\n  namespace: prod\nspec:\n  podSelector:\n    matchLabels:\n      app: frontend\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  # 외부(Ingress Controller)에서 접근\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: ingress-nginx\n    ports:\n    - port: 80\n  egress:\n  # Backend로만 나감\n  - to:\n    - podSelector:\n        matchLabels:\n          app: backend\n    ports:\n    - port: 8080\n  # DNS 허용\n  - to:\n    - namespaceSelector: {}\n      podSelector:\n        matchLabels:\n          k8s-app: kube-dns\n    ports:\n    - protocol: UDP\n      port: 53\n---\n# Backend: Frontend에서만 접근, Database로만 나감\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: backend-policy\n  namespace: prod\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n    ports:\n    - port: 8080\n  egress:\n  - to:\n    - podSelector:\n        matchLabels:\n          app: database\n    ports:\n    - port: 5432\n  - to:\n    - namespaceSelector: {}\n      podSelector:\n        matchLabels:\n          k8s-app: kube-dns\n    ports:\n    - protocol: UDP\n      port: 53\n---\n# Database: Backend에서만 접근, 외부 나감 차단\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: database-policy\n  namespace: prod\nspec:\n  podSelector:\n    matchLabels:\n      app: database\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: backend\n    ports:\n    - port: 5432\n  egress: []  # 아웃바운드 없음\n```\n\n---\n\n## Pod Security Standards (PSS)\n\n### PSS 개요\n\n> [!NOTE]\n> **Pod Security Admission**은 Kubernetes 1.25에서 GA가 되었으며, 더 이상 PodSecurityPolicy(PSP)를 사용하지 않습니다.\n\n### 3가지 보안 레벨\n\n```mermaid\nflowchart LR\n    subgraph Levels [\"Pod Security Standards\"]\n        P[\"Privileged\\n(제한 없음)\"]\n        B[\"Baseline\\n(기본 보안)\"]\n        R[\"Restricted\\n(최대 보안)\"]\n    end\n    \n    P --> |완화| B --> |완화| R\n```\n\n| 레벨 | 설명 | 차단 항목 |\n|-----|-----|----------|\n| `privileged` | 제한 없음 | 없음 |\n| `baseline` | 알려진 권한 상승 차단 | hostNetwork, hostPID, privileged 컨테이너 |\n| `restricted` | 최대 보안 | root 사용자, 쓰기 가능 루트 파일시스템 등 |\n\n### 3가지 적용 모드\n\n| 모드 | 설명 |\n|-----|-----|\n| `enforce` | 정책 위반 시 Pod 생성 차단 |\n| `audit` | 정책 위반 시 감사 로그 기록 |\n| `warn` | 정책 위반 시 경고 메시지 표시 |\n\n### 네임스페이스 라벨로 적용\n\n```bash\n# baseline 강제, restricted 경고/감사\nkubectl label namespace prod \\\n  pod-security.kubernetes.io/enforce=baseline \\\n  pod-security.kubernetes.io/enforce-version=latest \\\n  pod-security.kubernetes.io/warn=restricted \\\n  pod-security.kubernetes.io/warn-version=latest \\\n  pod-security.kubernetes.io/audit=restricted \\\n  pod-security.kubernetes.io/audit-version=latest\n```\n\n```yaml\n# 또는 YAML로 정의\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: prod\n  labels:\n    pod-security.kubernetes.io/enforce: baseline\n    pod-security.kubernetes.io/enforce-version: latest\n    pod-security.kubernetes.io/warn: restricted\n    pod-security.kubernetes.io/warn-version: latest\n    pod-security.kubernetes.io/audit: restricted\n    pod-security.kubernetes.io/audit-version: latest\n```\n\n### 클러스터 전체 기본값 설정\n\n```yaml\n# /etc/kubernetes/pss-config.yaml\napiVersion: apiserver.config.k8s.io/v1\nkind: AdmissionConfiguration\nplugins:\n- name: PodSecurity\n  configuration:\n    apiVersion: pod-security.admission.config.k8s.io/v1\n    kind: PodSecurityConfiguration\n    defaults:\n      enforce: \"baseline\"\n      enforce-version: \"latest\"\n      audit: \"restricted\"\n      audit-version: \"latest\"\n      warn: \"restricted\"\n      warn-version: \"latest\"\n    exemptions:\n      usernames: []\n      runtimeClasses: []\n      namespaces:\n      - kube-system\n      - kube-public\n```\n\n### Restricted 레벨 Pod 예시\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secure-pod\nspec:\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  \n  containers:\n  - name: app\n    image: my-app:latest\n    securityContext:\n      allowPrivilegeEscalation: false\n      readOnlyRootFilesystem: true\n      runAsNonRoot: true\n      runAsUser: 1000\n      capabilities:\n        drop:\n        - ALL\n    \n    # 쓰기 필요한 디렉토리는 emptyDir 사용\n    volumeMounts:\n    - name: tmp\n      mountPath: /tmp\n    - name: cache\n      mountPath: /app/cache\n  \n  volumes:\n  - name: tmp\n    emptyDir: {}\n  - name: cache\n    emptyDir: {}\n```\n\n---\n\n## ServiceAccount 보안\n\n### 자동 마운트 비활성화\n\n모든 Pod에는 기본적으로 ServiceAccount 토큰이 마운트됩니다. 필요 없다면 비활성화하세요.\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  automountServiceAccountToken: false\n  containers:\n  - name: app\n    image: my-app:latest\n```\n\n### 전용 ServiceAccount 사용\n\n```yaml\n# 최소 권한 ServiceAccount\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: app-sa\n  namespace: prod\nautomountServiceAccountToken: false  # 기본 비활성화\n---\n# 필요한 권한만 부여\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: app-role\n  namespace: prod\nrules:\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  resourceNames: [\"app-config\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: app-rolebinding\n  namespace: prod\nsubjects:\n- kind: ServiceAccount\n  name: app-sa\n  namespace: prod\nroleRef:\n  kind: Role\n  name: app-role\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### AWS IRSA (IAM Roles for Service Accounts)\n\n```yaml\n# ServiceAccount에 IAM Role 연결\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: s3-reader\n  namespace: prod\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/S3ReaderRole\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: s3-reader-pod\n  namespace: prod\nspec:\n  serviceAccountName: s3-reader\n  containers:\n  - name: app\n    image: amazon/aws-cli\n    command: [\"aws\", \"s3\", \"ls\"]\n    # AWS SDK가 자동으로 IRSA 토큰 사용\n```\n\n```mermaid\nsequenceDiagram\n    participant Pod as Pod\n    participant IRSA as AWS STS\n    participant S3 as AWS S3\n    \n    Pod->>Pod: SA 토큰 마운트됨\\n(/var/run/secrets/eks.amazonaws.com/serviceaccount/token)\n    Pod->>IRSA: AssumeRoleWithWebIdentity\\n(SA 토큰 전달)\n    IRSA-->>Pod: 임시 AWS 자격 증명\n    Pod->>S3: S3 API 호출\\n(임시 자격 증명 사용)\n    S3-->>Pod: 응답\n```\n\n---\n\n## 트러블슈팅 가이드\n\n### RBAC 권한 오류\n\n```bash\n# 에러: User \"jane\" cannot list pods in namespace \"prod\"\n\n# 1. 권한 확인\nkubectl auth can-i list pods --namespace=prod --as=jane\n\n# 2. RoleBinding 확인\nkubectl get rolebindings -n prod -o wide\n\n# 3. ClusterRoleBinding 확인\nkubectl get clusterrolebindings -o wide | grep jane\n\n# 4. 자세한 권한 조회\nkubectl auth can-i --list --namespace=prod --as=jane\n```\n\n### NetworkPolicy가 작동하지 않음\n\n```bash\n# 1. CNI 플러그인 확인 (Calico, Cilium 등)\nkubectl get pods -n kube-system | grep calico\n\n# 2. NetworkPolicy 확인\nkubectl get networkpolicy -n prod\n\n# 3. Pod 라벨 확인\nkubectl get pods -n prod --show-labels\n\n# 4. 정책 상세 확인\nkubectl describe networkpolicy backend-policy -n prod\n```\n\n**흔한 원인**:\n\n1. **CNI 미지원**: AWS VPC CNI 단독은 NetworkPolicy 미지원\n2. **라벨 불일치**: podSelector/namespaceSelector 라벨 오타\n3. **DNS 차단**: egress에서 DNS(UDP 53) 미허용\n\n### Pod Security 위반\n\n```bash\n# 경고 메시지 예시\n# Warning: would violate PodSecurity \"restricted:latest\"\n\n# 1. 네임스페이스 라벨 확인\nkubectl get ns prod --show-labels\n\n# 2. Pod 보안 컨텍스트 확인\nkubectl get pod my-pod -o yaml | grep -A 20 securityContext\n\n# 3. 위반 항목 확인 (dry-run)\nkubectl apply -f pod.yaml --dry-run=server\n```\n\n**흔한 원인**:\n\n1. **runAsNonRoot 미설정**\n2. **allowPrivilegeEscalation: true**\n3. **capabilities.drop: ALL 미설정**\n\n---\n\n## 정리\n\n| 보안 레이어 | 도구 | 핵심 기능 |\n|-----------|-----|----------|\n| **인증/인가** | RBAC | Role, ClusterRole, 최소 권한 |\n| **네트워크** | NetworkPolicy | ingress/egress 제어, Zero Trust |\n| **워크로드** | PSS | Privileged/Baseline/Restricted |\n| **ID 관리** | ServiceAccount | IRSA, 토큰 마운트 제어 |\n\n---\n\n## 시리즈 마무리\n\n이 시리즈에서 다룬 핵심 내용:\n\n| 편 | 주제 | 핵심 키워드 |\n|---|------|----------|\n| 1편 | 워크로드 컨트롤러 | Reconciliation Loop, Rolling Update, StatefulSet |\n| 2편 | 서비스 네트워킹 | ClusterIP/NodePort/LoadBalancer, nftables |\n| 3편 | 설정/시크릿 관리 | ConfigMap, Secrets at Rest, CSI Driver |\n| 4편 | Istio 서비스 메시 | VirtualService, DestinationRule, mTLS |\n| 5편 | 오토스케일링 | HPA v2, VPA, Karpenter, KEDA |\n| 6편 | 보안 | RBAC, NetworkPolicy, PSS, IRSA |\n\n---\n\n## 참고 자료\n\n- [Kubernetes RBAC](https://kubernetes.io/docs/reference/access-authn-authz/rbac/)\n- [Network Policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/)\n- [Pod Security Standards](https://kubernetes.io/docs/concepts/security/pod-security-standards/)\n- [Pod Security Admission](https://kubernetes.io/docs/concepts/security/pod-security-admission/)\n- [AWS IRSA](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "Kubernetes",
      "AWS",
      "Security"
    ],
    "readingTime": 11,
    "wordCount": 2159,
    "isFeatured": false,
    "isPublic": true,
    "series": "kubernetes",
    "date": "2026-01-04"
  },
  {
    "id": "k8s-05-autoscaling",
    "slug": "k8s-05-autoscaling",
    "path": "backend/kubernetes",
    "fullPath": "backend/kubernetes/k8s-05-autoscaling",
    "title": "Kubernetes 심화 시리즈 #5: 오토스케일링 완전 가이드",
    "excerpt": "HPA v2, VPA, Cluster Autoscaler, Karpenter, KEDA까지 Kubernetes 오토스케일링의 모든 것을 깊이 있게 다룹니다.",
    "content": "# Kubernetes 심화 시리즈 #5: 오토스케일링 완전 가이드\n\n## 시리즈 개요\n\n| # | 주제 | 핵심 내용 |\n|---|------|----------|\n| 1 | 워크로드 컨트롤러 심화 | Deployment, StatefulSet, DaemonSet, CronJob |\n| 2 | 서비스 네트워킹 심화 | Service 타입, kube-proxy, AWS ALB/NLB |\n| 3 | 설정 및 시크릿 관리 | ConfigMap, Secrets, AWS Secrets Manager CSI Driver |\n| 4 | Istio 서비스 메시 | VirtualService, DestinationRule, 와일드카드 서브도메인 |\n| **5** | **오토스케일링 심화** | HPA, VPA, Cluster Autoscaler, Karpenter, KEDA |\n| 6 | 보안 심화 | RBAC, NetworkPolicy, Pod Security Standards |\n\n---\n\n## 오토스케일링의 3가지 축\n\nKubernetes 오토스케일링은 **무엇을 스케일링하느냐**에 따라 세 가지로 구분됩니다.\n\n```mermaid\nflowchart TB\n    subgraph Horizontal [\"수평 스케일링 (HPA)\"]\n        H1[\"Pod 1\"]\n        H2[\"Pod 2\"]\n        H3[\"Pod 3\"]\n        H4[\"Pod 4 (신규)\"]\n    end\n    \n    subgraph Vertical [\"수직 스케일링 (VPA)\"]\n        V1[\"CPU: 100m → 500m\\nMemory: 128Mi → 512Mi\"]\n    end\n    \n    subgraph Cluster [\"클러스터 스케일링\"]\n        N1[\"Node 1\"]\n        N2[\"Node 2\"]\n        N3[\"Node 3 (신규)\"]\n    end\n```\n\n| 스케일링 유형 | 대상 | 도구 |\n|-------------|-----|------|\n| **수평 (Horizontal)** | Pod 개수 | HPA, KEDA |\n| **수직 (Vertical)** | Pod 리소스 (CPU/Memory) | VPA |\n| **클러스터** | Node 개수 | Cluster Autoscaler, Karpenter |\n\n---\n\n## HPA (HorizontalPodAutoscaler)\n\n### HPA v2: 현재 Stable API\n\n> [!IMPORTANT]\n> HPA는 현재 `autoscaling/v2`가 stable입니다. `v2beta2`는 deprecated되었습니다.\n\n### 기본 설정: CPU 기반\n\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: api-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: api-server\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70  # 평균 CPU 사용률 70%\n```\n\n### 다중 메트릭: CPU + Memory\n\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: api-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: api-server\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  # CPU 기반\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  # Memory 기반\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: AverageValue\n        averageValue: 500Mi\n```\n\n> [!NOTE]\n> 다중 메트릭 사용 시 HPA는 **가장 큰 replica 수를 요구하는 메트릭**을 기준으로 스케일링합니다.\n\n### 메트릭 타입 완전 정리\n\n```mermaid\nflowchart LR\n    subgraph Metrics [\"HPA 메트릭 타입\"]\n        R[\"Resource\\n(CPU, Memory)\"]\n        P[\"Pods\\n(커스텀 Pod 메트릭)\"]\n        O[\"Object\\n(다른 K8s 오브젝트)\"]\n        E[\"External\\n(외부 시스템)\"]\n    end\n    \n    R --> MS[\"Metrics Server\"]\n    P --> PA[\"Prometheus Adapter\"]\n    O --> PA\n    E --> PA\n```\n\n| 타입 | 설명 | 예시 |\n|-----|-----|-----|\n| `Resource` | CPU, Memory (Metrics Server 필요) | 평균 CPU 70% |\n| `Pods` | Pod당 커스텀 메트릭 | requests-per-second |\n| `Object` | 다른 K8s 오브젝트의 메트릭 | Ingress의 requests-per-second |\n| `External` | 외부 시스템 메트릭 | SQS 큐 메시지 수 |\n\n### External 메트릭 예시: AWS SQS\n\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: worker-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: worker\n  minReplicas: 1\n  maxReplicas: 50\n  metrics:\n  - type: External\n    external:\n      metric:\n        name: sqs_messages_visible\n        selector:\n          matchLabels:\n            queue_name: \"order-processing\"\n      target:\n        type: AverageValue\n        averageValue: \"30\"  # Pod당 30개 메시지 처리\n```\n\n### Scaling Behavior: 급격한 스케일링 방지\n\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: api-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: api-server\n  minReplicas: 2\n  maxReplicas: 100\n  \n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300  # 5분간 안정화\n      policies:\n      - type: Percent\n        value: 10        # 최대 10%씩 감소\n        periodSeconds: 60\n      selectPolicy: Max\n      \n    scaleUp:\n      stabilizationWindowSeconds: 0  # 즉시 스케일 업\n      policies:\n      - type: Percent\n        value: 100       # 최대 100%씩 증가\n        periodSeconds: 15\n      - type: Pods\n        value: 4         # 또는 최대 4개씩 증가\n        periodSeconds: 15\n      selectPolicy: Max\n  \n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n```\n\n```mermaid\nsequenceDiagram\n    participant HPA as HPA Controller\n    participant Metrics as Metrics Server\n    participant Deploy as Deployment\n    \n    loop 매 15초\n        HPA->>Metrics: 메트릭 조회\n        Metrics-->>HPA: CPU 사용률 반환\n        \n        alt CPU > 70%\n            HPA->>Deploy: Scale Up (behavior 정책 적용)\n        else CPU < 70%\n            Note over HPA: stabilizationWindow 대기\n            HPA->>Deploy: Scale Down (10%씩)\n        end\n    end\n```\n\n---\n\n## VPA (VerticalPodAutoscaler)\n\n### VPA 설치 (별도 설치 필요)\n\n> [!CAUTION]\n> VPA는 Kubernetes 기본 제공이 아닙니다. 별도 설치가 필요합니다.\n\n```bash\n# VPA 설치 (공식 리포지토리)\ngit clone https://github.com/kubernetes/autoscaler.git\ncd autoscaler/vertical-pod-autoscaler\n./hack/vpa-up.sh\n```\n\n### VPA 4가지 모드\n\n```mermaid\nflowchart TB\n    subgraph Modes [\"VPA 모드\"]\n        Auto[\"Auto / Recreate\\nPod 재생성으로 리소스 변경\"]\n        Initial[\"Initial\\n생성 시에만 적용\"]\n        Off[\"Off\\n추천만 제공\"]\n    end\n    \n    Auto --> |Pod 삭제 후 재생성| Pod1[\"새 Pod\\n(큰 리소스)\"]\n    Initial --> |최초 생성 시| Pod2[\"Pod\\n(추천 리소스)\"]\n    Off --> |변경 없음| Pod3[\"기존 Pod\"]\n```\n\n| 모드 | 동작 | 사용 사례 |\n|-----|-----|----------|\n| `Auto` | Pod 재생성으로 리소스 변경 | 일반적인 워크로드 |\n| `Recreate` | Auto와 동일 | Auto의 별칭 |\n| `Initial` | 생성 시에만 적용 | StatefulSet, 중단 민감 워크로드 |\n| `Off` | 추천만 제공, 변경 없음 | 모니터링 용도 |\n\n### VPA 설정 예시\n\n```yaml\napiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: api-vpa\nspec:\n  targetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: api-server\n  \n  updatePolicy:\n    updateMode: \"Auto\"  # Auto, Recreate, Initial, Off\n  \n  resourcePolicy:\n    containerPolicies:\n    - containerName: api\n      # 리소스 범위 제한\n      minAllowed:\n        cpu: 100m\n        memory: 128Mi\n      maxAllowed:\n        cpu: 2\n        memory: 4Gi\n      # 특정 리소스만 조정\n      controlledResources: [\"cpu\", \"memory\"]\n```\n\n### VPA 추천 확인\n\n```bash\nkubectl describe vpa api-vpa\n\n# Status:\n#   Recommendation:\n#     Container Recommendations:\n#       Container Name: api\n#       Lower Bound:\n#         Cpu:     25m\n#         Memory:  128Mi\n#       Target:\n#         Cpu:     100m    # 추천 CPU\n#         Memory:  256Mi   # 추천 Memory\n#       Upper Bound:\n#         Cpu:     1\n#         Memory:  1Gi\n```\n\n### In-place Pod Resizing (K8s 1.33+)\n\n> [!TIP]\n> Kubernetes 1.33부터 **In-place Pod Resizing**이 beta로 기본 활성화됩니다. Pod를 재생성하지 않고 리소스를 변경할 수 있습니다. 단, 2024년 기준 VPA는 아직 In-place Resizing을 지원하지 않으며, 통합이 진행 중입니다.\n\n```yaml\n# 수동 In-place Resizing 예시\napiVersion: v1\nkind: Pod\nmetadata:\n  name: api-pod\nspec:\n  containers:\n  - name: api\n    image: api:latest\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 512Mi\n    resizePolicy:        # K8s 1.33+ 필요\n    - resourceName: cpu\n      restartPolicy: NotRequired  # 재시작 없이 변경\n    - resourceName: memory\n      restartPolicy: RestartContainer  # 메모리는 재시작 필요\n```\n\n### HPA + VPA 동시 사용 시 주의\n\n> [!WARNING]\n> HPA와 VPA를 **같은 메트릭(CPU/Memory)**으로 동시에 사용하면 충돌이 발생합니다. HPA가 Pod 수를 늘리면 VPA가 리소스를 줄이고, 다시 HPA가 Pod를 늘리는 악순환이 발생할 수 있습니다.\n\n**권장 패턴**:\n\n- HPA: Custom/External 메트릭 (requests-per-second, 큐 길이)\n- VPA: CPU/Memory (Off 모드로 추천만 활용)\n\n---\n\n## Cluster Autoscaler vs Karpenter\n\n### 공식 지원 Node Autoscaler\n\n> [!NOTE]\n> Kubernetes SIG Autoscaling은 **Cluster Autoscaler**와 **Karpenter** 두 가지를 공식 지원합니다.\n\n```mermaid\nflowchart TB\n    subgraph CA [\"Cluster Autoscaler\"]\n        NG1[\"Node Group 1\\n(m5.large)\"]\n        NG2[\"Node Group 2\\n(m5.xlarge)\"]\n        NG3[\"Node Group 3\\n(c5.large)\"]\n    end\n    \n    subgraph KP [\"Karpenter\"]\n        NP[\"NodePool\\n(constraints만 정의)\"]\n        N1[\"m5.large\\n(자동 선택)\"]\n        N2[\"c5.xlarge\\n(자동 선택)\"]\n        N3[\"r5.2xlarge\\n(자동 선택)\"]\n    end\n```\n\n### 핵심 차이점\n\n| 특성 | Cluster Autoscaler | Karpenter |\n|-----|-------------------|-----------|\n| **프로비저닝** | Node Group 사전 정의 필요 | 자동 프로비저닝 (constraints만) |\n| **인스턴스 선택** | Node Group당 고정 | 워크로드에 맞게 자동 선택 |\n| **스케일 속도** | 느림 (Node Group 확인) | 빠름 (직접 EC2 API 호출) |\n| **Node Lifecycle** | 스케일링만 | Disruption, 자동 업그레이드 |\n| **클라우드 지원** | 다양 (AWS, GCP, Azure 등) | AWS, Azure |\n| **Spot 통합** | 별도 Node Group 필요 | NodePool에서 바로 설정 |\n\n### Karpenter 설정 예시 (AWS)\n\n```yaml\n# NodePool 정의\napiVersion: karpenter.sh/v1\nkind: NodePool\nmetadata:\n  name: default\nspec:\n  template:\n    spec:\n      requirements:\n      - key: kubernetes.io/arch\n        operator: In\n        values: [\"amd64\", \"arm64\"]\n      - key: karpenter.sh/capacity-type\n        operator: In\n        values: [\"spot\", \"on-demand\"]  # Spot 우선\n      - key: karpenter.k8s.aws/instance-category\n        operator: In\n        values: [\"c\", \"m\", \"r\"]\n      - key: karpenter.k8s.aws/instance-size\n        operator: In\n        values: [\"medium\", \"large\", \"xlarge\", \"2xlarge\"]\n      \n      nodeClassRef:\n        group: karpenter.k8s.aws\n        kind: EC2NodeClass\n        name: default\n  \n  # 자동 정리 (Disruption)\n  disruption:\n    consolidationPolicy: WhenEmptyOrUnderutilized\n    consolidateAfter: 1m\n  \n  limits:\n    cpu: 1000\n    memory: 1000Gi\n---\n# EC2NodeClass 정의\napiVersion: karpenter.k8s.aws/v1\nkind: EC2NodeClass\nmetadata:\n  name: default\nspec:\n  amiFamily: AL2023\n  subnetSelectorTerms:\n  - tags:\n      karpenter.sh/discovery: my-cluster\n  securityGroupSelectorTerms:\n  - tags:\n      karpenter.sh/discovery: my-cluster\n  role: KarpenterNodeRole-my-cluster\n```\n\n### Karpenter Disruption (자동 최적화)\n\n```mermaid\nsequenceDiagram\n    participant KP as Karpenter\n    participant Node as Under-utilized Node\n    participant Pod as Pods\n    participant NewNode as Optimal Node\n    \n    Note over KP: consolidateAfter: 1m 경과\n    KP->>Node: Node 활용도 분석\n    Note over KP: 사용률 낮음 감지\n    \n    KP->>NewNode: 최적 크기 Node 프로비저닝\n    KP->>Pod: Pod 이동 (drain)\n    Pod->>NewNode: Pod 스케줄링\n    KP->>Node: Node 종료\n    Note over KP: 비용 최적화 완료\n```\n\n---\n\n## KEDA (Kubernetes Event-driven Autoscaling)\n\n### KEDA란?\n\nKEDA는 HPA를 확장하여 **이벤트 기반 스케일링**을 제공합니다. 60개 이상의 트리거를 지원합니다.\n\n```mermaid\nflowchart LR\n    subgraph Triggers [\"KEDA 트리거\"]\n        SQS[\"AWS SQS\"]\n        Kafka[\"Kafka\"]\n        Prometheus[\"Prometheus\"]\n        Cron[\"Cron\"]\n        HTTP[\"HTTP\"]\n    end\n    \n    subgraph KEDA\n        SO[\"ScaledObject\"]\n        HPA[\"HPA (자동 생성)\"]\n    end\n    \n    subgraph Target\n        Deploy[\"Deployment\"]\n    end\n    \n    Triggers --> SO --> HPA --> Deploy\n```\n\n### KEDA 설치\n\n```bash\n# Helm으로 설치\nhelm repo add kedacore https://kedacore.github.io/charts\nhelm install keda kedacore/keda --namespace keda --create-namespace\n```\n\n### ScaledObject: AWS SQS 예시\n\n```yaml\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: order-processor-scaler\nspec:\n  scaleTargetRef:\n    name: order-processor\n  \n  minReplicaCount: 0   # 0까지 스케일 다운 가능!\n  maxReplicaCount: 100\n  \n  pollingInterval: 30  # 30초마다 확인\n  cooldownPeriod: 300  # 5분 대기 후 스케일 다운\n  \n  triggers:\n  - type: aws-sqs-queue\n    metadata:\n      queueURL: https://sqs.ap-northeast-2.amazonaws.com/123456789012/orders\n      queueLength: \"10\"  # 메시지 10개당 1개 Pod\n      awsRegion: ap-northeast-2\n    authenticationRef:\n      name: keda-aws-credentials\n---\n# 인증 설정\napiVersion: keda.sh/v1alpha1\nkind: TriggerAuthentication\nmetadata:\n  name: keda-aws-credentials\nspec:\n  podIdentity:\n    provider: aws-eks  # IRSA 사용\n```\n\n### ScaledObject: Kafka 예시\n\n```yaml\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: kafka-consumer-scaler\nspec:\n  scaleTargetRef:\n    name: kafka-consumer\n  minReplicaCount: 1\n  maxReplicaCount: 50\n  \n  triggers:\n  - type: kafka\n    metadata:\n      bootstrapServers: kafka.default.svc:9092\n      consumerGroup: my-consumer-group\n      topic: events\n      lagThreshold: \"100\"  # lag 100 초과 시 스케일 업\n```\n\n### ScaledObject: Cron (시간 기반)\n\n```yaml\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: business-hours-scaler\nspec:\n  scaleTargetRef:\n    name: api-server\n  minReplicaCount: 2\n  maxReplicaCount: 20\n  \n  triggers:\n  # 업무 시간 (9시-18시 평일)\n  - type: cron\n    metadata:\n      timezone: Asia/Seoul\n      start: 0 9 * * 1-5   # 월-금 9시\n      end: 0 18 * * 1-5    # 월-금 18시\n      desiredReplicas: \"10\"\n  \n  # 비업무 시간\n  - type: cron\n    metadata:\n      timezone: Asia/Seoul\n      start: 0 18 * * 1-5\n      end: 0 9 * * 2-6\n      desiredReplicas: \"2\"\n```\n\n### KEDA의 핵심 장점\n\n| 기능 | HPA | KEDA |\n|-----|-----|------|\n| **0으로 스케일 다운** | ❌ (min 1) | ✅ |\n| **외부 메트릭 연동** | 복잡 (Adapter 필요) | 간단 (내장) |\n| **이벤트 기반** | ❌ | ✅ |\n| **Cron 스케줄링** | ❌ | ✅ |\n\n---\n\n## 트러블슈팅 가이드\n\n### HPA가 스케일링하지 않음\n\n```bash\n# 1. HPA 상태 확인\nkubectl get hpa api-hpa\n\n# TARGETS가 <unknown>인 경우\n# NAME      REFERENCE             TARGETS         MINPODS   MAXPODS\n# api-hpa   Deployment/api-server <unknown>/70%   2         10\n\n# 2. Metrics Server 확인\nkubectl get deployment metrics-server -n kube-system\n\n# 3. Pod의 resource requests 확인 (필수!)\nkubectl get pod api-xxx -o yaml | grep -A 5 resources\n```\n\n**흔한 원인**:\n\n1. **Metrics Server 미설치**\n2. **Pod에 resource requests 미설정** (HPA 작동 불가)\n3. **메트릭 수집 지연** (15초 주기)\n\n### VPA 추천이 적용되지 않음\n\n```bash\n# 1. VPA 상태 확인\nkubectl describe vpa api-vpa\n\n# 2. updateMode 확인\n# Off 모드는 추천만 제공\n\n# 3. Pod 재생성 여부 확인\nkubectl get pods -w\n```\n\n**흔한 원인**:\n\n1. **updateMode: Off** 설정\n2. **minAllowed/maxAllowed 범위 밖**\n3. **PDB(PodDisruptionBudget)로 인한 재생성 차단**\n\n### Karpenter Node가 프로비저닝되지 않음\n\n```bash\n# 1. Karpenter 로그 확인\nkubectl logs -n karpenter -l app.kubernetes.io/name=karpenter\n\n# 2. NodePool 상태 확인\nkubectl describe nodepool default\n\n# 3. Pending Pod 확인\nkubectl get pods --field-selector=status.phase=Pending\n```\n\n**흔한 원인**:\n\n1. **requirements 충족 인스턴스 타입 없음**\n2. **서브넷/보안 그룹 태그 누락**\n3. **IAM 권한 부족**\n\n---\n\n## 정리\n\n| 도구 | 스케일링 대상 | 핵심 기능 |\n|-----|-------------|----------|\n| **HPA** | Pod 수 | Resource/Custom/External 메트릭 |\n| **VPA** | Pod 리소스 | 리소스 추천 및 자동 조정 |\n| **Cluster Autoscaler** | Node 수 | Node Group 기반 |\n| **Karpenter** | Node 수 | 자동 프로비저닝, Disruption |\n| **KEDA** | Pod 수 | 이벤트 기반, 0 스케일 다운 |\n\n---\n\n## 다음 편 예고\n\n**6편: 보안 심화**에서는 다음을 다룹니다:\n\n- RBAC 설계 패턴 (최소 권한 원칙)\n- NetworkPolicy로 Zero Trust 구현\n- Pod Security Standards (PSS)\n- ServiceAccount 보안과 IRSA\n\n---\n\n## 참고 자료\n\n- [Kubernetes HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)\n- [Kubernetes VPA](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler)\n- [Cluster Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler)\n- [Karpenter](https://karpenter.sh/)\n- [KEDA](https://keda.sh/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "Kubernetes",
      "AWS",
      "Autoscaling"
    ],
    "readingTime": 10,
    "wordCount": 1929,
    "isFeatured": false,
    "isPublic": true,
    "series": "kubernetes",
    "date": "2026-01-04"
  },
  {
    "id": "k8s-04-istio-service-mesh",
    "slug": "k8s-04-istio-service-mesh",
    "path": "backend/kubernetes",
    "fullPath": "backend/kubernetes/k8s-04-istio-service-mesh",
    "title": "Kubernetes 심화 시리즈 #4: Istio 서비스 메시 완전 가이드",
    "excerpt": "Istio의 VirtualService, DestinationRule, Gateway를 깊이 있게 다루며, 와일드카드 서브도메인 + AWS Route 53/ACM 연동까지 실전 패턴을 알아봅니다.",
    "content": "# Kubernetes 심화 시리즈 #4: Istio 서비스 메시 완전 가이드\n\n## 시리즈 개요\n\n| # | 주제 | 핵심 내용 |\n|---|------|----------|\n| 1 | 워크로드 컨트롤러 심화 | Deployment, StatefulSet, DaemonSet, CronJob |\n| 2 | 서비스 네트워킹 심화 | Service 타입, kube-proxy, AWS ALB/NLB |\n| 3 | 설정 및 시크릿 관리 | ConfigMap, Secrets, AWS Secrets Manager CSI Driver |\n| **4** | **Istio 서비스 메시** | VirtualService, DestinationRule, 와일드카드 서브도메인 |\n| 5 | 오토스케일링 심화 | HPA, VPA, Cluster Autoscaler, Karpenter, KEDA |\n| 6 | 보안 심화 | RBAC, NetworkPolicy, Pod Security Standards |\n\n---\n\n## 왜 서비스 메시인가?\n\n마이크로서비스 아키텍처에서는 서비스 간 통신이 핵심입니다. 하지만 각 서비스에 다음을 구현하면:\n\n```\n❌ 재시도/타임아웃 로직 중복\n❌ 서킷 브레이커 각각 구현\n❌ mTLS 직접 관리\n❌ 트래픽 분할/카나리 배포 어려움\n❌ 분산 추적 연동 복잡\n```\n\n**서비스 메시**는 이러한 **횡단 관심사(Cross-cutting Concerns)**를 애플리케이션에서 분리하여 인프라 레이어에서 처리합니다.\n\n```mermaid\nflowchart TB\n    subgraph Without [서비스 메시 없이]\n        A1[Service A<br/>+ 재시도 로직<br/>+ 서킷 브레이커<br/>+ TLS 관리]\n        B1[Service B<br/>+ 재시도 로직<br/>+ 서킷 브레이커<br/>+ TLS 관리]\n        A1 <--> B1\n    end\n    \n    subgraph With [서비스 메시 사용]\n        A2[Service A]\n        EA[Envoy Proxy]\n        B2[Service B]\n        EB[Envoy Proxy]\n        A2 --- EA <-->|mTLS, 재시도, 서킷 브레이커| EB --- B2\n    end\n```\n\n---\n\n## Istio 아키텍처\n\n### Control Plane vs Data Plane\n\n```mermaid\nflowchart TB\n    subgraph ControlPlane [Control Plane]\n        Istiod[istiod<br/>- Pilot: 설정 배포<br/>- Citadel: 인증서 관리<br/>- Galley: 설정 검증]\n    end\n    \n    subgraph DataPlane [Data Plane]\n        subgraph PodA [Service A Pod]\n            AppA[Application]\n            EnvoyA[Envoy Sidecar]\n        end\n        \n        subgraph PodB [Service B Pod]\n            AppB[Application]\n            EnvoyB[Envoy Sidecar]\n        end\n        \n        subgraph PodC [Service C Pod]\n            AppC[Application]\n            EnvoyC[Envoy Sidecar]\n        end\n    end\n    \n    subgraph IngressEgress [Ingress/Egress]\n        IG[Ingress Gateway<br/>Envoy]\n        EG[Egress Gateway<br/>Envoy]\n    end\n    \n    Istiod -->|xDS 프로토콜| EnvoyA & EnvoyB & EnvoyC & IG & EG\n    \n    External[외부 트래픽] --> IG --> EnvoyA\n    EnvoyA <--> EnvoyB <--> EnvoyC\n    EnvoyC --> EG --> ExtService[외부 서비스]\n```\n\n### Envoy Sidecar 자동 주입\n\n```yaml\n# 네임스페이스에 라벨 추가\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-app\n  labels:\n    istio-injection: enabled  # 자동 sidecar 주입\n```\n\n모든 Pod에 Envoy 컨테이너가 자동으로 추가됩니다:\n\n```yaml\n# 실제 생성된 Pod 확인\nkubectl get pod my-app-xxx -o yaml\n\n# 컨테이너 2개: app, istio-proxy\ncontainers:\n- name: my-app\n  image: my-app:v1\n- name: istio-proxy\n  image: docker.io/istio/proxyv2:1.20.0\n```\n\n---\n\n## Gateway: 외부 트래픽 진입점\n\n### 기본 Gateway 설정\n\n```yaml\napiVersion: networking.istio.io/v1\nkind: Gateway\nmetadata:\n  name: my-gateway\n  namespace: istio-system\nspec:\n  selector:\n    istio: ingressgateway  # Istio Ingress Gateway Pod 선택\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"api.example.com\"\n    - \"dashboard.example.com\"\n```\n\n### HTTPS + AWS ACM 연동\n\n```yaml\napiVersion: networking.istio.io/v1\nkind: Gateway\nmetadata:\n  name: secure-gateway\n  namespace: istio-system\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-tls-secret  # Kubernetes TLS Secret\n    hosts:\n    - \"*.example.com\"  # 와일드카드!\n```\n\n---\n\n## 와일드카드 서브도메인 라우팅 (실전 패턴)\n\n여러 서비스를 `api.example.com`, `dashboard.example.com`, `admin.example.com` 형태로 쉽게 배포할 수 있는 이유는 **Istio + AWS Route 53 + ACM 통합** 덕분입니다. 이 패턴의 핵심 구조를 살펴보겠습니다.\n\n### 전체 아키텍처\n\n```mermaid\nflowchart TB\n    subgraph Internet [인터넷]\n        User[사용자]\n    end\n    \n    subgraph AWS [AWS]\n        R53[Route 53<br/>*.example.com → ALB]\n        ACM[ACM 인증서<br/>*.example.com]\n        ALB[Application Load Balancer]\n    end\n    \n    subgraph EKS [EKS 클러스터]\n        IG[Istio Ingress Gateway<br/>istio-ingressgateway]\n        \n        GW[Gateway<br/>hosts: '*.example.com']\n        \n        VS1[VirtualService<br/>api.example.com]\n        VS2[VirtualService<br/>dashboard.example.com]\n        VS3[VirtualService<br/>admin.example.com]\n        \n        SVC1[api-service]\n        SVC2[dashboard-service]\n        SVC3[admin-service]\n    end\n    \n    User --> R53 --> ALB --> IG\n    IG --> GW\n    GW --> VS1 & VS2 & VS3\n    VS1 --> SVC1\n    VS2 --> SVC2\n    VS3 --> SVC3\n```\n\n### Step 1: AWS Route 53 와일드카드 레코드\n\n```bash\n# Route 53에 와일드카드 A 레코드 생성\naws route53 change-resource-record-sets \\\n  --hosted-zone-id Z1234567890 \\\n  --change-batch '{\n    \"Changes\": [{\n      \"Action\": \"CREATE\",\n      \"ResourceRecordSet\": {\n        \"Name\": \"*.example.com\",\n        \"Type\": \"A\",\n        \"AliasTarget\": {\n          \"HostedZoneId\": \"Z35SXDOTRQ7X7K\",\n          \"DNSName\": \"my-alb-123456789.ap-northeast-2.elb.amazonaws.com\",\n          \"EvaluateTargetHealth\": true\n        }\n      }\n    }]\n  }'\n```\n\n### Step 2: AWS ACM 와일드카드 인증서\n\n```bash\n# 와일드카드 인증서 요청\naws acm request-certificate \\\n  --domain-name \"*.example.com\" \\\n  --subject-alternative-names \"example.com\" \\\n  --validation-method DNS\n```\n\n### Step 3: ALB + Istio Ingress Gateway 연결\n\n```yaml\n# Istio Ingress Gateway Service (LoadBalancer)\napiVersion: v1\nkind: Service\nmetadata:\n  name: istio-ingressgateway\n  namespace: istio-system\n  annotations:\n    # ALB 사용\n    service.beta.kubernetes.io/aws-load-balancer-type: \"external\"\n    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: \"ip\"\n    service.beta.kubernetes.io/aws-load-balancer-scheme: \"internet-facing\"\n    \n    # HTTPS 종료는 ALB에서 (ACM 인증서 사용)\n    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \"arn:aws:acm:ap-northeast-2:123456789012:certificate/xxx\"\n    service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \"443\"\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 443\n    targetPort: 8080  # Istio Gateway는 8080에서 수신\n    name: https\n```\n\n### Step 4: Istio Gateway (와일드카드)\n\n```yaml\napiVersion: networking.istio.io/v1\nkind: Gateway\nmetadata:\n  name: wildcard-gateway\n  namespace: istio-system\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 8080      # ALB에서 TLS 종료 후 8080으로 전달\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"*.example.com\"   # 와일드카드로 모든 서브도메인 수신\n```\n\n### Step 5: 서비스별 VirtualService\n\n```yaml\n# API 서비스\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: api-vs\n  namespace: my-app\nspec:\n  hosts:\n  - \"api.example.com\"\n  gateways:\n  - istio-system/wildcard-gateway\n  http:\n  - route:\n    - destination:\n        host: api-service\n        port:\n          number: 80\n---\n# Dashboard 서비스\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: dashboard-vs\n  namespace: my-app\nspec:\n  hosts:\n  - \"dashboard.example.com\"\n  gateways:\n  - istio-system/wildcard-gateway\n  http:\n  - route:\n    - destination:\n        host: dashboard-service\n        port:\n          number: 80\n---\n# Admin 서비스\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: admin-vs\n  namespace: my-app\nspec:\n  hosts:\n  - \"admin.example.com\"\n  gateways:\n  - istio-system/wildcard-gateway\n  http:\n  - route:\n    - destination:\n        host: admin-service\n        port:\n          number: 80\n```\n\n> [!TIP]\n> 이 구조에서 **새 서비스 추가**는 VirtualService 하나만 추가하면 됩니다. DNS나 인증서 작업이 필요 없습니다!\n\n---\n\n## VirtualService: L7 라우팅 규칙\n\n### 경로 기반 라우팅\n\n```yaml\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: api-routing\nspec:\n  hosts:\n  - \"api.example.com\"\n  gateways:\n  - istio-system/wildcard-gateway\n  http:\n  # /v1/* → v1 서비스\n  - match:\n    - uri:\n        prefix: /v1/\n    route:\n    - destination:\n        host: api-v1\n        \n  # /v2/* → v2 서비스\n  - match:\n    - uri:\n        prefix: /v2/\n    route:\n    - destination:\n        host: api-v2\n        \n  # 기본 → latest\n  - route:\n    - destination:\n        host: api-latest\n```\n\n### 가중치 기반 트래픽 분할 (카나리 배포)\n\n```yaml\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: canary-rollout\nspec:\n  hosts:\n  - api-service\n  http:\n  - route:\n    - destination:\n        host: api-service\n        subset: stable\n      weight: 90\n    - destination:\n        host: api-service\n        subset: canary\n      weight: 10\n```\n\n```mermaid\nflowchart LR\n    Client[클라이언트] --> VS[VirtualService]\n    VS -->|90%| Stable[stable\\nv1.0.0]\n    VS -->|10%| Canary[canary\\nv1.1.0]\n```\n\n### 헤더 기반 라우팅 (A/B 테스팅)\n\n```yaml\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: ab-testing\nspec:\n  hosts:\n  - api-service\n  http:\n  # Beta 사용자 → 새 버전\n  - match:\n    - headers:\n        x-user-group:\n          exact: beta\n    route:\n    - destination:\n        host: api-service\n        subset: v2\n        \n  # 나머지 → 기존 버전\n  - route:\n    - destination:\n        host: api-service\n        subset: v1\n```\n\n### 타임아웃과 재시도\n\n```yaml\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: timeout-retry\nspec:\n  hosts:\n  - api-service\n  http:\n  - route:\n    - destination:\n        host: api-service\n    timeout: 10s  # 10초 타임아웃\n    retries:\n      attempts: 3          # 최대 3회 재시도\n      perTryTimeout: 2s    # 각 시도당 2초 타임아웃\n      retryOn: \"5xx,reset,connect-failure\"\n```\n\n---\n\n## DestinationRule: 트래픽 정책\n\n### Subset 정의\n\n```yaml\napiVersion: networking.istio.io/v1\nkind: DestinationRule\nmetadata:\n  name: api-service-dr\nspec:\n  host: api-service\n  subsets:\n  - name: stable\n    labels:\n      version: v1\n  - name: canary\n    labels:\n      version: v2\n```\n\n### 로드밸런싱 알고리즘\n\n```yaml\napiVersion: networking.istio.io/v1\nkind: DestinationRule\nmetadata:\n  name: lb-policy\nspec:\n  host: api-service\n  trafficPolicy:\n    loadBalancer:\n      simple: LEAST_CONN  # 최소 연결 수\n      # ROUND_ROBIN (기본값)\n      # RANDOM\n      # PASSTHROUGH (원래 IP로 전달)\n```\n\n### 서킷 브레이커\n\n```yaml\napiVersion: networking.istio.io/v1\nkind: DestinationRule\nmetadata:\n  name: circuit-breaker\nspec:\n  host: api-service\n  trafficPolicy:\n    connectionPool:\n      tcp:\n        maxConnections: 100      # 최대 연결 수\n      http:\n        h2UpgradePolicy: UPGRADE\n        http1MaxPendingRequests: 100\n        http2MaxRequests: 1000\n        \n    outlierDetection:\n      consecutive5xxErrors: 5    # 연속 5xx 에러 5번\n      interval: 30s              # 30초 간격으로 체크\n      baseEjectionTime: 30s      # 30초간 트래픽 제외\n      maxEjectionPercent: 50     # 최대 50%까지 제외\n```\n\n```mermaid\nsequenceDiagram\n    participant Client as 클라이언트\n    participant Envoy as Envoy Proxy\n    participant PodA as Pod A (정상)\n    participant PodB as Pod B (장애)\n    \n    Client->>Envoy: 요청 1\n    Envoy->>PodB: 전달\n    PodB-->>Envoy: 500 Error\n    \n    Client->>Envoy: 요청 2-5\n    Envoy->>PodB: 전달\n    PodB-->>Envoy: 500 Error (5회 연속)\n    \n    Note over Envoy,PodB: 서킷 오픈! Pod B 제외\n    \n    Client->>Envoy: 요청 6\n    Envoy->>PodA: Pod A로만 전달\n    PodA-->>Envoy: 200 OK\n    \n    Note over Envoy,PodB: 30초 후 재시도\n    Envoy->>PodB: 헬스체크\n    PodB-->>Envoy: 200 OK\n    Note over Envoy,PodB: 서킷 닫힘, 트래픽 복구\n```\n\n---\n\n## mTLS: 서비스 간 암호화\n\n### PeerAuthentication\n\n```yaml\n# 네임스페이스 전체에 mTLS 강제\napiVersion: security.istio.io/v1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: my-app\nspec:\n  mtls:\n    mode: STRICT  # mTLS 필수\n```\n\n| 모드 | 설명 |\n|------|-----|\n| `DISABLE` | mTLS 비활성화 |\n| `PERMISSIVE` | mTLS와 평문 모두 허용 (마이그레이션 시 유용) |\n| `STRICT` | mTLS만 허용 |\n\n### AuthorizationPolicy (접근 제어)\n\n```yaml\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\n  name: api-access-policy\n  namespace: my-app\nspec:\n  selector:\n    matchLabels:\n      app: api-service\n  rules:\n  # frontend에서만 접근 허용\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/my-app/sa/frontend\"]\n    to:\n    - operation:\n        methods: [\"GET\", \"POST\"]\n        paths: [\"/api/*\"]\n```\n\n---\n\n## Observability 연동\n\nIstio는 자동으로 메트릭, 로그, 트레이스를 수집합니다.\n\n### Kiali: 서비스 메시 시각화\n\n```bash\n# Kiali 설치\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.20/samples/addons/kiali.yaml\n\n# 대시보드 접근\nkubectl port-forward -n istio-system svc/kiali 20001:20001\n```\n\n### Jaeger: 분산 추적\n\n```bash\n# Jaeger 설치\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.20/samples/addons/jaeger.yaml\n\n# 대시보드 접근\nkubectl port-forward -n istio-system svc/tracing 16686:80\n```\n\n```mermaid\nflowchart LR\n    subgraph Trace [분산 추적]\n        A[Frontend<br/>trace-id: abc123] --> B[API Gateway<br/>span: 1]\n        B --> C[User Service<br/>span: 2]\n        B --> D[Order Service<br/>span: 3]\n        D --> E[Payment Service<br/>span: 4]\n    end\n```\n\n---\n\n## 트러블슈팅 가이드\n\n### VirtualService가 적용되지 않음\n\n```bash\n# 1. Gateway와 VirtualService 연결 확인\nkubectl get vs my-vs -o yaml | grep gateways -A 5\n\n# 2. Proxy 설정 동기화 확인\nistioctl proxy-status\n\n# 3. Envoy 설정 확인\nistioctl proxy-config routes deploy/my-app\n```\n\n**흔한 원인**:\n\n1. Gateway 이름/네임스페이스 불일치\n2. hosts 설정 불일치\n3. 네임스페이스에 `istio-injection` 라벨 누락\n\n### 503 Service Unavailable\n\n```bash\n# 1. 대상 서비스 확인\nkubectl get svc api-service\nkubectl get endpoints api-service\n\n# 2. DestinationRule subset 확인\nkubectl get dr api-service-dr -o yaml\n\n# 3. Pod 라벨 확인\nkubectl get pods --show-labels\n```\n\n**흔한 원인**:\n\n1. DestinationRule의 subset 라벨과 Pod 라벨 불일치\n2. 서비스 포트 설정 오류\n3. mTLS 모드 불일치\n\n### mTLS 관련 연결 실패\n\n```bash\n# 1. PeerAuthentication 확인\nkubectl get peerauthentication -A\n\n# 2. 특정 워크로드의 mTLS 상태 확인\nistioctl authn tls-check deploy/my-app\n\n# 3. 인증서 상태 확인\nistioctl proxy-config secret deploy/my-app -o json\n```\n\n---\n\n## 정리\n\n| 구성요소 | 역할 |\n|---------|-----|\n| **Gateway** | 외부 트래픽 진입점, 포트/프로토콜/TLS 설정 |\n| **VirtualService** | L7 라우팅 규칙 (경로, 헤더, 가중치) |\n| **DestinationRule** | 트래픽 정책 (LB, 서킷 브레이커, subset) |\n| **PeerAuthentication** | mTLS 모드 설정 |\n| **AuthorizationPolicy** | 접근 제어 (RBAC) |\n\n### 와일드카드 서브도메인 패턴 요약\n\n```\nRoute 53: *.example.com → ALB\nACM: *.example.com 와일드카드 인증서\nALB → Istio Ingress Gateway (TLS 종료)\nGateway: hosts: [\"*.example.com\"]\nVirtualService: 각 서비스별 서브도메인 라우팅\n```\n\n---\n\n## 다음 편 예고\n\n**5편: 오토스케일링 심화**에서는 다음을 다룹니다:\n\n- HPA v2: 다중 메트릭과 Scaling Behavior\n- VPA: 수직 스케일링과 In-place Resizing\n- Cluster Autoscaler vs Karpenter\n- KEDA: 이벤트 기반 오토스케일링\n\n---\n\n## 참고 자료\n\n- [Istio Documentation](https://istio.io/latest/docs/)\n- [Istio Traffic Management](https://istio.io/latest/docs/concepts/traffic-management/)\n- [Istio Security](https://istio.io/latest/docs/concepts/security/)\n- [AWS Load Balancer Controller + Istio](https://aws.amazon.com/blogs/containers/using-aws-load-balancer-controller-with-istio/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "Kubernetes",
      "Istio",
      "AWS"
    ],
    "readingTime": 9,
    "wordCount": 1740,
    "isFeatured": false,
    "isPublic": true,
    "series": "kubernetes",
    "date": "2026-01-03"
  },
  {
    "id": "k8s-03-config-secrets",
    "slug": "k8s-03-config-secrets",
    "path": "backend/kubernetes",
    "fullPath": "backend/kubernetes/k8s-03-config-secrets",
    "title": "Kubernetes 심화 시리즈 #3: 설정 및 시크릿 관리 완전 가이드",
    "excerpt": "ConfigMap과 Secrets의 내부 동작부터 AWS Secrets Manager CSI Driver 통합까지, 프로덕션 환경의 설정 관리 전략을 깊이 있게 다룹니다.",
    "content": "# Kubernetes 심화 시리즈 #3: 설정 및 시크릿 관리 완전 가이드\n\n## 시리즈 개요\n\n| # | 주제 | 핵심 내용 |\n|---|------|----------|\n| 1 | 워크로드 컨트롤러 심화 | Deployment, StatefulSet, DaemonSet, CronJob |\n| 2 | 서비스 네트워킹 심화 | Service 타입, kube-proxy, AWS ALB/NLB |\n| **3** | **설정 및 시크릿 관리** | ConfigMap, Secrets, AWS Secrets Manager CSI Driver |\n| 4 | Istio 서비스 메시 | VirtualService, DestinationRule, 와일드카드 서브도메인 |\n| 5 | 오토스케일링 심화 | HPA, VPA, Cluster Autoscaler, Karpenter, KEDA |\n| 6 | 보안 심화 | RBAC, NetworkPolicy, Pod Security Standards |\n\n---\n\n## ConfigMap: 설정의 외부화\n\n### 왜 ConfigMap인가?\n\n컨테이너 이미지에 설정을 하드코딩하면:\n\n```\n❌ 환경별 이미지 빌드 필요 (dev, staging, prod)\n❌ 설정 변경 시 재배포 필요\n❌ 설정 재사용 불가\n```\n\nConfigMap으로 **설정과 코드를 분리**합니다.\n\n```mermaid\nflowchart LR\n    subgraph Before [하드코딩된 설정]\n        IMG1[이미지-dev]\n        IMG2[이미지-staging]\n        IMG3[이미지-prod]\n    end\n    \n    subgraph After [ConfigMap 사용]\n        IMG[단일 이미지]\n        CM1[ConfigMap-dev]\n        CM2[ConfigMap-staging]\n        CM3[ConfigMap-prod]\n        IMG --> CM1 & CM2 & CM3\n    end\n```\n\n### ConfigMap 생성 방법\n\n```yaml\n# 직접 정의\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  # 키-값 쌍\n  DATABASE_HOST: mysql.default.svc.cluster.local\n  DATABASE_PORT: \"3306\"\n  LOG_LEVEL: info\n  \n  # 파일 내용\n  nginx.conf: |\n    server {\n      listen 80;\n      location / {\n        proxy_pass http://backend:8080;\n      }\n    }\n```\n\n```bash\n# 파일에서 생성\nkubectl create configmap nginx-config --from-file=nginx.conf\n\n# 리터럴에서 생성\nkubectl create configmap app-config \\\n  --from-literal=DATABASE_HOST=mysql \\\n  --from-literal=LOG_LEVEL=debug\n```\n\n### ConfigMap 사용 패턴\n\n#### 1. 환경변수로 주입\n\n```yaml\nspec:\n  containers:\n  - name: app\n    env:\n    # 개별 키 선택\n    - name: DB_HOST\n      valueFrom:\n        configMapKeyRef:\n          name: app-config\n          key: DATABASE_HOST\n    \n    # 전체 ConfigMap을 환경변수로\n    envFrom:\n    - configMapRef:\n        name: app-config\n```\n\n#### 2. 볼륨으로 마운트\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    volumeMounts:\n    - name: config-volume\n      mountPath: /etc/nginx/nginx.conf\n      subPath: nginx.conf  # 특정 키만 파일로\n      \n  volumes:\n  - name: config-volume\n    configMap:\n      name: nginx-config\n```\n\n### 불변(Immutable) ConfigMap\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\nimmutable: true  # K8s 1.21+\ndata:\n  LOG_LEVEL: info\n```\n\n**장점**:\n\n- API Server 부하 감소 (watch 제거)\n- 실수로 인한 변경 방지\n\n**단점**:\n\n- 변경하려면 새 ConfigMap 생성 필요\n- Pod 재배포 필요\n\n---\n\n## Secrets: 민감 정보 관리\n\n### ConfigMap vs Secrets\n\n| 특성 | ConfigMap | Secrets |\n|------|-----------|---------|\n| 용도 | 일반 설정 | 민감 정보 |\n| 저장 | 평문 | Base64 인코딩 |\n| 크기 제한 | 1MB | 1MB |\n| etcd 저장 | 평문 | 암호화 가능 (별도 설정) |\n| kubectl 출력 | 그대로 표시 | 기본적으로 숨김 |\n\n> [!CAUTION]\n> Base64는 **인코딩**이지 **암호화**가 아닙니다! 누구나 쉽게 디코딩할 수 있으므로, 반드시 **Secrets at Rest Encryption**을 활성화하세요.\n\n### Secret 타입\n\n```yaml\n# Opaque (기본값) - 일반 시크릿\napiVersion: v1\nkind: Secret\nmetadata:\n  name: db-credentials\ntype: Opaque\ndata:\n  username: YWRtaW4=      # echo -n 'admin' | base64\n  password: cGFzc3dvcmQ=  # echo -n 'password' | base64\n\n# 또는 stringData 사용 (자동 인코딩)\nstringData:\n  username: admin\n  password: password\n```\n\n```yaml\n# TLS 인증서\napiVersion: v1\nkind: Secret\nmetadata:\n  name: tls-secret\ntype: kubernetes.io/tls\ndata:\n  tls.crt: <base64-encoded-cert>\n  tls.key: <base64-encoded-key>\n```\n\n```yaml\n# Docker Registry 인증\napiVersion: v1\nkind: Secret\nmetadata:\n  name: regcred\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: <base64-encoded-docker-config>\n```\n\n### Secrets 사용 패턴\n\n```yaml\nspec:\n  containers:\n  - name: app\n    # 환경변수로 주입\n    env:\n    - name: DB_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: db-credentials\n          key: password\n    \n    # 볼륨으로 마운트 (파일)\n    volumeMounts:\n    - name: secrets-volume\n      mountPath: /etc/secrets\n      readOnly: true\n      \n  volumes:\n  - name: secrets-volume\n    secret:\n      secretName: db-credentials\n```\n\n### ServiceAccount Token 자동 마운트 비활성화\n\n모든 Pod에 ServiceAccount 토큰이 자동 마운트됩니다. 필요 없다면 비활성화하세요.\n\n```yaml\nspec:\n  automountServiceAccountToken: false\n  containers:\n  - name: app\n```\n\n---\n\n## Secrets at Rest Encryption\n\netcd에 저장된 Secrets를 암호화합니다.\n\n### EncryptionConfiguration\n\n```yaml\n# /etc/kubernetes/encryption-config.yaml\napiVersion: apiserver.config.k8s.io/v1\nkind: EncryptionConfiguration\nresources:\n  - resources:\n      - secrets\n    providers:\n      # AWS KMS 사용\n      - kms:\n          apiVersion: v2\n          name: aws-encryption-provider\n          endpoint: unix:///var/run/kmsplugin/socket.sock\n          cachesize: 1000\n          timeout: 3s\n      # 폴백: identity (암호화 안 함)\n      - identity: {}\n```\n\n### EKS에서 Envelope Encryption\n\nEKS는 **AWS KMS**를 사용한 Envelope Encryption을 지원합니다.\n\n```mermaid\nflowchart LR\n    subgraph K8s [Kubernetes]\n        Secret[Secret 데이터]\n        DEK[Data Encryption Key<br/>랜덤 생성]\n    end\n    \n    subgraph KMS [AWS KMS]\n        CMK[Customer Master Key]\n    end\n    \n    subgraph etcd [etcd]\n        Encrypted[암호화된 Secret +<br/>암호화된 DEK]\n    end\n    \n    Secret -->|DEK로 암호화| Encrypted\n    DEK -->|CMK로 암호화| Encrypted\n    CMK -.->|복호화 시 사용| DEK\n```\n\n```bash\n# EKS 클러스터에 KMS 키 연결\naws eks associate-encryption-config \\\n  --cluster-name my-cluster \\\n  --encryption-config '[{\n    \"resources\": [\"secrets\"],\n    \"provider\": {\n      \"keyArn\": \"arn:aws:kms:ap-northeast-2:123456789012:key/12345678-1234-1234-1234-123456789012\"\n    }\n  }]'\n```\n\n---\n\n## AWS Secrets Manager + CSI Driver\n\n### 문제: Kubernetes Secrets의 한계\n\n```\n❌ 시크릿이 etcd에 저장됨 (클러스터 침해 시 노출)\n❌ 시크릿 로테이션 어려움\n❌ 감사(Audit) 기능 제한적\n❌ 버전 관리 없음\n```\n\n### 해결: External Secrets Store\n\n```mermaid\nflowchart TB\n    subgraph AWS [AWS]\n        SM[\"Secrets Manager\"]\n    end\n    \n    subgraph K8s [\"Kubernetes 클러스터\"]\n        CSI[\"Secrets Store CSI Driver\"]\n        ASCP[\"AWS Secrets & Config Provider\"]\n        Pod[\"Application Pod\"]\n        \n        subgraph Mount [\"Pod 내부\"]\n            File[\"파일: db-password\"]\n            Env[\"ENV: DB_PASSWORD\"]\n        end\n    end\n    \n    SM --> ASCP --> CSI --> Pod\n    Pod --> Mount\n```\n\n### 설치\n\n```bash\n# Secrets Store CSI Driver 설치\nhelm repo add secrets-store-csi-driver \\\n  https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts\n\nhelm install csi-secrets-store \\\n  secrets-store-csi-driver/secrets-store-csi-driver \\\n  --namespace kube-system \\\n  --set syncSecret.enabled=true \\\n  --set enableSecretRotation=true\n\n# AWS Provider 설치\nkubectl apply -f https://raw.githubusercontent.com/aws/secrets-store-csi-driver-provider-aws/main/deployment/aws-provider-installer.yaml\n```\n\n### IRSA (IAM Roles for Service Accounts) 설정\n\n```bash\n# OIDC Provider 연결 (EKS 클러스터 생성 시 자동)\neksctl utils associate-iam-oidc-provider \\\n  --cluster my-cluster \\\n  --approve\n\n# ServiceAccount용 IAM Role 생성\neksctl create iamserviceaccount \\\n  --cluster my-cluster \\\n  --namespace default \\\n  --name my-app-sa \\\n  --attach-policy-arn arn:aws:iam::123456789012:policy/SecretsManagerReadPolicy \\\n  --approve\n```\n\n### SecretProviderClass 정의\n\n```yaml\napiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n  name: aws-secrets\nspec:\n  provider: aws\n  parameters:\n    objects: |\n      - objectName: \"prod/myapp/db-credentials\"\n        objectType: \"secretsmanager\"\n        jmesPath:\n          - path: username\n            objectAlias: db-username\n          - path: password\n            objectAlias: db-password\n      - objectName: \"prod/myapp/api-key\"\n        objectType: \"secretsmanager\"\n        \n  # Kubernetes Secret으로도 동기화 (optional)\n  secretObjects:\n  - secretName: db-credentials-k8s\n    type: Opaque\n    data:\n    - objectName: db-username\n      key: username\n    - objectName: db-password\n      key: password\n```\n\n### Pod에서 사용\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: my-app\nspec:\n  serviceAccountName: my-app-sa  # IRSA 연결된 SA\n  \n  containers:\n  - name: app\n    image: my-app:latest\n    \n    # 파일로 마운트된 시크릿 경로\n    volumeMounts:\n    - name: secrets\n      mountPath: /mnt/secrets\n      readOnly: true\n    \n    # 환경변수로도 사용 가능 (secretObjects 사용 시)\n    env:\n    - name: DB_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: db-credentials-k8s\n          key: password\n          \n  volumes:\n  - name: secrets\n    csi:\n      driver: secrets-store.csi.k8s.io\n      readOnly: true\n      volumeAttributes:\n        secretProviderClass: aws-secrets\n```\n\n### 시크릿 로테이션\n\n```yaml\n# Secrets Store CSI Driver 설치 시 옵션\nhelm upgrade csi-secrets-store \\\n  secrets-store-csi-driver/secrets-store-csi-driver \\\n  --namespace kube-system \\\n  --set enableSecretRotation=true \\\n  --set rotationPollInterval=2m  # 2분마다 체크\n```\n\n```mermaid\nsequenceDiagram\n    participant SM as AWS Secrets Manager\n    participant CSI as CSI Driver\n    participant Vol as Volume (파일)\n    participant App as Application\n    \n    Note over SM: 시크릿 로테이션 발생\n    SM->>SM: 새 버전 생성\n    \n    loop 매 2분\n        CSI->>SM: 현재 시크릿 조회\n        SM-->>CSI: 새 값 반환\n        CSI->>Vol: 파일 업데이트\n    end\n    \n    App->>Vol: 파일 다시 읽기\n    Note over App: 새 시크릿 사용\n```\n\n> [!TIP]\n> 애플리케이션이 파일 변경을 감지하거나 주기적으로 파일을 다시 읽도록 구현해야 합니다. 환경변수로 동기화하는 경우에는 Pod 재시작이 필요합니다.\n\n---\n\n## External Secrets Operator (대안)\n\nCSI Driver의 대안으로, 외부 시크릿 저장소의 값을 **Kubernetes Secret으로 직접 동기화**하는 방식입니다.\n\n```mermaid\nflowchart TB\n    subgraph AWS [AWS]\n        SM[Secrets Manager]\n    end\n    \n    subgraph K8s [Kubernetes]\n        ESO[External Secrets Operator]\n        ES[ExternalSecret CR]\n        Secret[Kubernetes Secret]\n        Pod[Pod]\n    end\n    \n    ES --> ESO\n    ESO --> SM\n    SM --> ESO --> Secret --> Pod\n```\n\n### CSI Driver vs External Secrets Operator\n\n| 특성 | CSI Driver | External Secrets Operator |\n|------|-----------|--------------------------|\n| 시크릿 저장 위치 | Pod 로컬 볼륨 (tmpfs) | Kubernetes Secret (etcd) |\n| 설치 복잡도 | DaemonSet + Provider | Deployment |\n| GitOps 친화성 | 낮음 | 높음 (ExternalSecret CRD) |\n| etcd에 저장 | 안 됨 | 됨 |\n| 호환성 | CSI 지원 필요 | 모든 환경 |\n\n---\n\n## 베스트 프랙티스\n\n### 1. 환경별 네임스페이스 분리\n\n```yaml\n# dev 네임스페이스\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\n  namespace: dev\ndata:\n  LOG_LEVEL: debug\n---\n# prod 네임스페이스\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\n  namespace: prod\ndata:\n  LOG_LEVEL: warn\n```\n\n### 2. RBAC로 시크릿 접근 제한\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: secret-reader\n  namespace: prod\nrules:\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  resourceNames: [\"db-credentials\"]  # 특정 시크릿만\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: app-secret-binding\n  namespace: prod\nsubjects:\n- kind: ServiceAccount\n  name: my-app-sa\n  namespace: prod\nroleRef:\n  kind: Role\n  name: secret-reader\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### 3. 감사 로깅 활성화\n\n```yaml\n# Audit Policy\napiVersion: audit.k8s.io/v1\nkind: Policy\nrules:\n- level: Metadata\n  resources:\n  - group: \"\"\n    resources: [\"secrets\"]\n```\n\n### 4. 시크릿 값 변경 시 Pod 재시작\n\n```yaml\n# Deployment 템플릿에 checksum 추가\nspec:\n  template:\n    metadata:\n      annotations:\n        checksum/secret: {{ include (print $.Template.BasePath \"/secret.yaml\") . | sha256sum }}\n```\n\n---\n\n## 트러블슈팅 가이드\n\n### CSI Driver Pod가 시크릿을 읽지 못함\n\n```bash\n# 1. Provider Pod 로그 확인\nkubectl logs -n kube-system -l app=csi-secrets-store-provider-aws\n\n# 2. IRSA 설정 확인\nkubectl describe sa my-app-sa\n\n# 3. IAM Role의 Trust Policy 확인\naws iam get-role --role-name MyAppRole --query 'Role.AssumeRolePolicyDocument'\n```\n\n**흔한 원인**:\n\n1. OIDC Provider 미설정\n2. ServiceAccount와 IAM Role 연결 누락\n3. Secrets Manager 접근 권한 누락\n\n### ConfigMap 변경이 Pod에 반영되지 않음\n\n```bash\n# 1. 환경변수로 주입한 경우: Pod 재시작 필요\nkubectl rollout restart deployment/my-app\n\n# 2. 볼륨으로 마운트한 경우: kubelet sync 대기 (기본 1분)\n# 즉시 반영하려면:\nkubectl exec -it my-pod -- cat /etc/config/my-key\n```\n\n> [!IMPORTANT]\n> 볼륨 마운트 시 파일은 자동 업데이트되지만, 애플리케이션이 파일을 캐싱하고 있다면 재시작이 필요합니다.\n\n---\n\n## 정리\n\n| 구성요소 | 용도 | 보안 수준 |\n|---------|-----|---------|\n| **ConfigMap** | 일반 설정 | 평문 |\n| **Secrets** | 민감 정보 (etcd) | Base64 + (암호화 가능) |\n| **Secrets at Rest** | etcd 암호화 | KMS 암호화 |\n| **CSI Driver** | 외부 저장소 연동 | 저장소 외부화 |\n| **External Secrets** | K8s Secret 동기화 | GitOps 친화적 |\n\n---\n\n## 다음 편 예고\n\n**4편: Istio 서비스 메시**에서는 다음을 다룹니다:\n\n- Istio 아키텍처 (istiod, Envoy Sidecar)\n- VirtualService와 DestinationRule\n- Gateway + 와일드카드 서브도메인 + Route 53 통합\n- mTLS와 보안 정책\n\n---\n\n## 참고 자료\n\n- [Kubernetes ConfigMaps](https://kubernetes.io/docs/concepts/configuration/configmap/)\n- [Kubernetes Secrets](https://kubernetes.io/docs/concepts/configuration/secret/)\n- [Secrets Store CSI Driver](https://secrets-store-csi-driver.sigs.k8s.io/)\n- [AWS Secrets Manager Provider](https://github.com/aws/secrets-store-csi-driver-provider-aws)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "Kubernetes",
      "AWS",
      "Security"
    ],
    "readingTime": 9,
    "wordCount": 1627,
    "isFeatured": false,
    "isPublic": true,
    "series": "kubernetes",
    "date": "2026-01-03"
  },
  {
    "id": "k8s-02-service-networking",
    "slug": "k8s-02-service-networking",
    "path": "backend/kubernetes",
    "fullPath": "backend/kubernetes/k8s-02-service-networking",
    "title": "Kubernetes 심화 시리즈 #2: 서비스 네트워킹의 내부 동작 원리",
    "excerpt": "ClusterIP, NodePort, LoadBalancer의 내부 동작 원리를 kube-proxy(iptables/IPVS), AWS ALB/NLB 통합과 함께 깊이 있게 이해합니다.",
    "content": "# Kubernetes 심화 시리즈 #2: 서비스 네트워킹의 내부 동작 원리\n\n## 시리즈 개요\n\n| # | 주제 | 핵심 내용 |\n|---|------|----------|\n| 1 | 워크로드 컨트롤러 심화 | Deployment, StatefulSet, DaemonSet, CronJob |\n| **2** | **서비스 네트워킹 심화** | Service 타입, kube-proxy, AWS ALB/NLB |\n| 3 | 설정 및 시크릿 관리 | ConfigMap, Secrets, AWS Secrets Manager CSI Driver |\n| 4 | Istio 서비스 메시 | VirtualService, DestinationRule, 와일드카드 서브도메인 |\n| 5 | 오토스케일링 심화 | HPA, VPA, Cluster Autoscaler, Karpenter, KEDA |\n| 6 | 보안 심화 | RBAC, NetworkPolicy, Pod Security Standards |\n\n---\n\n## Service의 본질: Pod 추상화\n\nPod는 생성될 때마다 **IP가 바뀝니다**. Service는 이 변화하는 Pod들 앞에 **안정적인 엔드포인트**를 제공합니다.\n\n```mermaid\nflowchart LR\n    subgraph Before [Pod IP 직접 사용]\n        Client1[클라이언트]\n        P1[Pod 10.0.1.5]\n        P2[Pod 10.0.1.6]\n        Client1 --> P1\n        Client1 -.->|Pod 재시작<br/>IP 변경!| P2\n    end\n    \n    subgraph After [Service 사용]\n        Client2[클라이언트]\n        SVC[Service<br/>10.96.0.100]\n        P3[Pod 10.0.1.5]\n        P4[Pod 10.0.1.6]\n        P5[Pod 10.0.1.7]\n        Client2 --> SVC --> P3 & P4 & P5\n    end\n```\n\n---\n\n## Service 타입별 동작 원리\n\n### ClusterIP (기본값): 클러스터 내부 통신\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: backend-api\nspec:\n  type: ClusterIP  # 기본값, 생략 가능\n  selector:\n    app: backend\n  ports:\n  - port: 80         # Service 포트\n    targetPort: 8080 # Pod 포트\n```\n\n**동작 원리**:\n\n1. Service 생성 시 **ClusterIP** 할당 (예: 10.96.0.100)\n2. **Endpoints** 객체 자동 생성 (selector와 일치하는 Pod IP 목록)\n3. kube-proxy가 **iptables/IPVS 규칙** 생성\n4. ClusterIP로 오는 트래픽을 Pod IP로 로드밸런싱\n\n```bash\n# Endpoints 확인 (실제 Pod IP 목록)\nkubectl get endpoints backend-api\n\n# 출력 예시:\n# NAME          ENDPOINTS                                   AGE\n# backend-api   10.0.1.5:8080,10.0.1.6:8080,10.0.1.7:8080   1h\n```\n\n### NodePort: 외부에서 노드 IP로 접근\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: backend-api\nspec:\n  type: NodePort\n  selector:\n    app: backend\n  ports:\n  - port: 80\n    targetPort: 8080\n    nodePort: 30080  # 30000-32767 범위, 생략 시 자동 할당\n```\n\n```mermaid\nflowchart LR\n    subgraph External [외부 네트워크]\n        Client[클라이언트]\n    end\n    \n    subgraph Cluster [Kubernetes 클러스터]\n        subgraph Node1 [Node 1 - 192.168.1.10]\n            NP1[NodePort :30080]\n            P1[Pod :8080]\n        end\n        \n        subgraph Node2 [Node 2 - 192.168.1.11]\n            NP2[NodePort :30080]\n            P2[Pod :8080]\n        end\n        \n        subgraph Node3 [Node 3 - 192.168.1.12]\n            NP3[NodePort :30080]\n        end\n    end\n    \n    Client -->|192.168.1.10:30080| NP1\n    Client -->|192.168.1.11:30080| NP2\n    Client -->|192.168.1.12:30080| NP3\n    \n    NP1 --> P1 & P2\n    NP2 --> P1 & P2\n    NP3 --> P1 & P2\n```\n\n> [!IMPORTANT]\n> NodePort는 **모든 노드**에서 열립니다. Node3에 Pod가 없어도 30080 포트로 접근하면 다른 노드의 Pod로 트래픽이 전달됩니다.\n\n### externalTrafficPolicy: 트래픽 경로 제어\n\n```yaml\nspec:\n  type: NodePort\n  externalTrafficPolicy: Local  # 기본값: Cluster\n```\n\n| 정책 | 동작 | 장점 | 단점 |\n|------|-----|------|------|\n| `Cluster` | 다른 노드 Pod로도 전달 | 균등한 로드밸런싱 | 추가 홉, 클라이언트 IP 손실 |\n| `Local` | 해당 노드의 Pod로만 전달 | 클라이언트 IP 보존, 낮은 지연 | 불균등 분배 가능 |\n\n```mermaid\nflowchart TB\n    subgraph Cluster_Policy [externalTrafficPolicy: Cluster]\n        C1[Client → Node1:30080]\n        N1_1[Node1]\n        N2_1[Node2]\n        P1_1[Pod on Node2]\n        C1 --> N1_1 -->|SNAT| N2_1 --> P1_1\n        Note1[클라이언트 IP가 Node1 IP로 변경됨]\n    end\n    \n    subgraph Local_Policy [externalTrafficPolicy: Local]\n        C2[Client → Node2:30080]\n        N2_2[Node2]\n        P1_2[Pod on Node2]\n        C2 --> N2_2 --> P1_2\n        Note2[클라이언트 IP 보존됨]\n    end\n```\n\n### LoadBalancer: 클라우드 로드밸런서 통합\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: backend-api\n  annotations:\n    # AWS NLB 사용 (기본은 CLB)\n    service.beta.kubernetes.io/aws-load-balancer-type: nlb\n    # 인터널 LB\n    service.beta.kubernetes.io/aws-load-balancer-internal: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: backend\n  ports:\n  - port: 80\n    targetPort: 8080\n```\n\n```mermaid\nflowchart LR\n    subgraph AWS [AWS]\n        Internet[인터넷]\n        NLB[Network Load Balancer<br/>abc123.elb.amazonaws.com]\n    end\n    \n    subgraph K8s [Kubernetes 클러스터]\n        subgraph Node1 [Node 1]\n            NP1[NodePort :30080]\n            P1[Pod]\n        end\n        subgraph Node2 [Node 2]\n            NP2[NodePort :30080]\n            P2[Pod]\n        end\n    end\n    \n    Internet --> NLB --> NP1 & NP2\n    NP1 --> P1\n    NP2 --> P2\n```\n\n**LoadBalancer = NodePort + 클라우드 LB 자동 프로비저닝**\n\n### ExternalName: 외부 서비스 CNAME\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: external-db\nspec:\n  type: ExternalName\n  externalName: mydb.us-east-1.rds.amazonaws.com\n```\n\n클러스터 내에서 `external-db`로 DNS 조회하면 RDS 주소가 반환됩니다.\n\n```bash\n# 클러스터 포드 내에서\nnslookup external-db.default.svc.cluster.local\n# → mydb.us-east-1.rds.amazonaws.com\n```\n\n---\n\n## kube-proxy: Service 구현체\n\n### 모드 비교: iptables vs IPVS vs nftables\n\n```mermaid\nflowchart TB\n    subgraph IptablesMode [\"iptables 모드\"]\n        IT[\"iptables 규칙\"]\n        Rule1[\"규칙 1\"]\n        Rule2[\"규칙 2\"]\n        RuleN[\"규칙 N\"]\n        Target[\"Pod\"]\n        IT --> Rule1 --> Rule2 --> RuleN --> Target\n    end\n    \n    subgraph IPVSMode [\"IPVS 모드\"]\n        IV[\"IPVS 해시 테이블\"]\n        Target2[\"Pod\"]\n        IV --> Target2\n    end\n    \n    subgraph NftablesMode [\"nftables 모드 (권장)\"]\n        NFT[\"nftables 규칙\"]\n        Target3[\"Pod\"]\n        NFT --> Target3\n    end\n```\n\n| 특성 | iptables | IPVS | **nftables** |\n|------|---------|------|-------------|\n| **조회 시간** | O(n) | O(1) | O(1) |\n| **권장 여부** | 안정적 | ⚠️ Deprecated 예정 | ✅ **권장** |\n| **로드밸런싱** | 랜덤 | rr, lc, dh 등 | 다양 |\n| **성능** | 개선됨 | 양호 | **최고** |\n| **최소 버전** | 모든 버전 | - | K8s 1.31+ (1.33 Stable) |\n\n> [!WARNING]\n> **IPVS 모드 Deprecated 예정**: Kubernetes 공식 문서에 따르면 IPVS 모드는 Kubernetes Services API와의 불일치로 인해 향후 deprecated될 예정입니다. 새로운 클러스터에서는 **nftables 모드**(K8s 1.33+ stable) 또는 **iptables 모드**를 권장합니다.\n\n### kube-proxy 모드 설정\n\n```yaml\n# nftables 모드 (K8s 1.31+, 권장)\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kube-proxy\n  namespace: kube-system\ndata:\n  config.conf: |\n    mode: \"nftables\"\n```\n\n```yaml\n# IPVS 모드 (⚠️ deprecated 예정)\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kube-proxy\n  namespace: kube-system\ndata:\n  config.conf: |\n    mode: \"ipvs\"\n    ipvs:\n      scheduler: \"rr\"  # round-robin\n```\n\n---\n\n## AWS Load Balancer Controller\n\n### 개요\n\nAWS Load Balancer Controller는 Ingress와 Service를 **ALB/NLB**로 프로비저닝합니다.\n\n```mermaid\nflowchart TB\n    subgraph K8s [Kubernetes]\n        Ingress[Ingress 리소스]\n        SVC[Service type: LoadBalancer]\n        CTRL[AWS Load Balancer Controller]\n    end\n    \n    subgraph AWS [AWS]\n        ALB[Application Load Balancer]\n        NLB[Network Load Balancer]\n        TG[Target Group]\n        R53[Route 53]\n    end\n    \n    Ingress --> CTRL\n    SVC --> CTRL\n    CTRL --> ALB & NLB\n    ALB & NLB --> TG\n    TG --> |Pod IP 직접 등록| Pods[Pods]\n```\n\n### Ingress로 ALB 생성\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\n  annotations:\n    # ALB Ingress Controller\n    kubernetes.io/ingress.class: alb\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/target-type: ip  # Pod IP 직접 연결\n    \n    # SSL/TLS\n    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:...\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTPS\":443}]'\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\n    \n    # 헬스체크\n    alb.ingress.kubernetes.io/healthcheck-path: /health\n    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '15'\nspec:\n  rules:\n  - host: api.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: backend-api\n            port:\n              number: 80\n```\n\n### target-type: ip vs instance\n\n```mermaid\nflowchart LR\n    subgraph Instance [target-type: instance]\n        ALB1[ALB] --> NP[NodePort] --> Pod1[Pod]\n    end\n    \n    subgraph IP [target-type: ip]\n        ALB2[ALB] --> Pod2[Pod]\n    end\n```\n\n| target-type | 경로 | 장점 | 단점 |\n|-------------|-----|------|------|\n| `instance` | ALB → NodePort → Pod | 간단, 모든 네트워크에서 동작 | 추가 홉, 지연 |\n| `ip` | ALB → Pod (직접) | 낮은 지연, 효율적 | VPC CNI 필요, ENI 제한 |\n\n### NLB for TCP/UDP\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: tcp-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: \"external\"\n    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: \"ip\"\n    service.beta.kubernetes.io/aws-load-balancer-scheme: \"internet-facing\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n  - port: 9000\n    targetPort: 9000\n    protocol: TCP\n```\n\n---\n\n## Headless Service 심화\n\nSelector가 있지만 ClusterIP가 없는 Service입니다. DNS 조회 시 Service IP가 아닌 Pod IP 목록을 직접 반환합니다.\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\nspec:\n  clusterIP: None  # Headless!\n  selector:\n    app: mysql\n  ports:\n  - port: 3306\n```\n\n### DNS 해석 차이\n\n```bash\n# 일반 Service: ClusterIP 반환\nnslookup backend-api.default.svc.cluster.local\n# → 10.96.0.100\n\n# Headless Service: 모든 Pod IP 반환\nnslookup mysql.default.svc.cluster.local\n# → 10.0.1.5\n# → 10.0.1.6\n# → 10.0.1.7\n```\n\n```mermaid\nflowchart TB\n    subgraph Normal [일반 Service]\n        DNS1[DNS 조회] --> ClusterIP[10.96.0.100]\n        ClusterIP --> |kube-proxy 로드밸런싱| P1 & P2 & P3\n    end\n    \n    subgraph Headless [Headless Service]\n        DNS2[DNS 조회] --> PodIPs[10.0.1.5, 10.0.1.6, 10.0.1.7]\n        PodIPs --> |클라이언트가 직접 선택| P4[Pod]\n    end\n```\n\n### 사용 사례\n\n| 사용 사례 | 이유 |\n|----------|-----|\n| StatefulSet | 각 Pod에 고유 DNS 필요 (`mysql-0.mysql.svc`) |\n| 클라이언트 측 로드밸런싱 | gRPC, 커스텀 LB 알고리즘 |\n| 서비스 디스커버리 | Consul, etcd 같은 클러스터 |\n\n---\n\n## EndpointSlices: 대규모 클러스터 최적화\n\nEndpoints 객체는 Service당 하나이며, **모든 Pod IP**를 담습니다. Pod가 수천 개면 문제가 됩니다.\n\n```mermaid\nflowchart TB\n    subgraph Endpoints [기존 Endpoints]\n        EP[Endpoints 객체<br/>5000개 Pod IP<br/>매우 큼!]\n    end\n    \n    subgraph EndpointSlices [EndpointSlices]\n        ES1[EndpointSlice 1<br/>100개 Pod]\n        ES2[EndpointSlice 2<br/>100개 Pod]\n        ES3[...<br/>...]\n        ES50[EndpointSlice 50<br/>100개 Pod]\n    end\n```\n\n**EndpointSlices 장점**:\n\n- 최대 100개 엔드포인트씩 분할\n- 변경 시 해당 슬라이스만 업데이트\n- etcd 부하 감소\n\n```bash\n# EndpointSlices 확인\nkubectl get endpointslices -l kubernetes.io/service-name=my-service\n```\n\n---\n\n## 트러블슈팅 가이드\n\n### Service에 연결되지 않음\n\n```bash\n# 1. Endpoints 확인 (비어있으면 selector 문제)\nkubectl get endpoints my-service\n\n# 2. Pod label 확인\nkubectl get pods --show-labels\n\n# 3. Pod readiness 확인\nkubectl get pods -o wide\n```\n\n### NodePort로 외부 접속 불가\n\n```bash\n# 1. NodePort 확인\nkubectl get svc my-service\n\n# 2. 방화벽/Security Group 확인\n# AWS: EC2 Security Group에서 NodePort 범위 허용\n\n# 3. iptables 규칙 확인\nsudo iptables -t nat -L KUBE-SERVICES -n\n```\n\n### LoadBalancer EXTERNAL-IP가 `<pending>`\n\n```bash\n# 1. AWS Load Balancer Controller 로그 확인\nkubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller\n\n# 2. 서비스 이벤트 확인\nkubectl describe svc my-service\n\n# 3. IAM 권한 확인 (IRSA)\n```\n\n**흔한 원인**:\n\n1. Cloud Controller Manager 미설치 (On-prem)\n2. IAM 권한 부족\n3. 서브넷 태그 누락 (`kubernetes.io/role/elb`)\n\n---\n\n## 정리\n\n| 구성요소 | 역할 |\n|---------|-----|\n| **ClusterIP** | 클러스터 내부 가상 IP |\n| **NodePort** | 모든 노드에서 고정 포트 노출 |\n| **LoadBalancer** | 클라우드 LB 자동 프로비저닝 |\n| **ExternalName** | 외부 서비스 DNS CNAME |\n| **kube-proxy** | iptables/IPVS로 패킷 라우팅 |\n| **AWS LB Controller** | Ingress → ALB, Service → NLB |\n| **Headless Service** | 각 Pod에 직접 DNS 접근 |\n\n---\n\n## 다음 편 예고\n\n**3편: 설정 및 시크릿 관리**에서는 다음을 다룹니다:\n\n- ConfigMap과 Secrets의 내부 동작\n- Secrets at Rest Encryption\n- AWS Secrets Manager + CSI Driver 통합\n- External Secrets Operator\n\n---\n\n## 참고 자료\n\n- [Kubernetes Services](https://kubernetes.io/docs/concepts/services-networking/service/)\n- [kube-proxy Modes](https://kubernetes.io/docs/reference/networking/virtual-ips/)\n- [AWS Load Balancer Controller](https://kubernetes-sigs.github.io/aws-load-balancer-controller/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "Kubernetes",
      "Networking",
      "AWS"
    ],
    "readingTime": 8,
    "wordCount": 1566,
    "isFeatured": false,
    "isPublic": true,
    "series": "kubernetes",
    "date": "2026-01-03"
  },
  {
    "id": "k8s-01-workload-controllers",
    "slug": "k8s-01-workload-controllers",
    "path": "backend/kubernetes",
    "fullPath": "backend/kubernetes/k8s-01-workload-controllers",
    "title": "Kubernetes 심화 시리즈 #1: 워크로드 컨트롤러의 내부 동작 원리",
    "excerpt": "Deployment, StatefulSet, DaemonSet, CronJob의 내부 동작 원리를 깊이 있게 이해합니다. Reconciliation Loop, 컨트롤러 패턴, 그리고 실무 트러블슈팅까지.",
    "content": "# Kubernetes 심화 시리즈 #1: 워크로드 컨트롤러의 내부 동작 원리\n\n## 시리즈 개요\n\n| # | 주제 | 핵심 내용 |\n|---|------|----------|\n| **1** | **워크로드 컨트롤러 심화** | Deployment, StatefulSet, DaemonSet, CronJob |\n| 2 | 서비스 네트워킹 심화 | Service 타입, kube-proxy, AWS ALB/NLB |\n| 3 | 설정 및 시크릿 관리 | ConfigMap, Secrets, AWS Secrets Manager CSI Driver |\n| 4 | Istio 서비스 메시 | VirtualService, DestinationRule, 와일드카드 서브도메인 |\n| 5 | 오토스케일링 심화 | HPA, VPA, Cluster Autoscaler, Karpenter, KEDA |\n| 6 | 보안 심화 | RBAC, NetworkPolicy, Pod Security Standards |\n\n---\n\n## 컨트롤러 패턴: Kubernetes의 핵심 철학\n\nKubernetes의 모든 것은 **컨트롤러 패턴**으로 동작합니다. 우리가 `kubectl apply`로 Deployment를 생성하면, 실제 Pod를 만드는 것은 Deployment Controller입니다.\n\n```mermaid\nflowchart LR\n    subgraph User [사용자]\n        YAML[Deployment YAML]\n    end\n    \n    subgraph ControlPlane [Control Plane]\n        API[API Server]\n        ETCD[(etcd)]\n        CM[Controller Manager]\n        \n        subgraph Controllers [컨트롤러들]\n            DC[Deployment Controller]\n            RSC[ReplicaSet Controller]\n            STC[StatefulSet Controller]\n            DSC[DaemonSet Controller]\n            JC[Job Controller]\n            CJC[CronJob Controller]\n        end\n    end\n    \n    subgraph Node [Worker Node]\n        Kubelet[kubelet]\n        Pod1[Pod]\n        Pod2[Pod]\n    end\n    \n    YAML --> API --> ETCD\n    API <--> CM --> Controllers\n    DC --> API\n    RSC --> API\n    API <--> Kubelet --> Pod1 & Pod2\n```\n\n### Reconciliation Loop (조정 루프)\n\n모든 컨트롤러는 동일한 패턴으로 동작합니다:\n\n```\n무한 루프:\n  1. 현재 상태(Current State) 관찰\n  2. 원하는 상태(Desired State)와 비교\n  3. 차이가 있으면 조정(Reconcile)\n  4. 다음 이벤트 대기\n```\n\n```go\n// 실제 Kubernetes 컨트롤러의 핵심 구조 (간략화)\nfunc (c *Controller) Run(ctx context.Context) {\n    for {\n        select {\n        case <-ctx.Done():\n            return\n        default:\n            // 1. 작업 큐에서 아이템 가져오기\n            key, shutdown := c.workqueue.Get()\n            if shutdown {\n                return\n            }\n            \n            // 2. Reconcile 실행\n            err := c.reconcile(key.(string))\n            if err != nil {\n                // 재시도 큐에 추가\n                c.workqueue.AddRateLimited(key)\n            } else {\n                c.workqueue.Forget(key)\n            }\n            c.workqueue.Done(key)\n        }\n    }\n}\n```\n\n> [!IMPORTANT]\n> **Level-triggered vs Edge-triggered**: Kubernetes 컨트롤러는 **Level-triggered** 방식입니다. \"상태가 변했다\"(Edge)가 아니라 \"현재 상태가 이것이다\"(Level)를 기준으로 동작합니다. 덕분에 컨트롤러가 재시작되어도 현재 상태를 다시 읽어서 정상적으로 조정할 수 있습니다.\n\n---\n\n## Deployment: 가장 널리 쓰이는 워크로드\n\n### 내부 구조: Deployment → ReplicaSet → Pod\n\nDeployment는 Pod를 직접 관리하지 않습니다. **ReplicaSet**을 통해 간접적으로 관리합니다.\n\n```mermaid\nflowchart TB\n    subgraph Deployment [Deployment: my-app]\n        D[spec.replicas: 3<br/>spec.strategy: RollingUpdate]\n    end\n    \n    subgraph RS [ReplicaSet: my-app-7d4f5b8c9]\n        RS1[spec.replicas: 3<br/>ownerReference: my-app]\n    end\n    \n    subgraph Pods [Pods]\n        P1[my-app-7d4f5b8c9-abc12]\n        P2[my-app-7d4f5b8c9-def34]\n        P3[my-app-7d4f5b8c9-ghi56]\n    end\n    \n    Deployment --> RS --> Pods\n```\n\n### Rolling Update 동작 원리\n\nDeployment를 업데이트하면 새 ReplicaSet이 생성되고, 점진적으로 트래픽이 이동합니다.\n\n```mermaid\nsequenceDiagram\n    participant User as 사용자\n    participant API as API Server\n    participant DC as Deployment Controller\n    participant RSC as ReplicaSet Controller\n    \n    User->>API: kubectl apply (새 이미지)\n    API->>DC: Deployment 변경 감지\n    \n    DC->>API: 새 ReplicaSet 생성 (replicas: 1)\n    DC->>API: 기존 ReplicaSet scale down (replicas: 2)\n    \n    loop maxSurge/maxUnavailable 조건 충족까지\n        RSC->>API: 새 Pod 생성\n        RSC->>API: 기존 Pod 삭제\n        DC->>API: ReplicaSet replicas 조정\n    end\n    \n    DC->>API: 기존 ReplicaSet replicas: 0\n    Note over API: 롤아웃 완료\n```\n\n### 업데이트 전략 비교\n\n```yaml\n# RollingUpdate (기본값) - 무중단 배포\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 25%        # 최대 추가 Pod 수\n      maxUnavailable: 25%  # 최대 사용 불가 Pod 수\n```\n\n```yaml\n# Recreate - 모든 Pod 삭제 후 재생성\nspec:\n  strategy:\n    type: Recreate\n    # 주의: 다운타임 발생!\n    # 사용 사례: DB 마이그레이션, 싱글 인스턴스 제약\n```\n\n| 전략 | 다운타임 | 리소스 사용 | 사용 사례 |\n|------|---------|-----------|----------|\n| RollingUpdate | 없음 | 일시적 증가 | 대부분의 상황 |\n| Recreate | 있음 | 동일 | 싱글 인스턴스, 볼륨 공유 불가 시 |\n\n### 롤백 동작 원리\n\n```bash\n# 롤아웃 히스토리 확인\nkubectl rollout history deployment/my-app\n\n# 특정 리비전으로 롤백\nkubectl rollout undo deployment/my-app --to-revision=2\n```\n\n롤백은 새 ReplicaSet을 생성하는 것이 아닙니다. **기존 ReplicaSet을 다시 Scale Up**합니다.\n\n```mermaid\nflowchart LR\n    subgraph Before [롤백 전]\n        RS1_B[RS v1: 0개]\n        RS2_B[RS v2: 0개]\n        RS3_B[RS v3: 3개 ✓]\n    end\n    \n    subgraph After [v2로 롤백 후]\n        RS1_A[RS v1: 0개]\n        RS2_A[RS v2: 3개 ✓]\n        RS3_A[RS v3: 0개]\n    end\n    \n    Before --> |kubectl rollout undo<br/>--to-revision=2| After\n```\n\n> [!TIP]\n> `revisionHistoryLimit` (기본값 10)으로 유지할 ReplicaSet 수를 제한할 수 있습니다. 너무 많으면 etcd 부담이 증가합니다.\n\n---\n\n## StatefulSet: 상태 유지가 필요한 워크로드\n\n### Deployment와의 핵심 차이\n\n| 특성 | Deployment | StatefulSet |\n|------|-----------|-------------|\n| Pod 이름 | 랜덤 (`app-7d4f5b8c9-abc12`) | 순차적 (`app-0`, `app-1`, `app-2`) |\n| 생성/삭제 순서 | 병렬 | 순차적 (0→1→2, 삭제는 역순) |\n| 네트워크 ID | 없음 | Headless Service로 고정 DNS |\n| 스토리지 | Pod 삭제 시 PVC도 삭제 가능 | Pod 삭제해도 PVC 유지 |\n\n### 순차적 생성과 삭제\n\n```mermaid\nsequenceDiagram\n    participant API as API Server\n    participant STC as StatefulSet Controller\n    \n    Note over STC: 생성 (순차적)\n    STC->>API: Pod-0 생성\n    API-->>STC: Pod-0 Ready\n    STC->>API: Pod-1 생성\n    API-->>STC: Pod-1 Ready\n    STC->>API: Pod-2 생성\n    API-->>STC: Pod-2 Ready\n    \n    Note over STC: 삭제 (역순)\n    STC->>API: Pod-2 삭제\n    API-->>STC: Pod-2 Terminated\n    STC->>API: Pod-1 삭제\n    API-->>STC: Pod-1 Terminated\n    STC->>API: Pod-0 삭제\n```\n\n### Headless Service와 DNS\n\n```yaml\n# Headless Service (ClusterIP: None)\napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\nspec:\n  clusterIP: None  # Headless!\n  selector:\n    app: mysql\n  ports:\n    - port: 3306\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  serviceName: mysql  # Headless Service 이름\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: mysql\n        image: mysql:8.0\n        ports:\n        - containerPort: 3306\n```\n\n이렇게 설정하면 각 Pod에 **고정 DNS**가 부여됩니다:\n\n```\nmysql-0.mysql.default.svc.cluster.local\nmysql-1.mysql.default.svc.cluster.local\nmysql-2.mysql.default.svc.cluster.local\n```\n\n> [!IMPORTANT]\n> 일반 Service는 Pod IP가 변해도 Service IP로 로드밸런싱됩니다. StatefulSet + Headless Service는 **각 Pod를 직접 지정**할 수 있어 MySQL 레플리케이션, Kafka 브로커 등에 필수입니다.\n\n### VolumeClaimTemplates\n\n```yaml\nspec:\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      storageClassName: gp3\n      resources:\n        requests:\n          storage: 100Gi\n```\n\nPod가 삭제되어도 PVC는 그대로 유지됩니다. `mysql-0`이 다시 생성되면 **같은 데이터**를 사용합니다.\n\n```mermaid\nflowchart TB\n    subgraph StatefulSet [StatefulSet: mysql]\n        P0[mysql-0]\n        P1[mysql-1]\n        P2[mysql-2]\n    end\n    \n    subgraph PVCs [PersistentVolumeClaims]\n        PVC0[data-mysql-0<br/>100Gi]\n        PVC1[data-mysql-1<br/>100Gi]\n        PVC2[data-mysql-2<br/>100Gi]\n    end\n    \n    subgraph PVs [PersistentVolumes - AWS EBS]\n        PV0[vol-abc...]\n        PV1[vol-def...]\n        PV2[vol-ghi...]\n    end\n    \n    P0 --> PVC0 --> PV0\n    P1 --> PVC1 --> PV1\n    P2 --> PVC2 --> PV2\n```\n\n### podManagementPolicy\n\n```yaml\nspec:\n  podManagementPolicy: Parallel  # 기본값: OrderedReady\n```\n\n| 정책 | 설명 | 사용 사례 |\n|------|-----|----------|\n| `OrderedReady` | 순차적 생성/삭제, 이전 Pod Ready 대기 | MySQL, ZooKeeper |\n| `Parallel` | 병렬 생성/삭제, 순서 무관 | Elasticsearch (빠른 스케일링 필요) |\n\n---\n\n## DaemonSet: 모든 노드에 Pod 배포\n\n### 동작 원리\n\nDaemonSet Controller는 각 노드에 정확히 하나의 Pod가 실행되도록 보장합니다.\n\n```mermaid\nflowchart TB\n    subgraph Cluster [Kubernetes 클러스터]\n        subgraph Node1 [Node 1]\n            P1[fluentd]\n        end\n        subgraph Node2 [Node 2]\n            P2[fluentd]\n        end\n        subgraph Node3 [Node 3]\n            P3[fluentd]\n        end\n        subgraph Node4 [Node 4 - 신규]\n            P4[fluentd]:::new\n        end\n    end\n    \n    DS[DaemonSet: fluentd]\n    DS --> P1 & P2 & P3 & P4\n    \n    classDef new fill:#90EE90\n```\n\n- **노드 추가 시**: 자동으로 해당 노드에 Pod 생성\n- **노드 삭제 시**: 해당 노드의 Pod도 함께 삭제\n\n### nodeSelector와 tolerations\n\n특정 노드에만 배포하거나, taint가 있는 노드에도 배포하려면:\n\n```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nvidia-driver\nspec:\n  selector:\n    matchLabels:\n      app: nvidia-driver\n  template:\n    spec:\n      # 특정 노드에만 배포\n      nodeSelector:\n        hardware: gpu\n      \n      # taint를 허용\n      tolerations:\n      - key: nvidia.com/gpu\n        operator: Exists\n        effect: NoSchedule\n      \n      containers:\n      - name: nvidia-driver\n        image: nvidia/driver:latest\n```\n\n### 주요 사용 사례\n\n| 사용 사례 | 예시 |\n|----------|-----|\n| **로그 수집** | Fluentd, Fluent Bit, Filebeat |\n| **모니터링** | Node Exporter, cAdvisor |\n| **네트워크** | Calico, Cilium (CNI 플러그인) |\n| **스토리지** | CSI 드라이버, Rook-Ceph |\n| **보안** | Falco, Sysdig |\n\n---\n\n## CronJob & Job: 배치 작업\n\n### Job: 일회성 작업\n\n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migration\nspec:\n  backoffLimit: 3           # 실패 시 최대 재시도 횟수\n  activeDeadlineSeconds: 600  # 최대 실행 시간 (10분)\n  ttlSecondsAfterFinished: 3600  # 완료 후 1시간 뒤 자동 삭제\n  template:\n    spec:\n      restartPolicy: Never   # Job에서는 OnFailure 또는 Never만 가능\n      containers:\n      - name: migration\n        image: my-app:latest\n        command: [\"./migrate.sh\"]\n```\n\n### CronJob: 정기적 작업\n\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: daily-report\nspec:\n  schedule: \"0 2 * * *\"      # 매일 새벽 2시 (UTC)\n  timeZone: \"Asia/Seoul\"     # K8s 1.27+ 지원\n  \n  # 동시 실행 정책\n  concurrencyPolicy: Forbid  # 이전 Job이 실행 중이면 새 Job 생성 안함\n  \n  # 히스토리 제한\n  successfulJobsHistoryLimit: 3\n  failedJobsHistoryLimit: 1\n  \n  # 스케줄 놓쳤을 때 정책\n  startingDeadlineSeconds: 300  # 5분 내 시작 못하면 스킵\n  \n  jobTemplate:\n    spec:\n      template:\n        spec:\n          restartPolicy: OnFailure\n          containers:\n          - name: report\n            image: report-generator:latest\n```\n\n### concurrencyPolicy 비교\n\n```mermaid\ngantt\n    title CronJob concurrencyPolicy 비교\n    dateFormat HH:mm\n    axisFormat %H:%M\n    \n    section Allow\n    Job 1    :active, 00:00, 30m\n    Job 2    :active, 00:30, 30m\n    Job 3 (동시실행됨) :active, 00:45, 30m\n    \n    section Forbid\n    Job 1    :active, 00:00, 60m\n    Job 2 (스킵됨) :crit, 00:30, 5m\n    Job 2 (실행) :active, 01:00, 30m\n    \n    section Replace\n    Job 1    :active, 00:00, 30m\n    Job 2 (Job 1 중단) :active, 00:30, 30m\n```\n\n| 정책 | 동작 | 사용 사례 |\n|------|-----|----------|\n| `Allow` | 동시 실행 허용 (기본값) | 독립적인 작업 |\n| `Forbid` | 이전 Job 실행 중이면 스킵 | 중복 실행 방지 필요 시 |\n| `Replace` | 이전 Job 중단하고 새로 시작 | 최신 데이터만 중요할 때 |\n\n---\n\n## 트러블슈팅 가이드\n\n### Deployment 롤아웃이 멈춤\n\n```bash\n# 상태 확인\nkubectl rollout status deployment/my-app\n\n# 이벤트 확인\nkubectl describe deployment my-app\n\n# ReplicaSet 상태 확인\nkubectl get rs -l app=my-app\n```\n\n**흔한 원인**:\n\n1. **이미지 Pull 실패**: ImagePullBackOff\n2. **리소스 부족**: Pending 상태\n3. **Liveness/Readiness Probe 실패**: CrashLoopBackOff\n4. **maxUnavailable 0**: 기존 Pod 삭제 안 됨\n\n### StatefulSet Pod가 Pending 상태\n\n```bash\n# PVC 상태 확인\nkubectl get pvc\n\n# 스토리지 이벤트 확인\nkubectl describe pvc data-mysql-0\n```\n\n**흔한 원인**:\n\n1. **StorageClass 없음**: 기본 StorageClass 미설정\n2. **가용 영역(AZ) 불일치**: EBS는 같은 AZ에서만 마운트\n3. **용량 부족**: AWS EBS 한도 초과\n\n### CronJob이 실행되지 않음\n\n```bash\n# CronJob 상태 확인\nkubectl get cronjob\n\n# 최근 Job 확인\nkubectl get jobs --sort-by=.metadata.creationTimestamp\n\n# 마지막 스케줄 시간 확인\nkubectl describe cronjob daily-report | grep \"Last Schedule\"\n```\n\n**흔한 원인**:\n\n1. **startingDeadlineSeconds 초과**: 스케줄 시간 놓침\n2. **concurrencyPolicy: Forbid + 긴 실행 시간**: 이전 Job이 계속 실행 중\n3. **suspend: true**: CronJob이 일시 중지됨\n\n---\n\n## 정리\n\n| 워크로드 | 핵심 특징 | 사용 사례 |\n|---------|----------|----------|\n| **Deployment** | ReplicaSet 관리, Rolling Update | 무상태 웹 서비스 |\n| **StatefulSet** | 순차적 생성, 고정 네트워크 ID, PVC 유지 | DB, 메시지 큐 |\n| **DaemonSet** | 노드당 1개 Pod 보장 | 로그/모니터링 에이전트 |\n| **Job** | 일회성 완료 작업 | 마이그레이션, 배치 |\n| **CronJob** | 정기적 Job 스케줄링 | 리포트, 정리 작업 |\n\n---\n\n## 다음 편 예고\n\n**2편: 서비스 네트워킹 심화**에서는 다음을 다룹니다:\n\n- Service 타입별 내부 동작 (ClusterIP, NodePort, LoadBalancer)\n- kube-proxy의 iptables vs IPVS 모드\n- AWS ALB/NLB Ingress Controller\n- ExternalDNS와 Route 53 통합\n\n---\n\n## 참고 자료\n\n- [Kubernetes Controllers](https://kubernetes.io/docs/concepts/architecture/controller/)\n- [Deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)\n- [StatefulSets](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/)\n- [CronJob](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "Kubernetes",
      "Controller"
    ],
    "readingTime": 9,
    "wordCount": 1721,
    "isFeatured": false,
    "isPublic": true,
    "series": "kubernetes",
    "date": "2026-01-03"
  },
  {
    "id": "asyncio-concurrency-guide",
    "slug": "asyncio-concurrency-guide",
    "path": "languages/python",
    "fullPath": "languages/python/asyncio-concurrency-guide",
    "title": "Python asyncio 비동기 프로그래밍 완벽 가이드",
    "excerpt": "Python asyncio를 활용한 비동기 프로그래밍의 핵심 개념과 실전 동시성 패턴을 알아봅니다.",
    "content": "# Python asyncio 비동기 프로그래밍 완벽 가이드\n\n## 개요\n\n**asyncio**는 Python의 비동기 I/O 프레임워크로, 코루틴(coroutine)을 기반으로 동시성을 구현합니다. I/O 바운드 작업(네트워크, 파일)에서 스레드보다 효율적인 동시 처리가 가능합니다.\n\n## 핵심 개념\n\n### 코루틴 (Coroutine)\n\n```python\nimport asyncio\n\n# async def로 정의된 함수 = 코루틴 함수\nasync def fetch_data(url: str) -> dict:\n    print(f\"Fetching {url}\")\n    await asyncio.sleep(1)  # 비동기 대기\n    return {\"url\": url, \"data\": \"...\"}\n\n# 코루틴 실행\nasync def main():\n    result = await fetch_data(\"https://api.example.com\")\n    print(result)\n\nasyncio.run(main())  # 진입점\n```\n\n### 이벤트 루프 (Event Loop)\n\n```python\n# asyncio.run()이 내부적으로 수행하는 작업\nloop = asyncio.new_event_loop()\nasyncio.set_event_loop(loop)\ntry:\n    loop.run_until_complete(main())\nfinally:\n    loop.close()\n```\n\n> [!NOTE]\n> Python 3.10+에서는 `asyncio.run()`만 사용하면 됩니다. 직접 이벤트 루프를 관리할 필요가 거의 없습니다.\n\n## 동시 실행 패턴\n\n### asyncio.gather - 병렬 실행\n\n```python\nasync def main():\n    # 동시에 3개 요청 실행\n    results = await asyncio.gather(\n        fetch_data(\"https://api1.example.com\"),\n        fetch_data(\"https://api2.example.com\"),\n        fetch_data(\"https://api3.example.com\"),\n    )\n    print(results)  # [result1, result2, result3]\n```\n\n**에러 처리**:\n\n```python\nasync def main():\n    results = await asyncio.gather(\n        fetch_data(\"url1\"),\n        fetch_data(\"url2\"),\n        return_exceptions=True,  # 예외를 결과로 반환\n    )\n    for result in results:\n        if isinstance(result, Exception):\n            print(f\"Error: {result}\")\n        else:\n            print(f\"Success: {result}\")\n```\n\n### asyncio.TaskGroup - 구조적 동시성 (3.11+)\n\n```python\nasync def main():\n    async with asyncio.TaskGroup() as tg:\n        task1 = tg.create_task(fetch_data(\"url1\"))\n        task2 = tg.create_task(fetch_data(\"url2\"))\n        task3 = tg.create_task(fetch_data(\"url3\"))\n    \n    # 모든 태스크 완료 후 결과 접근\n    print(task1.result(), task2.result(), task3.result())\n```\n\n> [!IMPORTANT]\n> `TaskGroup`은 하나의 태스크가 실패하면 나머지를 자동 취소합니다. 에러 전파가 명확합니다.\n\n### asyncio.create_task - 백그라운드 실행\n\n```python\nasync def background_job():\n    while True:\n        print(\"Background running...\")\n        await asyncio.sleep(5)\n\nasync def main():\n    # 백그라운드 태스크 시작\n    task = asyncio.create_task(background_job())\n    \n    # 메인 로직 실행\n    await do_main_work()\n    \n    # 태스크 취소\n    task.cancel()\n    try:\n        await task\n    except asyncio.CancelledError:\n        print(\"Background job cancelled\")\n```\n\n### as_completed - 완료 순서대로 처리\n\n```python\nasync def main():\n    tasks = [\n        asyncio.create_task(fetch_data(f\"url{i}\"))\n        for i in range(5)\n    ]\n    \n    # 완료되는 순서대로 결과 처리\n    for coro in asyncio.as_completed(tasks):\n        result = await coro\n        print(f\"Got: {result}\")\n```\n\n## 동기화 프리미티브\n\n### Semaphore - 동시 실행 제한\n\n```python\nasync def fetch_with_limit(sem: asyncio.Semaphore, url: str):\n    async with sem:  # 세마포어 획득\n        return await fetch_data(url)\n\nasync def main():\n    sem = asyncio.Semaphore(5)  # 최대 5개 동시 실행\n    urls = [f\"url{i}\" for i in range(100)]\n    \n    tasks = [fetch_with_limit(sem, url) for url in urls]\n    results = await asyncio.gather(*tasks)\n```\n\n### Lock - 상호 배제\n\n```python\nclass Counter:\n    def __init__(self):\n        self.value = 0\n        self._lock = asyncio.Lock()\n    \n    async def increment(self):\n        async with self._lock:\n            current = self.value\n            await asyncio.sleep(0.01)  # 시뮬레이션\n            self.value = current + 1\n```\n\n### Event - 이벤트 알림\n\n```python\nasync def waiter(event: asyncio.Event):\n    print(\"Waiting for event...\")\n    await event.wait()\n    print(\"Event received!\")\n\nasync def setter(event: asyncio.Event):\n    await asyncio.sleep(2)\n    event.set()\n    print(\"Event set!\")\n\nasync def main():\n    event = asyncio.Event()\n    await asyncio.gather(waiter(event), setter(event))\n```\n\n### Queue - 생산자-소비자 패턴\n\n```python\nasync def producer(queue: asyncio.Queue):\n    for i in range(10):\n        await queue.put(f\"item-{i}\")\n        print(f\"Produced: item-{i}\")\n        await asyncio.sleep(0.1)\n\nasync def consumer(queue: asyncio.Queue, name: str):\n    while True:\n        item = await queue.get()\n        print(f\"{name} consumed: {item}\")\n        queue.task_done()\n\nasync def main():\n    queue = asyncio.Queue(maxsize=5)\n    \n    producers = [asyncio.create_task(producer(queue))]\n    consumers = [\n        asyncio.create_task(consumer(queue, f\"consumer-{i}\"))\n        for i in range(3)\n    ]\n    \n    await asyncio.gather(*producers)\n    await queue.join()  # 모든 아이템 처리 대기\n    \n    for c in consumers:\n        c.cancel()\n```\n\n## 타임아웃 처리\n\n### asyncio.timeout (3.11+)\n\n```python\nasync def main():\n    try:\n        async with asyncio.timeout(5.0):  # 5초 제한\n            await long_running_task()\n    except TimeoutError:\n        print(\"Task timed out!\")\n```\n\n### asyncio.wait_for (레거시)\n\n```python\nasync def main():\n    try:\n        result = await asyncio.wait_for(\n            long_running_task(),\n            timeout=5.0\n        )\n    except asyncio.TimeoutError:\n        print(\"Task timed out!\")\n```\n\n## 동기 코드와 통합\n\n### run_in_executor - 블로킹 함수 실행\n\n```python\nimport concurrent.futures\n\ndef blocking_io():\n    \"\"\"동기 블로킹 함수\"\"\"\n    import time\n    time.sleep(2)\n    return \"IO completed\"\n\nasync def main():\n    loop = asyncio.get_event_loop()\n    \n    # ThreadPoolExecutor (I/O 바운드)\n    with concurrent.futures.ThreadPoolExecutor() as pool:\n        result = await loop.run_in_executor(pool, blocking_io)\n        print(result)\n    \n    # ProcessPoolExecutor (CPU 바운드)\n    with concurrent.futures.ProcessPoolExecutor() as pool:\n        result = await loop.run_in_executor(pool, cpu_intensive_task)\n```\n\n### asyncio.to_thread (3.9+)\n\n```python\nasync def main():\n    # 간편한 방법\n    result = await asyncio.to_thread(blocking_io)\n    print(result)\n```\n\n## 실전 패턴\n\n### Retry with Exponential Backoff\n\n```python\nasync def fetch_with_retry(\n    url: str,\n    max_retries: int = 3,\n    base_delay: float = 1.0\n) -> dict:\n    for attempt in range(max_retries):\n        try:\n            return await fetch_data(url)\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise\n            delay = base_delay * (2 ** attempt)\n            print(f\"Retry {attempt + 1} after {delay}s: {e}\")\n            await asyncio.sleep(delay)\n```\n\n### Rate Limiter\n\n```python\nclass RateLimiter:\n    def __init__(self, rate: int, per: float):\n        self.rate = rate\n        self.per = per\n        self.tokens = rate\n        self.updated_at = asyncio.get_event_loop().time()\n        self._lock = asyncio.Lock()\n    \n    async def acquire(self):\n        async with self._lock:\n            now = asyncio.get_event_loop().time()\n            elapsed = now - self.updated_at\n            self.tokens = min(self.rate, self.tokens + elapsed * (self.rate / self.per))\n            self.updated_at = now\n            \n            if self.tokens < 1:\n                wait_time = (1 - self.tokens) * (self.per / self.rate)\n                await asyncio.sleep(wait_time)\n                self.tokens = 0\n            else:\n                self.tokens -= 1\n```\n\n### Graceful Shutdown\n\n```python\nimport signal\n\nasync def shutdown(loop, signal=None):\n    if signal:\n        print(f\"Received signal {signal.name}\")\n    \n    tasks = [t for t in asyncio.all_tasks() if t is not asyncio.current_task()]\n    [task.cancel() for task in tasks]\n    \n    await asyncio.gather(*tasks, return_exceptions=True)\n    loop.stop()\n\ndef main():\n    loop = asyncio.new_event_loop()\n    \n    for sig in (signal.SIGTERM, signal.SIGINT):\n        loop.add_signal_handler(\n            sig,\n            lambda s=sig: asyncio.create_task(shutdown(loop, s))\n        )\n    \n    try:\n        loop.run_until_complete(run_server())\n    finally:\n        loop.close()\n```\n\n## 디버깅 팁\n\n### 디버그 모드 활성화\n\n```python\n# 환경변수\n# PYTHONASYNCIODEBUG=1\n\n# 또는 코드에서\nasyncio.run(main(), debug=True)\n```\n\n### 느린 콜백 감지\n\n```python\nloop = asyncio.get_event_loop()\nloop.slow_callback_duration = 0.1  # 100ms 이상 경고\n```\n\n## 주의사항\n\n1. ⚠️ **async 함수 내에서 `time.sleep()` 금지** - `await asyncio.sleep()` 사용\n2. ⚠️ **CPU 바운드 작업은 ProcessPoolExecutor** - 이벤트 루프 블로킹 방지\n3. ⚠️ **코루틴 호출 후 await 필수** - `await` 없이 호출하면 실행되지 않음\n4. ⚠️ **태스크 취소 시 CancelledError 처리** - 리소스 정리 필요\n\n## 참고 자료\n\n- [asyncio 공식 문서](https://docs.python.org/3/library/asyncio.html)\n- [Real Python: Async IO in Python](https://realpython.com/async-io-python/)\n- [PEP 492 – Coroutines](https://peps.python.org/pep-0492/)",
    "docType": "original",
    "category": "Languages",
    "tags": [
      "Concurrency",
      "Python"
    ],
    "readingTime": 5,
    "wordCount": 917,
    "isFeatured": false,
    "isPublic": true,
    "date": "2026-01-02"
  },
  {
    "id": "postgresql-query-tuning-guide",
    "slug": "postgresql-query-tuning-guide",
    "path": "database/postgresql",
    "fullPath": "database/postgresql/postgresql-query-tuning-guide",
    "title": "PostgreSQL 쿼리 성능 튜닝 완벽 가이드",
    "excerpt": "EXPLAIN ANALYZE를 활용한 쿼리 분석과 인덱스 전략으로 PostgreSQL 성능을 극대화하는 방법을 알아봅니다.",
    "content": "# PostgreSQL 쿼리 성능 튜닝 완벽 가이드\n\n## 개요\n\nPostgreSQL의 쿼리 성능 튜닝은 **EXPLAIN ANALYZE를 통한 문제 진단**과 **적절한 인덱스 설계**가 핵심입니다. 이 가이드에서는 실전에서 바로 적용할 수 있는 튜닝 전략을 다룹니다.\n\n## EXPLAIN ANALYZE 기초\n\n### 기본 사용법\n\n```sql\n-- 기본 실행 계획\nEXPLAIN SELECT * FROM users WHERE email = 'test@example.com';\n\n-- 실제 실행 통계 포함 (권장)\nEXPLAIN ANALYZE SELECT * FROM users WHERE email = 'test@example.com';\n\n-- 상세 정보 포함\nEXPLAIN (ANALYZE, BUFFERS, FORMAT TEXT)\nSELECT * FROM users WHERE email = 'test@example.com';\n```\n\n### 출력 해석\n\n```\nSeq Scan on users  (cost=0.00..1520.00 rows=1 width=256) (actual time=12.345..45.678 rows=1 loops=1)\n  Filter: (email = 'test@example.com'::text)\n  Rows Removed by Filter: 49999\n  Buffers: shared hit=520 read=200\nPlanning Time: 0.150 ms\nExecution Time: 45.720 ms\n```\n\n| 항목 | 설명 |\n|-----|------|\n| `cost` | 예상 비용 (시작..총) |\n| `rows` | 예상 반환 행 수 |\n| `actual time` | 실제 실행 시간 (ms) |\n| `Buffers` | 버퍼 캐시 hit/read 통계 |\n| `Rows Removed by Filter` | 필터링으로 제거된 행 수 ⚠️ |\n\n> [!WARNING]\n> `Rows Removed by Filter`가 크면 인덱스가 없거나 비효율적인 상태입니다.\n\n### 주요 스캔 타입\n\n| 스캔 타입 | 설명 | 성능 |\n|---------|------|-----|\n| `Seq Scan` | 전체 테이블 스캔 | 🔴 느림 |\n| `Index Scan` | 인덱스 + 테이블 접근 | 🟢 빠름 |\n| `Index Only Scan` | 인덱스만으로 해결 | 🟢🟢 가장 빠름 |\n| `Bitmap Index Scan` | 여러 조건 병합 | 🟡 조건부 |\n\n## 인덱스 전략\n\n### B-Tree 인덱스 (기본)\n\n```sql\n-- 단일 컬럼 인덱스\nCREATE INDEX idx_users_email ON users(email);\n\n-- 복합 인덱스 (순서 중요!)\nCREATE INDEX idx_orders_user_created ON orders(user_id, created_at DESC);\n```\n\n> [!IMPORTANT]\n> 복합 인덱스의 첫 번째 컬럼이 WHERE 조건에 없으면 인덱스를 타지 않습니다.\n\n### 복합 인덱스 설계 원칙\n\n```sql\n-- ❌ 잘못된 예: 두 번째 컬럼만 조회\nSELECT * FROM orders WHERE created_at > '2025-01-01';  -- idx_orders_user_created 사용 불가\n\n-- ✅ 올바른 예: 첫 번째 컬럼 포함\nSELECT * FROM orders WHERE user_id = 123 AND created_at > '2025-01-01';\n```\n\n**복합 인덱스 컬럼 순서 가이드라인**:\n\n1. **등호(=) 조건** → 앞쪽\n2. **범위(>, <, BETWEEN)** → 뒤쪽\n3. **높은 카디널리티** → 앞쪽\n\n### Covering Index (포함 인덱스)\n\n```sql\n-- INCLUDE로 추가 컬럼 포함 → Index Only Scan 가능\nCREATE INDEX idx_users_email_include ON users(email) INCLUDE (name, created_at);\n\n-- 쿼리\nSELECT email, name, created_at FROM users WHERE email = 'test@example.com';\n-- → Index Only Scan 발생!\n```\n\n### Partial Index (부분 인덱스)\n\n```sql\n-- 특정 조건의 행만 인덱싱\nCREATE INDEX idx_orders_active ON orders(user_id)\nWHERE status = 'PENDING';\n\n-- 활성 주문만 자주 조회할 때 효율적\nSELECT * FROM orders WHERE user_id = 123 AND status = 'PENDING';\n```\n\n### Expression Index (표현식 인덱스)\n\n```sql\n-- 함수 결과에 인덱스 생성\nCREATE INDEX idx_users_lower_email ON users(LOWER(email));\n\n-- 사용\nSELECT * FROM users WHERE LOWER(email) = 'test@example.com';\n```\n\n## 조인 최적화\n\n### 조인 타입 이해\n\n```\nEXPLAIN ANALYZE\nSELECT u.name, o.total FROM users u\nJOIN orders o ON u.id = o.user_id;\n```\n\n| 조인 타입 | 설명 | 최적 상황 |\n|---------|------|---------|\n| `Nested Loop` | 중첩 반복 | 작은 테이블 조인 |\n| `Hash Join` | 해시 테이블 생성 | 큰 테이블 등가 조인 |\n| `Merge Join` | 정렬 후 병합 | 정렬된 대량 데이터 |\n\n### 조인 순서 힌트\n\n```sql\n-- 조인 순서 고정 (드물게 사용)\nSET join_collapse_limit = 1;\n\n-- 또는 쿼리에서 순서 지정\nSELECT /*+ Leading(users orders) */ ...\n```\n\n## 통계 관리\n\n### 통계 업데이트\n\n```sql\n-- 테이블 통계 갱신 (필수!)\nANALYZE users;\n\n-- 전체 데이터베이스\nANALYZE;\n\n-- 통계 정보 확인\nSELECT \n    tablename,\n    n_live_tup,\n    n_dead_tup,\n    last_analyze,\n    last_autoanalyze\nFROM pg_stat_user_tables;\n```\n\n### 통계 샘플 크기 조정\n\n```sql\n-- 특정 컬럼의 통계 정밀도 증가 (기본값: 100)\nALTER TABLE users ALTER COLUMN email SET STATISTICS 1000;\nANALYZE users;\n```\n\n## 실전 문제 진단\n\n### 느린 쿼리 찾기\n\n```sql\n-- pg_stat_statements 확장 활성화 필요\nSELECT \n    query,\n    calls,\n    mean_exec_time,\n    total_exec_time,\n    rows\nFROM pg_stat_statements\nORDER BY mean_exec_time DESC\nLIMIT 10;\n```\n\n### 사용되지 않는 인덱스 탐지\n\n```sql\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    idx_scan,\n    idx_tup_read\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0\nAND indexname NOT LIKE 'pg_%';\n```\n\n### 누락된 인덱스 힌트\n\n```sql\n-- 순차 스캔 비율이 높은 테이블\nSELECT \n    relname,\n    seq_scan,\n    idx_scan,\n    ROUND(100.0 * seq_scan / NULLIF(seq_scan + idx_scan, 0), 2) AS seq_scan_pct\nFROM pg_stat_user_tables\nWHERE seq_scan + idx_scan > 100\nORDER BY seq_scan_pct DESC;\n```\n\n## 성능 설정 튜닝\n\n### 메모리 관련\n\n```sql\n-- 정렬/해시 작업에 사용할 메모리 (세션별)\nSET work_mem = '256MB';\n\n-- shared_buffers (postgresql.conf, RAM의 25%)\nshared_buffers = '4GB'\n\n-- 효과적인 캐시 크기 (쿼리 플래너 힌트)\neffective_cache_size = '12GB'\n```\n\n### 플래너 관련\n\n```sql\n-- 순차 스캔 비용 조정 (SSD는 낮게)\nSET random_page_cost = 1.1;  -- 기본값 4.0\n\n-- 병렬 쿼리 활성화\nSET max_parallel_workers_per_gather = 4;\n```\n\n## 모범 사례 체크리스트\n\n1. ✅ **정기적으로 ANALYZE 실행** - autovacuum 설정 확인\n2. ✅ **복합 인덱스 컬럼 순서** - 등호 조건 → 범위 조건\n3. ✅ **Covering Index 활용** - Index Only Scan 유도\n4. ✅ **Partial Index** - 자주 조회하는 서브셋에 적용\n5. ✅ **pg_stat_statements 활용** - 느린 쿼리 모니터링\n6. ✅ **사용하지 않는 인덱스 제거** - 쓰기 성능 저하 방지\n\n## 참고 자료\n\n- [PostgreSQL EXPLAIN 공식 문서](https://www.postgresql.org/docs/current/sql-explain.html)\n- [Using EXPLAIN](https://www.postgresql.org/docs/current/using-explain.html)\n- [Index Types](https://www.postgresql.org/docs/current/indexes-types.html)",
    "docType": "original",
    "category": "Database",
    "tags": [
      "Database",
      "Performance",
      "PostgreSQL"
    ],
    "readingTime": 5,
    "wordCount": 856,
    "isFeatured": false,
    "isPublic": true,
    "date": "2026-01-02"
  },
  {
    "id": "de-12-data-quality",
    "slug": "de-12-data-quality",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-12-data-quality",
    "title": "데이터 엔지니어링 시리즈 #12: 데이터 품질 - 테스트, 모니터링, 관측성",
    "excerpt": "데이터 파이프라인의 품질을 보장하는 방법을 배웁니다. dbt 테스트, Great Expectations, 데이터 계보, 관측성까지.",
    "content": "# 데이터 엔지니어링 시리즈 #12: 데이터 품질 - 테스트, 모니터링, 관측성\n\n> **대상 독자**: 충분한 경험을 가진 백엔드/풀스택 엔지니어로, 소프트웨어 테스트에 익숙하지만 데이터 테스트는 처음인 분\n\n## 이 편에서 다루는 것\n\n\"대시보드 숫자가 왜 어제와 달라요?\" 이런 질문에 체계적으로 답할 수 있는 **데이터 품질 관리 체계**를 배웁니다.\n\n---\n\n## 데이터 품질이란?\n\n### 품질의 다섯 가지 차원\n\n```mermaid\nflowchart TB\n    subgraph Dimensions [\"데이터 품질 차원\"]\n        C[\"완전성<br/>(Completeness)<br/>NULL이 없는가?\"]\n        A[\"정확성<br/>(Accuracy)<br/>값이 올바른가?\"]\n        Con[\"일관성<br/>(Consistency)<br/>규칙에 맞는가?\"]\n        T[\"적시성<br/>(Timeliness)<br/>최신인가?\"]\n        V[\"유효성<br/>(Validity)<br/>형식이 맞는가?\"]\n    end\n```\n\n### 소프트웨어 테스트와의 비교\n\n| 특성 | 소프트웨어 테스트 | 데이터 테스트 |\n|------|------------------|--------------|\n| **대상** | 코드 | 데이터 |\n| **시점** | 배포 전 | 파이프라인 실행 중/후 |\n| **입력** | 고정 (mock) | 변동 (실제 데이터) |\n| **실패 대응** | 배포 중단 | 알림/재처리/격리 |\n| **도구** | JUnit, Jest | dbt, Great Expectations |\n\n---\n\n## dbt: 변환과 테스트의 통합\n\n### dbt란?\n\n```mermaid\nflowchart LR\n    subgraph dbt [\"dbt (data build tool)\"]\n        Models[\"SQL 모델\"]\n        Tests[\"테스트\"]\n        Docs[\"문서화\"]\n        \n        Models --> Tests --> Docs\n    end\n    \n    subgraph Workflow [\"워크플로우\"]\n        Source[\"원본 데이터\"]\n        Transform[\"변환\"]\n        Target[\"결과 테이블\"]\n        \n        Source --> Transform --> Target\n    end\n    \n    dbt --> Workflow\n```\n\n### 핵심 철학\n\n1. **SQL 기반**: 복잡한 코드 없이 SQL만으로 변환\n2. **버전 관리**: Git으로 모델 관리\n3. **테스트 내장**: 스키마에 테스트 정의\n4. **문서 자동화**: 모델 정보 자동 생성\n5. **의존성 관리**: ref() 함수로 모델 간 의존성\n\n### 프로젝트 구조\n\n```\nmy_dbt_project/\n├── models/\n│   ├── staging/\n│   │   ├── stg_orders.sql\n│   │   └── schema.yml\n│   ├── marts/\n│   │   ├── fct_orders.sql\n│   │   └── dim_customers.sql\n│   └── schema.yml\n├── tests/\n│   └── custom_tests.sql\n├── macros/\n├── dbt_project.yml\n└── profiles.yml\n```\n\n---\n\n## dbt 테스트\n\n### 테스트 종류\n\n```mermaid\nflowchart TB\n    subgraph Tests [\"dbt 테스트 유형\"]\n        subgraph Schema [\"스키마 테스트\"]\n            S1[\"unique<br/>중복 없음\"]\n            S2[\"not_null<br/>NULL 없음\"]\n            S3[\"accepted_values<br/>허용값 목록\"]\n            S4[\"relationships<br/>참조 무결성\"]\n        end\n        \n        subgraph Custom [\"커스텀 테스트\"]\n            C1[\"SQL 기반\"]\n            C2[\"복잡한 로직\"]\n            C3[\"매크로 재사용\"]\n        end\n    end\n```\n\n### schema.yml 작성\n\n```yaml\n# models/marts/schema.yml\nversion: 2\n\nmodels:\n  - name: fct_orders\n    description: \"주문 Fact 테이블\"\n    columns:\n      - name: order_id\n        description: \"주문 고유 ID\"\n        data_tests:\n          - unique\n          - not_null\n      \n      - name: customer_id\n        description: \"고객 ID\"\n        data_tests:\n          - not_null\n          - relationships:\n              to: ref('dim_customers')\n              field: customer_id\n      \n      - name: order_status\n        description: \"주문 상태\"\n        data_tests:\n          - accepted_values:\n              values: ['pending', 'shipped', 'delivered', 'cancelled']\n      \n      - name: total_amount\n        description: \"주문 총액\"\n        data_tests:\n          - not_null\n          # dbt_utils 패키지 사용\n          - dbt_utils.expression_is_true:\n              expression: \">= 0\"\n```\n\n### 커스텀 테스트\n\n```sql\n-- tests/assert_positive_revenue.sql\n-- 총 매출이 양수인지 확인\n\nSELECT \n    order_date,\n    SUM(total_amount) as daily_revenue\nFROM {{ ref('fct_orders') }}\nGROUP BY order_date\nHAVING SUM(total_amount) < 0\n```\n\n### 테스트 실행\n\n```bash\n# 모든 테스트 실행\ndbt test\n\n# 특정 모델 테스트\ndbt test --select fct_orders\n\n# 실패 시 상세 정보\ndbt test --store-failures\n```\n\n---\n\n## 데이터 Freshness\n\n### Source Freshness\n\n```yaml\n# models/staging/sources.yml\nversion: 2\n\nsources:\n  - name: raw\n    database: production\n    schema: public\n    freshness:\n      warn_after: {count: 12, period: hour}\n      error_after: {count: 24, period: hour}\n    \n    tables:\n      - name: orders\n        loaded_at_field: _etl_loaded_at\n        \n      - name: customers\n        loaded_at_field: updated_at\n```\n\n```bash\n# Freshness 체크\ndbt source freshness\n```\n\n### Freshness 결과\n\n```mermaid\nflowchart TB\n    subgraph Status [\"Freshness 상태\"]\n        Pass[\"✅ Pass<br/>12시간 이내\"]\n        Warn[\"⚠️ Warn<br/>12~24시간\"]\n        Error[\"❌ Error<br/>24시간 초과\"]\n    end\n```\n\n---\n\n## Great Expectations\n\n### dbt와의 비교\n\n| 특성 | dbt | Great Expectations |\n|------|-----|-------------------|\n| **언어** | SQL | Python |\n| **적합한 경우** | SQL 변환 후 테스트 | 원본 데이터 검증 |\n| **학습 곡선** | 낮음 | 중간 |\n| **유연성** | 제한적 | 높음 |\n| **문서화** | 자동 | 자동 (Data Docs) |\n\n### 기본 사용법\n\n```python\nimport great_expectations as gx\n\n# Context 생성\ncontext = gx.get_context()\n\n# 데이터 소스 연결\ndatasource = context.sources.add_pandas(\"my_datasource\")\ndata_asset = datasource.add_dataframe_asset(\"orders\")\n\n# Expectation Suite 정의\nsuite = context.add_expectation_suite(\"orders_suite\")\n\n# Expectations 추가\nsuite.add_expectation(\n    gx.expectations.ExpectColumnValuesToBeUnique(column=\"order_id\")\n)\nsuite.add_expectation(\n    gx.expectations.ExpectColumnValuesToNotBeNull(column=\"customer_id\")\n)\nsuite.add_expectation(\n    gx.expectations.ExpectColumnValuesToBeBetween(\n        column=\"total_amount\",\n        min_value=0,\n        max_value=1000000\n    )\n)\nsuite.add_expectation(\n    gx.expectations.ExpectColumnValuesToMatchRegex(\n        column=\"email\",\n        regex=r\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n    )\n)\n\n# Validation 실행\nbatch = data_asset.build_batch_request()\nresults = context.run_checkpoint(\n    checkpoint_name=\"orders_checkpoint\",\n    batch_request=batch,\n    expectation_suite_name=\"orders_suite\"\n)\n\n# 결과 확인\nprint(f\"Success: {results.success}\")\n```\n\n### Airflow 연동\n\n```python\nfrom airflow.decorators import dag, task\nfrom datetime import datetime\n\n@dag(\n    dag_id=\"data_quality_pipeline\",\n    schedule=\"@daily\",\n    start_date=datetime(2024, 1, 1)\n)\ndef quality_pipeline():\n    \n    @task\n    def run_great_expectations(**context):\n        import great_expectations as gx\n        \n        gx_context = gx.get_context()\n        results = gx_context.run_checkpoint(\n            checkpoint_name=\"orders_checkpoint\"\n        )\n        \n        if not results.success:\n            raise ValueError(\"Data quality check failed!\")\n        \n        return {\"success\": True, \"statistics\": results.statistics}\n    \n    @task\n    def run_dbt_tests():\n        import subprocess\n        result = subprocess.run([\"dbt\", \"test\"], capture_output=True)\n        \n        if result.returncode != 0:\n            raise ValueError(f\"dbt tests failed: {result.stderr}\")\n    \n    @task\n    def load_to_warehouse(quality_result):\n        # 품질 검증 통과 후에만 로드\n        print(\"Loading data to warehouse...\")\n    \n    quality = run_great_expectations()\n    dbt = run_dbt_tests()\n    load_to_warehouse(quality)\n    \n    # dbt도 통과해야 함\n    dbt >> load_to_warehouse\n\nquality_pipeline()\n```\n\n---\n\n## 데이터 계보 (Lineage)\n\n### 왜 계보가 중요한가?\n\n```mermaid\nflowchart TB\n    subgraph Problem [\"문제 상황\"]\n        P1[\"대시보드 숫자가 이상해요\"]\n        P2[\"어디서 잘못된 거죠?\"]\n        P3[\"어떤 테이블을 봐야 하죠?\"]\n    end\n    \n    subgraph Lineage [\"Lineage로 해결\"]\n        L1[\"데이터 흐름 추적\"]\n        L2[\"영향 범위 파악\"]\n        L3[\"원인 분석\"]\n    end\n    \n    Problem --> Lineage\n```\n\n### dbt의 자동 Lineage\n\n```mermaid\nflowchart LR\n    subgraph Sources [\"Sources\"]\n        S1[(\"raw.orders\")]\n        S2[(\"raw.customers\")]\n    end\n    \n    subgraph Staging [\"Staging\"]\n        ST1[\"stg_orders\"]\n        ST2[\"stg_customers\"]\n    end\n    \n    subgraph Marts [\"Marts\"]\n        M1[\"fct_orders\"]\n        M2[\"dim_customers\"]\n    end\n    \n    S1 --> ST1 --> M1\n    S2 --> ST2 --> M2\n    ST2 --> M1\n```\n\n```bash\n# Lineage 문서 생성\ndbt docs generate\ndbt docs serve\n```\n\n### OpenLineage 표준\n\n```mermaid\nflowchart TB\n    subgraph Tools [\"다양한 도구\"]\n        Airflow[\"Airflow\"]\n        Spark[\"Spark\"]\n        dbt[\"dbt\"]\n        Flink[\"Flink\"]\n    end\n    \n    subgraph Standard [\"OpenLineage\"]\n        OL[\"통합 포맷\"]\n    end\n    \n    subgraph Catalog [\"데이터 카탈로그\"]\n        Marquez[\"Marquez\"]\n        DataHub[\"DataHub\"]\n        Atlan[\"Atlan\"]\n    end\n    \n    Tools --> Standard --> Catalog\n```\n\n---\n\n## 모니터링과 관측성\n\n### 핵심 지표\n\n```mermaid\nflowchart TB\n    subgraph Metrics [\"데이터 파이프라인 지표\"]\n        subgraph Availability [\"가용성\"]\n            A1[\"파이프라인 성공률\"]\n            A2[\"SLA 준수율\"]\n        end\n        \n        subgraph Quality [\"품질\"]\n            Q1[\"테스트 통과율\"]\n            Q2[\"이상치 비율\"]\n        end\n        \n        subgraph Freshness [\"신선도\"]\n            F1[\"데이터 지연\"]\n            F2[\"마지막 업데이트\"]\n        end\n        \n        subgraph Volume [\"볼륨\"]\n            V1[\"행 수 변화\"]\n            V2[\"파일 크기\"]\n        end\n    end\n```\n\n### 이상 탐지\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import count, avg, stddev\n\nspark = SparkSession.builder.getOrCreate()\n\n# 오늘 데이터\ntoday = spark.read.parquet(\"/data/today\")\ntoday_stats = today.agg(\n    count(\"*\").alias(\"row_count\"),\n    avg(\"amount\").alias(\"avg_amount\")\n).collect()[0]\n\n# 히스토리 (최근 30일 평균)\nhistory = spark.read.parquet(\"/data/history_30d\")\nhistory_stats = history.agg(\n    avg(\"daily_count\").alias(\"avg_count\"),\n    stddev(\"daily_count\").alias(\"std_count\"),\n    avg(\"daily_avg_amount\").alias(\"avg_amount\")\n).collect()[0]\n\n# 이상 탐지 (3-sigma)\nif abs(today_stats[\"row_count\"] - history_stats[\"avg_count\"]) > 3 * history_stats[\"std_count\"]:\n    alert(\"Row count anomaly detected!\")\n```\n\n### 대시보드 구성\n\n```mermaid\nflowchart TB\n    subgraph Dashboard [\"데이터 품질 대시보드\"]\n        subgraph Overview [\"개요\"]\n            O1[\"🟢 95% 파이프라인 성공\"]\n            O2[\"🟡 2개 SLA 경고\"]\n            O3[\"🔴 1개 테스트 실패\"]\n        end\n        \n        subgraph Details [\"상세\"]\n            D1[\"파이프라인별 상태\"]\n            D2[\"테스트 결과\"]\n            D3[\"데이터 볼륨 트렌드\"]\n        end\n        \n        subgraph Alerts [\"알림\"]\n            AL1[\"실패 알림\"]\n            AL2[\"이상 탐지 알림\"]\n        end\n    end\n```\n\n---\n\n## 프로덕션 체크리스트\n\n### 배포 전 확인\n\n```mermaid\nflowchart TB\n    subgraph Checklist [\"프로덕션 체크리스트\"]\n        C1[\"✅ 모든 dbt 테스트 통과\"]\n        C2[\"✅ Source freshness 확인\"]\n        C3[\"✅ Lineage 문서화\"]\n        C4[\"✅ 알림 설정 완료\"]\n        C5[\"✅ 롤백 계획 수립\"]\n        C6[\"✅ 담당자 지정\"]\n    end\n```\n\n### 일일 운영\n\n| 시간 | 작업 | 담당 |\n|------|------|------|\n| 09:00 | 야간 배치 결과 확인 | 온콜 |\n| 09:30 | 테스트 실패 검토 | 데이터 팀 |\n| 10:00 | 이상 알림 처리 | 해당 담당자 |\n| 매시 | 자동 freshness 체크 | 자동화 |\n\n---\n\n## 정리\n\n```mermaid\nmindmap\n  root((데이터<br/>품질))\n    품질 차원\n      완전성\n      정확성\n      일관성\n      적시성\n      유효성\n    dbt\n      SQL 기반\n      스키마 테스트\n      Freshness\n      Lineage\n    Great Expectations\n      Python 기반\n      커스텀 검증\n      Data Docs\n    Lineage\n      데이터 흐름\n      영향 분석\n      OpenLineage\n    모니터링\n      성공률\n      지연 시간\n      이상 탐지\n      대시보드\n```\n\n---\n\n## 시리즈 마무리\n\n12편에 걸쳐 데이터 엔지니어링의 핵심을 다뤘습니다:\n\n| Part | 주제 | 핵심 기술 |\n|------|------|----------|\n| 1-2 | 개념 | OLTP/OLAP, 아키텍처 |\n| 3-5 | Spark | RDD, DataFrame, 최적화 |\n| 6-7 | Airflow | DAG, TaskFlow, 운영 |\n| 8-9 | 스트리밍 | Kafka, Spark Streaming |\n| 10-11 | 저장소 | Lakehouse, 모델링 |\n| 12 | 품질 | 테스트, 모니터링 |\n\n이제 데이터 파이프라인의 전체 그림을 이해하셨을 겁니다. 실제 프로젝트에 적용하면서 깊이를 더해 가시길 바랍니다!\n\n---\n\n## 참고 자료\n\n- [dbt Documentation](https://docs.getdbt.com/)\n- [Great Expectations Documentation](https://docs.greatexpectations.io/)\n- [OpenLineage](https://openlineage.io/)\n- Monte Carlo, \"Data Observability Explained\"\n- \"Fundamentals of Data Engineering\" (O'Reilly)",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Data Engineering",
      "Data Quality",
      "Great Expectations",
      "Lineage",
      "Observability",
      "Testing",
      "dbt"
    ],
    "readingTime": 7,
    "wordCount": 1269,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-11-dimensional-modeling",
    "slug": "de-11-dimensional-modeling",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-11-dimensional-modeling",
    "title": "데이터 엔지니어링 시리즈 #11: 데이터 모델링 - Star Schema와 Slowly Changing Dimensions",
    "excerpt": "분석용 데이터 모델링의 핵심을 배웁니다. Star Schema, Fact/Dimension 테이블, SCD Type 2 패턴을 Delta Lake MERGE로 구현합니다.",
    "content": "# 데이터 엔지니어링 시리즈 #11: 데이터 모델링 - Star Schema와 Slowly Changing Dimensions\n\n> **대상 독자**: 충분한 경험을 가진 백엔드/풀스택 엔지니어로, RDBMS 정규화에 익숙하지만 분석용 모델링은 처음인 분\n\n## 이 편에서 다루는 것\n\n백엔드에서의 DB 설계와 **분석용 데이터 모델링은 완전히 다릅니다**. 왜 다른지, 어떻게 설계하는지 배웁니다.\n\n---\n\n## OLTP vs OLAP 모델링의 차이\n\n### 설계 목표가 다르다\n\n```mermaid\nflowchart TB\n    subgraph OLTP [\"OLTP (운영 DB)\"]\n        T1[\"목표: 데이터 무결성\"]\n        T2[\"정규화 (3NF)\"]\n        T3[\"중복 최소화\"]\n        T4[\"빠른 단건 조회/수정\"]\n    end\n    \n    subgraph OLAP [\"OLAP (분석 DB)\"]\n        A1[\"목표: 쿼리 성능\"]\n        A2[\"비정규화\"]\n        A3[\"중복 허용\"]\n        A4[\"빠른 집계 쿼리\"]\n    end\n```\n\n### 예시로 비교\n\n**OLTP (정규화)**:\n\n```sql\n-- 주문 조회: 3개 테이블 조인 필요\nSELECT o.id, c.name, p.product_name\nFROM orders o\nJOIN customers c ON o.customer_id = c.id\nJOIN products p ON o.product_id = p.id\nWHERE o.id = 12345;\n```\n\n**OLAP (비정규화)**:\n\n```sql\n-- 이미 조인된 상태로 저장\nSELECT order_id, customer_name, product_name\nFROM fact_orders\nWHERE order_id = 12345;\n```\n\n### 왜 비정규화하는가?\n\n```mermaid\nflowchart TB\n    subgraph Problem [\"조인의 비용\"]\n        P1[\"100만 주문 × 10만 고객 × 1000 상품\"]\n        P2[\"3-way 조인 = 매우 느림\"]\n        P3[\"분석 쿼리마다 반복\"]\n    end\n    \n    subgraph Solution [\"비정규화\"]\n        S1[\"한 번 조인해서 저장\"]\n        S2[\"이후 쿼리는 스캔만\"]\n        S3[\"저장 공간 ↔ 쿼리 속도 트레이드오프\"]\n    end\n    \n    Problem --> Solution\n```\n\n---\n\n## Kimball vs Inmon\n\n### 두 가지 방법론\n\n```mermaid\nflowchart TB\n    subgraph Kimball [\"Kimball (Bottom-Up)\"]\n        K1[\"비즈니스 프로세스별 설계\"]\n        K2[\"Data Mart 먼저\"]\n        K3[\"Star Schema\"]\n        K4[\"빠른 구현\"]\n    end\n    \n    subgraph Inmon [\"Inmon (Top-Down)\"]\n        I1[\"전사 데이터 모델 먼저\"]\n        I2[\"Enterprise DW\"]\n        I3[\"3NF 유지\"]\n        I4[\"Data Mart는 나중에\"]\n    end\n```\n\n| 특성 | Kimball | Inmon |\n|------|---------|-------|\n| **접근** | Bottom-Up | Top-Down |\n| **시작점** | 비즈니스 요구 | 전사 모델 |\n| **구현 속도** | 빠름 | 느림 |\n| **일관성** | Mart별 다를 수 있음 | 높음 |\n| **복잡도** | 낮음 | 높음 |\n| **현대 트렌드** | ✅ 선호 | 일부 채택 |\n\n> **현대 트렌드**: Kimball 방식이 **더 실용적**으로 평가. 빠르게 가치 제공 후 점진적 확장.\n\n---\n\n## Star Schema\n\n### 구조\n\n```mermaid\nflowchart TB\n    subgraph Star [\"Star Schema\"]\n        Fact[\"📊 fact_orders<br/>(Fact Table)\"]\n        \n        DimCustomer[\"👤 dim_customers\"]\n        DimProduct[\"📦 dim_products\"]\n        DimDate[\"📅 dim_date\"]\n        DimStore[\"🏪 dim_stores\"]\n        \n        Fact --> DimCustomer\n        Fact --> DimProduct\n        Fact --> DimDate\n        Fact --> DimStore\n    end\n```\n\n### Fact Table (사실 테이블)\n\n```mermaid\nflowchart TB\n    subgraph FactTable [\"Fact Table: fact_orders\"]\n        direction TB\n        Keys[\"🔑 Foreign Keys<br/>customer_key, product_key,<br/>date_key, store_key\"]\n        Measures[\"📏 Measures (측정값)<br/>quantity, amount,<br/>discount, tax\"]\n        Grain[\"📍 Grain: 주문 1건\"]\n    end\n```\n\n```sql\nCREATE TABLE fact_orders (\n    -- Surrogate Keys (FK)\n    order_key       BIGINT,\n    customer_key    BIGINT,\n    product_key     BIGINT,\n    date_key        INT,\n    store_key       BIGINT,\n    \n    -- Measures\n    quantity        INT,\n    unit_price      DECIMAL(10,2),\n    discount        DECIMAL(5,2),\n    total_amount    DECIMAL(12,2),\n    \n    -- Degenerate Dimension (원본 키)\n    order_id        VARCHAR(50)\n);\n```\n\n### Dimension Table (차원 테이블)\n\n```mermaid\nflowchart TB\n    subgraph DimTable [\"Dimension Table: dim_customers\"]\n        direction TB\n        SK[\"🔑 Surrogate Key<br/>customer_key (자동 생성)\"]\n        NK[\"🏷️ Natural Key<br/>customer_id (원본 ID)\"]\n        Attrs[\"📋 Attributes<br/>name, email, segment,<br/>city, country\"]\n    end\n```\n\n```sql\nCREATE TABLE dim_customers (\n    -- Surrogate Key\n    customer_key    BIGINT PRIMARY KEY,\n    \n    -- Natural Key\n    customer_id     VARCHAR(50),\n    \n    -- Attributes\n    name            VARCHAR(200),\n    email           VARCHAR(200),\n    segment         VARCHAR(50),\n    city            VARCHAR(100),\n    country         VARCHAR(50),\n    \n    -- Metadata\n    created_at      TIMESTAMP,\n    updated_at      TIMESTAMP\n);\n```\n\n### 왜 Surrogate Key인가?\n\n```mermaid\nflowchart TB\n    subgraph Problem [\"Natural Key 문제\"]\n        P1[\"customer_id 변경되면?\"]\n        P2[\"여러 소스에서 중복?\"]\n        P3[\"데이터 타입이 다르면?\"]\n    end\n    \n    subgraph Solution [\"Surrogate Key 해결\"]\n        S1[\"내부 생성 정수 키\"]\n        S2[\"변경 없이 안정적\"]\n        S3[\"조인 성능 우수\"]\n    end\n    \n    Problem --> Solution\n```\n\n---\n\n## Slowly Changing Dimensions (SCD)\n\n### 문제 상황\n\n```mermaid\nflowchart TB\n    subgraph Problem [\"차원 데이터 변경\"]\n        P1[\"고객 '김철수'가<br/>서울 → 부산 이사\"]\n        P2[\"기존 주문은<br/>어느 도시로 보여야 할까?\"]\n    end\n    \n    subgraph Options [\"선택지\"]\n        O1[\"항상 '부산' (현재 값)\"]\n        O2[\"주문 당시 '서울' (히스토리)\"]\n    end\n```\n\n### SCD 유형\n\n| Type | 전략 | 설명 | 히스토리 |\n|------|------|------|----------|\n| **Type 0** | 유지 | 변경하지 않음 | ❌ |\n| **Type 1** | 덮어쓰기 | 최신 값으로 교체 | ❌ |\n| **Type 2** | 히스토리 | 새 행 추가 | ✅ |\n| **Type 3** | 이전값 컬럼 | 현재 + 이전 값 | 제한적 |\n\n### SCD Type 2 상세\n\n```mermaid\nflowchart TB\n    subgraph Before [\"변경 전\"]\n        R1[\"customer_key=1<br/>name='김철수'<br/>city='서울'<br/>is_current=true\"]\n    end\n    \n    subgraph After [\"변경 후\"]\n        R2[\"customer_key=1<br/>name='김철수'<br/>city='서울'<br/>is_current=false<br/>expiry_date='2024-03-01'\"]\n        R3[\"customer_key=2<br/>name='김철수'<br/>city='부산'<br/>is_current=true<br/>effective_date='2024-03-01'\"]\n    end\n    \n    Before --> After\n```\n\n### SCD Type 2 스키마\n\n```sql\nCREATE TABLE dim_customers (\n    -- Surrogate Key (각 버전마다 다름)\n    customer_key    BIGINT PRIMARY KEY,\n    \n    -- Natural Key (동일)\n    customer_id     VARCHAR(50),\n    \n    -- Attributes\n    name            VARCHAR(200),\n    email           VARCHAR(200),\n    city            VARCHAR(100),\n    country         VARCHAR(50),\n    \n    -- SCD Type 2 Tracking\n    effective_date  DATE,\n    expiry_date     DATE,        -- NULL = 현재 레코드\n    is_current      BOOLEAN\n);\n```\n\n### Delta Lake MERGE로 SCD Type 2 구현\n\n```python\nfrom delta.tables import DeltaTable\nfrom pyspark.sql.functions import current_date, lit, col\n\n# 기존 차원 테이블\ndim_customers = DeltaTable.forPath(spark, \"/delta/dim_customers\")\n\n# 새로운/변경된 데이터\nstaging = spark.read.parquet(\"/staging/customers\")\n\n# Step 1: 변경된 레코드 찾기 (기존 current 레코드와 비교)\nchanges = dim_customers.toDF().alias(\"dim\") \\\n    .join(staging.alias(\"stg\"), \n          (col(\"dim.customer_id\") == col(\"stg.customer_id\")) & \n          col(\"dim.is_current\")) \\\n    .filter(\n        (col(\"dim.name\") != col(\"stg.name\")) | \n        (col(\"dim.city\") != col(\"stg.city\"))\n    ) \\\n    .select(\"dim.customer_key\")\n\n# Step 2: 기존 레코드 만료 처리\ndim_customers.alias(\"dim\").merge(\n    changes.alias(\"chg\"),\n    \"dim.customer_key = chg.customer_key\"\n).whenMatched().update(\n    set={\n        \"is_current\": lit(False),\n        \"expiry_date\": current_date()\n    }\n).execute()\n\n# Step 3: 새 버전 삽입\nnew_records = staging.withColumn(\"effective_date\", current_date()) \\\n    .withColumn(\"expiry_date\", lit(None)) \\\n    .withColumn(\"is_current\", lit(True)) \\\n    .withColumn(\"customer_key\", monotonically_increasing_id())\n\nnew_records.write.format(\"delta\") \\\n    .mode(\"append\") \\\n    .save(\"/delta/dim_customers\")\n```\n\n---\n\n## 실전 예제: 이커머스 데이터 모델\n\n### 전체 스키마\n\n```mermaid\nflowchart TB\n    subgraph Facts [\"Fact Tables\"]\n        F1[\"fact_orders<br/>(주문)\"]\n        F2[\"fact_page_views<br/>(페이지뷰)\"]\n    end\n    \n    subgraph Dimensions [\"Dimension Tables\"]\n        D1[\"dim_customers<br/>(SCD2)\"]\n        D2[\"dim_products<br/>(SCD1)\"]\n        D3[\"dim_date\"]\n        D4[\"dim_promotions\"]\n    end\n    \n    F1 --> D1\n    F1 --> D2\n    F1 --> D3\n    F1 --> D4\n    \n    F2 --> D1\n    F2 --> D3\n```\n\n### 분석 쿼리 예시\n\n```sql\n-- 월별/세그먼트별 매출\nSELECT \n    d.year,\n    d.month,\n    c.segment,\n    SUM(f.total_amount) AS revenue,\n    COUNT(DISTINCT c.customer_key) AS customers\nFROM fact_orders f\nJOIN dim_date d ON f.date_key = d.date_key\nJOIN dim_customers c ON f.customer_key = c.customer_key\n    AND c.is_current = TRUE  -- 현재 세그먼트 기준\nWHERE d.year = 2024\nGROUP BY d.year, d.month, c.segment\nORDER BY d.year, d.month;\n\n-- 주문 당시 고객 정보로 분석 (히스토리)\nSELECT \n    d.year,\n    d.month,\n    c.city,  -- 주문 당시 거주 도시\n    SUM(f.total_amount) AS revenue\nFROM fact_orders f\nJOIN dim_date d ON f.date_key = d.date_key\nJOIN dim_customers c ON f.customer_key = c.customer_key\n    -- SCD2: 주문 날짜가 유효 기간 내\n    AND d.full_date >= c.effective_date\n    AND (d.full_date < c.expiry_date OR c.expiry_date IS NULL)\nGROUP BY d.year, d.month, c.city;\n```\n\n---\n\n## Date Dimension\n\n### 필수 패턴\n\n```sql\nCREATE TABLE dim_date (\n    date_key        INT PRIMARY KEY,    -- YYYYMMDD\n    full_date       DATE,\n    year            INT,\n    quarter         INT,\n    month           INT,\n    month_name      VARCHAR(20),\n    week            INT,\n    day_of_week     INT,\n    day_name        VARCHAR(20),\n    is_weekend      BOOLEAN,\n    is_holiday      BOOLEAN,\n    fiscal_year     INT,\n    fiscal_quarter  INT\n);\n```\n\n### 미리 채우기\n\n```python\nimport pandas as pd\nfrom pyspark.sql import SparkSession\n\n# 10년치 날짜 생성\ndates = pd.date_range('2020-01-01', '2030-12-31')\n\ndim_date = pd.DataFrame({\n    'date_key': dates.strftime('%Y%m%d').astype(int),\n    'full_date': dates,\n    'year': dates.year,\n    'quarter': dates.quarter,\n    'month': dates.month,\n    'month_name': dates.strftime('%B'),\n    'week': dates.isocalendar().week,\n    'day_of_week': dates.dayofweek,\n    'day_name': dates.strftime('%A'),\n    'is_weekend': dates.dayofweek >= 5,\n})\n\nspark.createDataFrame(dim_date).write.format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .save(\"/delta/dim_date\")\n```\n\n---\n\n## 정리\n\n```mermaid\nmindmap\n  root((데이터<br/>모델링))\n    OLTP vs OLAP\n      정규화 vs 비정규화\n      무결성 vs 성능\n    Kimball\n      Bottom-Up\n      Star Schema\n      빠른 구현\n    Star Schema\n      Fact Table\n        Measures\n        Foreign Keys\n      Dimension Table\n        Attributes\n        Surrogate Key\n    SCD\n      Type 0: 유지\n      Type 1: 덮어쓰기\n      Type 2: 히스토리\n      Type 3: 이전값 컬럼\n    Date Dimension\n      미리 생성\n      분석 편의\n```\n\n---\n\n## 다음 편 예고\n\n**12편: 데이터 품질**에서는 운영을 다룹니다:\n\n- dbt를 이용한 변환과 테스트\n- Great Expectations\n- 데이터 계보 (Lineage)\n- 모니터링과 관측성\n\n---\n\n## 참고 자료\n\n- Ralph Kimball, \"The Data Warehouse Toolkit\"\n- dbt Labs, \"Building Slowly Changing Dimensions\"\n- Databricks, \"Data Modeling Best Practices\"",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Data Engineering",
      "Data Modeling"
    ],
    "readingTime": 6,
    "wordCount": 1191,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-10-lakehouse-architecture",
    "slug": "de-10-lakehouse-architecture",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-10-lakehouse-architecture",
    "title": "데이터 엔지니어링 시리즈 #10: 데이터 레이크 vs 웨어하우스 - 레이크하우스 아키텍처",
    "excerpt": "데이터 저장소 아키텍처의 종류와 선택 기준을 배웁니다. Delta Lake의 ACID, Time Travel, Schema Evolution을 심층 분석합니다.",
    "content": "# 데이터 엔지니어링 시리즈 #10: 데이터 레이크 vs 웨어하우스 - 레이크하우스 아키텍처\n\n> **대상 독자**: 충분한 경험을 가진 백엔드/풀스택 엔지니어로, PostgreSQL ACID에 익숙하지만 데이터 레이크/웨어하우스는 처음인 분\n\n## 이 편에서 다루는 것\n\n\"S3에 Parquet 올려두면 되는 거 아닌가요?\" 라는 질문에서 시작합니다. **왜 Delta Lake 같은 테이블 포맷이 필요한지**, 그리고 **레이크하우스가 무엇인지** 배웁니다.\n\n---\n\n## 데이터 저장소의 진화\n\n### 세대별 변화\n\n```mermaid\nflowchart LR\n    subgraph Gen1 [\"1세대: 데이터 웨어하우스\"]\n        DW1[\"온프레미스<br/>Oracle, Teradata\"]\n        DW2[\"구조화된 데이터\"]\n        DW3[\"SQL 분석\"]\n    end\n    \n    subgraph Gen2 [\"2세대: 데이터 레이크\"]\n        DL1[\"클라우드 스토리지<br/>S3, GCS\"]\n        DL2[\"모든 형태의 데이터\"]\n        DL3[\"Spark 처리\"]\n    end\n    \n    subgraph Gen3 [\"3세대: 레이크하우스\"]\n        LH1[\"레이크 위에<br/>웨어하우스 기능\"]\n        LH2[\"Delta Lake, Iceberg\"]\n        LH3[\"ACID + 유연성\"]\n    end\n    \n    Gen1 -->|\"확장성 한계\"| Gen2 -->|\"품질 문제\"| Gen3\n```\n\n---\n\n## 데이터 웨어하우스 (Data Warehouse)\n\n### 특징\n\n```mermaid\nflowchart TB\n    subgraph DW [\"Data Warehouse\"]\n        direction TB\n        Feature1[\"✅ 스키마 정의 (Schema-on-Write)\"]\n        Feature2[\"✅ ACID 트랜잭션\"]\n        Feature3[\"✅ SQL 지원\"]\n        Feature4[\"✅ 빠른 쿼리\"]\n        \n        Limit1[\"❌ 구조화된 데이터만\"]\n        Limit2[\"❌ 비용 (저장+컴퓨팅)\"]\n        Limit3[\"❌ 벤더 종속\"]\n    end\n    \n    Examples[\"예시:<br/>• Snowflake<br/>• BigQuery<br/>• Redshift\"]\n```\n\n### PostgreSQL과의 비교\n\n| 특성 | PostgreSQL (OLTP) | BigQuery (DW) |\n|------|-------------------|---------------|\n| **목적** | 트랜잭션 처리 | 분석 쿼리 |\n| **스토리지** | Row-based | Column-based |\n| **스케일** | 수직 확장 | 무한 수평 확장 |\n| **비용** | 서버 비용 | 쿼리당 비용 |\n| **쿼리 속도** | 단건 빠름 | 집계 빠름 |\n\n---\n\n## 데이터 레이크 (Data Lake)\n\n### 특징\n\n```mermaid\nflowchart TB\n    subgraph DL [\"Data Lake\"]\n        direction TB\n        Feature1[\"✅ 모든 형태의 데이터\"]\n        Feature2[\"✅ 저렴한 저장 비용\"]\n        Feature3[\"✅ 분리된 저장/컴퓨팅\"]\n        Feature4[\"✅ 유연한 처리 (Spark 등)\"]\n        \n        Limit1[\"❌ ACID 없음\"]\n        Limit2[\"❌ 스키마 관리 어려움\"]\n        Limit3[\"❌ 데이터 품질 문제\"]\n    end\n    \n    Examples[\"예시:<br/>• S3 + Parquet<br/>• GCS + Avro<br/>• ADLS + JSON\"]\n```\n\n### 데이터 레이크의 문제점\n\n```mermaid\nflowchart TB\n    subgraph Problems [\"레이크의 고질적 문제\"]\n        P1[\"동시 쓰기 충돌\"]\n        P2[\"부분 실패 → 깨진 데이터\"]\n        P3[\"스키마 변경 → 호환성 문제\"]\n        P4[\"삭제/수정 어려움\"]\n        P5[\"작은 파일 문제\"]\n    end\n    \n    Result[\"결국... 데이터 늪(Data Swamp)\"]\n    \n    Problems --> Result\n```\n\n---\n\n## 레이크하우스 (Lakehouse)\n\n### 두 세계의 통합\n\n```mermaid\nflowchart TB\n    subgraph Lakehouse [\"Lakehouse Architecture\"]\n        subgraph Top [\"웨어하우스 기능\"]\n            T1[\"ACID 트랜잭션\"]\n            T2[\"스키마 관리\"]\n            T3[\"Time Travel\"]\n            T4[\"SQL 지원\"]\n        end\n        \n        subgraph Middle [\"테이블 포맷\"]\n            M1[\"Delta Lake\"]\n            M2[\"Apache Iceberg\"]\n            M3[\"Apache Hudi\"]\n        end\n        \n        subgraph Bottom [\"오픈 스토리지\"]\n            B1[\"S3\"]\n            B2[\"GCS\"]\n            B3[\"ADLS\"]\n        end\n        \n        Top --> Middle --> Bottom\n    end\n```\n\n### 핵심 가치\n\n| 특성 | 레이크 | 웨어하우스 | 레이크하우스 |\n|------|--------|-----------|-------------|\n| **저장 비용** | 저렴 ✅ | 비쌈 | 저렴 ✅ |\n| **ACID** | ❌ | ✅ | ✅ |\n| **오픈 포맷** | ✅ | ❌ (벤더) | ✅ |\n| **ML 지원** | ✅ | 제한적 | ✅ |\n| **SQL 분석** | 제한적 | ✅ | ✅ |\n\n---\n\n## Medallion Architecture (Bronze/Silver/Gold)\n\n레이크하우스에서 데이터를 **계층화하여 관리하는 표준 패턴**입니다. Databricks가 제안하고 현재 업계 표준으로 자리잡았습니다.\n\n> **출처**: [Databricks - Medallion Architecture](https://docs.databricks.com/en/lakehouse/medallion.html), Armbrust et al., \"Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores\" (VLDB 2020)\n\n### 세 레이어 구조\n\n```mermaid\nflowchart LR\n    subgraph Bronze [\"🥉 Bronze Layer\"]\n        B1[\"원본 그대로 저장\"]\n        B2[\"스키마 변경 보호\"]\n        B3[\"감사/재처리 가능\"]\n    end\n    \n    subgraph Silver [\"🥈 Silver Layer\"]\n        S1[\"정제/검증\"]\n        S2[\"조인/통합\"]\n        S3[\"비즈니스 엔티티\"]\n    end\n    \n    subgraph Gold [\"🥇 Gold Layer\"]\n        G1[\"집계/요약\"]\n        G2[\"비즈니스 리포트\"]\n        G3[\"ML Features\"]\n    end\n    \n    Bronze -->|\"정제\"| Silver -->|\"집계\"| Gold\n```\n\n### 각 레이어의 역할\n\n| Layer | 목적 | 데이터 특성 | 소비자 |\n|-------|------|------------|--------|\n| **Bronze** | 원본 보존 | Raw, 스키마 유연 | 데이터 엔지니어 |\n| **Silver** | 정제/통합 | Cleaned, 조인됨 | 데이터 분석가, DS |\n| **Gold** | 비즈니스 집계 | Aggregated, 최적화 | BI, 경영진 |\n\n### 코드 예시\n\n```python\n# Bronze: 원본 그대로 저장\nraw_events = spark.readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n    .option(\"subscribe\", \"user_events\") \\\n    .load()\n\nraw_events.writeStream \\\n    .format(\"delta\") \\\n    .option(\"checkpointLocation\", \"/checkpoints/bronze\") \\\n    .start(\"/delta/bronze/events\")\n\n# Silver: 정제 및 스키마 적용\nbronze_df = spark.read.format(\"delta\").load(\"/delta/bronze/events\")\n\nsilver_df = bronze_df \\\n    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n    .select(\"data.*\") \\\n    .filter(col(\"user_id\").isNotNull()) \\\n    .dropDuplicates([\"event_id\"])\n\nsilver_df.write.format(\"delta\").mode(\"overwrite\") \\\n    .save(\"/delta/silver/events\")\n\n# Gold: 비즈니스 집계\nsilver_df = spark.read.format(\"delta\").load(\"/delta/silver/events\")\n\ngold_df = silver_df \\\n    .groupBy(\"date\", \"event_type\") \\\n    .agg(\n        count(\"*\").alias(\"event_count\"),\n        countDistinct(\"user_id\").alias(\"unique_users\")\n    )\n\ngold_df.write.format(\"delta\").mode(\"overwrite\") \\\n    .save(\"/delta/gold/daily_metrics\")\n```\n\n### 왜 이 패턴인가?\n\n| 문제 | Medallion 해결책 |\n|------|-----------------|\n| 원본 데이터 유실 | Bronze에 원본 보존 |\n| 스키마 변경 대응 | Bronze는 스키마 유연, Silver에서 검증 |\n| 재처리 필요 | Bronze → Silver → Gold 순서대로 재실행 |\n| 다양한 소비자 니즈 | 레이어별 최적화된 데이터 제공 |\n\n---\n\n## Delta Lake 심층 분석\n\n### ACID 트랜잭션\n\n```mermaid\nflowchart TB\n    subgraph WithoutACID [\"ACID 없이 (일반 Parquet)\"]\n        W1[\"writer 1: 파일 A 쓰기\"]\n        W2[\"writer 2: 파일 B 쓰기\"]\n        W3[\"동시에 실행\"]\n        W4[\"충돌/덮어쓰기 발생 💥\"]\n        \n        W1 --> W3\n        W2 --> W3\n        W3 --> W4\n    end\n    \n    subgraph WithACID [\"Delta Lake (ACID)\"]\n        D1[\"writer 1: 트랜잭션 시작\"]\n        D2[\"writer 2: 트랜잭션 시작\"]\n        D3[\"optimistic concurrency\"]\n        D4[\"하나만 커밋 성공<br/>나머지 재시도\"]\n        \n        D1 --> D3\n        D2 --> D3\n        D3 --> D4\n    end\n```\n\n**Delta Lake의 방법**: 트랜잭션 로그 (`_delta_log/`)\n\n```\ntable/\n├── _delta_log/\n│   ├── 00000000000000000000.json  # 첫 트랜잭션\n│   ├── 00000000000000000001.json  # 두 번째\n│   └── 00000000000000000002.json  # 세 번째\n├── part-00000.parquet\n├── part-00001.parquet\n└── part-00002.parquet\n```\n\n### Time Travel\n\n```mermaid\nflowchart LR\n    subgraph History [\"버전 히스토리\"]\n        V0[\"v0: 초기 데이터\"]\n        V1[\"v1: 추가\"]\n        V2[\"v2: 수정\"]\n        V3[\"v3: 삭제 (현재)\"]\n        \n        V0 --> V1 --> V2 --> V3\n    end\n    \n    Query[\"어떤 버전이든<br/>쿼리 가능!\"]\n```\n\n```python\n# 특정 버전으로 읽기\ndf = spark.read.format(\"delta\") \\\n    .option(\"versionAsOf\", 2) \\\n    .load(\"/delta/users\")\n\n# 특정 시점으로 읽기\ndf = spark.read.format(\"delta\") \\\n    .option(\"timestampAsOf\", \"2024-01-01\") \\\n    .load(\"/delta/users\")\n\n# 히스토리 조회\nfrom delta.tables import DeltaTable\n\ndt = DeltaTable.forPath(spark, \"/delta/users\")\ndt.history().show()\n```\n\n### Schema Evolution\n\n```mermaid\nflowchart TB\n    subgraph Problem [\"스키마 변경 문제\"]\n        P1[\"기존: id, name, email\"]\n        P2[\"새로운: id, name, email, phone\"]\n        P3[\"기존 파일은 phone 없음\"]\n        P4[\"어떻게 함께 읽지?\"]\n    end\n    \n    subgraph Solution [\"Delta Lake 해결책\"]\n        S1[\"스키마 자동 병합\"]\n        S2[\"새 컬럼 NULL 허용\"]\n        S3[\"호환성 검사\"]\n    end\n```\n\n```python\n# 자동 스키마 병합\ndf_new.write.format(\"delta\") \\\n    .mode(\"append\") \\\n    .option(\"mergeSchema\", \"true\") \\\n    .save(\"/delta/users\")\n\n# 스키마 덮어쓰기 (주의!)\ndf_new.write.format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .option(\"overwriteSchema\", \"true\") \\\n    .save(\"/delta/users\")\n```\n\n### MERGE (Upsert)\n\n```mermaid\nflowchart TB\n    subgraph Before [\"MERGE 전\"]\n        Source[\"Source (새 데이터)\"]\n        Target[\"Target (기존 테이블)\"]\n    end\n    \n    subgraph Logic [\"MERGE 로직\"]\n        Match[\"ON 조건으로 매칭\"]\n        WhenMatched[\"WHEN MATCHED → UPDATE\"]\n        WhenNotMatched[\"WHEN NOT MATCHED → INSERT\"]\n    end\n    \n    subgraph After [\"MERGE 후\"]\n        Result[\"통합된 결과\"]\n    end\n    \n    Before --> Logic --> After\n```\n\n```python\nfrom delta.tables import DeltaTable\n\n# 타겟 테이블\ntarget = DeltaTable.forPath(spark, \"/delta/users\")\n\n# 소스 데이터 (업데이트할 데이터)\nsource = spark.read.parquet(\"/staging/users\")\n\n# MERGE 실행\ntarget.alias(\"t\").merge(\n    source.alias(\"s\"),\n    \"t.user_id = s.user_id\"\n).whenMatchedUpdate(\n    set={\n        \"name\": \"s.name\",\n        \"email\": \"s.email\",\n        \"updated_at\": \"current_timestamp()\"\n    }\n).whenNotMatchedInsert(\n    values={\n        \"user_id\": \"s.user_id\",\n        \"name\": \"s.name\",\n        \"email\": \"s.email\",\n        \"created_at\": \"current_timestamp()\"\n    }\n).execute()\n```\n\n---\n\n## Delta Lake vs Apache Iceberg\n\n### 비교\n\n```mermaid\nflowchart TB\n    subgraph Delta [\"Delta Lake\"]\n        D1[\"✅ Databricks 최적화\"]\n        D2[\"✅ Spark 통합 우수\"]\n        D3[\"✅ 성숙한 생태계\"]\n        D4[\"⚠️ Databricks 외 지원 제한적\"]\n    end\n    \n    subgraph Iceberg [\"Apache Iceberg\"]\n        I1[\"✅ 벤더 중립\"]\n        I2[\"✅ 다양한 엔진 지원\"]\n        I3[\"✅ Hidden Partitioning\"]\n        I4[\"⚠️ 상대적으로 신생\"]\n    end\n```\n\n| 특성 | Delta Lake | Apache Iceberg |\n|------|-----------|----------------|\n| **개발사** | Databricks | Netflix→Apache |\n| **Spark 지원** | 최고 | 좋음 |\n| **Flink 지원** | 제한적 | 좋음 |\n| **Trino 지원** | 좋음 | 좋음 |\n| **파티셔닝** | 명시적 | Hidden (투명) |\n| **채택율** | 높음 | 증가 중 |\n\n### 선택 가이드\n\n```mermaid\nflowchart TB\n    Q1{\"Databricks<br/>사용?\"}\n    Q2{\"Flink<br/>필요?\"}\n    Q3{\"벤더 중립<br/>중요?\"}\n    \n    Q1 -->|\"예\"| Delta[\"Delta Lake\"]\n    Q1 -->|\"아니오\"| Q2\n    Q2 -->|\"예\"| Iceberg[\"Apache Iceberg\"]\n    Q2 -->|\"아니오\"| Q3\n    Q3 -->|\"예\"| Iceberg\n    Q3 -->|\"아니오\"| Delta\n```\n\n---\n\n## 아키텍처 결정 가이드\n\n### 언제 무엇을 선택하나?\n\n```mermaid\nflowchart TB\n    subgraph Decision [\"결정 트리\"]\n        D1{\"데이터 크기?\"}\n        D2{\"팀 SQL 역량?\"}\n        D3{\"ML 워크로드?\"}\n        D4{\"예산?\"}\n        \n        D1 -->|\"< 100GB\"| DW[\"Data Warehouse<br/>(BigQuery, Snowflake)\"]\n        D1 -->|\">= 100GB\"| D2\n        D2 -->|\"SQL 위주\"| DW\n        D2 -->|\"Python/Spark 혼합\"| D3\n        D3 -->|\"ML 중요\"| LH[\"Lakehouse<br/>(Delta Lake)\"]\n        D3 -->|\"분석 위주\"| D4\n        D4 -->|\"비용 민감\"| LH\n        D4 -->|\"관리 편의\"| DW\n    end\n```\n\n---\n\n## 정리\n\n```mermaid\nmindmap\n  root((데이터<br/>저장소))\n    Data Warehouse\n      구조화된 데이터\n      ACID\n      SQL 최적화\n      비용 높음\n    Data Lake\n      모든 데이터\n      저렴\n      ACID 없음\n      품질 문제\n    Lakehouse\n      레이크 + ACID\n      Delta Lake\n      Iceberg\n      최신 트렌드\n    Medallion\n      Bronze: 원본\n      Silver: 정제\n      Gold: 집계\n    Delta Lake\n      트랜잭션 로그\n      Time Travel\n      Schema Evolution\n      MERGE\n```\n\n---\n\n## 다음 편 예고\n\n**11편: 데이터 모델링**에서는 분석용 모델링을 다룹니다:\n\n- Star Schema vs Snowflake Schema\n- Fact Table vs Dimension Table\n- Slowly Changing Dimensions (SCD)\n\n---\n\n## 참고 자료\n\n- [Delta Lake Documentation](https://docs.delta.io/)\n- [Databricks Medallion Architecture](https://docs.databricks.com/en/lakehouse/medallion.html)\n- [Apache Iceberg Documentation](https://iceberg.apache.org/docs/latest/)\n- Armbrust et al., \"Delta Lake: High-Performance ACID Table Storage\" (VLDB 2020)\n- Databricks, \"The Data Lakehouse\" White Paper\n- Martin Kleppmann, \"Designing Data-Intensive Applications\" - Chapter 3",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Data Architecture",
      "Data Engineering",
      "Delta Lake",
      "Iceberg",
      "Transaction"
    ],
    "readingTime": 7,
    "wordCount": 1364,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-09-spark-structured-streaming",
    "slug": "de-09-spark-structured-streaming",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-09-spark-structured-streaming",
    "title": "데이터 엔지니어링 시리즈 #9: Spark Structured Streaming - 실시간 데이터 처리",
    "excerpt": "Spark Structured Streaming으로 실시간 데이터 파이프라인을 구축합니다. Kafka 연동, Watermark, Window 연산, 체크포인팅까지.",
    "content": "# 데이터 엔지니어링 시리즈 #9: Spark Structured Streaming - 실시간 데이터 처리\n\n> **대상 독자**: 충분한 경험을 가진 백엔드/풀스택 엔지니어로, Spark과 Kafka 기본 개념을 익히고 실시간 처리를 배우려는 분\n\n## 이 편에서 다루는 것\n\n배치 처리와 스트리밍 처리를 **같은 API로** 다루는 Spark Structured Streaming의 핵심을 배웁니다.\n\n---\n\n## 배치와 스트리밍의 통합\n\n### Structured Streaming의 철학\n\n```mermaid\nflowchart TB\n    subgraph Traditional [\"전통적 접근\"]\n        Batch[\"배치 코드<br/>(Spark SQL)\"]\n        Stream[\"스트리밍 코드<br/>(DStream)\"]\n        Two[\"서로 다른 API 😓\"]\n    end\n    \n    subgraph Unified [\"Structured Streaming\"]\n        Single[\"동일한 DataFrame API\"]\n        Batch2[\"배치 처리\"]\n        Stream2[\"스트리밍 처리\"]\n        Single --> Batch2\n        Single --> Stream2\n    end\n```\n\n### 무한 테이블 개념\n\n```mermaid\nflowchart LR\n    subgraph Input [\"입력 스트림\"]\n        T1[\"t=1: row 1, 2\"]\n        T2[\"t=2: row 3\"]\n        T3[\"t=3: row 4, 5, 6\"]\n        \n        T1 --> T2 --> T3\n    end\n    \n    subgraph Table [\"무한 테이블\"]\n        Row1[\"row 1\"]\n        Row2[\"row 2\"]\n        Row3[\"row 3\"]\n        Row4[\"row 4\"]\n        Row5[\"row 5\"]\n        Row6[\"row 6\"]\n        Dots[\"...\"]\n        \n        Row1 --> Row2 --> Row3 --> Row4 --> Row5 --> Row6 --> Dots\n    end\n    \n    subgraph Output [\"결과\"]\n        Q[\"동일한 쿼리 적용\"]\n    end\n    \n    Input --> Table --> Output\n```\n\n**핵심 아이디어**: 스트림을 \"계속 추가되는 테이블\"로 생각\n\n---\n\n## Source와 Sink\n\n### 지원되는 Source\n\n```mermaid\nflowchart TB\n    subgraph Sources [\"Input Sources\"]\n        Kafka[\"Kafka<br/>✅ 프로덕션\"]\n        File[\"File Source<br/>(JSON, Parquet, CSV)\"]\n        Socket[\"Socket<br/>(테스트용)\"]\n        Rate[\"Rate Source<br/>(테스트용)\"]\n    end\n```\n\n### 지원되는 Sink\n\n```mermaid\nflowchart TB\n    subgraph Sinks [\"Output Sinks\"]\n        Kafka2[\"Kafka\"]\n        File2[\"File<br/>(Parquet, JSON)\"]\n        Console[\"Console<br/>(디버깅)\"]\n        Memory[\"Memory<br/>(테스트)\"]\n        ForeachBatch[\"foreachBatch<br/>(커스텀 로직)\"]\n    end\n```\n\n---\n\n## Kafka → Spark Streaming 연동\n\n### 기본 구조\n\n```mermaid\nflowchart LR\n    subgraph Kafka [\"Kafka\"]\n        Topic[\"Topic: events\"]\n    end\n    \n    subgraph Spark [\"Spark Streaming\"]\n        Read[\"readStream\"]\n        Transform[\"변환 로직\"]\n        Write[\"writeStream\"]\n    end\n    \n    subgraph Output [\"출력\"]\n        DeltaLake[\"Delta Lake\"]\n    end\n    \n    Kafka --> Read --> Transform --> Write --> Output\n```\n\n### 코드 예시\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import from_json, col\nfrom pyspark.sql.types import StructType, StringType, TimestampType, DoubleType\n\nspark = SparkSession.builder \\\n    .appName(\"StreamingApp\") \\\n    .getOrCreate()\n\n# 스키마 정의\nevent_schema = StructType() \\\n    .add(\"user_id\", StringType()) \\\n    .add(\"event_type\", StringType()) \\\n    .add(\"timestamp\", TimestampType()) \\\n    .add(\"amount\", DoubleType())\n\n# Kafka에서 읽기\ndf = spark.readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n    .option(\"subscribe\", \"user_events\") \\\n    .option(\"startingOffsets\", \"latest\") \\\n    .load()\n\n# value 파싱 (Kafka 메시지는 binary)\nparsed = df.select(\n    from_json(col(\"value\").cast(\"string\"), event_schema).alias(\"data\")\n).select(\"data.*\")\n\n# 변환 로직 (배치와 동일!)\nresult = parsed.filter(col(\"amount\") > 0)\n\n# 출력\nquery = result.writeStream \\\n    .format(\"delta\") \\\n    .option(\"checkpointLocation\", \"/checkpoints/events\") \\\n    .outputMode(\"append\") \\\n    .start(\"/delta/events\")\n\nquery.awaitTermination()\n```\n\n---\n\n## Output Modes\n\n### 세 가지 모드\n\n```mermaid\nflowchart TB\n    subgraph Append [\"Append Mode\"]\n        A1[\"새로 추가된 행만 출력\"]\n        A2[\"집계 없는 쿼리에 적합\"]\n        A3[\"예: 필터링, 맵핑\"]\n    end\n    \n    subgraph Complete [\"Complete Mode\"]\n        C1[\"전체 결과 매번 출력\"]\n        C2[\"집계 쿼리에 적합\"]\n        C3[\"예: groupBy().count()\"]\n    end\n    \n    subgraph Update [\"Update Mode\"]\n        U1[\"변경된 행만 출력\"]\n        U2[\"집계 쿼리에 효율적\"]\n        U3[\"예: 카운트 업데이트\"]\n    end\n```\n\n### 언제 어떤 모드?\n\n| 쿼리 유형 | Append | Complete | Update |\n|----------|--------|----------|--------|\n| **SELECT, WHERE** | ✅ | ❌ | ✅ |\n| **집계 (groupBy)** | ❌* | ✅ | ✅ |\n| **워터마크 + 집계** | ✅ | ✅ | ✅ |\n\n*워터마크 없는 집계는 Append 불가\n\n---\n\n## Event Time vs Processing Time\n\n### 두 시간의 차이\n\n```mermaid\nflowchart LR\n    subgraph EventTime [\"Event Time\"]\n        ET[\"이벤트가 실제로 발생한 시간<br/>(데이터에 포함된 timestamp)\"]\n    end\n    \n    subgraph ProcessingTime [\"Processing Time\"]\n        PT[\"Spark이 데이터를 처리하는 시간<br/>(시스템 시계)\"]\n    end\n    \n    subgraph Problem [\"문제 상황\"]\n        P1[\"Event: 10:00:00\"]\n        P2[\"네트워크 지연\"]\n        P3[\"Processing: 10:05:00\"]\n        P1 --> P2 --> P3\n        \n        Q[\"어떤 시간 기준으로 윈도우?\"]\n    end\n```\n\n### Event Time 처리\n\n```python\n# timestamp 컬럼을 Event Time으로 사용\nparsed = df.select(\n    from_json(col(\"value\").cast(\"string\"), event_schema).alias(\"data\")\n).select(\"data.*\")\n\n# Event Time 기준 윈도우 집계\nresult = parsed \\\n    .groupBy(\n        window(col(\"timestamp\"), \"5 minutes\"),\n        col(\"event_type\")\n    ) \\\n    .count()\n```\n\n---\n\n## Watermark와 Late Data\n\n### 왜 Watermark가 필요한가?\n\n```mermaid\nflowchart TB\n    subgraph Problem [\"문제: 지연 데이터\"]\n        W1[\"10:00 윈도우 처리 중\"]\n        W2[\"10:05에 도착한 데이터\"]\n        W3[\"근데 event_time은 09:55!\"]\n        W4[\"어떻게 처리?\"]\n        \n        W1 --> W2 --> W3 --> W4\n    end\n    \n    subgraph Solution [\"해결: Watermark\"]\n        S1[\"허용 지연 시간 설정<br/>(예: 10분)\"]\n        S2[\"Watermark = max(event_time) - 10분\"]\n        S3[\"Watermark 이전 윈도우는 닫힘\"]\n        \n        S1 --> S2 --> S3\n    end\n```\n\n### Watermark 동작 방식\n\n```mermaid\nflowchart LR\n    subgraph Timeline [\"시간 흐름\"]\n        T1[\"Event: 10:05\"]\n        T2[\"Event: 10:10\"]\n        T3[\"Event: 10:08\"]\n        T4[\"Event: 10:15\"]\n        T5[\"Late: 09:55\"]\n    end\n    \n    subgraph Watermark [\"Watermark (지연 10분)\"]\n        W1[\"max=10:05<br/>WM=09:55\"]\n        W2[\"max=10:10<br/>WM=10:00\"]\n        W3[\"max=10:10<br/>WM=10:00\"]\n        W4[\"max=10:15<br/>WM=10:05\"]\n        W5[\"❌ 09:55 < 10:05<br/>→ 버려짐\"]\n    end\n    \n    T1 --> W1\n    T2 --> W2\n    T3 --> W3\n    T4 --> W4\n    T5 --> W5\n```\n\n### 코드 예시\n\n```python\nfrom pyspark.sql.functions import window, col\n\n# Watermark 설정: 10분 지연 허용\nresult = parsed \\\n    .withWatermark(\"timestamp\", \"10 minutes\") \\\n    .groupBy(\n        window(col(\"timestamp\"), \"5 minutes\"),\n        col(\"page\")\n    ) \\\n    .agg(count(\"*\").alias(\"views\"))\n\n# Watermark 덕분에 Append 모드 가능\nquery = result.writeStream \\\n    .format(\"delta\") \\\n    .outputMode(\"append\") \\\n    .option(\"checkpointLocation\", \"/checkpoints/views\") \\\n    .start(\"/delta/page_views\")\n```\n\n---\n\n## Window 연산\n\n### Window 종류\n\n```mermaid\nflowchart TB\n    subgraph Tumbling [\"Tumbling Window\"]\n        T1[\"0-5분\"]\n        T2[\"5-10분\"]\n        T3[\"10-15분\"]\n        T1 --> T2 --> T3\n        TNote[\"겹치지 않음\"]\n    end\n    \n    subgraph Sliding [\"Sliding Window\"]\n        S1[\"0-5분\"]\n        S2[\"2.5-7.5분\"]\n        S3[\"5-10분\"]\n        SNote[\"겹침 (slide < window)\"]\n    end\n    \n    subgraph Session [\"Session Window\"]\n        SE1[\"활동 기간 A\"]\n        Gap[\"비활동 gap\"]\n        SE2[\"활동 기간 B\"]\n        SE1 --> Gap --> SE2\n        SENote[\"gap 기준 분리\"]\n    end\n```\n\n### Window 함수 사용\n\n```python\nfrom pyspark.sql.functions import window, sum, avg\n\n# Tumbling Window: 5분 윈도우\ntumbling = parsed \\\n    .groupBy(window(\"timestamp\", \"5 minutes\")) \\\n    .agg(sum(\"amount\").alias(\"total\"))\n\n# Sliding Window: 10분 윈도우, 5분 슬라이드\nsliding = parsed \\\n    .groupBy(window(\"timestamp\", \"10 minutes\", \"5 minutes\")) \\\n    .agg(avg(\"amount\").alias(\"avg_amount\"))\n\n# Session Window (Spark 3.2+)\nsession = parsed \\\n    .groupBy(\n        session_window(\"timestamp\", \"10 minutes\"),\n        col(\"user_id\")\n    ) \\\n    .agg(count(\"*\").alias(\"session_events\"))\n```\n\n---\n\n## 체크포인팅과 장애 복구\n\n### 체크포인트 구조\n\n```mermaid\nflowchart TB\n    subgraph Checkpoint [\"체크포인트 디렉토리\"]\n        Offsets[\"offsets/<br/>Kafka offset 정보\"]\n        Commits[\"commits/<br/>처리 완료 배치\"]\n        State[\"state/<br/>집계 상태\"]\n        Metadata[\"metadata/<br/>쿼리 메타데이터\"]\n    end\n    \n    subgraph Recovery [\"장애 복구\"]\n        R1[\"마지막 체크포인트 로드\"]\n        R2[\"미처리 offset부터 재시작\"]\n        R3[\"상태 복원\"]\n        R1 --> R2 --> R3\n    end\n```\n\n### Exactly-Once 보장\n\n```python\n# 체크포인트 필수 설정\nquery = result.writeStream \\\n    .format(\"delta\") \\\n    .outputMode(\"append\") \\\n    .option(\"checkpointLocation\", \"hdfs://path/checkpoints/my_query\") \\\n    .trigger(processingTime=\"1 minute\") \\\n    .start(\"/delta/output\")\n```\n\n**체크포인트가 보장하는 것**:\n\n- Kafka offset 추적 → 중복 읽기 방지\n- 상태 저장 → 집계 결과 유지\n- Atomic 커밋 → Exactly-Once\n\n---\n\n## 실전 예제: 실시간 클릭스트림 분석\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import (\n    from_json, col, window, count, sum, avg,\n    current_timestamp, expr\n)\nfrom pyspark.sql.types import StructType, StringType, TimestampType\n\nspark = SparkSession.builder \\\n    .appName(\"ClickstreamAnalysis\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .getOrCreate()\n\n# 스키마\nclick_schema = StructType() \\\n    .add(\"user_id\", StringType()) \\\n    .add(\"page\", StringType()) \\\n    .add(\"action\", StringType()) \\\n    .add(\"timestamp\", TimestampType())\n\n# Kafka에서 읽기\nclicks = spark.readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n    .option(\"subscribe\", \"clickstream\") \\\n    .option(\"startingOffsets\", \"latest\") \\\n    .load() \\\n    .select(\n        from_json(col(\"value\").cast(\"string\"), click_schema).alias(\"click\")\n    ).select(\"click.*\")\n\n# 5분 윈도우로 페이지별 통계\npage_stats = clicks \\\n    .withWatermark(\"timestamp\", \"10 minutes\") \\\n    .groupBy(\n        window(col(\"timestamp\"), \"5 minutes\"),\n        col(\"page\")\n    ) \\\n    .agg(\n        count(\"*\").alias(\"view_count\"),\n        count(\"user_id\").alias(\"unique_users\")\n    )\n\n# Delta Lake에 저장\nquery = page_stats.writeStream \\\n    .format(\"delta\") \\\n    .outputMode(\"append\") \\\n    .option(\"checkpointLocation\", \"/checkpoints/clickstream\") \\\n    .trigger(processingTime=\"1 minute\") \\\n    .start(\"/delta/page_stats\")\n\n# 콘솔에도 출력 (디버깅용)\ndebug_query = page_stats.writeStream \\\n    .format(\"console\") \\\n    .outputMode(\"update\") \\\n    .trigger(processingTime=\"30 seconds\") \\\n    .start()\n\nquery.awaitTermination()\n```\n\n---\n\n## 모니터링\n\n### Streaming Query 상태 확인\n\n```python\n# 쿼리 진행 상황\nprint(query.status)\n# {'message': 'Processing new data', 'isActive': True, ...}\n\n# 최근 진행 상황\nfor progress in query.recentProgress:\n    print(f\"Batch {progress['batchId']}\")\n    print(f\"  Input rows: {progress['numInputRows']}\")\n    print(f\"  Processing time: {progress['batchDuration']} ms\")\n```\n\n### Spark UI에서 확인\n\n```mermaid\nflowchart TB\n    subgraph SparkUI [\"Structured Streaming UI\"]\n        Tab[\"Streaming 탭\"]\n        Metrics[\"• Input Rate<br/>• Processing Rate<br/>• Batch Duration<br/>• State Rows\"]\n    end\n```\n\n---\n\n## 정리\n\n```mermaid\nmindmap\n  root((Spark<br/>Structured<br/>Streaming))\n    핵심 개념\n      무한 테이블\n      동일한 API\n      배치 & 스트리밍 통합\n    Source/Sink\n      Kafka\n      File\n      Delta Lake\n    Output Mode\n      Append\n      Complete\n      Update\n    시간 처리\n      Event Time\n      Processing Time\n      Watermark\n    Window\n      Tumbling\n      Sliding\n      Session\n    안정성\n      Checkpoint\n      Exactly-Once\n      장애 복구\n```\n\n---\n\n## 다음 편 예고\n\n**10편: 레이크하우스 아키텍처**에서는 데이터 저장소를 다룹니다:\n\n- Data Lake vs Data Warehouse\n- Delta Lake 심층 분석\n- ACID, Time Travel, Schema Evolution\n- Apache Iceberg 비교\n\n---\n\n## 참고 자료\n\n- [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n- [Kafka Integration](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)\n- Databricks, \"Real-time Streaming with Spark 3.0\"",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Data Engineering",
      "Kafka",
      "Spark",
      "Streaming",
      "Window"
    ],
    "readingTime": 7,
    "wordCount": 1243,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-08-kafka-fundamentals",
    "slug": "de-08-kafka-fundamentals",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-08-kafka-fundamentals",
    "title": "데이터 엔지니어링 시리즈 #8: Kafka 핵심 - 메시지 큐를 넘어 이벤트 스트리밍으로",
    "excerpt": "Kafka의 핵심 개념을 배웁니다. Redis Streams와 비교하며 Topic, Partition, Consumer Group, Exactly-Once Semantics를 이해합니다.",
    "content": "# 데이터 엔지니어링 시리즈 #8: Kafka 핵심 - 메시지 큐를 넘어 이벤트 스트리밍으로\n\n> **대상 독자**: 충분한 경험을 가진 백엔드/풀스택 엔지니어로, Redis Streams나 RabbitMQ에 익숙하지만 Kafka는 처음인 분\n\n## 이 편에서 다루는 것\n\nRedis Streams를 써봤다면 \"Kafka가 뭐가 다르지?\"라는 의문이 있을 겁니다. **왜 대규모 시스템에서 Kafka를 선택하는지**, 핵심 개념을 배웁니다.\n\n---\n\n## Redis Streams vs Kafka\n\n### 친숙한 Redis Streams와 비교\n\n```mermaid\nflowchart TB\n    subgraph Redis [\"Redis Streams\"]\n        RS_Stream[\"Stream: orders\"]\n        RS_Group1[\"Consumer Group A\"]\n        RS_Group2[\"Consumer Group B\"]\n        RS_C1[\"Consumer 1\"]\n        RS_C2[\"Consumer 2\"]\n        \n        RS_Stream --> RS_Group1 --> RS_C1\n        RS_Stream --> RS_Group2 --> RS_C2\n    end\n    \n    subgraph Kafka [\"Apache Kafka\"]\n        K_Topic[\"Topic: orders<br/>(Partitioned)\"]\n        K_Group1[\"Consumer Group A\"]\n        K_Group2[\"Consumer Group B\"]\n        K_C1[\"Consumer 1\"]\n        K_C2[\"Consumer 2\"]\n        \n        K_Topic --> K_Group1 --> K_C1\n        K_Topic --> K_Group2 --> K_C2\n    end\n    \n    Similar[\"유사한 개념!\"]\n```\n\n### 주요 차이점\n\n| 특성 | Redis Streams | Kafka |\n|------|--------------|-------|\n| **설계 목적** | 캐시 + 가벼운 스트리밍 | 대용량 이벤트 스트리밍 전용 |\n| **데이터 저장** | 메모리 (제한적 보존) | 디스크 (장기 보존 가능) |\n| **스케일링** | 수직 확장 위주 | 수평 확장 (Partition) |\n| **처리량** | 수만 TPS | **수백만 TPS** |\n| **복제** | Master-Replica | Multi-broker 복제 |\n| **순서 보장** | Stream 내 보장 | Partition 내 보장 |\n| **Consumer 관리** | 자체 관리 필요 | Coordinator 자동 관리 |\n\n### 스케일 비교\n\n```mermaid\nflowchart TB\n    subgraph RedisScale [\"Redis Streams 스케일링\"]\n        RS1[\"단일 인스턴스<br/>100K TPS\"]\n        RS2[\"Cluster Sharding<br/>복잡한 관리\"]\n    end\n    \n    subgraph KafkaScale [\"Kafka 스케일링\"]\n        K1[\"Partition 추가\"]\n        K2[\"Broker 추가\"]\n        K3[\"Consumer 추가\"]\n        \n        K1 --> K2 --> K3\n        Result[\"선형 확장 가능<br/>수백만 TPS\"]\n    end\n```\n\n---\n\n## Kafka 핵심 개념\n\n### 전체 구조\n\n```mermaid\nflowchart TB\n    subgraph Producers [\"Producers\"]\n        P1[\"Producer 1\"]\n        P2[\"Producer 2\"]\n    end\n    \n    subgraph Kafka [\"Kafka Cluster\"]\n        subgraph Broker1 [\"Broker 1\"]\n            T1P0[\"Topic A<br/>Partition 0\"]\n            T1P1[\"Topic A<br/>Partition 1\"]\n        end\n        \n        subgraph Broker2 [\"Broker 2\"]\n            T1P2[\"Topic A<br/>Partition 2\"]\n            T2P0[\"Topic B<br/>Partition 0\"]\n        end\n        \n        subgraph Broker3 [\"Broker 3\"]\n            T2P1[\"Topic B<br/>Partition 1\"]\n        end\n    end\n    \n    subgraph Consumers [\"Consumer Groups\"]\n        subgraph CG1 [\"Consumer Group 1\"]\n            C1[\"Consumer 1\"]\n            C2[\"Consumer 2\"]\n        end\n        \n        subgraph CG2 [\"Consumer Group 2\"]\n            C3[\"Consumer 3\"]\n        end\n    end\n    \n    Producers --> Kafka --> Consumers\n```\n\n### Topic\n\n```mermaid\nflowchart LR\n    subgraph Topic [\"Topic: user_events\"]\n        direction TB\n        Desc[\"• 이벤트의 카테고리/채널<br/>• N개의 Partition으로 구성<br/>• 설정된 기간만큼 보존\"]\n    end\n    \n    Examples[\"예시:<br/>• orders<br/>• user_signups<br/>• page_views\"]\n    \n    Topic --- Examples\n```\n\n### Partition\n\n```mermaid\nflowchart TB\n    subgraph Topic [\"Topic: orders\"]\n        subgraph P0 [\"Partition 0\"]\n            M0_0[\"offset 0\"] --> M0_1[\"offset 1\"] --> M0_2[\"offset 2\"]\n        end\n        \n        subgraph P1 [\"Partition 1\"]\n            M1_0[\"offset 0\"] --> M1_1[\"offset 1\"] --> M1_2[\"offset 2\"]\n        end\n        \n        subgraph P2 [\"Partition 2\"]\n            M2_0[\"offset 0\"] --> M2_1[\"offset 1\"]\n        end\n    end\n    \n    Features[\"• 순서 보장 단위<br/>• 병렬 처리 단위<br/>• 파티션 키로 분배\"]\n```\n\n**핵심 인사이트**:\n\n- **순서 보장은 Partition 내에서만!**\n- Partition 수 = 병렬 처리 수준\n- 같은 키는 같은 Partition으로\n\n### Offset\n\n```mermaid\nflowchart LR\n    subgraph Partition [\"Partition 0\"]\n        O0[\"offset 0<br/>msg_a\"]\n        O1[\"offset 1<br/>msg_b\"]\n        O2[\"offset 2<br/>msg_c\"]\n        O3[\"offset 3<br/>msg_d\"]\n        O4[\"offset 4<br/>msg_e\"]\n        \n        O0 --> O1 --> O2 --> O3 --> O4\n    end\n    \n    subgraph Consumers [\"Consumer 위치\"]\n        C1[\"Group A<br/>offset: 3\"]\n        C2[\"Group B<br/>offset: 1\"]\n    end\n    \n    O3 -.->|\"읽는 중\"| C1\n    O1 -.->|\"읽는 중\"| C2\n```\n\n**Offset의 역할**:\n\n- 각 Consumer Group이 어디까지 읽었는지 추적\n- 재시작 시 이어서 읽기 가능\n- 과거 데이터 다시 읽기 가능 (rewind)\n\n### Consumer Group\n\n```mermaid\nflowchart TB\n    subgraph Topic [\"Topic: orders (3 partitions)\"]\n        P0[\"Partition 0\"]\n        P1[\"Partition 1\"]\n        P2[\"Partition 2\"]\n    end\n    \n    subgraph Group1 [\"Consumer Group A (3 consumers)\"]\n        C1[\"Consumer 1\"]\n        C2[\"Consumer 2\"]\n        C3[\"Consumer 3\"]\n    end\n    \n    subgraph Group2 [\"Consumer Group B (1 consumer)\"]\n        C4[\"Consumer 4\"]\n    end\n    \n    P0 --> C1\n    P1 --> C2\n    P2 --> C3\n    \n    P0 --> C4\n    P1 --> C4\n    P2 --> C4\n    \n    Note1[\"Group A: 각 Consumer가<br/>1개 Partition 담당\"]\n    Note2[\"Group B: 1 Consumer가<br/>모든 Partition 담당\"]\n```\n\n**핵심 규칙**:\n\n- 한 Partition은 Group 내 **하나의 Consumer**만 읽을 수 있음\n- Consumer 수 > Partition 수 → 일부 Consumer 유휴\n- Consumer 수 < Partition 수 → 일부 Consumer가 여러 Partition 담당\n\n---\n\n## Producer: 메시지 보내기\n\n### 파티션 결정 전략\n\n```mermaid\nflowchart TB\n    Message[\"메시지 전송\"]\n    \n    HasKey{\"키가<br/>있는가?\"}\n    \n    Hash[\"hash(key) % partition_count<br/>→ 같은 키 = 같은 Partition\"]\n    RoundRobin[\"라운드 로빈<br/>→ 고르게 분배\"]\n    Sticky[\"Sticky Partitioner<br/>→ 배치 최적화\"]\n    \n    Message --> HasKey\n    HasKey -->|\"예\"| Hash\n    HasKey -->|\"아니오 (2.4+)\"| Sticky\n    HasKey -->|\"아니오 (구버전)\"| RoundRobin\n```\n\n### Python Producer 예시\n\n```python\nfrom confluent_kafka import Producer\n\ndef delivery_callback(err, msg):\n    if err:\n        print(f\"Delivery failed: {err}\")\n    else:\n        print(f\"Delivered to {msg.topic()} [{msg.partition()}] @ {msg.offset()}\")\n\n# Producer 설정\nproducer = Producer({\n    'bootstrap.servers': 'localhost:9092',\n    'acks': 'all',  # 모든 replica 확인\n    'enable.idempotence': True,  # 중복 방지\n})\n\n# 키가 있는 메시지 (같은 user_id = 같은 Partition)\nproducer.produce(\n    topic='user_events',\n    key='user_123',\n    value='{\"event\": \"purchase\", \"amount\": 100}',\n    callback=delivery_callback\n)\n\n# 키가 없는 메시지 (자동 분배)\nproducer.produce(\n    topic='logs',\n    value='{\"level\": \"info\", \"message\": \"hello\"}',\n    callback=delivery_callback\n)\n\nproducer.flush()\n```\n\n---\n\n## Consumer: 메시지 읽기\n\n### Consumer 라이프사이클\n\n```mermaid\nflowchart TB\n    subgraph Lifecycle [\"Consumer 라이프사이클\"]\n        Subscribe[\"Subscribe<br/>Topic 구독\"]\n        Poll[\"Poll<br/>메시지 가져오기\"]\n        Process[\"Process<br/>비즈니스 로직\"]\n        Commit[\"Commit<br/>Offset 저장\"]\n        \n        Subscribe --> Poll --> Process --> Commit --> Poll\n    end\n```\n\n### Python Consumer 예시\n\n```python\nfrom confluent_kafka import Consumer, KafkaError\n\n# Consumer 설정\nconsumer = Consumer({\n    'bootstrap.servers': 'localhost:9092',\n    'group.id': 'my-consumer-group',\n    'auto.offset.reset': 'earliest',  # 처음부터 읽기\n    'enable.auto.commit': False,  # 수동 커밋\n})\n\nconsumer.subscribe(['user_events'])\n\ntry:\n    while True:\n        msg = consumer.poll(timeout=1.0)\n        \n        if msg is None:\n            continue\n        if msg.error():\n            if msg.error().code() == KafkaError._PARTITION_EOF:\n                continue\n            raise KafkaException(msg.error())\n        \n        # 메시지 처리\n        key = msg.key().decode('utf-8') if msg.key() else None\n        value = msg.value().decode('utf-8')\n        \n        print(f\"Received: key={key}, value={value}\")\n        \n        # 처리 완료 후 커밋\n        consumer.commit(asynchronous=False)\n        \nfinally:\n    consumer.close()\n```\n\n---\n\n## Exactly-Once Semantics\n\n> ⚠️ **주의**: Kafka의 Exactly-Once는 **\"Kafka 내부\"**에서의 보장입니다. 외부 DB/API로의 End-to-End Exactly-Once는 **애플리케이션 레벨에서 추가 처리**가 필요합니다.\n\n### 메시지 전달 보장 수준\n\n```mermaid\nflowchart TB\n    subgraph Levels [\"전달 보장 수준\"]\n        AtMost[\"At-Most-Once<br/>최대 1번\"]\n        AtLeast[\"At-Least-Once<br/>최소 1번\"]\n        Exactly[\"Exactly-Once<br/>정확히 1번\"]\n    end\n    \n    AtMost -->|\"메시지 유실 가능\"| L1[\"❌ 데이터 손실\"]\n    AtLeast -->|\"중복 가능\"| L2[\"⚠️ 중복 처리\"]\n    Exactly -->|\"정확함\"| L3[\"✅ 완벽\"]\n    \n    Difficulty[\"구현 난이도: At-Most < At-Least < Exactly\"]\n```\n\n### Idempotent Producer\n\n```mermaid\nflowchart LR\n    subgraph Problem [\"문제 상황\"]\n        P1[\"Producer 전송\"]\n        P2[\"Broker 저장\"]\n        P3[\"ACK 유실\"]\n        P4[\"Producer 재전송\"]\n        P5[\"중복 저장!\"]\n        \n        P1 --> P2 --> P3 --> P4 --> P5\n    end\n    \n    subgraph Solution [\"Idempotent Producer\"]\n        S1[\"Producer 전송<br/>(PID + SeqNum)\"]\n        S2[\"Broker 저장<br/>(SeqNum 기록)\"]\n        S3[\"ACK 유실\"]\n        S4[\"재전송 시<br/>중복 감지\"]\n        S5[\"무시됨 ✅\"]\n        \n        S1 --> S2 --> S3 --> S4 --> S5\n    end\n```\n\n```python\n# Idempotent Producer 설정\nproducer = Producer({\n    'bootstrap.servers': 'localhost:9092',\n    'enable.idempotence': True,  # 핵심 설정!\n    'acks': 'all',\n    'retries': 5,\n})\n```\n\n### Transactional Producer\n\n```mermaid\nflowchart TB\n    subgraph Transaction [\"트랜잭션\"]\n        Begin[\"begin_transaction()\"]\n        Send1[\"produce(topic_a)\"]\n        Send2[\"produce(topic_b)\"]\n        Commit[\"commit_transaction()\"]\n        \n        Begin --> Send1 --> Send2 --> Commit\n    end\n    \n    Result[\"모두 성공 또는 모두 실패<br/>→ Atomic\"]\n```\n\n```python\nfrom confluent_kafka import Producer\n\nproducer = Producer({\n    'bootstrap.servers': 'localhost:9092',\n    'enable.idempotence': True,\n    'transactional.id': 'my-transactional-producer',\n})\n\n# 트랜잭션 초기화 (한 번만)\nproducer.init_transactions()\n\ntry:\n    producer.begin_transaction()\n    \n    producer.produce('orders', key='order_1', value='...')\n    producer.produce('payments', key='order_1', value='...')\n    \n    producer.commit_transaction()\nexcept Exception as e:\n    producer.abort_transaction()\n    raise\n```\n\n### Consumer 측 Exactly-Once\n\n```python\nconsumer = Consumer({\n    'bootstrap.servers': 'localhost:9092',\n    'group.id': 'exactly-once-group',\n    'isolation.level': 'read_committed',  # 커밋된 메시지만 읽기\n    'enable.auto.commit': False,\n})\n```\n\n---\n\n## KRaft: Zookeeper 없는 Kafka\n\n### 기존 아키텍처의 문제\n\n```mermaid\nflowchart TB\n    subgraph Old [\"기존 (Zookeeper 기반)\"]\n        ZK[\"Zookeeper Cluster\"]\n        B1[\"Broker 1\"]\n        B2[\"Broker 2\"]\n        B3[\"Broker 3\"]\n        \n        ZK <-->|\"메타데이터\"| B1\n        ZK <-->|\"메타데이터\"| B2\n        ZK <-->|\"메타데이터\"| B3\n        \n        Problems[\"문제점:<br/>• 별도 클러스터 관리<br/>• 메타데이터 동기화 지연<br/>• 운영 복잡도\"]\n    end\n```\n\n### KRaft 아키텍처\n\n```mermaid\nflowchart TB\n    subgraph New [\"KRaft (Kafka 3.0+)\"]\n        subgraph Controllers [\"Controller 역할\"]\n            C1[\"Controller 1\"]\n            C2[\"Controller 2\"]\n            C3[\"Controller 3\"]\n        end\n        \n        subgraph Brokers [\"Broker 역할\"]\n            B1[\"Broker 1\"]\n            B2[\"Broker 2\"]\n            B3[\"Broker 3\"]\n        end\n        \n        Controllers <-->|\"Raft 합의\"| Controllers\n        Controllers -->|\"메타데이터\"| Brokers\n        \n        Benefits[\"장점:<br/>• 단일 시스템<br/>• 빠른 메타데이터 전파<br/>• 쉬운 운영\"]\n    end\n```\n\n---\n\n## Schema Registry: 스키마 버전 관리\n\n프로덕션 Kafka에서 **스키마 진화(Schema Evolution)**를 안전하게 관리하기 위한 필수 컴포넌트입니다.\n\n> **출처**: [Confluent Schema Registry Documentation](https://docs.confluent.io/platform/current/schema-registry/), Kleppmann, \"Designing Data-Intensive Applications\" Chapter 4\n\n### 왜 필요한가?\n\n```mermaid\nflowchart TB\n    subgraph Problem [\"스키마 없이 운영\"]\n        P1[\"Producer: {name, age}\"]\n        P2[\"Consumer: {name, age, email} 기대\"]\n        P3[\"💥 파싱 실패\"]\n        \n        P1 --> P3\n        P2 --> P3\n    end\n    \n    subgraph Solution [\"Schema Registry 사용\"]\n        S1[\"스키마 중앙 저장\"]\n        S2[\"버전 관리\"]\n        S3[\"호환성 검증\"]\n        S4[\"✅ 안전한 진화\"]\n        \n        S1 --> S2 --> S3 --> S4\n    end\n```\n\n### 지원 포맷\n\n| 포맷 | 특징 | 사용 사례 |\n|------|------|----------|\n| **Avro** | 스키마 진화 우수, 압축 효율 | 가장 널리 사용 |\n| **Protobuf** | gRPC 호환, 강타입 | 마이크로서비스 |\n| **JSON Schema** | 읽기 쉬움 | 디버깅, 호환성 |\n\n### 호환성 모드\n\n```mermaid\nflowchart TB\n    subgraph Modes [\"호환성 모드\"]\n        BACKWARD[\"BACKWARD<br/>새 스키마가 이전 데이터 읽기 가능\"]\n        FORWARD[\"FORWARD<br/>이전 스키마가 새 데이터 읽기 가능\"]\n        FULL[\"FULL<br/>양방향 호환\"]\n        NONE[\"NONE<br/>검증 없음 (비권장)\"]\n    end\n    \n    Recommend[\"권장: BACKWARD 또는 FULL\"]\n```\n\n| 모드 | 허용 변경 | 예시 |\n|------|----------|------|\n| **BACKWARD** | 필드 삭제, 기본값 있는 필드 추가 | 새 Consumer가 이전 데이터 읽음 |\n| **FORWARD** | 필드 추가, 기본값 있는 필드 삭제 | 이전 Consumer가 새 데이터 읽음 |\n| **FULL** | 기본값 있는 필드만 추가/삭제 | 가장 안전 |\n\n### Python 사용 예시\n\n```python\nfrom confluent_kafka import SerializingProducer\nfrom confluent_kafka.schema_registry import SchemaRegistryClient\nfrom confluent_kafka.schema_registry.avro import AvroSerializer\n\n# Schema Registry 연결\nschema_registry = SchemaRegistryClient({\n    'url': 'http://schema-registry:8081'\n})\n\n# Avro 스키마 정의\nuser_schema = \"\"\"\n{\n    \"type\": \"record\",\n    \"name\": \"User\",\n    \"fields\": [\n        {\"name\": \"name\", \"type\": \"string\"},\n        {\"name\": \"age\", \"type\": \"int\"},\n        {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null}\n    ]\n}\n\"\"\"\n\n# Serializer 생성 (스키마 자동 등록)\navro_serializer = AvroSerializer(\n    schema_registry,\n    user_schema,\n    to_dict=lambda user, ctx: user\n)\n\n# Producer 설정\nproducer = SerializingProducer({\n    'bootstrap.servers': 'localhost:9092',\n    'value.serializer': avro_serializer\n})\n\n# 메시지 전송\nproducer.produce(\n    topic='users',\n    value={'name': 'Kim', 'age': 30, 'email': 'kim@example.com'}\n)\nproducer.flush()\n```\n\n---\n\n## 사용 사례\n\n```mermaid\nflowchart TB\n    subgraph UseCases [\"Kafka 사용 사례\"]\n        subgraph Logging [\"로그 수집\"]\n            L1[\"App Logs\"]\n            L2[\"Kafka\"]\n            L3[\"Elasticsearch\"]\n            L1 --> L2 --> L3\n        end\n        \n        subgraph Events [\"이벤트 소싱\"]\n            E1[\"User Actions\"]\n            E2[\"Kafka<br/>(Event Store)\"]\n            E3[\"State 재구성\"]\n            E1 --> E2 --> E3\n        end\n        \n        subgraph CDC [\"Change Data Capture\"]\n            D1[(DB)]\n            D2[\"Debezium\"]\n            D3[\"Kafka\"]\n            D4[\"Data Lake\"]\n            D1 --> D2 --> D3 --> D4\n        end\n        \n        subgraph Stream [\"실시간 분석\"]\n            S1[\"Click Stream\"]\n            S2[\"Kafka\"]\n            S3[\"Flink/Spark\"]\n            S4[\"Dashboard\"]\n            S1 --> S2 --> S3 --> S4\n        end\n    end\n```\n\n---\n\n## 정리\n\n```mermaid\nmindmap\n  root((Kafka<br/>핵심))\n    vs Redis Streams\n      더 큰 스케일\n      더 긴 보존\n      더 나은 복제\n    구성 요소\n      Topic\n      Partition\n      Offset\n      Consumer Group\n    Producer\n      파티션 결정\n      키 기반 분배\n      Idempotent\n    Consumer\n      Group 관리\n      Offset 커밋\n      Rebalancing\n    Exactly-Once\n      Idempotent Producer\n      Transactional\n      read_committed\n    KRaft\n      Zookeeper 제거\n      단순한 운영\n      빠른 메타데이터\n    Schema Registry\n      스키마 버전 관리\n      호환성 검증\n      Avro/Protobuf\n```\n\n---\n\n## 다음 편 예고\n\n**9편: Spark Structured Streaming**에서는 실시간 처리를 다룹니다:\n\n- Kafka + Spark 연동\n- Watermark와 Late Data\n- Window 연산\n- 체크포인팅\n\n---\n\n## 참고 자료\n\n- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)\n- [Confluent Schema Registry](https://docs.confluent.io/platform/current/schema-registry/)\n- [Confluent Developer](https://developer.confluent.io/)\n- \"Kafka: The Definitive Guide\" (O'Reilly)\n- Martin Kleppmann, \"Designing Data-Intensive Applications\" - Chapter 4\n- [KRaft Overview](https://kafka.apache.org/documentation/#kraft)",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Data Engineering",
      "Kafka",
      "Partitioning",
      "Streaming"
    ],
    "readingTime": 9,
    "wordCount": 1692,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-07-airflow-production",
    "slug": "de-07-airflow-production",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-07-airflow-production",
    "title": "데이터 엔지니어링 시리즈 #7: Airflow 실전 - 프로덕션급 파이프라인 구축",
    "excerpt": "프로덕션에서 Airflow를 운영하는 방법을 배웁니다. DAG 모듈화, 동적 Task 생성, 테스트, 에러 처리, 모니터링까지.",
    "content": "# 데이터 엔지니어링 시리즈 #7: Airflow 실전 - 프로덕션급 파이프라인 구축\n\n> **대상 독자**: 충분한 경험을 가진 백엔드/풀스택 엔지니어로, Airflow 기본 개념을 익히고 프로덕션 운영에 관심 있는 분\n\n## 이 편에서 다루는 것\n\n6편에서 Airflow 개념을 배웠다면, 이제 **실제 프로덕션에서 어떻게 운영하는지** 실전 패턴을 배웁니다.\n\n---\n\n## DAG 모듈화 전략\n\n### 왜 모듈화가 필요한가?\n\n```mermaid\nflowchart TB\n    subgraph Bad [\"❌ 모든 것이 한 파일에\"]\n        B1[\"dag_everything.py<br/>• DB 연결<br/>• 비즈니스 로직<br/>• 설정<br/>• 헬퍼 함수<br/>...<br/>2000줄 😱\"]\n    end\n    \n    subgraph Good [\"✅ 모듈화된 구조\"]\n        G1[\"dags/daily_etl.py\"]\n        G2[\"plugins/operators/\"]\n        G3[\"plugins/hooks/\"]\n        G4[\"config/\"]\n        G5[\"utils/\"]\n    end\n    \n    Bad -->|\"리팩토링\"| Good\n```\n\n### 권장 디렉토리 구조\n\n```\nairflow/\n├── dags/\n│   ├── __init__.py\n│   ├── daily_etl.py\n│   ├── hourly_metrics.py\n│   └── config/\n│       ├── __init__.py\n│       ├── daily_etl_config.py\n│       └── tables.py\n│\n├── plugins/\n│   ├── __init__.py\n│   ├── operators/\n│   │   ├── __init__.py\n│   │   └── custom_operators.py\n│   └── hooks/\n│       ├── __init__.py\n│       └── custom_hooks.py\n│\n├── tests/\n│   ├── dags/\n│   │   └── test_daily_etl.py\n│   └── plugins/\n│       └── test_operators.py\n│\n└── requirements.txt\n```\n\n### 공통 설정 추출\n\n```python\n# config/daily_etl_config.py\nfrom dataclasses import dataclass\nfrom datetime import timedelta\n\n@dataclass\nclass ETLConfig:\n    source_conn_id: str = \"production_db\"\n    target_conn_id: str = \"warehouse_db\"\n    retries: int = 3\n    retry_delay: timedelta = timedelta(minutes=5)\n    \n    @property\n    def default_args(self):\n        return {\n            \"owner\": \"data-team\",\n            \"retries\": self.retries,\n            \"retry_delay\": self.retry_delay,\n        }\n\n# 테이블별 설정\nTABLES = {\n    \"users\": {\"schedule\": \"@daily\", \"partition_key\": \"created_at\"},\n    \"orders\": {\"schedule\": \"@hourly\", \"partition_key\": \"order_date\"},\n    \"events\": {\"schedule\": \"*/15 * * * *\", \"partition_key\": \"event_time\"},\n}\n```\n\n---\n\n## DAG Factory 패턴\n\n### 동적 DAG 생성\n\n```mermaid\nflowchart TB\n    subgraph Factory [\"DAG Factory\"]\n        Config[\"설정 파일<br/>(tables.py)\"]\n        Template[\"DAG 템플릿<br/>(create_etl_dag)\"]\n    end\n    \n    subgraph Output [\"생성된 DAG들\"]\n        D1[\"etl_users\"]\n        D2[\"etl_orders\"]\n        D3[\"etl_events\"]\n    end\n    \n    Config --> Template --> Output\n```\n\n```python\n# dags/etl_factory.py\nfrom airflow.decorators import dag, task\nfrom datetime import datetime\nfrom config.daily_etl_config import ETLConfig, TABLES\n\ndef create_etl_dag(table_name: str, table_config: dict):\n    \"\"\"테이블별 ETL DAG를 동적으로 생성\"\"\"\n    \n    config = ETLConfig()\n    \n    @dag(\n        dag_id=f\"etl_{table_name}\",\n        schedule=table_config[\"schedule\"],\n        start_date=datetime(2024, 1, 1),\n        catchup=False,\n        default_args=config.default_args,\n        tags=[\"etl\", \"generated\"]\n    )\n    def etl_pipeline():\n        \n        @task\n        def extract(**context):\n            from airflow.providers.postgres.hooks.postgres import PostgresHook\n            \n            hook = PostgresHook(postgres_conn_id=config.source_conn_id)\n            date = context[\"data_interval_start\"].strftime(\"%Y-%m-%d\")\n            partition_key = table_config[\"partition_key\"]\n            \n            sql = f\"\"\"\n                SELECT * FROM {table_name}\n                WHERE DATE({partition_key}) = '{date}'\n            \"\"\"\n            return hook.get_records(sql)\n        \n        @task\n        def transform(raw_data):\n            # 변환 로직\n            return raw_data\n        \n        @task\n        def load(data, **context):\n            from airflow.providers.postgres.hooks.postgres import PostgresHook\n            \n            hook = PostgresHook(postgres_conn_id=config.target_conn_id)\n            # 로드 로직\n            print(f\"Loaded {len(data)} records to warehouse.{table_name}\")\n        \n        raw = extract()\n        transformed = transform(raw)\n        load(transformed)\n    \n    return etl_pipeline()\n\n# 모든 테이블에 대해 DAG 생성\nfor table_name, table_config in TABLES.items():\n    globals()[f\"etl_{table_name}\"] = create_etl_dag(table_name, table_config)\n```\n\n---\n\n## Dynamic Task Mapping (Airflow 2.3+)\n\n### 런타임에 Task 개수 결정\n\n```mermaid\nflowchart LR\n    subgraph Before [\"정적 방식\"]\n        B1[\"process_user_1\"]\n        B2[\"process_user_2\"]\n        B3[\"process_user_3\"]\n        Note1[\"미리 정해진 수\"]\n    end\n    \n    subgraph After [\"Dynamic Mapping\"]\n        List[\"get_users()<br/>→ [u1, u2, ... uN]\"]\n        Expand[\"process.expand(user=users)\"]\n        Tasks[\"N개의 Task 생성\"]\n        \n        List --> Expand --> Tasks\n        Note2[\"런타임에 결정\"]\n    end\n```\n\n```python\nfrom airflow.decorators import dag, task\nfrom datetime import datetime\n\n@dag(\n    dag_id=\"dynamic_processing\",\n    schedule=\"@daily\",\n    start_date=datetime(2024, 1, 1),\n    catchup=False\n)\ndef dynamic_processing():\n    \n    @task\n    def get_partitions(**context):\n        \"\"\"처리할 파티션 목록 동적 반환\"\"\"\n        date = context[\"data_interval_start\"]\n        # 예: 날짜에 따라 다른 개수\n        return [f\"partition_{i}\" for i in range(10)]  # 10개 파티션\n    \n    @task\n    def process_partition(partition: str):\n        \"\"\"각 파티션 병렬 처리\"\"\"\n        print(f\"Processing {partition}\")\n        return {\"partition\": partition, \"count\": 1000}\n    \n    @task\n    def aggregate(results: list):\n        \"\"\"모든 결과 집계\"\"\"\n        total = sum(r[\"count\"] for r in results)\n        print(f\"Total: {total} records from {len(results)} partitions\")\n    \n    # Dynamic Task Mapping\n    partitions = get_partitions()\n    results = process_partition.expand(partition=partitions)  # N개 Task 생성\n    aggregate(results)\n\ndynamic_processing()\n```\n\n---\n\n## 테스트 전략\n\n### 테스트 피라미드\n\n```mermaid\nflowchart TB\n    subgraph Pyramid [\"테스트 피라미드\"]\n        E2E[\"E2E 테스트<br/>(실제 환경)\"]\n        Integration[\"통합 테스트<br/>(DAG 유효성)\"]\n        Unit[\"단위 테스트<br/>(비즈니스 로직)\"]\n    end\n    \n    Unit -->|\"가장 많이\"| Integration -->|\"적당히\"| E2E\n```\n\n### DAG 유효성 테스트\n\n```python\n# tests/dags/test_dag_validity.py\nimport pytest\nfrom airflow.models import DagBag\n\nclass TestDAGValidity:\n    \"\"\"모든 DAG의 기본 유효성 검사\"\"\"\n    \n    @pytest.fixture\n    def dagbag(self):\n        return DagBag(include_examples=False)\n    \n    def test_no_import_errors(self, dagbag):\n        \"\"\"DAG import 오류 없음\"\"\"\n        assert dagbag.import_errors == {}, f\"Import errors: {dagbag.import_errors}\"\n    \n    def test_all_dags_have_tags(self, dagbag):\n        \"\"\"모든 DAG에 태그 있음\"\"\"\n        for dag_id, dag in dagbag.dags.items():\n            assert dag.tags, f\"DAG {dag_id} has no tags\"\n    \n    def test_no_cycles(self, dagbag):\n        \"\"\"순환 의존성 없음\"\"\"\n        for dag_id, dag in dagbag.dags.items():\n            # Airflow가 자동으로 검사하지만 명시적으로\n            assert not dag.test_cycle(), f\"DAG {dag_id} has a cycle\"\n    \n    def test_default_args(self, dagbag):\n        \"\"\"필수 default_args 존재\"\"\"\n        required_keys = [\"owner\", \"retries\"]\n        for dag_id, dag in dagbag.dags.items():\n            for key in required_keys:\n                assert key in dag.default_args, f\"DAG {dag_id} missing {key}\"\n```\n\n### 개별 Task 테스트\n\n```python\n# tests/dags/test_daily_etl.py\nimport pytest\nfrom unittest.mock import patch, MagicMock\nfrom dags.daily_etl import calculate_metrics\n\nclass TestDailyETL:\n    \n    def test_calculate_metrics(self):\n        \"\"\"메트릭 계산 로직 테스트\"\"\"\n        events = [\n            {\"user_id\": 1, \"event\": \"click\"},\n            {\"user_id\": 1, \"event\": \"view\"},\n            {\"user_id\": 2, \"event\": \"click\"},\n        ]\n        \n        result = calculate_metrics.function(events)\n        \n        assert result[\"total_events\"] == 3\n        assert result[\"unique_users\"] == 2\n        assert result[\"events_per_user\"] == 1.5\n    \n    @patch(\"dags.daily_etl.PostgresHook\")\n    def test_extract_users(self, mock_hook):\n        \"\"\"추출 Task 테스트 (Mock 사용)\"\"\"\n        mock_hook.return_value.get_pandas_df.return_value = pd.DataFrame({\n            \"user_id\": [1, 2],\n            \"event_type\": [\"click\", \"view\"]\n        })\n        \n        # Task 실행\n        result = extract_users.function(data_interval_start=datetime(2024, 1, 1))\n        \n        assert len(result) == 2\n        mock_hook.assert_called_once()\n```\n\n---\n\n## 에러 처리와 알림\n\n### 콜백 함수\n\n```mermaid\nflowchart TB\n    subgraph Callbacks [\"콜백 종류\"]\n        C1[\"on_success_callback<br/>성공 시\"]\n        C2[\"on_failure_callback<br/>실패 시\"]\n        C3[\"on_retry_callback<br/>재시도 시\"]\n        C4[\"sla_miss_callback<br/>SLA 초과 시\"]\n    end\n    \n    subgraph Actions [\"가능한 액션\"]\n        A1[\"Slack 알림\"]\n        A2[\"PagerDuty 호출\"]\n        A3[\"이메일 전송\"]\n        A4[\"메트릭 기록\"]\n    end\n    \n    Callbacks --> Actions\n```\n\n```python\nfrom airflow.decorators import dag, task\nfrom datetime import datetime, timedelta\n\ndef send_slack_alert(context):\n    \"\"\"실패 시 Slack 알림\"\"\"\n    from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook\n    \n    task_instance = context[\"task_instance\"]\n    dag_id = context[\"dag\"].dag_id\n    task_id = task_instance.task_id\n    execution_date = context[\"execution_date\"]\n    log_url = task_instance.log_url\n    \n    message = f\"\"\"\n    🚨 *Task Failed*\n    • DAG: `{dag_id}`\n    • Task: `{task_id}`\n    • Execution: {execution_date}\n    • <{log_url}|View Logs>\n    \"\"\"\n    \n    hook = SlackWebhookHook(slack_webhook_conn_id=\"slack_webhook\")\n    hook.send(text=message)\n\ndef send_success_notification(context):\n    \"\"\"성공 시 알림 (선택적)\"\"\"\n    # 중요한 DAG만 성공 알림\n    pass\n\n@dag(\n    dag_id=\"monitored_pipeline\",\n    schedule=\"@daily\",\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    default_args={\n        \"retries\": 3,\n        \"retry_delay\": timedelta(minutes=5),\n        \"on_failure_callback\": send_slack_alert,\n    },\n    on_success_callback=send_success_notification\n)\ndef monitored_pipeline():\n    \n    @task(\n        retries=5,  # Task 개별 설정도 가능\n        retry_delay=timedelta(minutes=2)\n    )\n    def critical_task():\n        # 중요 로직\n        pass\n    \n    critical_task()\n\nmonitored_pipeline()\n```\n\n### SLA (Service Level Agreement)\n\n```python\nfrom airflow.decorators import dag, task\nfrom datetime import datetime, timedelta\n\n@dag(\n    dag_id=\"sla_monitored\",\n    schedule=\"@hourly\",\n    start_date=datetime(2024, 1, 1),\n    sla_miss_callback=send_sla_alert\n)\ndef sla_monitored():\n    \n    @task(\n        sla=timedelta(minutes=30)  # 30분 내 완료되어야 함\n    )\n    def time_sensitive_task():\n        # SLA를 초과하면 sla_miss_callback 호출\n        pass\n```\n\n---\n\n## 모니터링과 관측성\n\n### Airflow 메트릭\n\n```mermaid\nflowchart TB\n    subgraph Metrics [\"주요 모니터링 지표\"]\n        subgraph DAG [\"DAG 레벨\"]\n            D1[\"DAG Run 성공률\"]\n            D2[\"평균 실행 시간\"]\n            D3[\"지연(Lag)\"]\n        end\n        \n        subgraph Task [\"Task 레벨\"]\n            T1[\"Task 성공/실패율\"]\n            T2[\"Task Duration\"]\n            T3[\"Queue 대기 시간\"]\n        end\n        \n        subgraph System [\"시스템\"]\n            S1[\"Scheduler Heartbeat\"]\n            S2[\"Worker 상태\"]\n            S3[\"DB Connection Pool\"]\n        end\n    end\n```\n\n### StatsD + Grafana 연동\n\n```python\n# airflow.cfg\n[metrics]\nstatsd_on = True\nstatsd_host = statsd-exporter\nstatsd_port = 9125\nstatsd_prefix = airflow\n```\n\n### 유용한 대시보드 쿼리\n\n```python\n# 실패한 DAG Run 조회\nfrom airflow.models import DagRun\n\nfailed_runs = DagRun.find(\n    state=\"failed\",\n    execution_start_date=datetime.now() - timedelta(days=1)\n)\n\nfor run in failed_runs:\n    print(f\"{run.dag_id}: {run.execution_date}\")\n```\n\n---\n\n## 멱등성 보장\n\n### 왜 멱등성이 중요한가?\n\n```mermaid\nflowchart TB\n    subgraph Problem [\"멱등하지 않은 경우\"]\n        Run1[\"첫 실행: 100건 삽입\"]\n        Run2[\"재실행: 100건 추가 삽입\"]\n        Result1[\"결과: 200건 (중복!)\"]\n    end\n    \n    subgraph Solution [\"멱등한 경우\"]\n        Run3[\"첫 실행: 100건 삽입\"]\n        Run4[\"재실행: 100건 덮어쓰기\"]\n        Result2[\"결과: 100건 (정확!)\"]\n    end\n```\n\n### 멱등성 확보 패턴\n\n```python\n@task\ndef load_idempotent(data, **context):\n    \"\"\"멱등한 로드\"\"\"\n    from airflow.providers.postgres.hooks.postgres import PostgresHook\n    \n    hook = PostgresHook(postgres_conn_id=\"warehouse\")\n    date = context[\"data_interval_start\"].strftime(\"%Y-%m-%d\")\n    table = \"daily_metrics\"\n    \n    # 패턴 1: DELETE + INSERT\n    hook.run(f\"DELETE FROM {table} WHERE date = '{date}'\")\n    hook.insert_rows(table, data)\n    \n    # 패턴 2: UPSERT (PostgreSQL)\n    hook.run(f\"\"\"\n        INSERT INTO {table} (date, value)\n        VALUES ('{date}', {data['value']})\n        ON CONFLICT (date) DO UPDATE SET\n            value = EXCLUDED.value,\n            updated_at = NOW()\n    \"\"\")\n    \n    # 패턴 3: Partition 교체 (S3/GCS)\n    # s3://bucket/table/date=2024-01-01/ 전체 교체\n```\n\n---\n\n## 정리\n\n```mermaid\nmindmap\n  root((Airflow<br/>실전))\n    모듈화\n      디렉토리 구조\n      설정 분리\n      재사용 가능\n    DAG Factory\n      동적 생성\n      설정 기반\n      유지보수 용이\n    Dynamic Mapping\n      런타임 결정\n      expand 사용\n      병렬 처리\n    테스트\n      DAG 유효성\n      Task 단위\n      Mock 활용\n    에러 처리\n      on_failure_callback\n      SLA 설정\n      Slack 연동\n    모니터링\n      메트릭 수집\n      Grafana\n      로그 집계\n    멱등성\n      DELETE + INSERT\n      UPSERT\n      파티션 교체\n```\n\n---\n\n## 다음 편 예고\n\n**8편: Kafka 핵심**에서는 이벤트 스트리밍을 다룹니다:\n\n- Redis Streams와의 비교\n- Topic, Partition, Consumer Group\n- Exactly-Once Semantics\n- KRaft 모드\n\n---\n\n## 참고 자료\n\n- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)\n- [Testing Airflow DAGs](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html#testing-a-dag)\n- [Dynamic Task Mapping](https://airflow.apache.org/docs/apache-airflow/stable/concepts/dynamic-task-mapping.html)\n- Astronomer, \"Airflow in Production\"",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Airflow",
      "Data Engineering",
      "Observability",
      "Orchestration",
      "Testing"
    ],
    "readingTime": 7,
    "wordCount": 1297,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-06-airflow-core-concepts",
    "slug": "de-06-airflow-core-concepts",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-06-airflow-core-concepts",
    "title": "데이터 엔지니어링 시리즈 #6: Airflow 핵심 개념 - DAG, Operator, Task",
    "excerpt": "왜 cron으로는 부족할까요? Airflow의 핵심 개념인 DAG, Operator, Task를 이해하고 TaskFlow API로 현대적인 워크플로우를 작성하는 법을 배웁니다.",
    "content": "# 데이터 엔지니어링 시리즈 #6: Airflow 핵심 개념 - DAG, Operator, Task\n\n> **대상 독자**: 충분한 경험을 가진 백엔드/풀스택 엔지니어로, CI/CD 파이프라인이나 cron job에 익숙하지만 Airflow는 처음인 분\n\n## 이 편에서 다루는 것\n\nGitHub Actions나 cron으로 배치 작업을 돌려본 경험이 있다면, **왜 데이터 팀은 Airflow를 쓰는지** 궁금했을 겁니다. 그 이유와 핵심 개념을 배웁니다.\n\n---\n\n## 왜 cron job으로는 부족한가?\n\n### cron의 한계\n\n```mermaid\nflowchart TB\n    subgraph Cron [\"cron 방식의 문제\"]\n        C1[\"0 1 * * * extract.sh\"]\n        C2[\"0 2 * * * transform.sh\"]\n        C3[\"0 3 * * * load.sh\"]\n        \n        Problem1[\"❓ extract가 늦어지면?\"]\n        Problem2[\"❓ 중간에 실패하면?\"]\n        Problem3[\"❓ 어제 데이터를 재처리하려면?\"]\n        Problem4[\"❓ 실행 상태를 어떻게 확인?\"]\n    end\n```\n\n| 문제 | cron | Airflow |\n|------|------|---------|\n| **의존성 관리** | 시간으로만 (불확실) | 명시적 의존성 ✅ |\n| **실패 처리** | 수동 확인/재실행 | 자동 재시도 ✅ |\n| **백필** | 스크립트 수동 수정 | 날짜 지정 재실행 ✅ |\n| **모니터링** | 로그 파일 뒤지기 | 웹 UI ✅ |\n| **알림** | 직접 구현 | Slack/Email 연동 ✅ |\n\n### 실제 시나리오\n\n```mermaid\nflowchart LR\n    subgraph Reality [\"현실에서 일어나는 일\"]\n        A[\"Extract<br/>(01:00 예정)\"]\n        B[\"Transform<br/>(02:00 예정)\"]\n        C[\"Load<br/>(03:00 예정)\"]\n        \n        A -->|\"01:30에 끝남\"| Delay\n        Delay[\"⚠️ Transform이<br/>불완전한 데이터로 시작\"]\n        Delay --> Bad[\"❌ 잘못된 결과\"]\n    end\n```\n\n**Airflow의 해결책**: Task 간 **의존성**을 정의하여 이전 Task가 완료되어야 다음이 시작\n\n---\n\n## Airflow 아키텍처\n\n### 구성 요소\n\n```mermaid\nflowchart TB\n    subgraph Airflow [\"Airflow 시스템\"]\n        Web[\"Webserver<br/>📊 UI 제공\"]\n        Sched[\"Scheduler<br/>⏰ DAG 파싱/스케줄링\"]\n        Worker[\"Worker(s)<br/>⚙️ Task 실행\"]\n        DB[(Metadata DB<br/>📁 상태 저장)]\n        \n        Web <--> DB\n        Sched <--> DB\n        Worker <--> DB\n        Sched -->|\"Task 할당\"| Worker\n    end\n    \n    subgraph DAGs [\"DAG 파일\"]\n        D1[\"dag1.py\"]\n        D2[\"dag2.py\"]\n        D3[\"dag3.py\"]\n    end\n    \n    DAGs -->|\"파싱\"| Sched\n```\n\n### Executor 종류\n\n| Executor | 특징 | 적합한 환경 |\n|----------|------|------------|\n| **LocalExecutor** | 단일 머신, 멀티 프로세스 | 개발, 소규모 |\n| **CeleryExecutor** | 분산 워커 (Redis/RabbitMQ) | 중규모 프로덕션 |\n| **KubernetesExecutor** | 각 Task를 Pod로 | 대규모, 클라우드 |\n\n---\n\n## DAG (Directed Acyclic Graph)\n\n### DAG란?\n\n```mermaid\nflowchart LR\n    subgraph DAG [\"DAG: Directed Acyclic Graph\"]\n        A[\"Task A\"]\n        B[\"Task B\"]\n        C[\"Task C\"]\n        D[\"Task D\"]\n        E[\"Task E\"]\n        \n        A --> B\n        A --> C\n        B --> D\n        C --> D\n        D --> E\n    end\n    \n    subgraph Rules [\"규칙\"]\n        R1[\"✅ Directed: 방향이 있음\"]\n        R2[\"✅ Acyclic: 순환 없음\"]\n        R3[\"❌ A → B → A (불가)\"]\n    end\n```\n\n**왜 그래프인가?**\n\n- 순차 실행만 있는 게 아님\n- 병렬 실행 가능 (B와 C 동시 실행)\n- 의존성 명확히 표현\n\n### DAG 정의 예시\n\n```python\nfrom airflow import DAG\nfrom datetime import datetime\n\n# DAG 정의\ndag = DAG(\n    dag_id=\"my_etl_pipeline\",\n    start_date=datetime(2024, 1, 1),\n    schedule=\"@daily\",  # 매일 실행\n    catchup=False,\n    tags=[\"etl\", \"production\"]\n)\n```\n\n---\n\n## Operator와 Task\n\n### Operator: 무엇을 할 것인가?\n\n```mermaid\nflowchart TB\n    subgraph Operators [\"주요 Operator 종류\"]\n        subgraph Basic [\"기본\"]\n            O1[\"BashOperator<br/>쉘 명령 실행\"]\n            O2[\"PythonOperator<br/>Python 함수 실행\"]\n            O3[\"EmptyOperator<br/>아무것도 안 함\"]\n        end\n        \n        subgraph Transfer [\"데이터 전송\"]\n            O4[\"S3ToRedshiftOperator\"]\n            O5[\"GCSToGCSOperator\"]\n        end\n        \n        subgraph External [\"외부 시스템\"]\n            O6[\"SparkSubmitOperator<br/>Spark 작업 제출\"]\n            O7[\"DockerOperator<br/>컨테이너 실행\"]\n            O8[\"PostgresOperator<br/>SQL 실행\"]\n        end\n        \n        subgraph Sensors [\"센서 (대기)\"]\n            O9[\"FileSensor<br/>파일 존재 대기\"]\n            O10[\"HttpSensor<br/>API 응답 대기\"]\n        end\n    end\n```\n\n### Task: Operator의 인스턴스\n\n```mermaid\nflowchart LR\n    subgraph Definition [\"정의\"]\n        Operator[\"PythonOperator<br/>(클래스)\"]\n    end\n    \n    subgraph Instance [\"인스턴스\"]\n        Task1[\"extract_task<br/>(Task)\"]\n        Task2[\"transform_task<br/>(Task)\"]\n        Task3[\"load_task<br/>(Task)\"]\n    end\n    \n    Operator --> Task1\n    Operator --> Task2\n    Operator --> Task3\n```\n\n```python\nfrom airflow.operators.python import PythonOperator\n\ndef extract_data():\n    # 데이터 추출 로직\n    return {\"records\": 1000}\n\ndef transform_data(**context):\n    # 이전 Task 결과 가져오기\n    data = context[\"ti\"].xcom_pull(task_ids=\"extract\")\n    # 변환 로직\n    return {\"processed\": data[\"records\"]}\n\n# Task 정의\nextract_task = PythonOperator(\n    task_id=\"extract\",\n    python_callable=extract_data,\n    dag=dag\n)\n\ntransform_task = PythonOperator(\n    task_id=\"transform\",\n    python_callable=transform_data,\n    dag=dag\n)\n\n# 의존성 정의\nextract_task >> transform_task\n```\n\n---\n\n## TaskFlow API (Airflow 2.0+)\n\n### 전통적 방식 vs TaskFlow\n\n```mermaid\nflowchart TB\n    subgraph Traditional [\"전통적 방식\"]\n        T1[\"Operator 정의\"]\n        T2[\"XCom으로 데이터 전달\"]\n        T3[\"의존성 별도 정의\"]\n        \n        T1 --> T2 --> T3\n        Note1[\"장황한 코드 😓\"]\n    end\n    \n    subgraph TaskFlow [\"TaskFlow API\"]\n        TF1[\"@task 데코레이터\"]\n        TF2[\"return으로 전달\"]\n        TF3[\"함수 호출로 의존성\"]\n        \n        TF1 --> TF2 --> TF3\n        Note2[\"깔끔한 코드 ✨\"]\n    end\n```\n\n### TaskFlow 예시\n\n```python\nfrom airflow.decorators import dag, task\nfrom datetime import datetime\n\n@dag(\n    dag_id=\"taskflow_etl\",\n    start_date=datetime(2024, 1, 1),\n    schedule=\"@daily\",\n    catchup=False\n)\ndef my_etl_pipeline():\n    \"\"\"TaskFlow API를 사용한 ETL 파이프라인\"\"\"\n    \n    @task\n    def extract():\n        \"\"\"데이터 추출\"\"\"\n        return {\"data\": [1, 2, 3, 4, 5]}\n    \n    @task\n    def transform(raw_data: dict):\n        \"\"\"데이터 변환\"\"\"\n        return {\n            \"data\": [x * 2 for x in raw_data[\"data\"]],\n            \"count\": len(raw_data[\"data\"])\n        }\n    \n    @task\n    def load(processed_data: dict):\n        \"\"\"데이터 적재\"\"\"\n        print(f\"Loaded {processed_data['count']} records\")\n    \n    # 의존성이 자연스럽게 정의됨\n    raw = extract()\n    processed = transform(raw)\n    load(processed)\n\n# DAG 인스턴스 생성\nmy_etl_pipeline()\n```\n\n### XCom 자동 처리\n\n```mermaid\nflowchart LR\n    subgraph Traditional [\"전통적 XCom\"]\n        E1[\"extract\"]\n        X1[\"xcom_push()\"]\n        X2[\"xcom_pull()\"]\n        T1[\"transform\"]\n        \n        E1 --> X1 --> X2 --> T1\n        Note1[\"명시적 push/pull 필요\"]\n    end\n    \n    subgraph TaskFlow [\"TaskFlow XCom\"]\n        E2[\"@task<br/>return data\"]\n        T2[\"@task<br/>def fn(data):\"]\n        \n        E2 -->|\"자동!\"| T2\n        Note2[\"return/파라미터로 자동 전달\"]\n    end\n```\n\n---\n\n## 스케줄링과 Data Interval\n\n### schedule 표현식\n\n| 표현식 | 의미 | cron 표현 |\n|--------|------|----------|\n| `@once` | 한 번만 | - |\n| `@hourly` | 매시 | `0 * * * *` |\n| `@daily` | 매일 | `0 0 * * *` |\n| `@weekly` | 매주 | `0 0 * * 0` |\n| `@monthly` | 매월 | `0 0 1 * *` |\n| `0 6 * * *` | 매일 6시 | - |\n| `None` | 수동 트리거만 | - |\n\n### Data Interval 개념 (중요!)\n\n```mermaid\nflowchart TB\n    subgraph Timeline [\"시간선\"]\n        T1[\"2024-01-01<br/>00:00\"]\n        T2[\"2024-01-02<br/>00:00\"]\n        T3[\"2024-01-03<br/>00:00\"]\n    end\n    \n    subgraph DAGRun [\"DAG 실행\"]\n        D1[\"DAG Run 1<br/>data_interval: 01-01 ~ 01-02\"]\n        D2[\"DAG Run 2<br/>data_interval: 01-02 ~ 01-03\"]\n    end\n    \n    T2 -->|\"실행 시점\"| D1\n    T3 -->|\"실행 시점\"| D2\n    \n    Note[\"⚠️ 1월 2일에 1월 1일 데이터를 처리!\"]\n```\n\n```python\n@task\ndef process_data(**context):\n    # 처리할 데이터의 날짜 범위\n    data_interval_start = context[\"data_interval_start\"]\n    data_interval_end = context[\"data_interval_end\"]\n    \n    # 예: 2024-01-01 00:00 ~ 2024-01-02 00:00\n    print(f\"Processing data from {data_interval_start} to {data_interval_end}\")\n```\n\n### Catchup과 Backfill\n\n```mermaid\nflowchart TB\n    subgraph Catchup [\"catchup=True\"]\n        C1[\"DAG 생성: 2024-01-05\"]\n        C2[\"start_date: 2024-01-01\"]\n        C3[\"누락된 4일치 자동 실행\"]\n        \n        C1 --> C2 --> C3\n    end\n    \n    subgraph NoCatchup [\"catchup=False\"]\n        N1[\"DAG 생성: 2024-01-05\"]\n        N2[\"start_date: 2024-01-01\"]\n        N3[\"오늘(01-05)부터만 실행\"]\n        \n        N1 --> N2 --> N3\n    end\n```\n\n```bash\n# 수동 Backfill\nairflow dags backfill \\\n    --start-date 2024-01-01 \\\n    --end-date 2024-01-10 \\\n    my_etl_pipeline\n```\n\n---\n\n## Task 의존성 패턴\n\n### 기본 패턴\n\n```mermaid\nflowchart LR\n    subgraph Sequential [\"순차\"]\n        S1[\"A\"] --> S2[\"B\"] --> S3[\"C\"]\n    end\n    \n    subgraph Parallel [\"병렬\"]\n        P1[\"A\"] --> P2[\"B\"]\n        P1 --> P3[\"C\"]\n        P2 --> P4[\"D\"]\n        P3 --> P4\n    end\n    \n    subgraph FanOut [\"Fan-out\"]\n        F1[\"A\"] --> F2[\"B1\"]\n        F1 --> F3[\"B2\"]\n        F1 --> F4[\"B3\"]\n    end\n```\n\n### 코드에서 의존성 정의\n\n```python\n# 방법 1: >> 연산자\ntask_a >> task_b >> task_c\n\n# 방법 2: << 연산자 (역방향)\ntask_c << task_b << task_a\n\n# 방법 3: 리스트로 병렬\ntask_a >> [task_b, task_c] >> task_d\n\n# 방법 4: set_downstream/set_upstream\ntask_a.set_downstream(task_b)\ntask_b.set_upstream(task_a)\n```\n\n### TaskFlow에서는 더 자연스럽게\n\n```python\n@dag(...)\ndef pipeline():\n    @task\n    def start(): pass\n    \n    @task\n    def process_a(data): pass\n    \n    @task\n    def process_b(data): pass\n    \n    @task\n    def end(a, b): pass\n    \n    data = start()\n    result_a = process_a(data)\n    result_b = process_b(data)\n    end(result_a, result_b)  # 자동으로 의존성 생성\n```\n\n---\n\n## 실전 예제: 데이터 파이프라인\n\n```python\nfrom airflow.decorators import dag, task\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    \"owner\": \"data-team\",\n    \"retries\": 3,\n    \"retry_delay\": timedelta(minutes=5),\n}\n\n@dag(\n    dag_id=\"daily_user_analytics\",\n    start_date=datetime(2024, 1, 1),\n    schedule=\"@daily\",\n    catchup=False,\n    default_args=default_args,\n    tags=[\"analytics\", \"production\"]\n)\ndef daily_user_analytics():\n    \"\"\"일일 사용자 분석 파이프라인\"\"\"\n    \n    @task\n    def extract_users(**context):\n        \"\"\"PostgreSQL에서 사용자 데이터 추출\"\"\"\n        from airflow.providers.postgres.hooks.postgres import PostgresHook\n        \n        date = context[\"data_interval_start\"].strftime(\"%Y-%m-%d\")\n        hook = PostgresHook(postgres_conn_id=\"production_db\")\n        \n        sql = f\"\"\"\n            SELECT user_id, event_type, created_at\n            FROM user_events\n            WHERE DATE(created_at) = '{date}'\n        \"\"\"\n        \n        df = hook.get_pandas_df(sql)\n        return df.to_dict(\"records\")\n    \n    @task\n    def calculate_metrics(events: list):\n        \"\"\"사용자 메트릭 계산\"\"\"\n        from collections import Counter\n        \n        user_events = Counter(e[\"user_id\"] for e in events)\n        \n        return {\n            \"total_events\": len(events),\n            \"unique_users\": len(user_events),\n            \"events_per_user\": len(events) / len(user_events) if user_events else 0\n        }\n    \n    @task\n    def save_to_warehouse(metrics: dict, **context):\n        \"\"\"결과를 데이터 웨어하우스에 저장\"\"\"\n        from airflow.providers.postgres.hooks.postgres import PostgresHook\n        \n        date = context[\"data_interval_start\"].strftime(\"%Y-%m-%d\")\n        hook = PostgresHook(postgres_conn_id=\"analytics_db\")\n        \n        hook.run(f\"\"\"\n            INSERT INTO daily_metrics (date, total_events, unique_users, events_per_user)\n            VALUES ('{date}', {metrics['total_events']}, {metrics['unique_users']}, {metrics['events_per_user']})\n            ON CONFLICT (date) DO UPDATE SET\n                total_events = EXCLUDED.total_events,\n                unique_users = EXCLUDED.unique_users,\n                events_per_user = EXCLUDED.events_per_user\n        \"\"\")\n    \n    @task\n    def notify_slack(metrics: dict):\n        \"\"\"Slack 알림 전송\"\"\"\n        from airflow.providers.slack.hooks.slack import SlackHook\n        \n        hook = SlackHook(slack_conn_id=\"slack\")\n        hook.send(\n            channel=\"#data-alerts\",\n            text=f\"📊 Daily Metrics: {metrics['unique_users']} users, {metrics['total_events']} events\"\n        )\n    \n    # 의존성 정의\n    events = extract_users()\n    metrics = calculate_metrics(events)\n    save_to_warehouse(metrics)\n    notify_slack(metrics)\n\ndaily_user_analytics()\n```\n\n---\n\n## 정리\n\n```mermaid\nmindmap\n  root((Airflow<br/>핵심 개념))\n    왜 Airflow?\n      의존성 관리\n      실패 처리\n      백필\n      모니터링\n    아키텍처\n      Webserver\n      Scheduler\n      Worker\n      Metadata DB\n    DAG\n      방향성 그래프\n      순환 없음\n      Task들의 모음\n    Operator/Task\n      Operator: 무엇을\n      Task: 인스턴스\n      의존성 정의\n    TaskFlow API\n      @task 데코레이터\n      자동 XCom\n      깔끔한 코드\n    스케줄링\n      Data Interval\n      Catchup\n      Backfill\n```\n\n---\n\n## 다음 편 예고\n\n**7편: Airflow 실전**에서는 프로덕션 운영을 다룹니다:\n\n- DAG 모듈화 전략\n- 동적 Task 생성\n- 테스트 방법\n- 에러 처리와 알림\n- 모니터링\n\n---\n\n## 참고 자료\n\n- [Apache Airflow Documentation](https://airflow.apache.org/docs/)\n- [TaskFlow API Tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html)\n- Astronomer, \"Airflow Best Practices\"\n- \"Data Pipelines with Apache Airflow\" (Manning)",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Airflow",
      "Data Engineering",
      "Data Pipeline",
      "Orchestration"
    ],
    "readingTime": 8,
    "wordCount": 1413,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-05-pyspark-patterns",
    "slug": "de-05-pyspark-patterns",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-05-pyspark-patterns",
    "title": "데이터 엔지니어링 시리즈 #5: PySpark 실전 - 데이터 처리 패턴과 최적화",
    "excerpt": "실무에서 자주 사용하는 PySpark 패턴을 배웁니다. DataFrame 연산, UDF 최적화, 조인 전략, 캐싱, 그리고 피해야 할 안티패턴까지.",
    "content": "# 데이터 엔지니어링 시리즈 #5: PySpark 실전 - 데이터 처리 패턴과 최적화\n\n> **대상 독자**: 충분한 경험을 가진 백엔드/풀스택 엔지니어로, Spark 개념을 익히고 실전 코드를 작성하려는 분\n\n## 이 편에서 다루는 것\n\n4편에서 Spark 내부를 이해했다면, 이제 **실제로 코드를 어떻게 작성해야 하는지** 패턴과 최적화 기법을 배웁니다.\n\n---\n\n## 자주 사용하는 DataFrame 연산\n\n### 기본 연산 맵\n\n```mermaid\nflowchart TB\n    subgraph Selection [\"선택/필터\"]\n        S1[\"select()\"]\n        S2[\"filter() / where()\"]\n        S3[\"drop()\"]\n    end\n    \n    subgraph Transform [\"변환\"]\n        T1[\"withColumn()\"]\n        T2[\"withColumnRenamed()\"]\n        T3[\"cast()\"]\n    end\n    \n    subgraph Aggregate [\"집계\"]\n        A1[\"groupBy()\"]\n        A2[\"agg()\"]\n        A3[\"pivot()\"]\n    end\n    \n    subgraph Join [\"조인\"]\n        J1[\"join()\"]\n        J2[\"crossJoin()\"]\n        J3[\"union()\"]\n    end\n    \n    subgraph Window [\"윈도우\"]\n        W1[\"over()\"]\n        W2[\"partitionBy()\"]\n        W3[\"orderBy()\"]\n    end\n    \n    Selection --> Transform --> Aggregate --> Output[\"결과\"]\n    Join --> Aggregate\n    Window --> Aggregate\n```\n\n### 선택과 필터링\n\n```python\nfrom pyspark.sql import functions as F\n\n# 컬럼 선택 - 필요한 것만!\ndf.select(\"user_id\", \"name\", \"email\")\n\n# 여러 방식의 컬럼 참조\ndf.select(\n    F.col(\"user_id\"),\n    df.name,\n    df[\"email\"]\n)\n\n# 필터링\ndf.filter(F.col(\"age\") > 20)\ndf.filter((F.col(\"age\") > 20) & (F.col(\"city\") == \"Seoul\"))\n\n# SQL 표현식도 가능\ndf.filter(\"age > 20 AND city = 'Seoul'\")\n```\n\n### 컬럼 변환\n\n```python\n# 새 컬럼 추가\ndf.withColumn(\"age_group\", \n    F.when(F.col(\"age\") < 20, \"teen\")\n     .when(F.col(\"age\") < 30, \"20s\")\n     .when(F.col(\"age\") < 40, \"30s\")\n     .otherwise(\"40+\")\n)\n\n# 타입 변환\ndf.withColumn(\"amount\", F.col(\"amount\").cast(\"double\"))\n\n# 문자열 처리\ndf.withColumn(\"email_domain\", \n    F.split(F.col(\"email\"), \"@\").getItem(1)\n)\n\n# 날짜 처리\ndf.withColumn(\"year\", F.year(\"created_at\"))\ndf.withColumn(\"month\", F.month(\"created_at\"))\ndf.withColumn(\"date_str\", F.date_format(\"created_at\", \"yyyy-MM-dd\"))\n```\n\n### 집계 연산\n\n```python\n# 기본 집계\ndf.groupBy(\"city\").agg(\n    F.count(\"*\").alias(\"user_count\"),\n    F.avg(\"age\").alias(\"avg_age\"),\n    F.sum(\"purchase_amount\").alias(\"total_purchase\"),\n    F.max(\"last_login\").alias(\"last_activity\")\n)\n\n# 여러 그룹 기준\ndf.groupBy(\"city\", \"gender\").count()\n\n# Pivot (행→열)\ndf.groupBy(\"year\").pivot(\"quarter\", [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]).sum(\"revenue\")\n```\n\n### 윈도우 함수\n\n```mermaid\nflowchart TB\n    subgraph WindowConcept [\"윈도우 함수 개념\"]\n        Data[\"데이터\"]\n        Partition[\"파티션별 그룹핑<br/>(groupBy와 유사)\"]\n        Order[\"정렬\"]\n        Frame[\"윈도우 프레임\"]\n        Calculate[\"계산 (rank, sum, etc.)\"]\n        \n        Data --> Partition --> Order --> Frame --> Calculate\n    end\n    \n    Note[\"✅ groupBy와 달리<br/>원본 행 유지\"]\n```\n\n```python\nfrom pyspark.sql.window import Window\n\n# 윈도우 정의\nwindow_spec = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n\n# 순위 (파티션 내 순서)\ndf.withColumn(\"row_num\", F.row_number().over(window_spec))\ndf.withColumn(\"rank\", F.rank().over(window_spec))\n\n# 이전/다음 값\ndf.withColumn(\"prev_value\", F.lag(\"value\", 1).over(window_spec))\ndf.withColumn(\"next_value\", F.lead(\"value\", 1).over(window_spec))\n\n# 누적 합계\ndf.withColumn(\"cumsum\", F.sum(\"amount\").over(window_spec))\n\n# 파티션 전체 기준 (정렬 없이)\nunbounded = Window.partitionBy(\"user_id\")\ndf.withColumn(\"user_total\", F.sum(\"amount\").over(unbounded))\n```\n\n---\n\n## UDF vs Built-in Functions\n\n### 왜 Built-in을 써야 하는가?\n\n```mermaid\nflowchart TB\n    subgraph UDF [\"Python UDF\"]\n        U1[\"Python 함수 정의\"]\n        U2[\"직렬화 (pickle)\"]\n        U3[\"JVM → Python 전송\"]\n        U4[\"Python에서 실행\"]\n        U5[\"결과 직렬화\"]\n        U6[\"Python → JVM\"]\n        \n        U1 --> U2 --> U3 --> U4 --> U5 --> U6\n    end\n    \n    subgraph BuiltIn [\"Built-in Function\"]\n        B1[\"API 호출\"]\n        B2[\"JVM에서 직접 실행\"]\n        B3[\"Catalyst 최적화\"]\n        \n        B1 --> B2 --> B3\n    end\n    \n    UDF -->|\"🐢 20~100x 느림\"| Slow[\"성능 저하\"]\n    BuiltIn -->|\"🚀 빠름\"| Fast[\"최적 성능\"]\n```\n\n### 비교 예시\n\n```python\n# ❌ 나쁜 예: UDF 사용\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\n@udf(returnType=StringType())\ndef extract_domain(email):\n    if email:\n        return email.split(\"@\")[-1]\n    return None\n\ndf.withColumn(\"domain\", extract_domain(F.col(\"email\")))\n\n# ✅ 좋은 예: Built-in 함수 사용\ndf.withColumn(\"domain\", \n    F.split(F.col(\"email\"), \"@\").getItem(1)\n)\n```\n\n### 정말 UDF가 필요한 경우: Pandas UDF\n\n```mermaid\nflowchart LR\n    subgraph Types [\"UDF 유형별 성능\"]\n        T1[\"Python UDF<br/>🐢 느림\"]\n        T2[\"Pandas UDF<br/>⚡ 빠름\"]\n        T3[\"Built-in<br/>🚀 가장 빠름\"]\n    end\n    \n    T1 -->|\"벡터화\"| T2 -->|\"가능하면\"| T3\n```\n\n```python\nimport pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n\n# Pandas UDF - Series → Series (벡터화)\n@pandas_udf(\"double\")\ndef calculate_zscore(values: pd.Series) -> pd.Series:\n    return (values - values.mean()) / values.std()\n\ndf.withColumn(\"zscore\", calculate_zscore(F.col(\"value\")))\n\n# Pandas UDF - GroupBy Aggregate\n@pandas_udf(\"double\")\ndef median_value(v: pd.Series) -> float:\n    return v.median()\n\ndf.groupBy(\"category\").agg(median_value(F.col(\"price\")))\n```\n\n---\n\n## 조인 최적화\n\n### 조인 종류와 선택\n\n```mermaid\nflowchart TB\n    subgraph JoinTypes [\"조인 종류\"]\n        direction TB\n        Broadcast[\"Broadcast Join<br/>작은 테이블을 전체 복사\"]\n        SortMerge[\"Sort-Merge Join<br/>정렬 후 병합\"]\n        Shuffle[\"Shuffle Hash Join<br/>해시 기반 재배치\"]\n    end\n    \n    Decision{\"작은 테이블이<br/>10MB 이하?\"}\n    \n    Decision -->|\"예\"| Broadcast\n    Decision -->|\"아니오\"| SortMerge\n```\n\n### Broadcast Join (필수!)\n\n```mermaid\nflowchart TB\n    subgraph NoBroadcast [\"일반 조인\"]\n        L1[\"Large Table<br/>100GB\"]\n        S1[\"Small Table<br/>10MB\"]\n        Shuffle1[\"Shuffle 🔀\"]\n        \n        L1 --> Shuffle1\n        S1 --> Shuffle1\n    end\n    \n    subgraph WithBroadcast [\"Broadcast 조인\"]\n        L2[\"Large Table<br/>100GB\"]\n        S2[\"Small Table<br/>10MB\"]\n        \n        S2 -->|\"각 Executor로 복사\"| E1[\"Executor 1\"]\n        S2 --> E2[\"Executor 2\"]\n        S2 --> E3[\"Executor 3\"]\n        \n        L2 -->|\"Shuffle 없음\"| E1 & E2 & E3\n    end\n    \n    NoBroadcast -->|\"❌ 느림\"| Slow\n    WithBroadcast -->|\"✅ 빠름\"| Fast\n```\n\n```python\nfrom pyspark.sql.functions import broadcast\n\n# 작은 테이블에 broadcast 힌트\nresult = large_df.join(\n    broadcast(small_df), \n    \"join_key\"\n)\n\n# 또는 설정으로 자동 적용\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10 * 1024 * 1024)  # 10MB\n```\n\n### 조인 순서 최적화\n\n```python\n# ❌ 나쁜 예: 필터 후조인이 아님\nresult = df1.join(df2, \"key\").filter(df1.status == \"active\")\n\n# ✅ 좋은 예: 조인 전에 필터\ndf1_filtered = df1.filter(df1.status == \"active\")\nresult = df1_filtered.join(df2, \"key\")\n```\n\n---\n\n## 캐싱과 체크포인팅\n\n### 언제 캐시하는가?\n\n```mermaid\nflowchart TB\n    Q1{\"같은 DataFrame을<br/>여러 번 사용?\"}\n    Q2{\"계산 비용이<br/>비싼가?\"}\n    Q3{\"메모리에<br/>들어가는가?\"}\n    \n    Q1 -->|\"예\"| Q2\n    Q1 -->|\"아니오\"| NoCache[\"캐시 불필요\"]\n    Q2 -->|\"예\"| Q3\n    Q2 -->|\"아니오\"| NoCache\n    Q3 -->|\"예\"| Cache[\"✅ cache() 사용\"]\n    Q3 -->|\"아니오\"| Persist[\"persist(DISK) 사용\"]\n```\n\n```python\n# 기본 캐시 (메모리)\nexpensive_df = df.groupBy(\"category\").agg(...)\nexpensive_df.cache()\n\n# 첫 번째 Action에서 캐시됨\nexpensive_df.count()  \n\n# 이후 재사용 시 캐시에서 읽음\nexpensive_df.filter(...).show()\nexpensive_df.select(...).write.parquet(...)\n\n# 캐시 해제\nexpensive_df.unpersist()\n```\n\n### cache() vs persist()\n\n```python\nfrom pyspark import StorageLevel\n\n# cache() = persist(MEMORY_AND_DISK)\ndf.cache()\n\n# 명시적 스토리지 레벨\ndf.persist(StorageLevel.MEMORY_ONLY)\ndf.persist(StorageLevel.MEMORY_AND_DISK)\ndf.persist(StorageLevel.DISK_ONLY)\ndf.persist(StorageLevel.MEMORY_AND_DISK_SER)  # 직렬화하여 저장\n```\n\n### 체크포인팅\n\n```python\n# checkpoint는 계보(lineage)를 끊음\nspark.sparkContext.setCheckpointDir(\"hdfs://path/checkpoints\")\n\n# 복잡한 변환 후\nresult = complex_transformations(df)\nresult.checkpoint()\n\n# 이후 장애 시 체크포인트에서 복구\n```\n\n---\n\n## 안티패턴 피하기\n\n### ❌ collect() 남용\n\n```mermaid\nflowchart LR\n    subgraph Cluster [\"클러스터\"]\n        E1[\"1TB\"]\n        E2[\"1TB\"]\n        E3[\"1TB\"]\n    end\n    \n    subgraph Driver [\"Driver\"]\n        D[\"메모리: 4GB\"]\n    end\n    \n    Cluster -->|\"collect()\"| Driver\n    Driver -->|\"💥 OOM\"| Crash[\"OutOfMemory!\"]\n```\n\n```python\n# ❌ 나쁜 예\nall_data = df.collect()  # 전체를 Driver로!\nfor row in all_data:\n    process(row)\n\n# ✅ 좋은 예: 집계 후 collect\nsummary = df.groupBy(\"category\").count().collect()\n\n# ✅ 좋은 예: limit 사용\nsample = df.limit(1000).collect()\n\n# ✅ 좋은 예: Iterator 사용\nfor row in df.toLocalIterator():\n    process(row)  # 한 번에 하나씩\n```\n\n### ❌ 작은 파일 문제\n\n```mermaid\nflowchart TB\n    subgraph Problem [\"문제 상황\"]\n        P1[\"10,000개 파일\"]\n        P2[\"각 1MB\"]\n        P3[\"총 10GB\"]\n        \n        P1 --> Overhead[\"❌ 파일 오픈 오버헤드<br/>❌ 메타데이터 처리 비용\"]\n    end\n    \n    subgraph Solution [\"해결책\"]\n        S1[\"100개 파일\"]\n        S2[\"각 100MB\"]\n        S3[\"총 10GB\"]\n        \n        S1 --> Efficient[\"✅ I/O 효율적\"]\n    end\n```\n\n```python\n# ❌ 나쁜 예: 파티션마다 파일 생성\ndf.write.parquet(\"output/\")  # 파티션 수만큼 파일\n\n# ✅ 좋은 예: coalesce로 파일 수 조절\ndf.coalesce(10).write.parquet(\"output/\")  # 10개 파일\n\n# ✅ 좋은 예: 적정 크기로 분할\ndf.repartition(100).write.parquet(\"output/\")  # 100개 파일\n```\n\n### ❌ 불필요한 Shuffle\n\n```python\n# ❌ 나쁜 예: groupBy 두 번\nresult = df.groupBy(\"a\").count() \\\n           .groupBy(\"a\").agg(F.sum(\"count\"))\n\n# ✅ 좋은 예: 한 번에 처리\nresult = df.groupBy(\"a\").agg(F.count(\"*\").alias(\"count\"))\n```\n\n---\n\n## 실전 예제: 로그 분석 파이프라인\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nspark = SparkSession.builder \\\n    .appName(\"LogAnalysis\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .getOrCreate()\n\n# 1. 데이터 로드 (필요한 컬럼만)\nlogs = spark.read.json(\"logs/*.json\").select(\n    \"timestamp\", \"user_id\", \"event_type\", \"page\", \"duration\"\n)\n\n# 2. 데이터 정제\ncleaned = logs \\\n    .filter(F.col(\"user_id\").isNotNull()) \\\n    .withColumn(\"event_date\", F.to_date(\"timestamp\")) \\\n    .withColumn(\"event_hour\", F.hour(\"timestamp\"))\n\n# 3. 여러 번 사용할 것이므로 캐시\ncleaned.cache()\n\n# 4. 일별 집계\ndaily_stats = cleaned.groupBy(\"event_date\").agg(\n    F.countDistinct(\"user_id\").alias(\"dau\"),\n    F.count(\"*\").alias(\"total_events\"),\n    F.avg(\"duration\").alias(\"avg_duration\")\n)\n\n# 5. 시간대별 패턴\nhourly_pattern = cleaned.groupBy(\"event_hour\").agg(\n    F.count(\"*\").alias(\"events\")\n).orderBy(\"event_hour\")\n\n# 6. 유저별 세션 분석 (윈도우 함수)\nuser_window = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n\nsessions = cleaned \\\n    .withColumn(\"prev_timestamp\", F.lag(\"timestamp\").over(user_window)) \\\n    .withColumn(\"time_gap\", \n        F.unix_timestamp(\"timestamp\") - F.unix_timestamp(\"prev_timestamp\")) \\\n    .withColumn(\"new_session\", \n        F.when(F.col(\"time_gap\") > 1800, 1).otherwise(0)) \\\n    .withColumn(\"session_id\", \n        F.sum(\"new_session\").over(user_window))\n\n# 7. 저장\ndaily_stats.write.mode(\"overwrite\").parquet(\"output/daily_stats\")\nhourly_pattern.write.mode(\"overwrite\").parquet(\"output/hourly_pattern\")\n\n# 8. 캐시 해제\ncleaned.unpersist()\n```\n\n---\n\n## 정리\n\n```mermaid\nmindmap\n  root((PySpark<br/>실전))\n    DataFrame 연산\n      select, filter\n      withColumn\n      groupBy, agg\n      Window 함수\n    UDF 최적화\n      Built-in 우선\n      Pandas UDF\n      Python UDF 피하기\n    조인\n      Broadcast Join\n      조인 전 필터\n      작은 테이블 힌트\n    캐싱\n      여러 번 사용 시\n      cache vs persist\n      unpersist 잊지 않기\n    안티패턴\n      collect 남용\n      작은 파일 문제\n      불필요한 Shuffle\n```\n\n---\n\n## 다음 편 예고\n\n**6편: Airflow 핵심 개념**에서는 워크플로우 오케스트레이션을 다룹니다:\n\n- 왜 cron으로는 부족한가?\n- DAG, Operator, Task 이해\n- TaskFlow API (Airflow 2.0+)\n- 스케줄링과 Backfill\n\n---\n\n## 참고 자료\n\n- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)\n- [Spark SQL Built-in Functions](https://spark.apache.org/docs/latest/api/sql/)\n- Databricks, \"PySpark Best Practices\"\n- [Window Functions Guide](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-window.html)",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Data Engineering",
      "Performance",
      "Spark",
      "UDF"
    ],
    "readingTime": 7,
    "wordCount": 1232,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-04-spark-internals",
    "slug": "de-04-spark-internals",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-04-spark-internals",
    "title": "데이터 엔지니어링 시리즈 #4: Spark 내부 동작 원리 - Job, Stage, Task",
    "excerpt": "Spark의 실행 모델을 이해합니다. Job, Stage, Task 계층, Shuffle의 비용, 파티셔닝 전략, 그리고 Spark UI를 읽는 법까지.",
    "content": "# 데이터 엔지니어링 시리즈 #4: Spark 내부 동작 원리 - Job, Stage, Task\n\n> **대상 독자**: 충분한 경험을 가진 백엔드/풀스택 엔지니어로, Spark의 기본 개념을 알고 성능 튜닝에 관심 있는 분\n\n## 이 편에서 다루는 것\n\nSpark 코드를 작성할 때 **왜 어떤 코드는 느리고 어떤 코드는 빠른지** 이해하려면, 내부 실행 모델을 알아야 합니다.\n\n---\n\n## 실행 계층 구조: Application → Job → Stage → Task\n\n### 전체 구조\n\n```mermaid\nflowchart TB\n    subgraph App [\"Application (앱 전체)\"]\n        subgraph Job1 [\"Job 1 (Action 1)\"]\n            subgraph Stage1 [\"Stage 1\"]\n                T1[\"Task 1\"]\n                T2[\"Task 2\"]\n                T3[\"Task 3\"]\n            end\n            subgraph Stage2 [\"Stage 2\"]\n                T4[\"Task 4\"]\n                T5[\"Task 5\"]\n            end\n        end\n        \n        subgraph Job2 [\"Job 2 (Action 2)\"]\n            subgraph Stage3 [\"Stage 3\"]\n                T6[\"Task 6\"]\n                T7[\"Task 7\"]\n            end\n        end\n    end\n    \n    Stage1 -->|\"Shuffle\"| Stage2\n```\n\n### 각 계층의 역할\n\n| 계층 | 무엇 | 언제 생성 | 병렬성 |\n|------|------|---------|--------|\n| **Application** | 전체 Spark 프로그램 | spark-submit 시 | 1개 |\n| **Job** | 하나의 Action 실행 단위 | count(), save() 호출 | 순차 |\n| **Stage** | Shuffle 기준 분리 | 자동 분리 | 순차 |\n| **Task** | 파티션당 실행 단위 | 파티션 수만큼 | **병렬** |\n\n### 코드 예시와 실행 흐름\n\n```python\n# 이 코드가 어떻게 실행될까?\ndf = spark.read.parquet(\"data/\")  # Transformation\nfiltered = df.filter(df.age > 20)  # Transformation\ngrouped = filtered.groupBy(\"city\").count()  # Transformation\ngrouped.show()  # Action → Job 생성!\n```\n\n```mermaid\nflowchart LR\n    subgraph Job [\"Job (show 호출)\"]\n        subgraph Stage1 [\"Stage 1: Wide 전까지\"]\n            Read[\"read.parquet\"]\n            Filter[\"filter(age > 20)\"]\n            Read --> Filter\n        end\n        \n        Shuffle[\"⚡ Shuffle<br/>(city 기준 재배치)\"]\n        \n        subgraph Stage2 [\"Stage 2: 집계\"]\n            GroupBy[\"groupBy + count\"]\n            Show[\"show()\"]\n            GroupBy --> Show\n        end\n        \n        Stage1 --> Shuffle --> Stage2\n    end\n```\n\n---\n\n## Narrow vs Wide Transformations\n\n### 이것이 성능의 핵심\n\n```mermaid\nflowchart TB\n    subgraph Narrow [\"Narrow Transformations\"]\n        direction TB\n        N1[\"map\"]\n        N2[\"filter\"]\n        N3[\"flatMap\"]\n        N4[\"select\"]\n        \n        NP1[\"Partition 1\"] --> NP1R[\"결과 1\"]\n        NP2[\"Partition 2\"] --> NP2R[\"결과 2\"]\n        NP3[\"Partition 3\"] --> NP3R[\"결과 3\"]\n        \n        Desc1[\"✅ 파티션 독립 처리<br/>✅ 네트워크 통신 없음<br/>✅ 매우 빠름\"]\n    end\n    \n    subgraph Wide [\"Wide Transformations\"]\n        direction TB\n        W1[\"groupBy\"]\n        W2[\"join\"]\n        W3[\"orderBy\"]\n        W4[\"repartition\"]\n        \n        WP1[\"Partition 1\"] --> WS[\"Shuffle<br/>🔀\"] --> WR1[\"결과 1\"]\n        WP2[\"Partition 2\"] --> WS --> WR2[\"결과 2\"]\n        WP3[\"Partition 3\"] --> WS --> WR3[\"결과 3\"]\n        \n        Desc2[\"⚠️ 데이터 재배치<br/>⚠️ 네트워크 I/O 발생<br/>⚠️ 느림\"]\n    end\n```\n\n### Shuffle이 비싼 이유\n\n```mermaid\nflowchart TB\n    subgraph Before [\"Shuffle 전\"]\n        P1[\"Executor 1<br/>키: A, B, C\"]\n        P2[\"Executor 2<br/>키: A, D, E\"]\n        P3[\"Executor 3<br/>키: B, C, F\"]\n    end\n    \n    subgraph Network [\"네트워크 전송\"]\n        direction TB\n        N1[\"A 데이터 → Reducer 1로\"]\n        N2[\"B 데이터 → Reducer 2로\"]\n        N3[\"C 데이터 → Reducer 3으로\"]\n        N4[\"...\"]\n    end\n    \n    subgraph After [\"Shuffle 후\"]\n        R1[\"Reducer 1<br/>키 A만\"]\n        R2[\"Reducer 2<br/>키 B만\"]\n        R3[\"Reducer 3<br/>키 C만\"]\n    end\n    \n    Before --> Network --> After\n    \n    Cost[\"💸 비용 발생<br/>• 디스크 쓰기<br/>• 네트워크 전송<br/>• 디스크 읽기<br/>• 정렬\"]\n```\n\n**Shuffle이 발생하면**:\n\n1. 각 Executor가 결과를 **디스크에 저장**\n2. 키 기준으로 **네트워크로 전송**\n3. 받는 쪽에서 **디스크에 저장**\n4. 키 기준 **정렬**\n5. 메모리로 **읽어서 처리**\n\n---\n\n## 파티셔닝 전략\n\n### 파티션이란?\n\n```mermaid\nflowchart TB\n    subgraph Data [\"원본 데이터\"]\n        BigData[\"1TB 데이터\"]\n    end\n    \n    subgraph Partitions [\"파티션 분할\"]\n        P1[\"Partition 1<br/>100GB\"]\n        P2[\"Partition 2<br/>100GB\"]\n        P3[\"Partition 3<br/>100GB\"]\n        PN[\"...<br/>100GB\"]\n    end\n    \n    subgraph Tasks [\"병렬 처리\"]\n        T1[\"Task 1<br/>→ Core 1\"]\n        T2[\"Task 2<br/>→ Core 2\"]\n        T3[\"Task 3<br/>→ Core 3\"]\n        TN[\"Task N<br/>→ Core N\"]\n    end\n    \n    BigData --> Partitions --> Tasks\n```\n\n### 파티션 수와 병렬성\n\n```mermaid\nflowchart TB\n    subgraph TooFew [\"파티션이 너무 적음\"]\n        F1[\"4 파티션\"]\n        F2[\"100 코어 클러스터\"]\n        F3[\"❌ 96 코어 놀고 있음\"]\n    end\n    \n    subgraph TooMany [\"파티션이 너무 많음\"]\n        M1[\"10000 파티션\"]\n        M2[\"100 코어 클러스터\"]\n        M3[\"❌ 스케줄링 오버헤드\"]\n    end\n    \n    subgraph JustRight [\"적절한 파티션\"]\n        R1[\"200~400 파티션\"]\n        R2[\"100 코어 클러스터\"]\n        R3[\"✅ 코어당 2~4 Task\"]\n    end\n```\n\n**경험칙**:\n\n- 파티션 수 = 코어 수 × 2~4\n- 파티션당 크기 = 100MB ~ 1GB\n\n### 데이터 스큐(Skew) 문제\n\n```mermaid\nflowchart TB\n    subgraph Skewed [\"스큐 발생\"]\n        S1[\"Partition 1<br/>10MB\"]\n        S2[\"Partition 2<br/>10MB\"]\n        S3[\"Partition 3<br/>10GB !!\"]\n        \n        ST1[\"Task 1<br/>1초\"]\n        ST2[\"Task 2<br/>1초\"]\n        ST3[\"Task 3<br/>100초 😱\"]\n        \n        S1 --> ST1\n        S2 --> ST2\n        S3 --> ST3\n    end\n    \n    Result[\"전체 시간 = 100초<br/>(가장 느린 Task 기준)\"]\n    \n    Skewed --> Result\n```\n\n**해결책**:\n\n1. **Salting**: 핫 키에 랜덤 접두사 추가\n2. **Broadcast Join**: 작은 테이블은 전체 복사\n3. **Adaptive Query Execution (AQE)**: Spark 3.0+ 자동 최적화\n\n---\n\n## 메모리 관리\n\n### Executor 메모리 구조\n\n```mermaid\nflowchart TB\n    subgraph Executor [\"Executor 메모리\"]\n        subgraph Reserved [\"Reserved (300MB)\"]\n            R[\"시스템용\"]\n        end\n        \n        subgraph Unified [\"Unified Memory (60%)\"]\n            Storage[\"Storage<br/>(캐시)\"]\n            Execution[\"Execution<br/>(Shuffle, 정렬)\"]\n            Storage <-->|\"동적 공유\"| Execution\n        end\n        \n        subgraph User [\"User Memory (40%)\"]\n            UDF[\"UDF 객체\"]\n            Meta[\"메타데이터\"]\n        end\n    end\n```\n\n### 메모리 부족 시: Spill to Disk\n\n```mermaid\nflowchart LR\n    subgraph Normal [\"정상 상태\"]\n        Mem1[\"메모리 사용<br/>3GB\"]\n        Limit1[\"할당량<br/>4GB\"]\n    end\n    \n    subgraph Spill [\"Spill 발생\"]\n        Mem2[\"메모리 사용<br/>4GB+\"]\n        Disk[\"디스크로<br/>내보내기 💾\"]\n        Slow[\"🐢 느려짐\"]\n        \n        Mem2 --> Disk --> Slow\n    end\n    \n    Normal -->|\"데이터 증가\"| Spill\n```\n\n**Spill 감지 방법**: Spark UI에서 \"Spill (Memory)\" / \"Spill (Disk)\" 확인\n\n---\n\n## Spark UI 읽는 법\n\n### 핵심 지표들\n\n```mermaid\nflowchart TB\n    subgraph SparkUI [\"Spark UI\"]\n        subgraph JobsTab [\"Jobs 탭\"]\n            J1[\"Job 성공/실패\"]\n            J2[\"전체 소요 시간\"]\n        end\n        \n        subgraph StagesTab [\"Stages 탭 ⭐\"]\n            S1[\"Stage별 시간\"]\n            S2[\"Shuffle Read/Write\"]\n            S3[\"Task 분포\"]\n        end\n        \n        subgraph SQLTab [\"SQL 탭\"]\n            SQL1[\"실행 계획\"]\n            SQL2[\"물리 계획\"]\n        end\n    end\n```\n\n### Stages 탭 해석\n\n```mermaid\nflowchart TB\n    subgraph StageMetrics [\"Stage 지표\"]\n        subgraph Good [\"✅ 정상\"]\n            G1[\"Task Duration 균일\"]\n            G2[\"Shuffle Write 적음\"]\n            G3[\"Spill 없음\"]\n        end\n        \n        subgraph Bad [\"⚠️ 문제\"]\n            B1[\"Task Duration 편차 큼<br/>→ 데이터 스큐\"]\n            B2[\"Shuffle 크기 거대<br/>→ 조인/그룹 최적화 필요\"]\n            B3[\"Spill 발생<br/>→ 메모리 부족\"]\n        end\n    end\n```\n\n### 실전 디버깅 플로우\n\n```mermaid\nflowchart TB\n    Start[\"Job이 느림\"]\n    \n    Q1{\"Shuffle이<br/>큰가?\"}\n    Q2{\"Task Duration<br/>편차가 큰가?\"}\n    Q3{\"Spill이<br/>발생하는가?\"}\n    Q4{\"GC 시간이<br/>긴가?\"}\n    \n    A1[\"조인/그룹 최적화<br/>Broadcast Join 고려\"]\n    A2[\"데이터 스큐 해결<br/>Salting, AQE\"]\n    A3[\"메모리 증가<br/>파티션 수 조정\"]\n    A4[\"Executor 메모리 증가<br/>GC 튜닝\"]\n    \n    Start --> Q1\n    Q1 -->|\"예\"| A1\n    Q1 -->|\"아니오\"| Q2\n    Q2 -->|\"예\"| A2\n    Q2 -->|\"아니오\"| Q3\n    Q3 -->|\"예\"| A3\n    Q3 -->|\"아니오\"| Q4\n    Q4 -->|\"예\"| A4\n    Q4 -->|\"아니오\"| Other[\"다른 원인 조사\"]\n```\n\n---\n\n## 실전 최적화 체크리스트\n\n### 코드 레벨\n\n| 항목 | 좋은 예 | 나쁜 예 |\n|------|--------|--------|\n| **조인** | Broadcast Join (작은 테이블) | 양쪽 다 큰 Shuffle Join |\n| **필터** | 조인 전에 filter | 조인 후에 filter |\n| **컬럼 선택** | 필요한 컬럼만 select | SELECT * |\n| **UDF** | Built-in 함수 사용 | Python UDF 남용 |\n| **collect** | 집계 후 collect | 큰 데이터 collect |\n\n### 설정 레벨\n\n```python\n# 권장 설정\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")  # AQE\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")  # 기본 200\n```\n\n---\n\n## 정리\n\n```mermaid\nmindmap\n  root((Spark<br/>내부 동작))\n    계층 구조\n      Application\n      Job (Action마다)\n      Stage (Shuffle마다)\n      Task (파티션마다)\n    Transformation\n      Narrow\n        map, filter\n        빠름\n      Wide\n        groupBy, join\n        Shuffle 발생\n        느림\n    Shuffle\n      네트워크 전송\n      디스크 I/O\n      성능 병목\n    파티셔닝\n      적정 수: 코어 x 2~4\n      스큐 주의\n      AQE 활용\n    메모리\n      Storage + Execution\n      Spill 발생 시 느려짐\n    Spark UI\n      Stage 탭 확인\n      Shuffle 크기\n      Task 분포\n```\n\n---\n\n## 다음 편 예고\n\n**5편: PySpark 실전**에서는 실무 패턴을 다룹니다:\n\n- 자주 쓰는 DataFrame 연산\n- UDF vs Built-in Functions\n- 조인 최적화 기법\n- 캐싱과 체크포인팅\n- 안티패턴 피하기\n\n---\n\n## 참고 자료\n\n- [Spark Web UI](https://spark.apache.org/docs/latest/web-ui.html)\n- [Tuning Spark](https://spark.apache.org/docs/latest/tuning.html)\n- [Adaptive Query Execution](https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution)\n- Jacek Laskowski, \"The Internals of Apache Spark\"",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Data Engineering",
      "Partitioning",
      "Performance",
      "Spark"
    ],
    "readingTime": 6,
    "wordCount": 1124,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-03-spark-core-concepts",
    "slug": "de-03-spark-core-concepts",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-03-spark-core-concepts",
    "title": "데이터 엔지니어링 시리즈 #3: Spark 핵심 개념 - RDD에서 DataFrame까지",
    "excerpt": "분산 처리의 핵심 개념과 Spark의 추상화 계층을 이해합니다. Goroutine, ThreadPoolExecutor와 비교하며 Spark가 해결하는 문제를 파악합니다.",
    "content": "# 데이터 엔지니어링 시리즈 #3: Spark 핵심 개념 - RDD에서 DataFrame까지\n\n> **대상 독자**: 충분한 경험을 가진 백엔드/풀스택 엔지니어로, Go의 Goroutine이나 Python의 ThreadPoolExecutor에 익숙하지만 Spark는 처음인 분\n\n## 이 편에서 다루는 것\n\n\"Spark가 빠르다\"는 말은 많이 들어봤을 겁니다. 하지만 **왜 빠른지**, **기존 병렬 처리와 무엇이 다른지** 이해하는 것이 먼저입니다.\n\n---\n\n## 왜 분산 처리가 필요한가?\n\n### 단일 서버의 한계\n\n백엔드 개발에서 성능을 높이려면 어떻게 하나요?\n\n```mermaid\nflowchart TB\n    subgraph SingleServer [\"단일 서버 최적화\"]\n        direction TB\n        S1[\"1. 알고리즘 개선\"]\n        S2[\"2. 인덱스 추가\"]\n        S3[\"3. 캐싱\"]\n        S4[\"4. 병렬 처리<br/>(Goroutine, ThreadPool)\"]\n        S5[\"5. 수직 확장<br/>(더 좋은 서버)\"]\n    end\n    \n    Limit[\"그래도 안 되면?\"]\n    \n    SingleServer --> Limit\n    Limit --> Distribute[\"수평 확장<br/>(여러 서버로 분산)\"]\n```\n\n언젠가는 **단일 서버로는 불가능한 순간**이 옵니다:\n\n| 상황 | 예시 |\n|------|------|\n| **데이터가 메모리에 안 들어감** | 1TB 데이터를 32GB 서버에서 처리 |\n| **처리 시간이 너무 김** | 단일 코어로 10억 건 처리에 10시간 |\n| **디스크 I/O 병목** | 초당 읽기 한계 도달 |\n\n### 분산 처리의 핵심 아이디어\n\n```mermaid\nflowchart LR\n    subgraph Before [\"단일 서버\"]\n        Server1[\"서버 1대<br/>🖥️\"]\n        Data1[\"1TB 데이터\"]\n        Time1[\"⏱️ 10시간\"]\n        Data1 --> Server1 --> Time1\n    end\n    \n    subgraph After [\"분산 처리\"]\n        Data2[\"1TB 데이터\"]\n        \n        subgraph Cluster [\"10대 클러스터\"]\n            C1[\"🖥️ 100GB\"]\n            C2[\"🖥️ 100GB\"]\n            C3[\"🖥️ 100GB\"]\n            CN[\"🖥️ ...\"]\n        end\n        \n        Time2[\"⏱️ 1시간\"]\n        \n        Data2 --> Cluster --> Time2\n    end\n    \n    Before -.->|\"10배 빠르게\"| After\n```\n\n**핵심**: 데이터와 연산을 여러 서버에 **나눠서** 동시에 처리\n\n---\n\n## Goroutine/ThreadPoolExecutor와 Spark의 차이\n\n### 기존 병렬 처리: 단일 서버 내\n\nGo와 Python에서의 병렬 처리는 **단일 서버의 CPU 코어를 활용**합니다.\n\n```mermaid\nflowchart TB\n    subgraph Go [\"Go: Goroutines\"]\n        GoRuntime[\"Go Runtime<br/>(단일 프로세스)\"]\n        G1[\"goroutine\"]\n        G2[\"goroutine\"]\n        G3[\"goroutine\"]\n        G4[\"goroutine\"]\n        \n        GoRuntime --> G1 & G2 & G3 & G4\n        \n        subgraph GoCPU [\"CPU 코어\"]\n            GC1[\"Core 1\"]\n            GC2[\"Core 2\"]\n            GC3[\"Core 3\"]\n            GC4[\"Core 4\"]\n        end\n        \n        G1 -.-> GC1\n        G2 -.-> GC2\n        G3 -.-> GC3\n        G4 -.-> GC4\n    end\n    \n    subgraph Python [\"Python: ThreadPoolExecutor\"]\n        Pool[\"ThreadPoolExecutor\"]\n        T1[\"Thread 1\"]\n        T2[\"Thread 2\"]\n        T3[\"Thread 3\"]\n        T4[\"Thread 4\"]\n        \n        Pool --> T1 & T2 & T3 & T4\n        \n        GIL[\"⚠️ GIL 제약\"]\n    end\n```\n\n**한계**:\n\n- **메모리 한계**: 서버 RAM 크기를 초과하는 데이터 처리 불가\n- **CPU 한계**: 코어 수 이상의 병렬성 불가\n- **GIL (Python)**: CPU-bound 작업 시 진정한 병렬성 어려움\n\n### Spark: 여러 서버에 분산\n\n```mermaid\nflowchart TB\n    subgraph Driver [\"Driver (마스터)\"]\n        App[\"Spark Application\"]\n    end\n    \n    subgraph Cluster [\"클러스터 (워커들)\"]\n        subgraph Worker1 [\"Worker 1 (서버 A)\"]\n            E1[\"Executor\"]\n            E1T1[\"Task\"]\n            E1T2[\"Task\"]\n            E1 --> E1T1 & E1T2\n        end\n        \n        subgraph Worker2 [\"Worker 2 (서버 B)\"]\n            E2[\"Executor\"]\n            E2T1[\"Task\"]\n            E2T2[\"Task\"]\n            E2 --> E2T1 & E2T2\n        end\n        \n        subgraph Worker3 [\"Worker 3 (서버 C)\"]\n            E3[\"Executor\"]\n            E3T1[\"Task\"]\n            E3T2[\"Task\"]\n            E3 --> E3T1 & E3T2\n        end\n    end\n    \n    App -->|\"작업 분배\"| Worker1 & Worker2 & Worker3\n```\n\n**Spark의 해결책**:\n\n- **메모리 분산**: 각 서버가 데이터 일부만 처리\n- **CPU 분산**: 총 CPU = 서버 수 × 서버당 코어\n- **장애 복구**: 한 서버가 죽어도 다른 서버가 재처리\n\n### 비교 정리\n\n| 특성 | Goroutine / ThreadPool | Spark |\n|------|----------------------|-------|\n| **범위** | 단일 서버 | 여러 서버 클러스터 |\n| **스케일링** | 수직 (더 좋은 서버) | 수평 (서버 추가) |\n| **메모리** | 서버 RAM 한계 | 클러스터 합산 RAM |\n| **장애 처리** | 프로세스 재시작 | 다른 노드가 재처리 |\n| **데이터 공유** | 메모리 직접 공유 | 네트워크 통신 |\n| **적합한 데이터** | GB 이하 | TB ~ PB |\n\n---\n\n## MapReduce 패러다임\n\nSpark를 이해하려면 먼저 **MapReduce**를 알아야 합니다.\n\n### 클래식 예제: Word Count\n\n\"Hello World Hello\" 라는 텍스트에서 단어별 개수를 세는 문제입니다.\n\n```mermaid\nflowchart LR\n    subgraph Input [\"입력\"]\n        I1[\"Hello World Hello\"]\n    end\n    \n    subgraph Map [\"Map (변환)\"]\n        M1[\"(Hello, 1)\"]\n        M2[\"(World, 1)\"]\n        M3[\"(Hello, 1)\"]\n    end\n    \n    subgraph Shuffle [\"Shuffle (재배치)\"]\n        S1[\"Hello → [(Hello, 1), (Hello, 1)]\"]\n        S2[\"World → [(World, 1)]\"]\n    end\n    \n    subgraph Reduce [\"Reduce (집계)\"]\n        R1[\"(Hello, 2)\"]\n        R2[\"(World, 1)\"]\n    end\n    \n    Input --> Map --> Shuffle --> Reduce\n```\n\n### 분산 환경에서의 MapReduce\n\n```mermaid\nflowchart TB\n    subgraph Data [\"분산된 데이터\"]\n        D1[\"Partition 1<br/>'Hello World'\"]\n        D2[\"Partition 2<br/>'Hello Spark'\"]\n        D3[\"Partition 3<br/>'World Spark'\"]\n    end\n    \n    subgraph MapPhase [\"Map Phase (병렬)\"]\n        M1[\"Worker 1<br/>(Hello,1) (World,1)\"]\n        M2[\"Worker 2<br/>(Hello,1) (Spark,1)\"]\n        M3[\"Worker 3<br/>(World,1) (Spark,1)\"]\n    end\n    \n    subgraph ShufflePhase [\"Shuffle Phase\"]\n        direction LR\n        SH[\"키 기준으로 재배치<br/>(네트워크 전송 발생)\"]\n    end\n    \n    subgraph ReducePhase [\"Reduce Phase (병렬)\"]\n        R1[\"Reducer 1<br/>Hello → 2\"]\n        R2[\"Reducer 2<br/>World → 2\"]\n        R3[\"Reducer 3<br/>Spark → 2\"]\n    end\n    \n    D1 --> M1\n    D2 --> M2\n    D3 --> M3\n    \n    M1 & M2 & M3 --> ShufflePhase --> R1 & R2 & R3\n```\n\n**핵심 인사이트**:\n\n- **Map**: 각 서버가 자기 파티션만 처리 (병렬, 빠름)\n- **Shuffle**: 키 기준으로 데이터 재배치 (네트워크 통신, 느림 ⚠️)\n- **Reduce**: 같은 키끼리 모여서 집계 (병렬)\n\n---\n\n## RDD (Resilient Distributed Dataset)\n\nSpark의 핵심 추상화입니다.\n\n### RDD란?\n\n```mermaid\nflowchart TB\n    subgraph RDD [\"RDD: Resilient Distributed Dataset\"]\n        R[\"불변(Immutable)\"]\n        D[\"분산(Distributed)\"]\n        F[\"장애 복구(Fault-tolerant)\"]\n    end\n    \n    subgraph Partitions [\"파티션으로 분산\"]\n        P1[\"Partition 1<br/>서버 A\"]\n        P2[\"Partition 2<br/>서버 B\"]\n        P3[\"Partition 3<br/>서버 C\"]\n    end\n    \n    RDD --> Partitions\n```\n\n**핵심 특성**:\n\n| 특성 | 의미 | 왜 중요한가? |\n|------|------|------------|\n| **Resilient** | 장애 복구 가능 | 노드가 죽어도 데이터 복구 |\n| **Distributed** | 클러스터에 분산 | 여러 서버에서 병렬 처리 |\n| **Immutable** | 변경 불가 | 연산 결과는 새 RDD 생성 |\n\n### Transformations vs Actions\n\nRDD 연산은 두 종류로 나뉩니다.\n\n```mermaid\nflowchart LR\n    subgraph Transformations [\"Transformations (변환)\"]\n        T1[\"map\"]\n        T2[\"filter\"]\n        T3[\"flatMap\"]\n        T4[\"groupBy\"]\n        T5[\"join\"]\n        \n        Lazy[\"⏸️ Lazy: 바로 실행 안 함\"]\n    end\n    \n    subgraph Actions [\"Actions (실행)\"]\n        A1[\"count\"]\n        A2[\"collect\"]\n        A3[\"save\"]\n        A4[\"reduce\"]\n        \n        Execute[\"▶️ 실행: 이때 계산 시작\"]\n    end\n    \n    RDD1[\"RDD\"] --> Transformations --> RDD2[\"새 RDD\"]\n    RDD2 --> Actions --> Result[\"결과\"]\n```\n\n### Lazy Evaluation의 힘\n\n```mermaid\nflowchart TB\n    subgraph Eager [\"Eager Evaluation (일반적인 방식)\"]\n        EE1[\"data = load()\"] -->|\"실행\"| EE2[\"filtered = filter()\"]\n        EE2 -->|\"실행\"| EE3[\"mapped = map()\"]\n        EE3 -->|\"실행\"| EE4[\"result = count()\"]\n    end\n    \n    subgraph Lazy [\"Lazy Evaluation (Spark)\"]\n        LE1[\"rdd = load()\"] -->|\"기록만\"| LE2[\"filtered = filter()\"]\n        LE2 -->|\"기록만\"| LE3[\"mapped = map()\"]\n        LE3 -->|\"기록만\"| LE4[\"count()\"]\n        LE4 -->|\"최적화 후 실행!\"| Result[\"결과\"]\n    end\n    \n    Lazy -->|\"장점\"| Optimize[\"✅ 실행 계획 최적화<br/>✅ 불필요한 연산 제거<br/>✅ 파이프라이닝\"]\n```\n\n**실제 예시**:\n\n```python\n# Spark는 이 시점에 아무것도 실행하지 않음\nrdd = spark.read.text(\"huge_file.txt\")  # 기록만\nfiltered = rdd.filter(lambda x: \"error\" in x)  # 기록만\nmapped = filtered.map(lambda x: (x, 1))  # 기록만\n\n# 이 시점에 최적화된 계획으로 한 번에 실행\ncount = mapped.count()  # 실행!\n```\n\n---\n\n## DataFrame: RDD의 진화\n\n### RDD의 한계\n\n```mermaid\nflowchart TB\n    subgraph Problem [\"RDD의 문제\"]\n        P1[\"타입 정보 없음<br/>(Python 객체)\"]\n        P2[\"최적화 어려움<br/>(블랙박스)\"]\n        P3[\"직렬화 오버헤드<br/>(Python ↔ JVM)\"]\n    end\n    \n    Problem --> Solution[\"DataFrame 등장\"]\n```\n\n### DataFrame이란?\n\n```mermaid\nflowchart TB\n    subgraph DataFrame [\"DataFrame\"]\n        direction TB\n        Schema[\"스키마 (컬럼명, 타입)\"]\n        Rows[\"Row 데이터\"]\n        Catalyst[\"Catalyst Optimizer\"]\n    end\n    \n    subgraph Analogy [\"익숙한 비유\"]\n        SQL[\"SQL 테이블\"]\n        Pandas[\"Pandas DataFrame\"]\n        Excel[\"엑셀 시트\"]\n    end\n    \n    DataFrame --> Analogy\n```\n\n**DataFrame vs RDD**:\n\n| 특성 | RDD | DataFrame |\n|------|-----|-----------|\n| **스키마** | 없음 (Python 객체) | 있음 (컬럼명, 타입) |\n| **최적화** | 수동 (개발자가) | 자동 (Catalyst) |\n| **API** | map, filter (함수형) | select, where (SQL형) |\n| **성능** | 느림 (직렬화) | 빠름 (최적화) |\n| **언어** | 언어별 차이 큼 | 언어별 차이 적음 |\n\n### 왜 DataFrame이 더 빠른가?\n\n```mermaid\nflowchart TB\n    subgraph RDDPath [\"RDD 경로\"]\n        R1[\"Python 함수\"] --> R2[\"직렬화<br/>(pickle)\"]\n        R2 --> R3[\"JVM 전송\"]\n        R3 --> R4[\"역직렬화\"]\n        R4 --> R5[\"실행\"]\n    end\n    \n    subgraph DFPath [\"DataFrame 경로\"]\n        D1[\"DataFrame API\"] --> D2[\"Catalyst<br/>최적화\"]\n        D2 --> D3[\"JVM 코드<br/>직접 실행\"]\n    end\n    \n    RDDPath -->|\"🐢\"| Slow[\"느림\"]\n    DFPath -->|\"🚀\"| Fast[\"빠름\"]\n```\n\n---\n\n## Spark Connect (4.0+)\n\nSpark 4.0의 새로운 아키텍처입니다.\n\n### 기존 방식 vs Spark Connect\n\n```mermaid\nflowchart TB\n    subgraph Before [\"기존 방식\"]\n        Client1[\"Python Driver\"] -->|\"같은 서버\"| Cluster1[\"Spark Cluster\"]\n    end\n    \n    subgraph After [\"Spark Connect\"]\n        Client2[\"Thin Client<br/>(어디서든)\"] -->|\"gRPC\"| Server[\"Spark Connect<br/>Server\"]\n        Server --> Cluster2[\"Spark Cluster\"]\n    end\n    \n    After -->|\"장점\"| Benefits[\"✅ 클라이언트 가벼움<br/>✅ 원격 연결 가능<br/>✅ 다양한 언어 지원\"]\n```\n\n```python\n# Spark Connect 사용 예\nfrom pyspark.sql import SparkSession\n\n# 원격 클러스터에 연결\nspark = SparkSession.builder \\\n    .remote(\"sc://spark-server:15002\") \\\n    .getOrCreate()\n\n# 나머지는 동일하게 사용\ndf = spark.range(1000000)\nresult = df.groupBy((df.id % 10).alias(\"group\")).count()\nresult.show()\n```\n\n---\n\n## 실전 코드: Word Count 비교\n\n### Python (ThreadPoolExecutor)\n\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nfrom collections import Counter\n\ndef count_words_in_chunk(text_chunk):\n    words = text_chunk.lower().split()\n    return Counter(words)\n\ndef word_count_threaded(text, num_workers=4):\n    # 텍스트를 청크로 분할\n    chunks = [text[i::num_workers] for i in range(num_workers)]\n    \n    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n        results = list(executor.map(count_words_in_chunk, chunks))\n    \n    # 결과 합치기\n    total = Counter()\n    for result in results:\n        total.update(result)\n    \n    return total\n\n# 한계: 메모리에 전체 텍스트가 올라와야 함\n```\n\n### Go (Goroutines)\n\n```go\nfunc wordCount(texts []string) map[string]int {\n    results := make(chan map[string]int, len(texts))\n    \n    // 각 청크를 goroutine으로 처리\n    for _, text := range texts {\n        go func(t string) {\n            counts := make(map[string]int)\n            for _, word := range strings.Fields(strings.ToLower(t)) {\n                counts[word]++\n            }\n            results <- counts\n        }(text)\n    }\n    \n    // 결과 합치기\n    total := make(map[string]int)\n    for i := 0; i < len(texts); i++ {\n        for word, count := range <-results {\n            total[word] += count\n        }\n    }\n    \n    return total\n}\n\n// 한계: 단일 서버 메모리 한계\n```\n\n### PySpark (분산 처리)\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode, split, lower, col\n\nspark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n\n# TB 단위 파일도 처리 가능\ndf = spark.read.text(\"hdfs://path/to/huge_files/*.txt\")\n\nword_counts = df \\\n    .select(explode(split(lower(col(\"value\")), \"\\\\s+\")).alias(\"word\")) \\\n    .groupBy(\"word\") \\\n    .count() \\\n    .orderBy(col(\"count\").desc())\n\nword_counts.show(20)\n\n# 장점: 자동으로 클러스터 전체에 분산 처리\n```\n\n---\n\n## 정리\n\n```mermaid\nmindmap\n  root((Spark<br/>핵심 개념))\n    왜 분산 처리?\n      단일 서버 한계\n      데이터 > 메모리\n      처리 시간 단축\n    vs 기존 병렬처리\n      Goroutine: 단일 서버\n      Spark: 여러 서버\n      스케일 차이\n    MapReduce\n      Map: 변환\n      Shuffle: 재배치\n      Reduce: 집계\n    RDD\n      Immutable\n      Distributed\n      Fault-tolerant\n    Lazy Evaluation\n      기록만 하다가\n      Action에서 실행\n      최적화 가능\n    DataFrame\n      스키마 있음\n      Catalyst 최적화\n      빠름\n```\n\n---\n\n## 다음 편 예고\n\n**4편: Spark 내부 동작 원리**에서는 더 깊이 들어갑니다:\n\n- Job → Stage → Task 계층\n- Shuffle이 느린 이유\n- 파티셔닝 전략\n- 메모리 관리와 Spill\n- Spark UI 읽는 법\n\n---\n\n## 참고 자료\n\n- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)\n- [Spark: The Definitive Guide](https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/) (O'Reilly)\n- [Spark Connect Overview](https://spark.apache.org/docs/latest/spark-connect-overview.html)\n- [Learning Spark, 2nd Edition](https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/)",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Data Engineering",
      "Distributed Systems",
      "Spark"
    ],
    "readingTime": 8,
    "wordCount": 1559,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-02-data-architecture-101",
    "slug": "de-02-data-architecture-101",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-02-data-architecture-101",
    "title": "데이터 엔지니어링 시리즈 #2: 데이터 아키텍처 101 - 전체 그림 이해하기",
    "excerpt": "데이터 파이프라인의 전체 구조를 조망합니다. ETL vs ELT, 배치 vs 스트리밍, Lambda vs Kappa 아키텍처, 그리고 Modern Data Stack까지.",
    "content": "# 데이터 엔지니어링 시리즈 #2: 데이터 아키텍처 101 - 전체 그림 이해하기\n\n> **대상 독자**: 충분한 경험을 가진 백엔드/풀스택 엔지니어로, 데이터 파이프라인 아키텍처를 처음 접하는 분\n\n## 이 편에서 다루는 것\n\n1편에서 **왜** 데이터 엔지니어링이 필요한지 알았다면, 이제 **어떻게** 구성되는지 전체 그림을 봅니다.\n\n---\n\n## 데이터 파이프라인의 5단계\n\n데이터는 생성부터 소비까지 5단계를 거칩니다.\n\n```mermaid\nflowchart LR\n    subgraph Stage1 [\"1️⃣ Sources\"]\n        S1[(PostgreSQL)]\n        S2[(MongoDB)]\n        S3[REST APIs]\n        S4[로그 파일]\n        S5[IoT 센서]\n    end\n    \n    subgraph Stage2 [\"2️⃣ Ingestion\"]\n        I1[Kafka]\n        I2[Debezium]\n        I3[Airbyte]\n        I4[Fivetran]\n    end\n    \n    subgraph Stage3 [\"3️⃣ Processing\"]\n        P1[Spark]\n        P2[Flink]\n        P3[dbt]\n    end\n    \n    subgraph Stage4 [\"4️⃣ Storage\"]\n        ST1[(Data Lake<br/>S3, GCS)]\n        ST2[(Data Warehouse<br/>BigQuery)]\n        ST3[(Lakehouse<br/>Delta Lake)]\n    end\n    \n    subgraph Stage5 [\"5️⃣ Serving\"]\n        SV1[Metabase]\n        SV2[Looker]\n        SV3[ML Models]\n        SV4[Reverse ETL]\n    end\n    \n    Stage1 --> Stage2 --> Stage3 --> Stage4 --> Stage5\n```\n\n### 각 단계별 역할\n\n| 단계 | 역할 | 백엔드 개발자에게 익숙한 비유 |\n|------|------|---------------------------|\n| **Sources** | 데이터 원천 | 운영 DB, 외부 API |\n| **Ingestion** | 데이터 수집/전송 | 메시지 큐, 웹훅 |\n| **Processing** | 변환/정제/집계 | 배치 작업, Worker |\n| **Storage** | 처리된 데이터 저장 | 캐시, Read Replica |\n| **Serving** | 최종 사용자에게 제공 | API Gateway, CDN |\n\n---\n\n## ETL vs ELT: 패러다임의 변화\n\n데이터를 옮기는 방식에는 두 가지 패러다임이 있습니다.\n\n### ETL (Extract → Transform → Load)\n\n**전통적 방식**: 데이터를 가져와서, 변환한 후, 저장합니다.\n\n```mermaid\nflowchart LR\n    subgraph Source [\"원천 시스템\"]\n        DB[(운영 DB)]\n    end\n    \n    subgraph ETL [\"ETL 서버\"]\n        E[\"Extract<br/>추출\"]\n        T[\"Transform<br/>변환\"]\n    end\n    \n    subgraph DW [\"Data Warehouse\"]\n        L[(저장)]\n    end\n    \n    DB -->|\"원본 데이터\"| E\n    E -->|\"원본\"| T\n    T -->|\"변환된 데이터\"| L\n    \n    style ETL fill:#ffcdd2\n```\n\n**특징**:\n\n- 변환 로직이 **ETL 서버**에 있음\n- Informatica, Talend 같은 전용 도구 사용\n- 저장 전에 데이터 품질 보장\n\n### ELT (Extract → Load → Transform)\n\n**현대적 방식**: 데이터를 먼저 저장하고, 저장소 내에서 변환합니다.\n\n```mermaid\nflowchart LR\n    subgraph Source [\"원천 시스템\"]\n        DB[(운영 DB)]\n    end\n    \n    subgraph Ingestion [\"수집 도구\"]\n        E[\"Extract\"]\n        L1[\"Load\"]\n    end\n    \n    subgraph DW [\"Data Warehouse\"]\n        Raw[(Raw Layer)]\n        T[\"Transform<br/>(dbt, SQL)\"]\n        Final[(Mart Layer)]\n    end\n    \n    DB -->|\"원본 데이터\"| E\n    E --> L1\n    L1 -->|\"원본 그대로\"| Raw\n    Raw --> T --> Final\n    \n    style DW fill:#c8e6c9\n```\n\n**특징**:\n\n- 변환 로직이 **웨어하우스 내부**에 있음 (SQL 기반)\n- dbt, BigQuery, Snowflake 활용\n- 원본 데이터 보존 (언제든 재변환 가능)\n\n### 언제 무엇을 선택할까?\n\n```mermaid\nflowchart TB\n    Q1{\"원본 데이터를<br/>보존해야 하나요?\"}\n    Q2{\"변환 로직이<br/>복잡한가요?\"}\n    Q3{\"웨어하우스가<br/>강력한가요?\"}\n    \n    Q1 -->|\"예\"| ELT[\"✅ ELT 권장\"]\n    Q1 -->|\"아니오\"| Q2\n    Q2 -->|\"매우 복잡\"| ETL[\"✅ ETL 권장\"]\n    Q2 -->|\"SQL로 가능\"| Q3\n    Q3 -->|\"BigQuery/Snowflake\"| ELT\n    Q3 -->|\"제한적\"| ETL\n```\n\n> **현대 트렌드**: 클라우드 웨어하우스(BigQuery, Snowflake)의 컴퓨팅 파워가 강력해지면서 **ELT가 표준**이 되어가고 있습니다.\n\n---\n\n## 배치 vs 스트리밍\n\n데이터 처리 주기에 따라 두 가지 방식이 있습니다.\n\n```mermaid\nflowchart TB\n    subgraph Batch [\"배치 처리 (Batch)\"]\n        B1[\"📊 대량의 데이터를\"]\n        B2[\"⏰ 정해진 시간에\"]\n        B3[\"🔄 한 번에 처리\"]\n        B1 --> B2 --> B3\n    end\n    \n    subgraph Stream [\"스트리밍 처리 (Streaming)\"]\n        S1[\"📨 데이터가 발생할 때\"]\n        S2[\"⚡ 실시간으로\"]\n        S3[\"🔁 지속적으로 처리\"]\n        S1 --> S2 --> S3\n    end\n    \n    Batch -->|\"예시\"| BE[\"일일 매출 리포트<br/>주간 코호트 분석\"]\n    Stream -->|\"예시\"| SE[\"실시간 이상 탐지<br/>라이브 대시보드\"]\n```\n\n### 비교\n\n| 특성 | 배치 | 스트리밍 |\n|------|------|---------|\n| **처리 주기** | 분/시간/일 단위 | 밀리초~초 단위 |\n| **지연 시간** | 높음 (분~시간) | 낮음 (초 이내) |\n| **구현 복잡도** | 상대적으로 단순 | 복잡 (상태 관리, 순서 보장) |\n| **리소스 사용** | 피크 타임에 집중 | 지속적으로 사용 |\n| **도구** | Spark, Airflow | Kafka, Flink, Spark Streaming |\n| **적합한 경우** | 리포팅, ML 학습 | 실시간 알림, 추천 |\n\n### 실제로는 둘 다 필요\n\n```mermaid\nflowchart LR\n    Source[(이벤트 발생)] --> Kafka\n    \n    subgraph Processing\n        Kafka --> Stream[\"실시간 처리<br/>(Flink)\"] --> RT[\"실시간 알림\"]\n        Kafka --> Lake[(Data Lake)]\n        Lake --> Batch[\"배치 처리<br/>(Spark)\"] --> Report[\"일일 리포트\"]\n    end\n```\n\n대부분의 시스템은 **배치와 스트리밍을 함께 사용**합니다. 이를 어떻게 조합할지가 바로 다음에 다룰 아키텍처 패턴입니다.\n\n---\n\n## Lambda Architecture vs Kappa Architecture\n\n### Lambda Architecture (람다 아키텍처)\n\n**배치 + 스트리밍 이중 파이프라인**을 운영하는 패턴입니다.\n\n```mermaid\nflowchart TB\n    Source[\"데이터 소스\"] --> Kafka[\"메시지 큐<br/>(Kafka)\"]\n    \n    subgraph Lambda [\"Lambda Architecture\"]\n        Kafka --> Speed[\"⚡ Speed Layer<br/>(실시간 처리)\"]\n        Kafka --> Batch[\"📊 Batch Layer<br/>(배치 처리)\"]\n        \n        Speed --> Serving[\"Serving Layer\"]\n        Batch --> Serving\n    end\n    \n    Serving --> Query[\"쿼리\"]\n    \n    subgraph Legend [\"역할\"]\n        L1[\"Speed: 최신 데이터 (근사치)\"]\n        L2[\"Batch: 전체 데이터 (정확한 값)\"]\n        L3[\"Serving: 두 결과를 합쳐서 제공\"]\n    end\n```\n\n**장점**:\n\n- 실시간성 + 정확성 모두 확보\n- 배치가 정확한 결과로 보정\n\n**단점**:\n\n- **동일한 로직을 두 번 구현** (배치 코드 + 스트리밍 코드)\n- 복잡한 운영\n- 결과 합치는 로직 필요\n\n### Kappa Architecture (카파 아키텍처)\n\n**스트리밍 단일 파이프라인**으로 모든 것을 처리하는 패턴입니다.\n\n```mermaid\nflowchart TB\n    Source[\"데이터 소스\"] --> Kafka[\"메시지 큐<br/>(Kafka)\"]\n    \n    subgraph Kappa [\"Kappa Architecture\"]\n        Kafka --> Stream[\"⚡ Stream Processing<br/>(모든 처리)\"]\n        Stream --> Serving[\"Serving Layer\"]\n    end\n    \n    Serving --> Query[\"쿼리\"]\n    \n    subgraph Reprocess [\"재처리가 필요하면?\"]\n        R1[\"Kafka에서 과거 데이터 다시 읽음\"]\n        R2[\"(Kafka의 log retention 활용)\"]\n    end\n```\n\n**장점**:\n\n- 단일 코드베이스\n- 단순한 아키텍처\n- 유지보수 용이\n\n**단점**:\n\n- 모든 것을 스트리밍으로 처리하기 어려울 수 있음\n- 과거 데이터 재처리 시 시간 소요\n\n### 현대적 관점에서의 선택\n\n```mermaid\nflowchart TB\n    Q1{\"스트리밍 처리만으로<br/>요구사항을 충족하나요?\"}\n    Q2{\"복잡한 ML/통계 분석이<br/>필요한가요?\"}\n    Q3{\"팀의 스트리밍<br/>경험이 충분한가요?\"}\n    \n    Q1 -->|\"예\"| Kappa[\"✅ Kappa\"]\n    Q1 -->|\"아니오\"| Q2\n    Q2 -->|\"예\"| Lambda[\"✅ Lambda\"]\n    Q2 -->|\"아니오\"| Q3\n    Q3 -->|\"예\"| Kappa\n    Q3 -->|\"아니오\"| Hybrid[\"🔄 배치 우선,<br/>점진적 스트리밍 도입\"]\n```\n\n> **현실적 조언**: 처음부터 Lambda를 구축하려 하지 마세요. **배치로 시작**하고, 정말 실시간이 필요한 부분만 스트리밍으로 확장하세요.\n\n---\n\n## Modern Data Stack\n\n최근 몇 년간 데이터 도구 생태계가 급격히 발전했습니다. 이를 **Modern Data Stack**이라 부릅니다.\n\n```mermaid\nflowchart TB\n    subgraph Sources [\"데이터 소스\"]\n        S1[(PostgreSQL)]\n        S2[SaaS APIs]\n        S3[이벤트 로그]\n    end\n    \n    subgraph Ingestion [\"수집 (Ingestion)\"]\n        direction TB\n        I1[\"Fivetran\"]\n        I2[\"Airbyte\"]\n        I3[\"Stitch\"]\n    end\n    \n    subgraph Storage [\"저장 & 처리\"]\n        direction TB\n        ST1[\"Snowflake\"]\n        ST2[\"BigQuery\"]\n        ST3[\"Databricks\"]\n    end\n    \n    subgraph Transform [\"변환\"]\n        T1[\"dbt\"]\n    end\n    \n    subgraph BI [\"분석 & 시각화\"]\n        direction TB\n        B1[\"Looker\"]\n        B2[\"Metabase\"]\n        B3[\"Superset\"]\n    end\n    \n    subgraph Orchestration [\"오케스트레이션\"]\n        O1[\"Airflow\"]\n        O2[\"Dagster\"]\n        O3[\"Prefect\"]\n    end\n    \n    subgraph Quality [\"품질 & 관측성\"]\n        Q1[\"Great Expectations\"]\n        Q2[\"Monte Carlo\"]\n        Q3[\"dbt tests\"]\n    end\n    \n    Sources --> Ingestion --> Storage\n    Storage --> Transform --> Storage\n    Storage --> BI\n    \n    Orchestration -.->|\"스케줄링\"| Ingestion\n    Orchestration -.->|\"스케줄링\"| Transform\n    Quality -.->|\"검증\"| Storage\n```\n\n### 핵심 도구들\n\n| 카테고리 | 도구 | 설명 |\n|---------|------|------|\n| **수집** | Airbyte, Fivetran | SaaS/DB에서 데이터 추출 |\n| **저장소** | Snowflake, BigQuery | 클라우드 네이티브 웨어하우스 |\n| **변환** | dbt | SQL 기반 변환, 테스트, 문서화 |\n| **오케스트레이션** | Airflow, Dagster | 워크플로우 스케줄링 |\n| **시각화** | Metabase, Looker | 대시보드, 리포팅 |\n| **품질** | Great Expectations | 데이터 테스트 자동화 |\n\n### 왜 \"Modern\"인가?\n\n1. **클라우드 네이티브**: 직접 인프라 관리 X, 사용량 기반 과금\n2. **분리된 컴퓨팅/스토리지**: 저장은 S3, 처리는 필요할 때만\n3. **SQL 중심**: 복잡한 코드 대신 SQL로 변환\n4. **Git 기반 워크플로우**: dbt는 코드처럼 버전 관리\n5. **API 우선**: 모든 도구가 API로 연결\n\n---\n\n## 백엔드 개발자로서 기억할 것\n\n### 1. 전체 파이프라인에서 내 시스템의 위치\n\n```mermaid\nflowchart LR\n    subgraph YourSystem [\"당신이 만든 시스템\"]\n        API[\"API 서버\"]\n        DB[(운영 DB)]\n        Events[\"이벤트 발행\"]\n    end\n    \n    subgraph DataTeam [\"데이터 팀이 가져가는 것\"]\n        CDC[\"DB 변경 캡처<br/>(Debezium)\"]\n        Kafka[\"이벤트 수집<br/>(Kafka)\"]\n        API2[\"API 호출\"]\n    end\n    \n    DB -->|\"CDC\"| CDC\n    Events --> Kafka\n    API -->|\"Pull\"| API2\n    \n    DataTeam --> Pipeline[\"데이터 파이프라인\"]\n```\n\n### 2. 데이터 추출을 고려한 설계\n\n**좋은 설계**:\n\n- 이벤트에 **타임스탬프** 포함\n- 레코드에 **created_at, updated_at** 컬럼\n- **Soft delete** 적용 (deleted_at)\n- 변경 이력 추적 가능한 설계\n\n**피해야 할 설계**:\n\n- Hard delete로 데이터 증발\n- 상태 변경 시 덮어쓰기\n- 타임스탬프 없는 레코드\n\n### 3. 협업 포인트\n\n| 상황 | 백엔드가 할 일 |\n|------|--------------|\n| CDC 도입 시 | 운영 DB 부하 모니터링 |\n| 이벤트 스키마 변경 | 데이터 팀과 사전 협의 |\n| API 변경 | 역호환성 유지 또는 버저닝 |\n| 장애 발생 | 데이터 파이프라인 영향도 확인 |\n\n---\n\n## 정리\n\n```mermaid\nmindmap\n  root((데이터<br/>아키텍처))\n    파이프라인 5단계\n      Sources\n      Ingestion\n      Processing\n      Storage\n      Serving\n    처리 패러다임\n      ETL\n        변환 후 적재\n        레거시 도구\n      ELT\n        적재 후 변환\n        현대적 방식\n    처리 주기\n      Batch\n        정해진 시간\n        대량 처리\n      Streaming\n        실시간\n        지속적 처리\n    아키텍처 패턴\n      Lambda\n        배치 + 스트리밍\n        이중 유지보수\n      Kappa\n        스트리밍 단일\n        단순함\n    Modern Data Stack\n      클라우드 네이티브\n      SQL 중심\n      API 연결\n```\n\n---\n\n## 다음 편 예고\n\n**3편: Spark 핵심 개념**에서는 분산 처리의 핵심을 다룹니다:\n\n- 왜 분산 처리가 필요한가?\n- RDD, DataFrame의 개념\n- Lazy Evaluation의 의미\n- Goroutine/ThreadPoolExecutor와 Spark의 차이\n\n---\n\n## 참고 자료\n\n- Databricks, \"The Data Lakehouse\"\n- Maxime Beauchemin, \"Functional Data Engineering\" (Medium)\n- Martin Kleppmann, \"Designing Data-Intensive Applications\" - Chapter 10, 11\n- [Kappa Architecture](https://www.oreilly.com/radar/questioning-the-lambda-architecture/)",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Data Architecture",
      "Data Engineering",
      "Data Pipeline"
    ],
    "readingTime": 7,
    "wordCount": 1341,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-01-why-data-engineering",
    "slug": "de-01-why-data-engineering",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-01-why-data-engineering",
    "title": "데이터 엔지니어링 시리즈 #1: 백엔드 개발자가 데이터 엔지니어링을 배워야 하는 이유",
    "excerpt": "왜 백엔드 개발자가 데이터 엔지니어링을 알아야 할까요? OLTP의 한계에서 시작하는 데이터 파이프라인의 세계로 안내합니다.",
    "content": "# 데이터 엔지니어링 시리즈 #1: 백엔드 개발자가 데이터 엔지니어링을 배워야 하는 이유\n\n> **대상 독자**: 6년 이상의 경험을 가진 백엔드/풀스택 엔지니어로, Spark, Airflow 등 데이터 엔지니어링 기술에 처음 접근하는 분\n\n## 시리즈 소개\n\n| # | 주제 | 익숙한 개념과의 연결 |\n|---|------|---------------------|\n| **1** | 왜 데이터 엔지니어링인가 | 슬로우 쿼리, DB 부하 |\n| 2 | 데이터 아키텍처 101 | 마이크로서비스 아키텍처 |\n| 3 | Spark 핵심 개념 | Goroutine, ThreadPoolExecutor |\n| 4 | Spark 내부 동작 | Task Queue, Worker Pool |\n| 5 | PySpark 실전 | ORM, Query Optimization |\n| 6 | Airflow 핵심 개념 | CI/CD Pipeline, cron |\n| 7 | Airflow 실전 | GitHub Actions, ArgoCD |\n| 8 | Kafka 핵심 | Redis Streams |\n| 9 | Spark Streaming | Event-Driven Architecture |\n| 10 | 레이크하우스 | PostgreSQL ACID |\n| 11 | 데이터 모델링 | ERD, 정규화 |\n| 12 | 데이터 품질 | 테스트 자동화, 모니터링 |\n\n---\n\n## 새벽 3시의 슬로우 쿼리\n\n어느 날 새벽, 당신에게 알림이 옵니다.\n\n> \"프로덕션 DB CPU 100%, 응답 시간 30초 초과\"\n\n원인을 찾아보니, 마케팅팀이 요청한 대시보드 쿼리였습니다:\n\n```sql\nSELECT \n    DATE_TRUNC('month', created_at) AS month,\n    COUNT(*) AS user_count,\n    AVG(lifetime_value) AS avg_ltv,\n    SUM(purchase_amount) AS total_revenue\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE created_at >= '2024-01-01'\nGROUP BY DATE_TRUNC('month', created_at)\nORDER BY month;\n```\n\n이 쿼리 하나가 프로덕션 서비스를 마비시켰습니다. **왜일까요?**\n\n---\n\n## OLTP의 한계: 설계 목적이 다르다\n\n우리가 익숙한 PostgreSQL, MySQL 같은 데이터베이스는 **OLTP(Online Transaction Processing)**에 최적화되어 있습니다.\n\n```mermaid\nflowchart LR\n    subgraph OLTP [\"OLTP (트랜잭션 처리)\"]\n        direction TB\n        A1[\"✅ 빠른 단건 조회<br/>SELECT * FROM users WHERE id = 123\"]\n        A2[\"✅ 빠른 삽입/수정<br/>INSERT, UPDATE\"]\n        A3[\"✅ ACID 보장\"]\n    end\n    \n    subgraph OLAP [\"OLAP (분석 처리)\"]\n        direction TB\n        B1[\"✅ 대량 데이터 집계<br/>GROUP BY, SUM, AVG\"]\n        B2[\"✅ 복잡한 조인\"]\n        B3[\"✅ 긴 쿼리 허용\"]\n    end\n    \n    OLTP -.->|\"이걸로 분석하면?\"| Problem[\"❌ 슬로우 쿼리<br/>❌ 락 경쟁<br/>❌ 서비스 장애\"]\n```\n\n### OLTP vs OLAP: 근본적 차이\n\n| 특성 | OLTP | OLAP |\n|------|------|------|\n| **목적** | 트랜잭션 처리 | 분석/리포팅 |\n| **쿼리 패턴** | 단건 조회/수정 | 대량 집계 |\n| **데이터 저장** | Row-based | Column-based |\n| **인덱스** | B-Tree (특정 행 찾기) | 컬럼 스캔에 최적화 |\n| **동시성** | 높은 동시성, 짧은 트랜잭션 | 낮은 동시성, 긴 쿼리 |\n| **예시 DB** | PostgreSQL, MySQL | BigQuery, Snowflake |\n\n### Row-based vs Column-based 저장\n\n```mermaid\nflowchart TB\n    subgraph RowBased [\"Row-Based Storage (PostgreSQL)\"]\n        direction TB\n        R1[\"Row 1: id=1, name='Kim', age=30, city='Seoul'\"]\n        R2[\"Row 2: id=2, name='Lee', age=25, city='Busan'\"]\n        R3[\"Row 3: id=3, name='Park', age=35, city='Seoul'\"]\n    end\n    \n    subgraph ColBased [\"Column-Based Storage (BigQuery)\"]\n        direction TB\n        C1[\"id: [1, 2, 3]\"]\n        C2[\"name: ['Kim', 'Lee', 'Park']\"]\n        C3[\"age: [30, 25, 35]\"]\n        C4[\"city: ['Seoul', 'Busan', 'Seoul']\"]\n    end\n    \n    Query[\"SELECT AVG(age) FROM users\"]\n    \n    RowBased -->|\"전체 행 스캔 필요\"| Slow[\"🐢 느림\"]\n    ColBased -->|\"age 컬럼만 읽음\"| Fast[\"🚀 빠름\"]\n```\n\n**핵심 인사이트**: OLTP 데이터베이스로 분석 쿼리를 돌리는 것은, 스포츠카로 이사짐을 나르는 것과 같습니다. 가능은 하지만, 적합한 도구가 아닙니다.\n\n---\n\n## 데이터 파이프라인의 등장\n\n그렇다면 어떻게 해야 할까요? **데이터를 분석에 적합한 형태로 복사해 두는 것**입니다.\n\n```mermaid\nflowchart LR\n    subgraph Source [\"운영 시스템\"]\n        DB1[(PostgreSQL)]\n        DB2[(MongoDB)]\n        API[REST APIs]\n    end\n    \n    subgraph Pipeline [\"데이터 파이프라인\"]\n        E[Extract<br/>추출]\n        T[Transform<br/>변환]\n        L[Load<br/>적재]\n    end\n    \n    subgraph Dest [\"분석 시스템\"]\n        DW[(Data Warehouse<br/>BigQuery, Snowflake)]\n        BI[대시보드<br/>Metabase, Looker]\n    end\n    \n    DB1 --> E\n    DB2 --> E\n    API --> E\n    E --> T --> L --> DW --> BI\n    \n    style Pipeline fill:#e1f5fe\n```\n\n### 왜 그냥 \"복사\"가 아닌가?\n\n단순히 `pg_dump`로 복사하면 안 될까요? 실제로는 이런 문제들이 있습니다:\n\n1. **스키마 불일치**: 운영 DB 스키마가 분석에 적합하지 않음\n2. **데이터 정제**: NULL 처리, 타입 변환, 중복 제거 필요\n3. **증분 처리**: 매번 전체를 복사하면 비효율적\n4. **의존성 관리**: 테이블 A → B → C 순서대로 처리해야 함\n5. **실패 복구**: 중간에 실패하면 어디서부터 다시?\n6. **모니터링**: 파이프라인이 제대로 동작하는지 확인\n\n이 모든 것을 해결하는 것이 바로 **데이터 엔지니어링**입니다.\n\n---\n\n## 백엔드 개발자에게 익숙한 개념과의 연결\n\n데이터 엔지니어링의 개념들은 백엔드 개발에서 이미 접해본 것들의 확장입니다.\n\n### 병렬 처리: Goroutine / ThreadPoolExecutor → Spark\n\n백엔드에서 성능을 위해 병렬 처리를 하는 것처럼, 데이터 처리도 병렬화합니다.\n\n```mermaid\nflowchart TB\n    subgraph Go [\"Go: Goroutine\"]\n        G1[\"goroutine 1\"]\n        G2[\"goroutine 2\"]\n        G3[\"goroutine 3\"]\n        G4[\"goroutine N\"]\n    end\n    \n    subgraph Python [\"Python: ThreadPoolExecutor\"]\n        P1[\"Thread 1\"]\n        P2[\"Thread 2\"]\n        P3[\"Thread 3\"]\n        P4[\"Thread N\"]\n    end\n    \n    subgraph Spark [\"Spark: Distributed Processing\"]\n        S1[\"Executor 1<br/>(다른 서버)\"]\n        S2[\"Executor 2<br/>(다른 서버)\"]\n        S3[\"Executor 3<br/>(다른 서버)\"]\n        S4[\"Executor N<br/>(다른 서버)\"]\n    end\n    \n    SingleServer[\"단일 서버 내 병렬화\"]\n    MultiServer[\"여러 서버로 분산\"]\n    \n    Go --> SingleServer\n    Python --> SingleServer\n    SingleServer -.->|\"데이터가 너무 크면?\"| MultiServer\n    MultiServer --> Spark\n```\n\n**핵심 차이**:\n\n- Goroutine/ThreadPoolExecutor: **단일 서버** 내에서 CPU 코어를 활용\n- Spark: **여러 서버**에 걸쳐 데이터와 연산을 분산\n\n### 스케줄링: cron / GitHub Actions → Airflow\n\n정기적인 작업 실행을 관리하는 것도 비슷합니다.\n\n```mermaid\nflowchart TB\n    subgraph Cron [\"cron / GitHub Actions\"]\n        C1[\"Task A (00:00)\"]\n        C2[\"Task B (01:00)\"]\n        C3[\"Task C (02:00)\"]\n        C1 -.->|\"의존성 관리?\"| Problem1[\"❌ 수동 관리\"]\n        C2 -.->|\"실패 시?\"| Problem2[\"❌ 수동 재실행\"]\n    end\n    \n    subgraph Airflow [\"Apache Airflow\"]\n        direction LR\n        A1[\"Task A\"] --> A2[\"Task B\"] --> A3[\"Task C\"]\n        A1 --> A4[\"Task D\"] --> A3\n    end\n    \n    Airflow -->|\"의존성\"| Dep[\"✅ DAG로 자동 관리\"]\n    Airflow -->|\"실패 시\"| Retry[\"✅ 자동 재시도\"]\n    Airflow -->|\"모니터링\"| UI[\"✅ 웹 UI 제공\"]\n```\n\n### 메시지 스트리밍: Redis Streams → Kafka\n\nRedis Streams를 사용해 봤다면, Kafka의 개념이 익숙할 것입니다.\n\n```mermaid\nflowchart LR\n    subgraph Redis [\"Redis Streams\"]\n        RS[\"Stream: orders\"]\n        RC1[\"Consumer Group A\"]\n        RC2[\"Consumer Group B\"]\n        RS --> RC1\n        RS --> RC2\n    end\n    \n    subgraph Kafka [\"Apache Kafka\"]\n        KT[\"Topic: orders<br/>(Partitioned)\"]\n        KC1[\"Consumer Group A\"]\n        KC2[\"Consumer Group B\"]\n        KT --> KC1\n        KT --> KC2\n    end\n    \n    Redis -.->|\"데이터 규모가 커지면\"| Kafka\n```\n\n| 특성 | Redis Streams | Kafka |\n|------|---------------|-------|\n| **설계 목적** | 캐시 + 가벼운 스트리밍 | 대용량 이벤트 스트리밍 |\n| **데이터 보존** | 메모리 기반 (제한적) | 디스크 기반 (무제한) |\n| **확장성** | 수직 확장 위주 | 수평 확장 (파티셔닝) |\n| **처리량** | 수만 TPS | 수백만 TPS |\n| **복제** | Master-Replica | Multi-broker 복제 |\n\n---\n\n## 데이터 엔지니어 vs 백엔드 개발자\n\n```mermaid\nflowchart TB\n    subgraph Backend [\"백엔드 개발자 영역\"]\n        BE1[\"API 서버\"]\n        BE2[\"비즈니스 로직\"]\n        BE3[\"운영 DB\"]\n        BE4[\"캐시\"]\n    end\n    \n    subgraph DE [\"데이터 엔지니어 영역\"]\n        DE1[\"데이터 파이프라인\"]\n        DE2[\"분석 플랫폼\"]\n        DE3[\"ML 인프라\"]\n        DE4[\"데이터 품질\"]\n    end\n    \n    subgraph Overlap [\"겹치는 영역\"]\n        O1[\"이벤트 스트리밍\"]\n        O2[\"로그 수집\"]\n        O3[\"메트릭/모니터링\"]\n    end\n    \n    Backend --- Overlap --- DE\n    \n    style Overlap fill:#fff3e0\n```\n\n### 왜 백엔드 개발자도 알아야 하는가?\n\n1. **협업**: 데이터 팀과 효과적으로 소통하려면 그들의 언어를 알아야 합니다\n2. **설계**: 데이터 추출이 용이한 API와 이벤트 설계를 할 수 있습니다\n3. **문제 해결**: \"왜 대시보드 숫자가 다르죠?\" 같은 질문에 함께 답할 수 있습니다\n4. **커리어**: Full-stack Data Engineer의 가치가 높아지고 있습니다\n\n---\n\n## 이 시리즈에서 배울 것들\n\n```mermaid\nflowchart TB\n    subgraph Part1 [\"Part 1: 개념\"]\n        P1[\"#1 왜 데이터 엔지니어링?\"]\n        P2[\"#2 데이터 아키텍처 전체 그림\"]\n    end\n    \n    subgraph Part2 [\"Part 2: Spark\"]\n        P3[\"#3 Spark 핵심 개념\"]\n        P4[\"#4 내부 동작 원리\"]\n        P5[\"#5 PySpark 실전\"]\n    end\n    \n    subgraph Part3 [\"Part 3: Airflow\"]\n        P6[\"#6 Airflow 핵심 개념\"]\n        P7[\"#7 프로덕션 파이프라인\"]\n    end\n    \n    subgraph Part4 [\"Part 4: 스트리밍\"]\n        P8[\"#8 Kafka 핵심\"]\n        P9[\"#9 Spark Streaming\"]\n    end\n    \n    subgraph Part5 [\"Part 5: 저장소\"]\n        P10[\"#10 레이크하우스\"]\n        P11[\"#11 데이터 모델링\"]\n    end\n    \n    subgraph Part6 [\"Part 6: 운영\"]\n        P12[\"#12 데이터 품질\"]\n    end\n    \n    Part1 --> Part2 --> Part3 --> Part4 --> Part5 --> Part6\n```\n\n각 편에서는:\n\n- **\"왜?\"**에서 시작합니다 - 기술이 해결하는 문제\n- **시각화**로 구조를 보여줍니다 - Mermaid 다이어그램\n- **익숙한 개념과 연결**합니다 - Go, Python 경험 활용\n- **실전 예제**로 마무리합니다 - 바로 적용 가능한 코드\n\n---\n\n## 정리\n\n| 개념 | 설명 |\n|------|------|\n| **OLTP** | 트랜잭션 처리에 최적화 (PostgreSQL, MySQL) |\n| **OLAP** | 분석 처리에 최적화 (BigQuery, Snowflake) |\n| **데이터 파이프라인** | 데이터를 추출 → 변환 → 적재하는 자동화된 흐름 |\n| **ETL** | Extract, Transform, Load |\n| **Row vs Column Storage** | 행 기반(단건 조회) vs 열 기반(집계 분석) |\n\n---\n\n## 다음 편 예고\n\n**2편: 데이터 아키텍처 101**에서는 데이터 파이프라인의 전체 구조를 다룹니다:\n\n- ETL vs ELT 패러다임\n- 배치 vs 스트리밍\n- Lambda vs Kappa 아키텍처\n- Modern Data Stack 소개\n\n---\n\n## 참고 자료\n\n- Martin Kleppmann, \"Designing Data-Intensive Applications\" (O'Reilly)\n- Maxime Beauchemin, \"The Rise of the Data Engineer\" (Airbnb Engineering Blog)\n- [OLTP vs OLAP: What's the Difference?](https://www.ibm.com/topics/oltp)",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Backend",
      "Data Engineering",
      "Data Pipeline",
      "OLAP",
      "OLTP"
    ],
    "readingTime": 7,
    "wordCount": 1316,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "interface-design-principles",
    "slug": "interface-design-principles",
    "path": "backend/go",
    "fullPath": "backend/go/interface-design-principles",
    "title": "Go 인터페이스 설계 원칙 - Accept Interfaces, Return Structs",
    "excerpt": "'Accept interfaces, return structs' 원칙과 포인터 vs 값 수신자 선택 기준을 알아봅니다.",
    "content": "# Go 인터페이스 설계 원칙\n\n## 개요\n\nGo의 **\"Accept interfaces, return structs\"** 원칙은 유연하고 테스트 가능한 코드를 작성하는 핵심 가이드라인입니다. 이 글에서는 이 원칙의 철학과 실전 적용법을 다룹니다.\n\n## Accept Interfaces, Return Structs\n\n### 원칙 설명\n\n```go\n// ✅ 좋은 예: 인터페이스를 받고, 구체 타입을 반환\nfunc NewUserService(repo UserRepository) *UserService {\n    return &UserService{repo: repo}\n}\n\n// ❌ 나쁜 예: 구체 타입을 받음\nfunc NewUserService(repo *PostgresUserRepo) *UserService {\n    return &UserService{repo: repo}\n}\n```\n\n### 왜 이렇게 해야 할까?\n\n| 관점 | 인터페이스 수용 | 구체 타입 반환 |\n|-----|---------------|--------------|\n| **유연성** | 어떤 구현체든 주입 가능 | 호출자가 구체 메서드 접근 가능 |\n| **테스트** | Mock 쉽게 주입 | 타입 단언 없이 사용 |\n| **결합도** | 낮음 (구현에 독립적) | API 명확성 |\n\n### 실전 예시\n\n```go\n// 인터페이스 정의 (소비자 측에서 정의)\ntype UserRepository interface {\n    FindByID(ctx context.Context, id string) (*User, error)\n    Save(ctx context.Context, user *User) error\n}\n\n// 구체 구현\ntype PostgresUserRepo struct {\n    db *sql.DB\n}\n\nfunc NewPostgresUserRepo(db *sql.DB) *PostgresUserRepo {\n    return &PostgresUserRepo{db: db}\n}\n\nfunc (r *PostgresUserRepo) FindByID(ctx context.Context, id string) (*User, error) {\n    // 구현\n}\n\nfunc (r *PostgresUserRepo) Save(ctx context.Context, user *User) error {\n    // 구현\n}\n\n// 추가 메서드 (인터페이스에 없음)\nfunc (r *PostgresUserRepo) BulkInsert(ctx context.Context, users []*User) error {\n    // PostgreSQL 전용 최적화\n}\n\n// 서비스 - 인터페이스를 받음\ntype UserService struct {\n    repo UserRepository\n}\n\nfunc NewUserService(repo UserRepository) *UserService {\n    return &UserService{repo: repo}\n}\n```\n\n## 인터페이스 정의 위치\n\n### 소비자 측에서 정의 (권장)\n\n[Go Wiki](https://go.dev/wiki/CodeReviewComments#interfaces)에서는 다음과 같이 권장합니다:\n\n> \"Go interfaces generally belong in the package that **uses** values of the interface type, not the package that implements those values.\"\n\n```go\n// ❌ 구현자가 인터페이스 정의 (Java 스타일)\n// repository/interfaces.go\ntype UserRepository interface { ... }\n\n// repository/postgres.go\ntype PostgresUserRepo struct { ... }\n\n// ✅ 소비자가 인터페이스 정의 (Go 스타일)\n// service/user.go\ntype UserRepository interface {\n    FindByID(ctx context.Context, id string) (*User, error)\n}\n\ntype UserService struct {\n    repo UserRepository\n}\n```\n\n> [!IMPORTANT]\n> Go의 암묵적 인터페이스 구현 덕분에, 소비자가 필요한 메서드만 정의할 수 있습니다.\n\n### 작은 인터페이스 선호\n\nRob Pike의 [Go Proverbs](https://go-proverbs.github.io/)에서:\n\n> \"The bigger the interface, the weaker the abstraction.\"\n\n```go\n// ❌ 너무 큰 인터페이스\ntype UserRepository interface {\n    FindByID(id string) (*User, error)\n    FindByEmail(email string) (*User, error)\n    FindAll() ([]*User, error)\n    Save(user *User) error\n    Delete(id string) error\n    UpdateProfile(id string, profile *Profile) error\n    // ... 10개 더\n}\n\n// ✅ 작은 인터페이스 (인터페이스 분리 원칙)\ntype UserFinder interface {\n    FindByID(id string) (*User, error)\n}\n\ntype UserSaver interface {\n    Save(user *User) error\n}\n\n// 필요하면 조합\ntype UserRepository interface {\n    UserFinder\n    UserSaver\n}\n```\n\n## 포인터 vs 값 수신자\n\n### 기본 가이드라인\n\n| 상황 | 선택 | 이유 |\n|-----|-----|-----|\n| 상태 변경 필요 | `*T` 포인터 | 원본 수정 가능 |\n| 큰 구조체 | `*T` 포인터 | 복사 비용 절감 |\n| 작은 불변 값 | `T` 값 | 안전하고 간단 |\n| 일관성 유지 | 하나로 통일 | 혼란 방지 |\n\n### 예시\n\n```go\n// 값 수신자 - 작고 불변\ntype Point struct {\n    X, Y int\n}\n\nfunc (p Point) Distance() float64 {\n    return math.Sqrt(float64(p.X*p.X + p.Y*p.Y))\n}\n\n// 포인터 수신자 - 상태 변경\ntype Counter struct {\n    value int\n}\n\nfunc (c *Counter) Increment() {\n    c.value++\n}\n\nfunc (c *Counter) Value() int {\n    return c.value\n}\n```\n\n### 인터페이스와 포인터/값\n\n```go\ntype Stringer interface {\n    String() string\n}\n\ntype MyType struct {\n    Name string\n}\n\n// 값 수신자로 정의\nfunc (m MyType) String() string {\n    return m.Name\n}\n\nvar s Stringer\n\ns = MyType{Name: \"hello\"}  // ✅ 값 할당 가능\ns = &MyType{Name: \"world\"} // ✅ 포인터도 할당 가능\n```\n\n```go\n// 포인터 수신자로 정의\nfunc (m *MyType) String() string {\n    return m.Name\n}\n\ns = MyType{Name: \"hello\"}  // ❌ 컴파일 에러!\ns = &MyType{Name: \"world\"} // ✅ 포인터만 할당 가능\n```\n\n> [!WARNING]\n> 포인터 수신자로 인터페이스를 구현하면, 값은 해당 인터페이스를 만족하지 않습니다.\n\n### 일관성 규칙\n\n```go\n// ❌ 혼합 사용 - 혼란스러움\nfunc (u User) Name() string { ... }\nfunc (u *User) SetName(name string) { ... }\nfunc (u User) Age() int { ... }\nfunc (u *User) SetAge(age int) { ... }\n\n// ✅ 통일 - 명확함\nfunc (u *User) Name() string { ... }\nfunc (u *User) SetName(name string) { ... }\nfunc (u *User) Age() int { ... }\nfunc (u *User) SetAge(age int) { ... }\n```\n\n## 인터페이스 반환이 적합한 경우\n\n일반적으로 구체 타입을 반환하지만, 예외 상황도 있습니다.\n\n### 1. 표준 라이브러리 인터페이스\n\n```go\n// io.Reader, io.Writer 등 표준 인터페이스\nfunc NewReader(data []byte) io.Reader {\n    return bytes.NewReader(data)\n}\n```\n\n### 2. 팩토리 패턴\n\n```go\ntype Database interface {\n    Query(query string) ([]Row, error)\n    Close() error\n}\n\n// 설정에 따라 다른 구현 반환\nfunc NewDatabase(config Config) (Database, error) {\n    switch config.Driver {\n    case \"postgres\":\n        return newPostgresDB(config)\n    case \"mysql\":\n        return newMySQLDB(config)\n    default:\n        return nil, errors.New(\"unknown driver\")\n    }\n}\n```\n\n### 3. 내부 구현 숨기기\n\n```go\n// unexported 구현\ntype client struct {\n    httpClient *http.Client\n}\n\n// exported 인터페이스\ntype Client interface {\n    Get(url string) (*Response, error)\n}\n\nfunc NewClient() Client {\n    return &client{httpClient: &http.Client{}}\n}\n```\n\n## 테스트를 위한 인터페이스\n\n### Mock 구현\n\n```go\n// 테스트용 Mock\ntype MockUserRepository struct {\n    FindByIDFunc func(ctx context.Context, id string) (*User, error)\n    SaveFunc     func(ctx context.Context, user *User) error\n}\n\nfunc (m *MockUserRepository) FindByID(ctx context.Context, id string) (*User, error) {\n    return m.FindByIDFunc(ctx, id)\n}\n\nfunc (m *MockUserRepository) Save(ctx context.Context, user *User) error {\n    return m.SaveFunc(ctx, user)\n}\n\n// 테스트\nfunc TestUserService_GetUser(t *testing.T) {\n    mockRepo := &MockUserRepository{\n        FindByIDFunc: func(ctx context.Context, id string) (*User, error) {\n            return &User{ID: id, Name: \"Test User\"}, nil\n        },\n    }\n    \n    service := NewUserService(mockRepo)\n    \n    user, err := service.GetUser(context.Background(), \"123\")\n    assert.NoError(t, err)\n    assert.Equal(t, \"Test User\", user.Name)\n}\n```\n\n### gomock 사용 (권장)\n\n[go.uber.org/mock](https://github.com/uber-go/mock)을 활용한 코드 생성 방식:\n\n```bash\n# mockgen 설치\ngo install go.uber.org/mock/mockgen@latest\n\n# 인터페이스에서 Mock 생성\nmockgen -source=repository.go -destination=mocks/repository_mock.go -package=mocks\n```\n\n```go\n// 생성된 mocks/repository_mock.go 사용\nimport (\n    \"testing\"\n    \"go.uber.org/mock/gomock\"\n    \"myapp/mocks\"\n)\n\nfunc TestUserService_GetUser(t *testing.T) {\n    ctrl := gomock.NewController(t)\n    defer ctrl.Finish()\n    \n    mockRepo := mocks.NewMockUserRepository(ctrl)\n    \n    // 기대 동작 설정\n    mockRepo.EXPECT().\n        FindByID(gomock.Any(), \"123\").\n        Return(&User{ID: \"123\", Name: \"Test User\"}, nil)\n    \n    service := NewUserService(mockRepo)\n    user, err := service.GetUser(context.Background(), \"123\")\n    \n    assert.NoError(t, err)\n    assert.Equal(t, \"Test User\", user.Name)\n}\n```\n\n> [!TIP]\n> gomock은 컴파일 타임에 타입 안전성을 보장하며, IDE 자동완성도 지원됩니다.\n\n## 체크리스트\n\n### 인터페이스 설계\n\n- [ ] 소비자 측에서 인터페이스 정의\n- [ ] 가능한 작은 인터페이스 (1-3 메서드)\n- [ ] 함수 파라미터로 인터페이스 수용\n- [ ] 구체 타입 반환 (특별한 이유 없으면)\n\n### 수신자 선택\n\n- [ ] 상태 변경 필요하면 포인터 수신자\n- [ ] struct 크기 > 64 bytes면 포인터 수신자\n- [ ] 동일 타입에서 수신자 통일\n- [ ] 인터페이스 구현 시 포인터/값 주의\n\n## 참고 자료\n\n- [Go Wiki: CodeReviewComments](https://go.dev/wiki/CodeReviewComments#interfaces)\n- [Effective Go: Interfaces](https://go.dev/doc/effective_go#interfaces)\n- [Go Proverbs: The bigger the interface, the weaker the abstraction](https://go-proverbs.github.io/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Design Patterns",
      "Go",
      "Interface"
    ],
    "readingTime": 6,
    "wordCount": 1120,
    "isFeatured": false,
    "isPublic": true,
    "date": "2026-01-02"
  },
  {
    "id": "error-handling-strategy",
    "slug": "error-handling-strategy",
    "path": "backend/go",
    "fullPath": "backend/go/error-handling-strategy",
    "title": "Go 에러 핸들링 전략 완벽 가이드",
    "excerpt": "errors.Is, errors.As, 커스텀 에러 타입, 스택 트레이스를 활용한 Go의 효과적인 에러 핸들링 전략을 알아봅니다.",
    "content": "# Go 에러 핸들링 전략 완벽 가이드\n\n## 개요\n\nGo의 에러 처리는 명시적이고 값 기반입니다. Go 1.13부터 도입된 에러 래핑(wrapping)과 `errors.Is`, `errors.As`를 활용하면 체계적인 에러 핸들링이 가능합니다.\n\n## 에러 래핑 (Error Wrapping)\n\n### fmt.Errorf와 %w\n\n```go\nimport (\n    \"errors\"\n    \"fmt\"\n)\n\nfunc readConfig(path string) error {\n    data, err := os.ReadFile(path)\n    if err != nil {\n        // %w로 원본 에러 래핑 (체인 유지)\n        return fmt.Errorf(\"config 파일 읽기 실패 [%s]: %w\", path, err)\n    }\n    return nil\n}\n```\n\n> [!IMPORTANT]\n> `%v` 대신 `%w`를 사용해야 원본 에러 체인이 유지됩니다. `%v`는 문자열로만 변환됩니다.\n\n### 에러 체인 언래핑\n\n```go\nerr := readConfig(\"config.yaml\")\nif err != nil {\n    // 가장 바깥 에러 메시지\n    fmt.Println(err)\n    // → config 파일 읽기 실패 [config.yaml]: open config.yaml: no such file or directory\n    \n    // 원본 에러 추출\n    unwrapped := errors.Unwrap(err)\n    fmt.Println(unwrapped)\n    // → open config.yaml: no such file or directory\n}\n```\n\n## errors.Is - 에러 동등성 검사\n\n`errors.Is`는 에러 체인 전체를 순회하며 **특정 에러와 동등한지** 검사합니다.\n\n### 센티넬 에러 비교\n\n```go\nimport (\n    \"errors\"\n    \"io\"\n    \"os\"\n)\n\nvar ErrNotFound = errors.New(\"리소스를 찾을 수 없습니다\")\n\nfunc findUser(id string) (*User, error) {\n    user, err := db.Find(id)\n    if err != nil {\n        if errors.Is(err, sql.ErrNoRows) {\n            return nil, ErrNotFound  // 도메인 에러로 변환\n        }\n        return nil, fmt.Errorf(\"DB 조회 실패: %w\", err)\n    }\n    return user, nil\n}\n\n// 호출측\nuser, err := findUser(\"123\")\nif errors.Is(err, ErrNotFound) {\n    // 404 응답\n}\n```\n\n### 체인 내 에러 검사\n\n```go\nfunc processFile(path string) error {\n    data, err := os.ReadFile(path)\n    if err != nil {\n        return fmt.Errorf(\"파일 처리 실패: %w\", err)\n    }\n    // ...\n}\n\nerr := processFile(\"data.txt\")\nif errors.Is(err, os.ErrNotExist) {\n    fmt.Println(\"파일이 존재하지 않습니다\")\n}\nif errors.Is(err, os.ErrPermission) {\n    fmt.Println(\"권한이 없습니다\")\n}\n```\n\n### Is 메서드 커스터마이징\n\n```go\ntype TemporaryError struct {\n    Msg string\n}\n\nfunc (e *TemporaryError) Error() string { return e.Msg }\n\n// Is 메서드 구현으로 커스텀 비교 로직\nfunc (e *TemporaryError) Is(target error) bool {\n    _, ok := target.(*TemporaryError)\n    return ok\n}\n\n// 사용\nvar errTemp = &TemporaryError{Msg: \"일시적 오류\"}\nerr := fmt.Errorf(\"작업 실패: %w\", errTemp)\n\nif errors.Is(err, &TemporaryError{}) {\n    // 재시도 로직\n}\n```\n\n## errors.As - 에러 타입 추출\n\n`errors.As`는 에러 체인에서 **특정 타입의 에러를 추출**합니다.\n\n### 기본 사용법\n\n```go\ntype ValidationError struct {\n    Field   string\n    Message string\n}\n\nfunc (e *ValidationError) Error() string {\n    return fmt.Sprintf(\"유효성 검사 실패 [%s]: %s\", e.Field, e.Message)\n}\n\nfunc validateUser(u User) error {\n    if u.Email == \"\" {\n        return &ValidationError{Field: \"email\", Message: \"필수 입력값입니다\"}\n    }\n    return nil\n}\n\n// 에러 타입 추출\nerr := validateUser(User{})\nvar validErr *ValidationError\nif errors.As(err, &validErr) {\n    fmt.Printf(\"필드: %s, 메시지: %s\\n\", validErr.Field, validErr.Message)\n}\n```\n\n### 래핑된 에러에서 추출\n\n```go\nfunc createUser(u User) error {\n    if err := validateUser(u); err != nil {\n        return fmt.Errorf(\"사용자 생성 실패: %w\", err)\n    }\n    // ...\n}\n\nerr := createUser(User{})\nvar validErr *ValidationError\nif errors.As(err, &validErr) {\n    // 래핑되어 있어도 추출 가능!\n    log.Printf(\"검증 실패 필드: %s\", validErr.Field)\n}\n```\n\n## 커스텀 에러 타입\n\n### 도메인 에러 정의\n\n```go\npackage domain\n\ntype ErrorCode string\n\nconst (\n    ErrCodeNotFound     ErrorCode = \"NOT_FOUND\"\n    ErrCodeUnauthorized ErrorCode = \"UNAUTHORIZED\"\n    ErrCodeConflict     ErrorCode = \"CONFLICT\"\n)\n\ntype DomainError struct {\n    Code    ErrorCode\n    Message string\n    Cause   error  // 원인 에러\n}\n\nfunc (e *DomainError) Error() string {\n    if e.Cause != nil {\n        return fmt.Sprintf(\"[%s] %s: %v\", e.Code, e.Message, e.Cause)\n    }\n    return fmt.Sprintf(\"[%s] %s\", e.Code, e.Message)\n}\n\n// Unwrap으로 체인 지원\nfunc (e *DomainError) Unwrap() error {\n    return e.Cause\n}\n\n// 생성자 함수들\nfunc NewNotFoundError(resource string, cause error) *DomainError {\n    return &DomainError{\n        Code:    ErrCodeNotFound,\n        Message: fmt.Sprintf(\"%s를 찾을 수 없습니다\", resource),\n        Cause:   cause,\n    }\n}\n\nfunc NewConflictError(msg string) *DomainError {\n    return &DomainError{\n        Code:    ErrCodeConflict,\n        Message: msg,\n    }\n}\n```\n\n### HTTP 상태 코드 매핑\n\n```go\nfunc (e *DomainError) HTTPStatus() int {\n    switch e.Code {\n    case ErrCodeNotFound:\n        return http.StatusNotFound\n    case ErrCodeUnauthorized:\n        return http.StatusUnauthorized\n    case ErrCodeConflict:\n        return http.StatusConflict\n    default:\n        return http.StatusInternalServerError\n    }\n}\n\n// 핸들러에서 사용\nfunc userHandler(w http.ResponseWriter, r *http.Request) {\n    user, err := userService.Find(r.Context(), userID)\n    if err != nil {\n        var domainErr *DomainError\n        if errors.As(err, &domainErr) {\n            http.Error(w, domainErr.Message, domainErr.HTTPStatus())\n            return\n        }\n        http.Error(w, \"서버 오류\", http.StatusInternalServerError)\n        return\n    }\n    // ...\n}\n```\n\n## 다중 에러 래핑 (Go 1.20+)\n\n### errors.Join\n\n```go\nfunc validateForm(data FormData) error {\n    var errs []error\n    \n    if data.Name == \"\" {\n        errs = append(errs, errors.New(\"이름은 필수입니다\"))\n    }\n    if data.Email == \"\" {\n        errs = append(errs, errors.New(\"이메일은 필수입니다\"))\n    }\n    if data.Age < 0 {\n        errs = append(errs, errors.New(\"나이는 0 이상이어야 합니다\"))\n    }\n    \n    return errors.Join(errs...)  // nil이면 nil 반환\n}\n\nerr := validateForm(FormData{})\n// 출력: 이름은 필수입니다\n//       이메일은 필수입니다\n```\n\n### 다중 에러 검사\n\n```go\nvar (\n    ErrNameRequired  = errors.New(\"이름은 필수입니다\")\n    ErrEmailRequired = errors.New(\"이메일은 필수입니다\")\n)\n\nerr := errors.Join(ErrNameRequired, ErrEmailRequired)\n\n// 각각 검사 가능\nerrors.Is(err, ErrNameRequired)  // true\nerrors.Is(err, ErrEmailRequired) // true\n```\n\n## 스택 트레이스\n\n표준 라이브러리는 스택 트레이스를 제공하지 않습니다. 외부 라이브러리를 활용합니다.\n\n### pkg/errors (레거시)\n\n```go\nimport \"github.com/pkg/errors\"\n\nfunc readFile(path string) error {\n    data, err := os.ReadFile(path)\n    if err != nil {\n        return errors.Wrap(err, \"파일 읽기 실패\")  // 스택 트레이스 포함\n    }\n    return nil\n}\n\n// 스택 출력\nerr := readFile(\"config.yaml\")\nfmt.Printf(\"%+v\\n\", err)  // 스택 트레이스 포함 출력\n```\n\n### cockroachdb/errors (권장)\n\n```go\nimport \"github.com/cockroachdb/errors\"\n\nfunc processData() error {\n    if err := readConfig(); err != nil {\n        return errors.Wrap(err, \"데이터 처리 실패\")\n    }\n    return nil\n}\n\n// errors.Is, errors.As 호환\n// 스택 트레이스 자동 포함\n```\n\n### 구조화된 로깅과 함께\n\n```go\nimport (\n    \"github.com/cockroachdb/errors\"\n    \"log/slog\"\n)\n\nfunc handleRequest() {\n    if err := processData(); err != nil {\n        slog.Error(\"요청 처리 실패\",\n            \"error\", err,\n            \"stack\", fmt.Sprintf(\"%+v\", err),\n        )\n    }\n}\n```\n\n## 에러 핸들링 패턴\n\n### 패턴 1: 계층별 래핑\n\n```go\n// Repository 계층\nfunc (r *UserRepo) FindByID(id string) (*User, error) {\n    user, err := r.db.Get(id)\n    if err != nil {\n        return nil, fmt.Errorf(\"UserRepo.FindByID: %w\", err)\n    }\n    return user, nil\n}\n\n// Service 계층\nfunc (s *UserService) GetUser(id string) (*User, error) {\n    user, err := s.repo.FindByID(id)\n    if err != nil {\n        if errors.Is(err, sql.ErrNoRows) {\n            return nil, domain.NewNotFoundError(\"user\", err)\n        }\n        return nil, fmt.Errorf(\"UserService.GetUser: %w\", err)\n    }\n    return user, nil\n}\n\n// Handler 계층\nfunc (h *UserHandler) Get(w http.ResponseWriter, r *http.Request) {\n    user, err := h.service.GetUser(userID)\n    if err != nil {\n        var domainErr *domain.DomainError\n        if errors.As(err, &domainErr) {\n            respondError(w, domainErr)\n            return\n        }\n        slog.Error(\"예상치 못한 에러\", \"error\", err)\n        http.Error(w, \"Internal Server Error\", 500)\n        return\n    }\n    respondJSON(w, user)\n}\n```\n\n### 패턴 2: 에러 로깅 위치\n\n```go\n// ❌ 모든 곳에서 로깅 (중복)\nfunc foo() error {\n    err := bar()\n    if err != nil {\n        log.Error(\"bar 실패\", err)  // 중복!\n        return err\n    }\n}\n\n// ✅ 최상위에서만 로깅\nfunc handler() {\n    err := foo()\n    if err != nil {\n        log.Error(\"요청 처리 실패\", err)  // 한 곳에서만\n        // 응답 처리\n    }\n}\n```\n\n### 패턴 3: 재시도 가능 에러\n\n```go\ntype RetryableError struct {\n    Err       error\n    RetryAfter time.Duration\n}\n\nfunc (e *RetryableError) Error() string {\n    return fmt.Sprintf(\"재시도 가능: %v (after %v)\", e.Err, e.RetryAfter)\n}\n\nfunc (e *RetryableError) Unwrap() error { return e.Err }\n\n// 사용\nfunc callExternalAPI() error {\n    resp, err := http.Get(url)\n    if err != nil {\n        return &RetryableError{Err: err, RetryAfter: 5 * time.Second}\n    }\n    if resp.StatusCode == 429 {\n        return &RetryableError{\n            Err:        errors.New(\"rate limited\"),\n            RetryAfter: parseRetryAfter(resp.Header),\n        }\n    }\n    return nil\n}\n\n// 호출측\nfor i := 0; i < maxRetries; i++ {\n    err := callExternalAPI()\n    var retryErr *RetryableError\n    if errors.As(err, &retryErr) {\n        time.Sleep(retryErr.RetryAfter)\n        continue\n    }\n    if err != nil {\n        return err  // 재시도 불가능한 에러\n    }\n    return nil  // 성공\n}\n```\n\n## 주의사항\n\n1. ⚠️ **`%w` vs `%v`** - 체인 유지가 필요하면 반드시 `%w`\n2. ⚠️ **errors.As는 포인터의 포인터** - `var err *CustomError; errors.As(e, &err)`\n3. ⚠️ **센티넬 에러는 패키지 레벨** - 전역 변수로 선언\n4. ⚠️ **스택 트레이스는 성능 비용** - 프로덕션에서 신중히 사용\n\n## 참고 자료\n\n- [Go Blog: Working with Errors in Go 1.13](https://go.dev/blog/go1.13-errors)\n- [errors 패키지 문서](https://pkg.go.dev/errors)\n- [cockroachdb/errors](https://github.com/cockroachdb/errors)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Error Handling",
      "Go"
    ],
    "readingTime": 7,
    "wordCount": 1273,
    "isFeatured": false,
    "isPublic": true,
    "date": "2026-01-02"
  },
  {
    "id": "docker-multistage-build-optimization",
    "slug": "docker-multistage-build-optimization",
    "path": "backend/devops",
    "fullPath": "backend/devops/docker-multistage-build-optimization",
    "title": "Docker 멀티스테이지 빌드 최적화 가이드",
    "excerpt": "멀티스테이지 빌드로 Docker 이미지 크기를 줄이고, 빌드 속도와 보안을 개선하는 실전 전략을 알아봅니다.",
    "content": "# Docker 멀티스테이지 빌드 최적화 가이드\n\n## 개요\n\n**멀티스테이지 빌드**는 단일 Dockerfile에서 여러 `FROM` 명령어를 사용하여 빌드 환경과 런타임 환경을 분리하는 기법입니다. 이를 통해 **이미지 크기 감소**, **보안 강화**, **빌드 속도 향상**을 동시에 달성할 수 있습니다.\n\n## 기본 구조\n\n### 싱글 스테이지 vs 멀티 스테이지\n\n```dockerfile\n# ❌ 싱글 스테이지 - 빌드 도구가 포함됨\nFROM golang:1.23\nWORKDIR /app\nCOPY . .\nRUN go build -o main .\nCMD [\"./main\"]\n# 결과: ~1GB 이미지\n```\n\n```dockerfile\n# ✅ 멀티 스테이지 - 런타임만 포함\nFROM golang:1.23 AS builder\nWORKDIR /app\nCOPY . .\nRUN CGO_ENABLED=0 go build -o main .\n\nFROM scratch\nCOPY --from=builder /app/main /main\nCMD [\"/main\"]\n# 결과: ~10MB 이미지\n```\n\n> [!IMPORTANT]\n> `COPY --from=builder`로 빌드 아티팩트만 복사하여 빌드 도구, 소스 코드, 중간 파일을 모두 제외합니다.\n\n## 언어별 최적화 예시\n\n### Go\n\n```dockerfile\n# 빌드 스테이지\nFROM golang:1.23-alpine AS builder\nWORKDIR /app\n\n# 의존성 먼저 (캐시 활용)\nCOPY go.mod go.sum ./\nRUN go mod download\n\n# 소스 복사 및 빌드\nCOPY . .\nRUN CGO_ENABLED=0 GOOS=linux go build -ldflags=\"-s -w\" -o /app/server ./cmd/server\n\n# 런타임 스테이지\nFROM scratch\nCOPY --from=builder /app/server /server\nCOPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/\nEXPOSE 8080\nENTRYPOINT [\"/server\"]\n```\n\n### Node.js\n\n```dockerfile\n# 의존성 설치 스테이지\nFROM node:22-alpine AS deps\nWORKDIR /app\nCOPY package.json pnpm-lock.yaml ./\nRUN corepack enable && pnpm install --frozen-lockfile\n\n# 빌드 스테이지\nFROM node:22-alpine AS builder\nWORKDIR /app\nCOPY --from=deps /app/node_modules ./node_modules\nCOPY . .\nRUN npm run build\n\n# 런타임 스테이지 (프로덕션 의존성만)\nFROM node:22-alpine AS runner\nWORKDIR /app\nENV NODE_ENV=production\n\nCOPY --from=deps /app/node_modules ./node_modules\nCOPY --from=builder /app/dist ./dist\nCOPY --from=builder /app/package.json ./\n\nUSER node\nEXPOSE 3000\nCMD [\"node\", \"dist/main.js\"]\n```\n\n### Python\n\n```dockerfile\n# 빌드 스테이지\nFROM python:3.12-slim AS builder\nWORKDIR /app\n\nRUN pip install --no-cache-dir poetry\nCOPY pyproject.toml poetry.lock ./\nRUN poetry export -f requirements.txt -o requirements.txt --without-hashes\n\n# 런타임 스테이지\nFROM python:3.12-slim\nWORKDIR /app\n\nCOPY --from=builder /app/requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\nUSER nobody\nCMD [\"python\", \"-m\", \"app.main\"]\n```\n\n## 베이스 이미지 선택\n\n| 이미지 | 크기 | 용도 |\n|-------|-----|-----|\n| `scratch` | 0MB | 정적 바이너리 (Go, Rust) |\n| `distroless` | ~2MB | 보안 중시, 셸 없음 |\n| `alpine` | ~5MB | 범용 경량 리눅스 |\n| `slim` | ~80MB | 호환성 필요 시 |\n\n### Distroless 예시\n\n```dockerfile\nFROM golang:1.23 AS builder\nWORKDIR /app\nCOPY . .\nRUN CGO_ENABLED=0 go build -o /server .\n\n# Google Distroless - 셸 없음, 보안 강화\nFROM gcr.io/distroless/static-debian12\nCOPY --from=builder /server /server\nUSER nonroot:nonroot\nENTRYPOINT [\"/server\"]\n```\n\n## 캐시 최적화\n\n### 레이어 순서 최적화\n\n```dockerfile\n# ❌ 나쁜 순서 - 소스 변경 시 의존성 재설치\nCOPY . .\nRUN npm install\nRUN npm run build\n\n# ✅ 좋은 순서 - 의존성 캐시 활용\nCOPY package.json package-lock.json ./\nRUN npm ci\nCOPY . .\nRUN npm run build\n```\n\n### BuildKit 캐시 마운트\n\n```dockerfile\n# syntax=docker/dockerfile:1\n\nFROM golang:1.23 AS builder\nWORKDIR /app\nCOPY . .\n\n# 모듈 캐시를 볼륨으로 마운트 (빌드 간 유지)\nRUN --mount=type=cache,target=/go/pkg/mod \\\n    --mount=type=cache,target=/root/.cache/go-build \\\n    go build -o /server .\n```\n\n```dockerfile\n# Node.js 예시\nFROM node:22 AS builder\nWORKDIR /app\nCOPY package*.json ./\n\nRUN --mount=type=cache,target=/root/.npm \\\n    npm ci\n```\n\n## 빌드 인자 활용\n\n### 타겟 스테이지 지정\n\n```dockerfile\nFROM node:22-alpine AS base\nWORKDIR /app\nCOPY package*.json ./\n\nFROM base AS development\nRUN npm install\nCMD [\"npm\", \"run\", \"dev\"]\n\nFROM base AS production\nRUN npm ci --only=production\nCOPY . .\nRUN npm run build\nCMD [\"npm\", \"start\"]\n```\n\n```bash\n# 개발 이미지 빌드\ndocker build --target development -t myapp:dev .\n\n# 프로덕션 이미지 빌드\ndocker build --target production -t myapp:prod .\n```\n\n### ARG로 조건부 빌드\n\n```dockerfile\nARG BUILD_ENV=production\n\nFROM node:22-alpine AS builder\nWORKDIR /app\nCOPY . .\n\nRUN if [ \"$BUILD_ENV\" = \"development\" ]; then \\\n      npm install; \\\n    else \\\n      npm ci --only=production; \\\n    fi\n```\n\n## BuildKit 고급 기능\n\n### 병렬 빌드\n\n```dockerfile\n# syntax=docker/dockerfile:1\n\nFROM alpine AS stage1\nRUN sleep 10 && echo \"stage1\" > /out1\n\nFROM alpine AS stage2\nRUN sleep 10 && echo \"stage2\" > /out2\n\n# BuildKit이 stage1, stage2를 병렬 실행\nFROM alpine\nCOPY --from=stage1 /out1 /\nCOPY --from=stage2 /out2 /\n```\n\n### Secret 마운트 (민감 정보)\n\n```dockerfile\n# syntax=docker/dockerfile:1\n\nFROM node:22-alpine\nWORKDIR /app\n\n# .npmrc를 이미지에 포함하지 않음\nRUN --mount=type=secret,id=npmrc,target=/root/.npmrc \\\n    npm ci\n```\n\n```bash\ndocker build --secret id=npmrc,src=.npmrc -t myapp .\n```\n\n## 모범 사례 체크리스트\n\n### 이미지 크기 최적화\n\n- [ ] 런타임에 불필요한 빌드 도구 제외\n- [ ] `scratch` 또는 `distroless` 사용 고려\n- [ ] `.dockerignore` 설정으로 불필요한 파일 제외\n- [ ] `--no-cache-dir` (pip), `--frozen-lockfile` (pnpm) 사용\n\n### 빌드 속도 최적화\n\n- [ ] 변경 빈도 낮은 레이어를 상단에 배치\n- [ ] BuildKit 캐시 마운트 활용\n- [ ] 의존성 파일 먼저 복사 후 설치\n\n### 보안\n\n- [ ] 베이스 이미지 버전 고정 (`:latest` 금지)\n- [ ] `USER` 명령어로 non-root 실행\n- [ ] 불필요한 포트 노출 금지\n- [ ] Secret 마운트로 민감 정보 관리\n\n## 참고 자료\n\n- [Docker Multi-stage builds](https://docs.docker.com/build/building/multi-stage/)\n- [Docker BuildKit](https://docs.docker.com/build/buildkit/)\n- [Google Distroless](https://github.com/GoogleContainerTools/distroless)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "Docker",
      "Performance"
    ],
    "readingTime": 5,
    "wordCount": 826,
    "isFeatured": false,
    "isPublic": true,
    "date": "2026-01-02"
  },
  {
    "id": "langchain-langgraph-practical-guide",
    "slug": "langchain-langgraph-practical-guide",
    "path": "ai-ml",
    "fullPath": "ai-ml/langchain-langgraph-practical-guide",
    "title": "LangChain & LangGraph 실전 가이드 (2025)",
    "excerpt": "LangChain과 LangGraph의 최신 API를 활용한 LLM 애플리케이션 및 에이전트 개발 실전 가이드입니다.",
    "content": "# LangChain & LangGraph 실전 가이드 (2025)\n\n## 개요\n\n**LangChain**은 LLM 애플리케이션 개발을 위한 프레임워크이고, **LangGraph**는 상태 기반 에이전트 워크플로우를 구축하는 런타임입니다. 2025년 LangGraph 1.0 릴리스와 함께 두 프레임워크의 역할이 명확해졌습니다.\n\n| 프레임워크 | 역할 | 주요 사용처 |\n|-----------|-----|-----------|\n| **LangChain** | LLM 인터페이스, 프롬프트, 체인 | RAG, 단순 체인, 도구 호출 |\n| **LangGraph** | 상태 관리, 그래프 기반 워크플로우 | 복잡한 에이전트, 멀티 에이전트 |\n\n## 설치\n\n```bash\n# LangChain 핵심 + OpenAI 통합\npip install langchain langchain-openai\n\n# LangGraph (에이전트 워크플로우)\npip install langgraph\n\n# 전체 스택\npip install langchain langchain-openai langgraph langchain-community\n```\n\n---\n\n## Part 1: LangChain 기초\n\n### ChatOpenAI 설정\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# 기본 설정\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    temperature=0,\n    # openai_api_key=\"...\"  # 또는 OPENAI_API_KEY 환경변수\n)\n\n# 스트리밍 활성화\nllm_streaming = ChatOpenAI(model=\"gpt-4o\", streaming=True)\n```\n\n### LCEL (LangChain Expression Language)\n\nLCEL은 체인을 선언적으로 구성하는 방식입니다.\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"당신은 {role} 전문가입니다.\"),\n    (\"human\", \"{question}\")\n])\n\n# 파이프(|)로 체인 연결\nchain = prompt | llm | StrOutputParser()\n\n# 실행\nresult = chain.invoke({\n    \"role\": \"Python\",\n    \"question\": \"asyncio의 장점은?\"\n})\nprint(result)\n```\n\n### 도구 바인딩 (Tool Calling)\n\n```python\nfrom langchain_core.tools import tool\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"도시의 현재 날씨를 조회합니다.\"\"\"\n    # 실제 API 호출 로직\n    return f\"{city}의 날씨: 맑음, 22°C\"\n\n@tool\ndef calculate(expression: str) -> str:\n    \"\"\"수학 표현식을 계산합니다.\"\"\"\n    return str(eval(expression))\n\n# LLM에 도구 바인딩\nllm_with_tools = llm.bind_tools([get_weather, calculate])\n\n# 실행\nresponse = llm_with_tools.invoke(\"서울 날씨 어때?\")\nprint(response.tool_calls)\n# [{'name': 'get_weather', 'args': {'city': '서울'}, 'id': '...'}]\n```\n\n### RAG 체인\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.runnables import RunnablePassthrough\n\n# 벡터 스토어 설정 (예시)\nembeddings = OpenAIEmbeddings()\nvectorstore = FAISS.from_texts(\n    [\"LangChain은 LLM 프레임워크입니다.\", \"LangGraph는 에이전트 런타임입니다.\"],\n    embeddings\n)\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n\n# RAG 프롬프트\nrag_prompt = ChatPromptTemplate.from_template(\"\"\"\n다음 컨텍스트를 기반으로 질문에 답하세요.\n\n컨텍스트:\n{context}\n\n질문: {question}\n\"\"\")\n\n# RAG 체인\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | rag_prompt\n    | llm\n    | StrOutputParser()\n)\n\nresult = rag_chain.invoke(\"LangGraph가 뭐야?\")\n```\n\n---\n\n## Part 2: LangGraph 에이전트\n\n### StateGraph 기본 구조\n\nLangGraph의 핵심은 **상태(State)**와 **노드(Node)**입니다.\n\n```python\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\n\n# 1. 상태 스키마 정의\nclass AgentState(TypedDict):\n    messages: Annotated[list, add_messages]  # 메시지 리듀서\n    user_info: str\n\n# 2. 그래프 빌더 생성\ngraph_builder = StateGraph(AgentState)\n```\n\n> [!NOTE]\n> `Annotated[list, add_messages]`는 리듀서 함수로, 새 메시지가 기존 리스트에 추가됩니다.\n\n### 노드 정의 단위: 함수 vs 에이전트\n\n노드를 어떤 단위로 정의할지는 프로젝트의 성격에 따라 달라집니다.\n\n#### 함수 단위 (Fine-grained) - 파이프라인 스타일\n\n[LangGraph 공식 README](https://github.com/langchain-ai/langgraph)의 기본 예시:\n\n```python\nfrom langgraph.graph import START, StateGraph\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    text: str\n\ndef node_a(state: State) -> dict:\n    return {\"text\": state[\"text\"] + \"a\"}\n\ndef node_b(state: State) -> dict:\n    return {\"text\": state[\"text\"] + \"b\"}\n\ngraph = StateGraph(State)\ngraph.add_node(\"node_a\", node_a)\ngraph.add_node(\"node_b\", node_b)\ngraph.add_edge(START, \"node_a\")\ngraph.add_edge(\"node_a\", \"node_b\")\n\nprint(graph.compile().invoke({\"text\": \"\"}))\n# {'text': 'ab'}\n```\n\n**적합한 상황**: ETL 파이프라인, 데이터 처리 워크플로우, DAG 스타일 작업\n\n#### 에이전트 단위 (Coarse-grained) - 역할 기반 스타일\n\n```python\ndef researcher(state): ...  # 리서치 담당\ndef writer(state): ...      # 글쓰기 담당\ndef reviewer(state): ...    # 검토 담당\n```\n\n**적합한 상황**: 멀티 에이전트 협업, 역할 분리가 명확한 경우\n\n#### 선택 가이드\n\n| 접근법 | 노드 단위 | 장점 | 체크포인트 |\n|-------|---------|-----|----------|\n| **함수 단위** | fetch, transform, validate 등 | 재사용성 ↑, 테스트 용이 | 세밀한 복구 가능 |\n| **에이전트 단위** | researcher, writer 등 | 직관적, 역할 분리 | 큰 단위로 복구 |\n\n> [!IMPORTANT]\n> 체크포인트는 **노드 단위**로 저장됩니다. 실패 시 해당 노드부터 재실행되므로, 노드 크기를 \"상태 변경의 원자성\"으로 결정하세요.\n\n### 노드와 엣지 정의\n\n```python\nfrom langchain_core.messages import HumanMessage, AIMessage\n\n# 노드 함수들\ndef fetch_user_info(state: AgentState) -> dict:\n    \"\"\"사용자 정보를 가져오는 노드\"\"\"\n    return {\"user_info\": \"VIP 고객, 가입일: 2023-01-15\"}\n\ndef chatbot(state: AgentState) -> dict:\n    \"\"\"LLM 응답을 생성하는 노드\"\"\"\n    system_prompt = f\"사용자 정보: {state['user_info']}\"\n    response = llm.invoke([\n        {\"role\": \"system\", \"content\": system_prompt},\n        *state[\"messages\"]\n    ])\n    return {\"messages\": [response]}\n\n# 노드 추가\ngraph_builder.add_node(\"fetch_user\", fetch_user_info)\ngraph_builder.add_node(\"chatbot\", chatbot)\n\n# 엣지 연결 (흐름 정의)\ngraph_builder.add_edge(START, \"fetch_user\")\ngraph_builder.add_edge(\"fetch_user\", \"chatbot\")\ngraph_builder.add_edge(\"chatbot\", END)\n\n# 컴파일\ngraph = graph_builder.compile()\n```\n\n### 조건부 라우팅\n\n```python\nfrom langgraph.graph import END\n\ndef should_continue(state: AgentState) -> str:\n    \"\"\"다음 노드를 결정하는 조건 함수\"\"\"\n    last_message = state[\"messages\"][-1]\n    \n    # 도구 호출이 있으면 도구 실행\n    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n        return \"tools\"\n    # 없으면 종료\n    return END\n\n# 조건부 엣지 추가\ngraph_builder.add_conditional_edges(\n    \"chatbot\",\n    should_continue,\n    {\n        \"tools\": \"tool_executor\",\n        END: END\n    }\n)\n```\n\n### 체크포인트 (메모리 영속성)\n\n```python\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# 체크포인터 설정\ncheckpointer = InMemorySaver()\ngraph = graph_builder.compile(checkpointer=checkpointer)\n\n# thread_id로 대화 세션 관리\nconfig = {\"configurable\": {\"thread_id\": \"user-123\"}}\n\n# 첫 번째 메시지\nresult1 = graph.invoke(\n    {\"messages\": [HumanMessage(content=\"안녕하세요\")]},\n    config=config\n)\n\n# 같은 thread_id로 이어서 대화\nresult2 = graph.invoke(\n    {\"messages\": [HumanMessage(content=\"이전에 뭐라고 했죠?\")]},\n    config=config\n)\n# → 이전 대화 컨텍스트 유지됨!\n```\n\n### ReAct 에이전트 (Pre-built)\n\nLangGraph는 일반적인 에이전트 패턴을 위한 프리빌트 함수를 제공합니다.\n\n```python\nfrom langgraph.prebuilt import create_react_agent\n\n# 도구 정의\ntools = [get_weather, calculate]\n\n# ReAct 에이전트 생성\nagent = create_react_agent(llm, tools=tools)\n\n# 실행\nresult = agent.invoke({\n    \"messages\": [HumanMessage(content=\"서울 날씨 알려주고, 15+27 계산해줘\")]\n})\n\nfor msg in result[\"messages\"]:\n    print(f\"{msg.type}: {msg.content}\")\n```\n\n### 스트리밍\n\n```python\n# 노드별 스트리밍\nfor event in graph.stream(\n    {\"messages\": [HumanMessage(content=\"안녕\")]},\n    config=config\n):\n    for node_name, output in event.items():\n        print(f\"[{node_name}] {output}\")\n\n# 토큰별 스트리밍 (LLM 출력)\nasync for event in graph.astream_events(\n    {\"messages\": [HumanMessage(content=\"긴 이야기 해줘\")]},\n    config=config,\n    version=\"v2\"\n):\n    if event[\"event\"] == \"on_chat_model_stream\":\n        print(event[\"data\"][\"chunk\"].content, end=\"\", flush=True)\n```\n\n---\n\n## Part 3: 실전 패턴\n\n### 패턴 1: Human-in-the-Loop\n\n```python\nfrom langgraph.types import interrupt\n\ndef review_step(state: AgentState) -> dict:\n    \"\"\"사람 승인이 필요한 노드\"\"\"\n    # interrupt()로 실행 중단, 사용자 입력 대기\n    user_approval = interrupt(\"이 작업을 진행할까요? (yes/no)\")\n    \n    if user_approval != \"yes\":\n        return {\"messages\": [AIMessage(content=\"작업이 취소되었습니다.\")]}\n    \n    return {\"messages\": [AIMessage(content=\"작업을 진행합니다.\")]}\n```\n\n### 패턴 2: 멀티 에이전트\n\n```python\nfrom langgraph.graph import StateGraph\n\nclass MultiAgentState(TypedDict):\n    messages: Annotated[list, add_messages]\n    current_agent: str\n\ndef researcher(state: MultiAgentState) -> dict:\n    \"\"\"리서치 담당 에이전트\"\"\"\n    # 리서치 로직...\n    return {\"messages\": [...], \"current_agent\": \"writer\"}\n\ndef writer(state: MultiAgentState) -> dict:\n    \"\"\"글쓰기 담당 에이전트\"\"\"\n    # 글쓰기 로직...\n    return {\"messages\": [...], \"current_agent\": \"reviewer\"}\n\ndef router(state: MultiAgentState) -> str:\n    return state[\"current_agent\"]\n\n# 그래프 구성\nworkflow = StateGraph(MultiAgentState)\nworkflow.add_node(\"researcher\", researcher)\nworkflow.add_node(\"writer\", writer)\nworkflow.add_node(\"reviewer\", reviewer)\n\nworkflow.add_edge(START, \"researcher\")\nworkflow.add_conditional_edges(\"researcher\", router)\nworkflow.add_conditional_edges(\"writer\", router)\n```\n\n### 패턴 3: 에러 핸들링\n\n```python\nfrom langgraph.errors import NodeInterrupt\n\ndef safe_tool_executor(state: AgentState) -> dict:\n    try:\n        # 도구 실행\n        result = execute_tools(state[\"messages\"][-1].tool_calls)\n        return {\"messages\": result}\n    except Exception as e:\n        # 에러 메시지를 상태에 추가\n        error_msg = AIMessage(content=f\"도구 실행 실패: {str(e)}\")\n        return {\"messages\": [error_msg]}\n```\n\n---\n\n## LangChain vs LangGraph 선택 가이드\n\n| 요구사항 | 선택 |\n|---------|-----|\n| 단순 프롬프트 → LLM → 출력 | LangChain LCEL |\n| RAG 파이프라인 | LangChain LCEL |\n| 단일 도구 호출 | LangChain `bind_tools` |\n| 복잡한 에이전트 루프 | **LangGraph** |\n| 멀티 에이전트 협업 | **LangGraph** |\n| 대화 상태 영속성 필요 | **LangGraph** checkpointer |\n| Human-in-the-loop | **LangGraph** interrupt |\n\n## 참고 자료\n\n- [LangChain Docs](https://python.langchain.com/docs/)\n- [LangGraph Docs](https://langchain-ai.github.io/langgraph/)\n- [LangGraph 1.0 Release](https://blog.langchain.dev/langgraph-v1/)\n- [LCEL Conceptual Guide](https://python.langchain.com/docs/concepts/lcel/)",
    "docType": "original",
    "category": "AI/ML",
    "tags": [
      "AI/ML",
      "Python"
    ],
    "readingTime": 6,
    "wordCount": 1135,
    "isFeatured": false,
    "isPublic": true,
    "date": "2026-01-02"
  },
  {
    "id": "enterprise-go-9-makefile",
    "slug": "enterprise-go-9-makefile",
    "path": "backend/go",
    "fullPath": "backend/go/enterprise-go-9-makefile",
    "title": "Enterprise Go 시리즈 #9: Makefile로 개발 워크플로우 자동화",
    "excerpt": "복잡한 Go 명령어들을 Makefile로 추상화하여 팀 전체의 개발 경험을 향상시킵니다. Façade 패턴의 CLI 버전입니다.",
    "content": "# Enterprise Go 시리즈 #9: Makefile로 개발 워크플로우 자동화\n\n> **핵심 아이디어**: Makefile은 **CLI의 Façade 패턴**입니다.\n> 복잡한 명령어 조합을 단순한 인터페이스 뒤에 숨깁니다.\n\n## 왜 Makefile인가?\n\n### 문제 상황\n\n```bash\n# 새 팀원이 프로젝트에 합류했을 때\n$ go build -o bin/user-api ./apps/user-api/cmd\n$ go build -o bin/order-api ./apps/order-api/cmd\n$ go generate ./...\n$ mockgen -source=... -destination=...\n$ wire ./apps/user-api/cmd\n$ go test -race -cover ./...\n```\n\n### Façade 적용 후\n\n```bash\nmake build      # 전체 빌드\nmake test       # 테스트\nmake generate   # 코드 생성 (mock, wire)\n```\n\n**Façade 패턴**처럼, 복잡한 하위 시스템을 단순한 인터페이스로 감쌉니다.\n\n---\n\n## 기본 구조\n\n```makefile\n# 변수 정의\nGO := go\nAPPS := user-api order-api worker\nBIN_DIR := bin\n\n# 기본 타겟\n.PHONY: all build test lint clean\n\nall: build\n\nbuild: $(APPS)\n\n$(APPS):\n $(GO) build -o $(BIN_DIR)/$@ ./apps/$@/cmd\n\ntest:\n $(GO) test -race -cover ./...\n\nlint:\n golangci-lint run ./...\n\nclean:\n rm -rf $(BIN_DIR)\n```\n\n---\n\n## 모노레포용 타겟\n\n### 서비스별 빌드\n\n```makefile\n# 특정 서비스만 빌드\n.PHONY: build-user build-order\n\nbuild-user:\n $(GO) build -o $(BIN_DIR)/user-api ./apps/user-api/cmd\n\nbuild-order:\n $(GO) build -o $(BIN_DIR)/order-api ./apps/order-api/cmd\n\n# 변경된 서비스만 빌드 (CI용)\nbuild-changed:\n @for app in $(shell git diff --name-only HEAD~1 | grep \"apps/\" | cut -d/ -f2 | sort -u); do \\\n  echo \"Building $$app...\"; \\\n  $(GO) build -o $(BIN_DIR)/$$app ./apps/$$app/cmd; \\\n done\n```\n\n---\n\n## 코드 생성 자동화\n\n### Mock, Wire 통합\n\n```makefile\n.PHONY: generate mock wire\n\ngenerate: mock wire\n\nmock:\n @echo \"Generating mocks...\"\n $(GO) generate ./...\n\nwire:\n @echo \"Running wire...\"\n @for app in $(APPS); do \\\n  wire ./apps/$$app/cmd; \\\n done\n```\n\n---\n\n## 개발 환경\n\n### 로컬 실행\n\n```makefile\n.PHONY: run dev\n\n# 단일 서비스 실행\nrun:\n $(GO) run ./apps/user-api/cmd serve\n\n# 핫 리로드 (air 사용)\ndev:\n air -c .air.toml\n```\n\n### Docker\n\n```makefile\n.PHONY: docker-build docker-up docker-down\n\ndocker-build:\n docker build -t myapp/user-api -f apps/user-api/Dockerfile .\n\ndocker-up:\n docker-compose up -d\n\ndocker-down:\n docker-compose down\n```\n\n---\n\n## CI/CD 연동\n\n### GitHub Actions 예시\n\n```yaml\n# .github/workflows/ci.yml\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-go@v5\n        with:\n          go-version: '1.21'\n      \n      - name: Lint\n        run: make lint\n      \n      - name: Test\n        run: make test\n      \n      - name: Build\n        run: make build\n```\n\nMakefile을 사용하면 **로컬과 CI 환경이 동일**합니다.\n\n---\n\n## 유용한 패턴\n\n### Help 타겟\n\n```makefile\n.PHONY: help\n\nhelp:\n @echo \"사용 가능한 타겟:\"\n @echo \"  build    - 전체 빌드\"\n @echo \"  test     - 테스트 실행\"\n @echo \"  lint     - 린트 검사\"\n @echo \"  generate - 코드 생성 (mock, wire)\"\n @echo \"  clean    - 빌드 결과물 삭제\"\n```\n\n### 의존성 설치\n\n```makefile\n.PHONY: tools\n\ntools:\n $(GO) install github.com/golangci/golangci-lint/cmd/golangci-lint@latest\n $(GO) install go.uber.org/mock/mockgen@latest\n $(GO) install github.com/google/wire/cmd/wire@latest\n```\n\n---\n\n## 정리: Façade로서의 Makefile\n\n```mermaid\ngraph TB\n    subgraph \"개발자 인터페이스\"\n        MAKE[\"make build<br/>make test<br/>make generate\"]\n    end\n    \n    subgraph \"숨겨진 복잡성\"\n        BUILD[\"go build -o ... ./apps/.../cmd\"]\n        TEST[\"go test -race -cover ./...\"]\n        MOCK[\"go generate + mockgen\"]\n        WIRE[\"wire ./apps/.../cmd\"]\n    end\n    \n    MAKE --> BUILD\n    MAKE --> TEST\n    MAKE --> MOCK\n    MAKE --> WIRE\n```\n\n| 장점 | 설명 |\n|------|------|\n| **온보딩 단순화** | 새 팀원도 `make build`로 시작 |\n| **일관성** | 로컬 = CI 환경 |\n| **문서화** | Makefile 자체가 실행 가능한 문서 |\n\n---\n\n## 참고 자료\n\n- [GNU Make Manual](https://www.gnu.org/software/make/manual/)\n- [Makefile Tutorial](https://makefiletutorial.com/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Architecture",
      "Backend",
      "Enterprise",
      "Go",
      "Tooling"
    ],
    "readingTime": 3,
    "wordCount": 518,
    "isFeatured": false,
    "isPublic": true,
    "series": "enterprise-go",
    "date": "2026-01-01"
  },
  {
    "id": "enterprise-go-8-observability",
    "slug": "enterprise-go-8-observability",
    "path": "backend/go",
    "fullPath": "backend/go/enterprise-go-8-observability",
    "title": "Enterprise Go 시리즈 #8: Observability와 Debugging",
    "excerpt": "Micrometer, Winston에 익숙한 개발자를 위한 Go Observability 가이드. Grafana 대시보드와 Alert 연동까지 다룹니다.",
    "content": "# Enterprise Go 시리즈 #8: Observability와 Debugging\n\n> **다른 생태계 경험자를 위한 매핑**\n>\n> - Java: Micrometer, Logback, Zipkin\n> - Node.js: Winston, prom-client, Jaeger\n\n## 핵심 질문\n\n프로덕션에서:\n\n- 장애 발생 시 원인을 어떻게 파악하나?\n- 어떤 메트릭을 수집해야 하나?\n- Grafana 대시보드와 Alert는 어떻게 구성하나?\n\n---\n\n## Observability 3요소\n\n```mermaid\ngraph TB\n    subgraph \"Observability\"\n        LOGS[\"Logs<br/>무슨 일이 일어났나?\"]\n        METRICS[\"Metrics<br/>얼마나 일어났나?\"]\n        TRACES[\"Traces<br/>어디서 일어났나?\"]\n    end\n    \n    LOGS --> DEBUG[디버깅, 감사]\n    METRICS --> MONITOR[모니터링, 알림]\n    TRACES --> ANALYSIS[병목 분석, 의존성 파악]\n```\n\n---\n\n## Logging (uber/zap)\n\n### 구조화된 로깅\n\n| 패턴 | Java | Go |\n|------|------|-----|\n| 구조화 로깅 | Logback + JSON | uber/zap |\n| 컨텍스트 전파 | MDC | Context + zap.With |\n\n### 로그 레벨 가이드\n\n| 레벨 | 사용 시점 | 프로덕션 |\n|------|----------|---------|\n| DEBUG | 개발/디버깅 | OFF |\n| INFO | 정상 흐름 기록 | ON |\n| WARN | 복구 가능한 문제 | ON |\n| ERROR | 실패, 조치 필요 | ON + Alert |\n\n### Request ID 전파\n\n```mermaid\nsequenceDiagram\n    participant Middleware\n    participant Handler\n    participant UseCase\n    participant Logger\n    \n    Middleware->>Middleware: requestID 생성\n    Middleware->>Handler: ctx에 requestID 저장\n    Handler->>UseCase: ctx 전달\n    UseCase->>Logger: logger.With(ctx)\n    Logger->>Logger: requestID 자동 포함\n```\n\n### 로거 초기화 패턴\n\n| 패턴 | 방식 | 장단점 |\n|------|------|--------|\n| **ReplaceGlobals** | `zap.ReplaceGlobals(logger)` | 간편, 테스트 어려움 |\n| **DI 주입** | Wire로 주입 | 테스트 용이, 명시적 |\n\n**ReplaceGlobals**: 빠른 프로토타이핑에 적합\n\n```go\nlogger, _ := zap.NewProduction()\nzap.ReplaceGlobals(logger)\n// 이후 어디서든 zap.L().Info(...)\n```\n\n**DI (권장)**: 정식 프로젝트에서는 Wire로 주입\n\n```go\n// wire.go\nfunc InitializeServer(logger *zap.Logger) *Server { ... }\n```\n\n테스트 시 Mock 로거 주입 가능, 의존성 명시적\n\n---\n\n## Metrics\n\n### 수집해야 할 핵심 메트릭\n\n#### RED Method (Request-driven)\n\n| 메트릭 | 설명 | Prometheus 타입 |\n|--------|------|----------------|\n| **R**ate | 초당 요청 수 | Counter |\n| **E**rrors | 에러율 | Counter |\n| **D**uration | 응답 시간 분포 | Histogram |\n\n#### USE Method (Resource-driven)\n\n| 메트릭 | 설명 | 예시 |\n|--------|------|------|\n| **U**tilization | 리소스 사용률 | CPU, Memory |\n| **S**aturation | 포화도, 대기열 | Goroutine 수 |\n| **E**rrors | 리소스 에러 | Connection 실패 |\n\n### Grafana 대시보드 구성\n\n```mermaid\ngraph TB\n    subgraph \"대시보드 레이아웃\"\n        ROW1[\"Row 1: Overview\"]\n        ROW2[\"Row 2: HTTP\"]\n        ROW3[\"Row 3: Database\"]\n        ROW4[\"Row 4: External APIs\"]\n    end\n    \n    ROW1 --> P1[\"요청률<br/>Panel\"]\n    ROW1 --> P2[\"에러율<br/>Panel\"]\n    ROW1 --> P3[\"P99 레이턴시<br/>Panel\"]\n    \n    ROW2 --> P4[\"상태코드별<br/>요청 분포\"]\n    ROW2 --> P5[\"엔드포인트별<br/>레이턴시\"]\n```\n\n### Alert 설정 예시\n\n| Alert 이름 | 조건 | 심각도 | 채널 |\n|-----------|------|--------|------|\n| HighErrorRate | 에러율 > 5% (5분) | Critical | Slack + PagerDuty |\n| HighLatency | P99 > 1s (5분) | Warning | Slack |\n| PodRestart | 재시작 > 3회 (1시간) | Critical | Slack |\n\n---\n\n## Tracing\n\n### 분산 추적이 필요한 상황\n\n```mermaid\nsequenceDiagram\n    participant Gateway\n    participant UserService\n    participant OrderService\n    participant PaymentService\n    \n    Note over Gateway,PaymentService: 같은 Trace ID 공유\n    \n    Gateway->>UserService: traceID: abc123\n    Gateway->>OrderService: traceID: abc123\n    OrderService->>PaymentService: traceID: abc123\n```\n\n### OpenTelemetry 연동\n\n| Java | Go |\n|------|-----|\n| Spring Cloud Sleuth | OpenTelemetry SDK |\n| Zipkin/Jaeger Exporter | OTLP Exporter |\n| @NewSpan | tracer.Start(ctx, name) |\n\n---\n\n## pprof: 성능 프로파일링\n\n### 프로파일 종류\n\n| 프로파일 | 분석 대상 | 사용 시점 |\n|---------|----------|----------|\n| CPU | 함수별 CPU 사용 | 높은 CPU 사용률 |\n| Heap | 메모리 할당 | 메모리 증가 |\n| Goroutine | 활성 goroutine | 누수 의심 |\n| Block | 블로킹 지점 | 동기화 문제 |\n\n### 프로덕션 활성화\n\n```mermaid\ngraph LR\n    APP[Application] -->|\":6060\"| PPROF[pprof 서버]\n    PPROF -->|인증 필요| ANALYSIS[분석 도구]\n```\n\n**보안 주의**: pprof 엔드포인트는 내부망/VPN에서만 접근\n\n---\n\n## 통합 아키텍처\n\n```mermaid\ngraph TB\n    APP[Go Application]\n    \n    APP -->|zap| LOKI[Loki]\n    APP -->|Prometheus Client| PROM[Prometheus]\n    APP -->|OTLP| TEMPO[Tempo]\n    \n    LOKI --> GRAFANA[Grafana]\n    PROM --> GRAFANA\n    TEMPO --> GRAFANA\n    \n    GRAFANA --> ALERT[Alert Manager]\n    ALERT --> SLACK[Slack]\n    ALERT --> PAGER[PagerDuty]\n```\n\n---\n\n## SPoF 방지\n\n### 단일 장애 지점 식별\n\n```mermaid\ngraph TD\n    subgraph \"확인 사항\"\n        Q1[\"외부 의존성별<br/>Circuit Breaker?\"]\n        Q2[\"DB 연결 실패 시<br/>Fallback?\"]\n        Q3[\"다운스트림 장애 시<br/>Graceful Degradation?\"]\n    end\n```\n\n### 메트릭 기반 SPoF 탐지\n\n| 메트릭 | 임계치 | 의미 |\n|--------|-------|------|\n| 의존성별 에러율 | > 50% | 해당 의존성 장애 |\n| DB 커넥션 풀 사용률 | > 90% | 커넥션 고갈 위험 |\n| Goroutine 수 | 급격한 증가 | 누수 또는 블로킹 |\n\n---\n\n## 정리\n\n| 요소 | 도구 | 용도 |\n|------|------|------|\n| Logs | zap → Loki | 디버깅, 감사 |\n| Metrics | Prometheus → Grafana | 모니터링, Alert |\n| Traces | OpenTelemetry → Tempo | 병목 분석 |\n| Profile | pprof | 성능 최적화 |\n\n---\n\n## 시리즈 마무리\n\n8편에 걸쳐 엔터프라이즈 Go 개발의 핵심을 다뤘습니다:\n\n| 편 | 주제 | 핵심 요점 |\n|----|------|----------|\n| 1 | 프로젝트 설계 | Hollow Main, internal/app, 진화 단계 |\n| 2 | HTTP 서버 | 미들웨어 순서, run.Group, Graceful Shutdown |\n| 3 | Context | 타임아웃, 취소 전파, Request ID |\n| 4 | 동시성 | Goroutine 제한, errgroup |\n| 5 | 데이터베이스 | WithTx로 @Transactional 경험 |\n| 6 | 외부 통신 | 패턴 조합 순서 |\n| 7 | 테스트 | Mock, Testcontainers, Ginkgo |\n| 8 | Observability | RED/USE, Grafana Alert |\n| 9 | Makefile | 개발 워크플로우 자동화, Façade |\n\n---\n\n## 참고 자료\n\n- [uber/zap](https://github.com/uber-go/zap)\n- [Prometheus Go Client](https://github.com/prometheus/client_golang)\n- [OpenTelemetry Go](https://opentelemetry.io/docs/instrumentation/go/)\n- [Grafana Loki](https://grafana.com/oss/loki/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Enterprise",
      "Go",
      "Observability",
      "Performance"
    ],
    "readingTime": 5,
    "wordCount": 883,
    "isFeatured": false,
    "isPublic": true,
    "series": "enterprise-go",
    "date": "2026-01-01"
  },
  {
    "id": "enterprise-go-7-testing",
    "slug": "enterprise-go-7-testing",
    "path": "backend/go",
    "fullPath": "backend/go/enterprise-go-7-testing",
    "title": "Enterprise Go 시리즈 #7: 테스트 전략과 실전",
    "excerpt": "JUnit, Jest에 익숙한 개발자를 위한 Go 테스트 생태계 가이드. Mock, 통합 테스트, BDD 스타일을 다룹니다.",
    "content": "# Enterprise Go 시리즈 #7: 테스트 전략과 실전\n\n> **다른 생태계 경험자를 위한 매핑**\n>\n> - Java: JUnit, Mockito, Testcontainers\n> - Node.js: Jest, Supertest\n> - Python: pytest, unittest.mock\n\n## 테스트 도구 매핑\n\n| 기능 | Java | Node.js | Go |\n|------|------|---------|-----|\n| Mock 생성 | Mockito | jest.mock | **uber/gomock** |\n| 통합 테스트 | Testcontainers | Testcontainers | **Testcontainers-go** |\n| BDD 스타일 | Cucumber | Jest describe | **Ginkgo** |\n| 어설션 | AssertJ | Jest expect | **Gomega** |\n\n---\n\n## Mock: uber/gomock\n\n### Mockito vs gomock\n\n| Mockito | gomock |\n|---------|--------|\n| when(...).thenReturn(...) | EXPECT().Return(...) |\n| verify(..., times(n)) | EXPECT().Times(n) |\n| @Mock 어노테이션 | mockgen 코드 생성 |\n\n### 인터페이스 기반 Mock\n\n```mermaid\ngraph LR\n    INTERFACE[UserRepository<br/>인터페이스] --> IMPL[UserGormRepo<br/>실제 구현]\n    INTERFACE --> MOCK[MockUserRepository<br/>Mock 구현]\n```\n\n**Spring과의 차이점:**\n\n- Spring: @MockBean으로 런타임 주입\n- Go: **인터페이스 정의 필수**, 컴파일 타임 타입 체크\n\n---\n\n## 통합 테스트: Testcontainers\n\n### Java vs Go\n\n| Java Testcontainers | Go Testcontainers |\n|---------------------|-------------------|\n| @Container 어노테이션 | container.Run() 함수 |\n| @DynamicPropertySource | ConnectionString() |\n| JUnit 생명주기 | Ginkgo BeforeSuite/AfterSuite |\n\n### 언제 사용하나?\n\n```mermaid\ngraph TD\n    Q{무엇을 테스트?}\n    Q -->|비즈니스 로직| MOCK[Mock 사용]\n    Q -->|SQL 쿼리| CONTAINER[Testcontainers]\n    Q -->|Repository 통합| CONTAINER\n```\n\n---\n\n## BDD: Ginkgo\n\n### Jest vs Ginkgo\n\n| Jest | Ginkgo |\n|------|--------|\n| describe('...', () => {}) | Describe(\"...\", func() {}) |\n| beforeEach(() => {}) | BeforeEach(func() {}) |\n| it('should...', () => {}) | It(\"should...\", func() {}) |\n| expect(...).toBe(...) | Expect(...).To(Equal(...)) |\n\n### 테스트 구조\n\n```mermaid\ngraph TB\n    DESCRIBE[\"Describe('UserService')\"]\n    DESCRIBE --> BEFORE[\"BeforeEach<br/>Mock 설정\"]\n    DESCRIBE --> CONTEXT1[\"Context('존재하는 사용자')\"]\n    DESCRIBE --> CONTEXT2[\"Context('존재하지 않는 사용자')\"]\n    CONTEXT1 --> IT1[\"It('정보 반환')\"]\n    CONTEXT2 --> IT2[\"It('에러 반환')\"]\n```\n\n---\n\n## 테스트 분리: Label\n\n### JUnit @Tag vs Ginkgo Label\n\n| JUnit | Ginkgo |\n|-------|--------|\n| @Tag(\"integration\") | Label(\"integration\") |\n| -Dgroups=integration | --label-filter=\"integration\" |\n\n### CI 파이프라인\n\n```mermaid\ngraph LR\n    PR[PR Build] -->|빠르게| UNIT[\"--label-filter='!integration'\"]\n    MAIN[Main Build] -->|전체| ALL[\"ginkgo -r\"]\n```\n\n---\n\n## 정리\n\n| 기능 | 추천 도구 | 대응 |\n|------|----------|------|\n| Mock | uber/gomock | Mockito |\n| 통합 테스트 | Testcontainers | @Testcontainers |\n| BDD | Ginkgo + Gomega | Jest describe |\n| 분리 실행 | Label | @Tag |\n\n---\n\n## 다음 편 예고\n\n**8편: Observability와 Debugging**에서는 Micrometer, Winston에 대응하는 Go의 관찰가능성 도구를 다룹니다.\n\n---\n\n## 참고 자료\n\n- [uber/gomock](https://github.com/uber-go/mock)\n- [Testcontainers Go](https://golang.testcontainers.org/)\n- [Ginkgo](https://onsi.github.io/ginkgo/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Enterprise",
      "Go",
      "Testing"
    ],
    "readingTime": 3,
    "wordCount": 401,
    "isFeatured": false,
    "isPublic": true,
    "series": "enterprise-go",
    "date": "2026-01-01"
  },
  {
    "id": "enterprise-go-6-resilient-communication",
    "slug": "enterprise-go-6-resilient-communication",
    "path": "backend/go",
    "fullPath": "backend/go/enterprise-go-6-resilient-communication",
    "title": "Enterprise Go 시리즈 #6: Resilient한 외부 통신",
    "excerpt": "Resilience4j, Polly에 익숙한 개발자를 위한 Go Resilience 패턴 가이드. Circuit Breaker, Retry, Rate Limiting을 올바르게 조합합니다.",
    "content": "# Enterprise Go 시리즈 #6: Resilient한 외부 통신\n\n> **다른 생태계 경험자를 위한 매핑**\n>\n> - Java: Resilience4j, Hystrix\n> - .NET: Polly\n> - Node.js: cockatiel, opossum\n\n## 핵심 질문\n\n마이크로서비스 환경에서:\n\n- 외부 API가 느려지면 우리 서비스는 어떻게 되나?\n- 재시도는 언제 해야 하고 언제 하면 안 되나?\n- 장애 전파를 어떻게 막을 것인가?\n\n---\n\n## Resilience 패턴 조합\n\n### 올바른 적용 순서\n\n```mermaid\nflowchart LR\n    REQ[Request] --> RL[Rate Limit]\n    RL --> CB[Circuit Breaker]\n    CB --> RETRY[Retry]\n    RETRY --> TIMEOUT[Timeout]\n    TIMEOUT --> CLIENT[HTTP Client]\n```\n\n| 순서 | 패턴 | 목적 | 위치 이유 |\n|------|------|------|----------|\n| 1 | Rate Limit | 요청 속도 제한 | 가장 바깥에서 과부하 방지 |\n| 2 | Circuit Breaker | 장애 서비스 격리 | 불필요한 재시도 차단 |\n| 3 | Retry | 일시적 실패 복구 | 타임아웃 전에 재시도 결정 |\n| 4 | Timeout | 무한 대기 방지 | 가장 안쪽에서 개별 요청 제어 |\n\n---\n\n## Timeout\n\n### 다계층 타임아웃 설정\n\n```mermaid\ngraph TB\n    subgraph \"타임아웃 계층\"\n        DIAL[\"Connection: 5s\"]\n        TLS[\"TLS Handshake: 5s\"]\n        RESP[\"Response Header: 10s\"]\n        TOTAL[\"Total Request: 30s\"]\n    end\n    \n    DIAL --> TLS --> RESP --> TOTAL\n```\n\n| 설정 | 권장값 | 대응 상황 |\n|------|--------|----------|\n| 연결 타임아웃 | 5초 | DNS 장애, 네트워크 단절 |\n| TLS 핸드셰이크 | 5초 | 인증서 문제 |\n| 응답 헤더 | 10초 | 서버 처리 지연 |\n| 전체 요청 | 30초 | 대용량 응답 포함 |\n\n---\n\n## Retry\n\n### 재시도 가능 조건\n\n```mermaid\ngraph TD\n    ERR{에러 종류}\n    \n    ERR -->|5xx Server Error| YES[재시도 O]\n    ERR -->|408 Request Timeout| YES\n    ERR -->|429 Too Many Requests| WAIT[대기 후 재시도]\n    ERR -->|네트워크 오류| YES\n    \n    ERR -->|4xx Client Error| NO[재시도 X]\n    ERR -->|400, 401, 403, 404| NO\n```\n\n### Exponential Backoff with Jitter\n\n```mermaid\ngraph LR\n    R1[\"1차: 100ms\"] --> R2[\"2차: 200ms + jitter\"]\n    R2 --> R3[\"3차: 400ms + jitter\"]\n    R3 --> R4[\"4차: 800ms + jitter\"]\n    R4 --> FAIL[최대 도달]\n```\n\n**Jitter가 필요한 이유:**\n\n- 동시에 실패한 클라이언트들이 동시에 재시도 → Thundering Herd\n- 랜덤 지연으로 재시도 분산\n\n---\n\n## Circuit Breaker\n\n### 상태 머신\n\n```mermaid\nstateDiagram-v2\n    [*] --> Closed: 정상 동작\n    \n    Closed --> Open: 실패율 >= 60%<br/>(최소 5회 요청)\n    \n    Open --> HalfOpen: 30초 경과\n    \n    HalfOpen --> Closed: 테스트 성공\n    HalfOpen --> Open: 테스트 실패\n```\n\n### 상태별 동작\n\n| 상태 | 요청 처리 | 메트릭 수집 |\n|------|----------|------------|\n| **Closed** | 정상 통과 | O |\n| **Open** | 즉시 실패 (Fallback) | X |\n| **Half-Open** | 제한된 수만 통과 | O |\n\n### Resilience4j 대응\n\n| Resilience4j | Go (sony/gobreaker) |\n|--------------|---------------------|\n| failureRateThreshold | ReadyToTrip 콜백 |\n| waitDurationInOpenState | Timeout |\n| permittedNumberOfCallsInHalfOpenState | MaxRequests |\n\n---\n\n## Rate Limiting\n\n### Token Bucket 알고리즘\n\n```mermaid\ngraph LR\n    FILL[\"토큰 보충<br/>N개/초\"] --> BUCKET[\"버킷<br/>최대 M개\"]\n    BUCKET --> REQ[\"요청 도착\"]\n    \n    REQ -->|토큰 있음| OK[처리]\n    REQ -->|토큰 없음| WAIT[대기/거부]\n```\n\n### 외부 API Rate Limit 대응\n\n외부 API가 100 req/s 제한이라면:\n\n- **우리 설정**: 80 req/s (여유분 20%)\n- **이유**: 버스트 트래픽, 다른 클라이언트 고려\n\n---\n\n## 패턴 조합 전략\n\n### 언제 어떤 패턴을?\n\n| 상황 | 적용 패턴 |\n|------|----------|\n| 네트워크 일시 장애 | Retry + Exponential Backoff |\n| 외부 서비스 장애 | Circuit Breaker |\n| 외부 API 호출량 제한 | Rate Limiting |\n| 모든 외부 호출 | Timeout (필수) |\n\n### 빠른 프로토타이핑 → 정식 채택\n\n빠른 개발을 위해 Timeout만 설정했으나, 프로덕션 배포 후 개선:\n\n1. **1차**: Timeout만 적용 → 외부 장애 시 요청 누적\n2. **2차**: Retry 추가 → 재시도 폭풍 발생\n3. **3차**: Circuit Breaker 추가 → 장애 격리 성공\n4. **최종**: Rate Limiting으로 안정성 확보\n\n---\n\n## Go 라이브러리 선택\n\n| 패턴 | 권장 라이브러리 |\n|------|---------------|\n| Circuit Breaker | sony/gobreaker |\n| Retry | 직접 구현 또는 avast/retry-go |\n| Rate Limiting | golang.org/x/time/rate |\n| 통합 | failsafe-go |\n\n---\n\n## 정리\n\n| 패턴 | 목적 | 적용 위치 |\n|------|------|----------|\n| **Timeout** | 무한 대기 방지 | 필수, 모든 외부 호출 |\n| **Retry** | 일시적 실패 복구 | 5xx, 네트워크 오류만 |\n| **Circuit Breaker** | 장애 전파 차단 | 외부 의존성 호출 |\n| **Rate Limiting** | 과부하 방지 | API 호출량 제한 시 |\n\n---\n\n## 다음 편 예고\n\n**7편: 테스트 전략과 실전**에서는 JUnit, Jest에 대응하는 Go의 테스트 생태계를 다룹니다.\n\n---\n\n## 참고 자료\n\n- [sony/gobreaker](https://github.com/sony/gobreaker)\n- [failsafe-go](https://failsafe-go.dev/)\n- [golang.org/x/time/rate](https://pkg.go.dev/golang.org/x/time/rate)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Enterprise",
      "Go",
      "HTTP",
      "Resilience"
    ],
    "readingTime": 4,
    "wordCount": 716,
    "isFeatured": false,
    "isPublic": true,
    "series": "enterprise-go",
    "date": "2026-01-01"
  },
  {
    "id": "enterprise-go-5-database",
    "slug": "enterprise-go-5-database",
    "path": "backend/go",
    "fullPath": "backend/go/enterprise-go-5-database",
    "title": "Enterprise Go 시리즈 #5: 데이터베이스 연동 패턴",
    "excerpt": "Spring의 @Transactional처럼 편리하게 트랜잭션을 관리하는 Go 패턴을 다룹니다. 어노테이션 하나로 해결되던 것을 Go에서 어떻게 구현할까요?",
    "content": "# Enterprise Go 시리즈 #5: 데이터베이스 연동 패턴\n\n> **Spring 개발자를 위한 핵심 질문**\n> `@Transactional` 어노테이션 하나로 메서드들을 하나의 트랜잭션으로 묶던 편리함, Go에서도 가능할까요?\n\n## @Transactional의 본질\n\n### Spring에서의 경험\n\n```java\n@Transactional\npublic void transfer(String from, String to, BigDecimal amount) {\n    accountRepository.withdraw(from, amount);  // 같은 TX\n    accountRepository.deposit(to, amount);     // 같은 TX\n    logRepository.save(new TransferLog(...));  // 같은 TX\n}\n```\n\n**핵심 편의성:**\n\n- 어노테이션만 붙이면 끝\n- 메서드 내 모든 Repository 호출이 **자동으로 같은 트랜잭션**\n- 예외 발생 시 **자동 롤백**\n- 개발자는 비즈니스 로직에만 집중\n\n### Go의 현실\n\nGo에는 AOP가 없으므로 `@Transactional`과 100% 동일한 경험은 불가능합니다. 하지만 **비슷한 수준의 편의성**을 달성할 수 있습니다.\n\n---\n\n## 목표: @Transactional과 유사한 경험\n\n### 우리가 원하는 것\n\n```mermaid\ngraph LR\n    subgraph \"Spring\"\n        A[\"@Transactional<br/>어노테이션 한 줄\"]\n    end\n    \n    subgraph \"Go 목표\"\n        B[\"WithTx(ctx, func(ctx) {...})<br/>래퍼 함수 한 줄\"]\n    end\n    \n    A --> RESULT[Repository들이<br/>같은 TX 사용]\n    B --> RESULT\n```\n\n### 핵심 아이디어\n\n```mermaid\nsequenceDiagram\n    participant Service\n    participant WithTx\n    participant Repository1\n    participant Repository2\n    participant DB\n    \n    Service->>WithTx: WithTx(ctx, func(ctx) error)\n    WithTx->>DB: BEGIN\n    WithTx->>WithTx: ctx에 TX 삽입\n    \n    WithTx->>Service: ctx (TX 포함)\n    \n    Service->>Repository1: Withdraw(ctx, ...)\n    Repository1->>Repository1: ctx에서 TX 추출\n    Repository1->>DB: UPDATE (TX 사용)\n    \n    Service->>Repository2: Deposit(ctx, ...)\n    Repository2->>Repository2: ctx에서 TX 추출\n    Repository2->>DB: UPDATE (같은 TX)\n    \n    alt 성공\n        WithTx->>DB: COMMIT\n    else 에러\n        WithTx->>DB: ROLLBACK\n    end\n```\n\n---\n\n## 구현 패턴\n\n### 사용 코드 (UseCase)\n\nSpring의 `@Transactional`과 유사하게, **래퍼 함수 한 줄**로 트랜잭션 경계를 정의합니다:\n\n```go\nfunc (u *TransferUseCase) Transfer(ctx context.Context, from, to string, amount int64) error {\n    // WithTx 한 줄로 트랜잭션 시작 - @Transactional과 유사!\n    return database.WithTx(ctx, u.db, func(ctx context.Context) error {\n        // 이 안의 모든 Repository 호출은 같은 트랜잭션\n        if err := u.accountRepo.Withdraw(ctx, from, amount); err != nil {\n            return err  // 자동 롤백\n        }\n        if err := u.accountRepo.Deposit(ctx, to, amount); err != nil {\n            return err  // 자동 롤백\n        }\n        return u.logRepo.Save(ctx, &TransferLog{...})\n    })\n    // 성공 시 자동 커밋, 실패 시 자동 롤백\n}\n```\n\n### Repository는 트랜잭션을 모름\n\n```go\nfunc (r *AccountRepository) Withdraw(ctx context.Context, id string, amount int64) error {\n    // ctx에서 TX가 있으면 사용, 없으면 일반 DB\n    db := database.GetDB(ctx, r.db)\n    return db.Model(&Account{}).Where(\"id = ?\", id).\n        Update(\"balance\", gorm.Expr(\"balance - ?\", amount)).Error\n}\n```\n\n---\n\n## Spring vs Go 비교\n\n| 측면 | Spring @Transactional | Go WithTx |\n|------|----------------------|-----------|\n| 문법 | 어노테이션 | 래퍼 함수 |\n| 명시성 | 암묵적 | 명시적 |\n| 트랜잭션 전파 | 선언적 (REQUIRED 등) | Context 전달 |\n| 롤백 조건 | 예외 타입 기반 | error 반환 |\n| 학습 곡선 | AOP 이해 필요 | Context 이해 필요 |\n\n### Go의 장점\n\n- **명시적**: 트랜잭션 경계가 코드에 보임\n- **테스트 용이**: Mock Context 주입 가능\n- **디버깅 용이**: 스택 트레이스 명확\n\n### Go의 단점\n\n- **반복 코드**: 매번 WithTx 호출 필요\n- **규율 필요**: GetDB 호출 누락 시 별도 TX 사용\n\n---\n\n## Connection Pool 설정\n\n### 핵심 설정\n\n| 설정 | 권장 | 이유 |\n|------|------|------|\n| MaxOpenConns | 25-50 | DB 동시 연결 제한 고려 |\n| MaxIdleConns | 10-25 | Open의 40-50% |\n| ConnMaxLifetime | 5분 | 방화벽/LB 타임아웃 고려 |\n\n### Spring 대응\n\n| Spring | Go |\n|--------|-----|\n| HikariCP maximumPoolSize | MaxOpenConns |\n| minimumIdle | MaxIdleConns |\n| maxLifetime | ConnMaxLifetime |\n\n---\n\n## 정리\n\n| 요소 | 역할 |\n|------|------|\n| **WithTx** | @Transactional 대응, 트랜잭션 경계 |\n| **GetDB** | Context에서 TX 추출 |\n| **Repository** | TX를 모름, Context만 받음 |\n\n**핵심 메시지**: Go에서도 `WithTx` 래퍼 하나로 Spring의 `@Transactional`과 유사한 편의성을 얻을 수 있습니다.\n\n---\n\n## 다음 편 예고\n\n**6편: Resilient한 외부 통신**에서는 Resilience4j에 대응하는 Go의 Circuit Breaker, Retry 패턴을 다룹니다.\n\n---\n\n## 참고 자료\n\n- [GORM 공식 문서](https://gorm.io/)\n- [database/sql](https://golang.org/pkg/database/sql/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Database",
      "Enterprise",
      "GORM",
      "Go",
      "Repository",
      "Transaction"
    ],
    "readingTime": 3,
    "wordCount": 594,
    "isFeatured": false,
    "isPublic": true,
    "series": "enterprise-go",
    "date": "2026-01-01"
  },
  {
    "id": "enterprise-go-4-goroutine-channel",
    "slug": "enterprise-go-4-goroutine-channel",
    "path": "backend/go",
    "fullPath": "backend/go/enterprise-go-4-goroutine-channel",
    "title": "Enterprise Go 시리즈 #4: Goroutine과 Channel 실전 활용",
    "excerpt": "ExecutorService, Worker Threads에 익숙한 개발자를 위한 Go 동시성 모델 가이드. Goroutine의 메모리 특성과 안전한 패턴을 다룹니다.",
    "content": "# Enterprise Go 시리즈 #4: Goroutine과 Channel 실전 활용\n\n> **다른 언어 경험자를 위한 매핑**\n>\n> - Java: `ExecutorService`, `CompletableFuture`\n> - Node.js: Worker Threads, `Promise.all`\n> - Rust: `tokio::spawn`, `mpsc`\n\n## Goroutine의 실체\n\n### Java Thread vs Go Goroutine\n\n| 특성 | Java Thread | Go Goroutine |\n|------|------------|--------------|\n| 초기 스택 | ~1MB (OS 할당) | **~2KB** (Go 런타임) |\n| 최대 스택 | 고정 | **동적 확장** (최대 1GB) |\n| 스케줄링 | OS 커널 | Go 런타임 (M:N) |\n| 생성 비용 | 높음 | 낮음 |\n| 실용적 상한 | 수천 개 | **수십만 개** |\n\n### 왜 제한이 필요한가?\n\nGoroutine이 가볍다고 해서 무제한은 아닙니다.\n\n**실증적 근거:**\n\n- 초기 스택 2KB + 디스크립터 ~400바이트 ≈ **약 2.4KB / goroutine**\n- 100,000개 goroutine = **최소 240MB** 메모리\n- 컨테이너 메모리 제한(512MB~2GB)에서 OOM 위험\n\n```mermaid\ngraph LR\n    subgraph \"메모리 계산\"\n        G[100,000 goroutines] --> MEM[\"240MB+ 메모리\"]\n        MEM --> LIMIT[\"컨테이너 제한 초과\"]\n        LIMIT --> OOM[\"OOM Killer\"]\n    end\n```\n\n**실제 OOM 시나리오:**\n\n1. 요청마다 goroutine 생성\n2. 외부 API 응답 지연으로 goroutine 대기\n3. 대기 중인 goroutine 누적\n4. 컨테이너 메모리 제한 도달 → 강제 종료\n\n---\n\n## Java 개발자를 위한 패턴 매핑\n\n### ExecutorService → Worker Pool\n\n```mermaid\ngraph TB\n    subgraph \"Java ExecutorService\"\n        JES[newFixedThreadPool 10]\n        JQ[BlockingQueue]\n        JES --> JQ\n    end\n    \n    subgraph \"Go Worker Pool\"\n        GW[10 Worker Goroutines]\n        GC[Buffered Channel]\n        GW --> GC\n    end\n```\n\n| Java | Go |\n|------|-----|\n| `Executors.newFixedThreadPool(10)` | Worker Pool 패턴 |\n| `executor.submit(task)` | `jobs <- task` |\n| `Future.get()` | `<-results` |\n\n### CompletableFuture.allOf → errgroup\n\n```mermaid\nsequenceDiagram\n    participant Main\n    participant G1 as Goroutine 1\n    participant G2 as Goroutine 2\n    participant G3 as Goroutine 3\n    \n    Main->>G1: g.Go(fn1)\n    Main->>G2: g.Go(fn2)\n    Main->>G3: g.Go(fn3)\n    \n    G2->>Main: return error\n    Note over Main: Context 취소 전파\n    \n    G1-->>Main: ctx.Done() 감지\n    G3-->>Main: ctx.Done() 감지\n    \n    Main->>Main: g.Wait() - 첫 에러 반환\n```\n\n**Java와의 차이점:**\n\n- `CompletableFuture.allOf`는 모든 완료를 기다림\n- `errgroup`은 첫 에러 시 나머지 **취소 가능** (Context 연동)\n\n---\n\n## Channel 설계 원칙\n\n### Buffered vs Unbuffered\n\n| 타입 | Java 대응 | 사용 시점 |\n|------|----------|----------|\n| Unbuffered | `SynchronousQueue` | 핸드셰이크, 동기 통신 |\n| Buffered | `ArrayBlockingQueue` | 백프레셔, 비동기 |\n\n### 닫기 규칙\n\n```mermaid\ngraph LR\n    PRODUCER[\"Producer<br/>(Channel 생성자)\"] -->|close| CH[Channel]\n    CH --> C1[Consumer 1]\n    CH --> C2[Consumer 2]\n```\n\n**Java와의 차이:**\n\n- Java: `BlockingQueue`는 보통 닫지 않음\n- Go: **Producer가 close() 호출** (규칙)\n\n---\n\n## 동기화 도구 선택\n\n### Mutex vs Channel\n\n```mermaid\ngraph TD\n    Q{무엇이 필요한가?}\n    Q -->|상태 보호| MUTEX[sync.Mutex]\n    Q -->|데이터 전달| CHANNEL[Channel]\n    \n    MUTEX --> M1[\"캐시, 카운터\"]\n    CHANNEL --> C1[\"작업 분배, 이벤트\"]\n```\n\n| 상황 | Java | Go |\n|------|------|-----|\n| 공유 상태 보호 | `synchronized` | `sync.Mutex` |\n| 작업 분배 | `BlockingQueue` | Channel |\n| 병렬 실행 후 수집 | `CompletableFuture` | errgroup |\n\n---\n\n## Race Detector\n\nGo의 강력한 장점 중 하나입니다. Java의 ThreadSanitizer에 대응되나, **표준 도구로 내장**되어 있습니다:\n\n```bash\ngo test -race ./...\ngo build -race ./cmd/myapp\n```\n\n**CI 필수 적용 권장** - 런타임 오버헤드가 있으나 테스트 환경에서는 문제없음\n\n---\n\n## 정리\n\n| 개념 | Java 대응 | Go |\n|------|----------|-----|\n| 스레드 풀 | ExecutorService | Worker Pool + Channel |\n| 병렬 실행 | CompletableFuture | errgroup |\n| 공유 상태 | synchronized | sync.Mutex |\n| 메시지 전달 | BlockingQueue | Channel |\n| 경쟁 상태 탐지 | ThreadSanitizer | -race 플래그 |\n\n---\n\n## 다음 편 예고\n\n**5편: 데이터베이스 연동 패턴**에서는 Spring의 `@Transactional`에 대응하는 Go의 트랜잭션 관리 전략을 다룹니다.\n\n---\n\n## 참고 자료\n\n- [Go Concurrency Patterns](https://talks.golang.org/2012/concurrency.slide)\n- [errgroup Package](https://pkg.go.dev/golang.org/x/sync/errgroup)\n- [Goroutine Stack Size](https://go.dev/blog/go1.4)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Concurrency",
      "Enterprise",
      "Go"
    ],
    "readingTime": 3,
    "wordCount": 583,
    "isFeatured": false,
    "isPublic": true,
    "series": "enterprise-go",
    "date": "2026-01-01"
  },
  {
    "id": "enterprise-go-3-context",
    "slug": "enterprise-go-3-context",
    "path": "backend/go",
    "fullPath": "backend/go/enterprise-go-3-context",
    "title": "Enterprise Go 시리즈 #3: Context로 요청 생명주기 관리",
    "excerpt": "Java의 ThreadLocal, Node.js의 AsyncLocalStorage에 익숙한 개발자를 위한 Go Context 패턴 가이드입니다.",
    "content": "# Enterprise Go 시리즈 #3: Context로 요청 생명주기 관리\n\n> **다른 생태계 경험자를 위한 매핑**\n>\n> - Java: ThreadLocal, MDC\n> - Node.js: AsyncLocalStorage\n> - Python: contextvars\n\n## Context의 역할\n\n### 다른 언어와의 비교\n\n| 기능 | Java | Node.js | Go |\n|------|------|---------|-----|\n| 요청 범위 값 | ThreadLocal | AsyncLocalStorage | context.WithValue |\n| 타임아웃 | ExecutorService timeout | AbortController | context.WithTimeout |\n| 취소 전파 | Future.cancel() | AbortSignal | context.WithCancel |\n\n### 핵심 차이점\n\nGo Context는 세 가지 역할을 **하나의 인터페이스**에서 처리:\n\n1. 값 전파 (Request ID, Trace ID)\n2. 타임아웃\n3. 취소 신호 전파\n\n---\n\n## 타임아웃 설계\n\n### 계층별 전략\n\n```mermaid\ngraph TB\n    HTTP[\"HTTP 전체: 30s\"] --> UC[\"UseCase: 10s\"]\n    UC --> DB[\"DB 쿼리: 3s\"]\n    UC --> EXT[\"외부 API: 5s\"]\n```\n\n**원칙**: 하위 타임아웃 합 < 상위 타임아웃\n\n### Spring vs Go\n\n| Spring | Go |\n|--------|-----|\n| @Transactional(timeout=3) | WithTimeout(ctx, 3*time.Second) |\n| RestTemplate.setConnectTimeout | http.Client Timeout |\n\n---\n\n## 취소 전파\n\n### 왜 중요한가?\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant Server\n    participant DB\n    participant API\n    \n    Client->>Server: 요청\n    Server->>DB: 쿼리 시작\n    Server->>API: API 호출 시작\n    Client-xServer: 연결 끊김\n    \n    Note over DB,API: Context 전파 O<br/>→ 모두 취소\n    Note over DB,API: Context 전파 X<br/>→ 계속 실행 (낭비)\n```\n\n### Java 대응\n\n| Java | Go |\n|------|-----|\n| Future.cancel(true) | cancel() 호출 |\n| InterruptedException | ctx.Done() 채널 |\n\n---\n\n## Request ID 전파\n\n### MDC vs Context\n\n```mermaid\ngraph LR\n    subgraph \"Java MDC\"\n        FILTER[Filter] --> MDC[\"MDC.put('requestId')\"]\n        MDC --> SERVICE[Service]\n        SERVICE --> LOG[\"자동 포함\"]\n    end\n    \n    subgraph \"Go Context\"\n        MW[Middleware] --> CTX[\"WithValue(requestID)\"]\n        CTX --> HANDLER[Handler]\n        HANDLER --> LOGGER[\"logger.With(ctx)\"]\n    end\n```\n\n---\n\n## 안티패턴\n\n### Context를 구조체에 저장\n\n```mermaid\ngraph TD\n    BAD[\"struct { ctx Context }\"] --> PROBLEM1[\"생명주기 불일치\"]\n    BAD --> PROBLEM2[\"취소 신호 무시\"]\n    \n    GOOD[\"func(ctx Context)\"] --> OK[\"매번 최신 ctx 사용\"]\n```\n\n### Spring 개발자 주의점\n\n| Spring 습관 | Go 규칙 |\n|------------|---------|\n| @Autowired로 주입 | **파라미터로 전달** |\n| ThreadLocal에 저장 | **저장하지 않음** |\n| 선택적 사용 | **첫 번째 파라미터 (필수)** |\n\n---\n\n## 정리\n\n| 개념 | Java/Spring | Go |\n|------|-------------|-----|\n| 요청 범위 값 | ThreadLocal/MDC | WithValue |\n| 타임아웃 | @Transactional timeout | WithTimeout |\n| 취소 | Future.cancel | cancel() |\n| 사용 패턴 | 암묵적 (ThreadLocal) | **명시적 (파라미터)** |\n\n---\n\n## 다음 편 예고\n\n**4편: Goroutine과 Channel 실전 활용**에서는 Java의 ExecutorService, Node.js의 Worker Threads에 대응하는 Go 동시성 패턴을 다룹니다.\n\n---\n\n## 참고 자료\n\n- [Go Blog: Context](https://blog.golang.org/context)\n- [context Package](https://pkg.go.dev/context)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Cancellation",
      "Concurrency",
      "Enterprise",
      "Go",
      "Observability",
      "Timeout"
    ],
    "readingTime": 3,
    "wordCount": 409,
    "isFeatured": false,
    "isPublic": true,
    "series": "enterprise-go",
    "date": "2026-01-01"
  },
  {
    "id": "enterprise-go-2-http-server",
    "slug": "enterprise-go-2-http-server",
    "path": "backend/go",
    "fullPath": "backend/go/enterprise-go-2-http-server",
    "title": "Enterprise Go 시리즈 #2: 견고한 HTTP 서버 구축",
    "excerpt": "Spring MVC의 Filter/Interceptor, Express의 Middleware에 익숙한 개발자를 위한 Echo 미들웨어 설계 가이드입니다.",
    "content": "# Enterprise Go 시리즈 #2: 견고한 HTTP 서버 구축\n\n> **다른 생태계 경험자를 위한 매핑**\n>\n> - Java: Spring MVC Filter, HandlerInterceptor\n> - Node.js: Express Middleware\n> - Python: Django Middleware, FastAPI Middleware\n\n## 미들웨어 체인: 순서가 중요하다\n\n### 실행 흐름\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant Recover\n    participant RequestID\n    participant Logger\n    participant Auth\n    participant Handler\n    \n    Client->>Recover: Request\n    Recover->>RequestID: \n    RequestID->>Logger: \n    Logger->>Auth: \n    Auth->>Handler: \n    Handler-->>Auth: Response\n    Auth-->>Logger: \n    Logger-->>RequestID: \n    RequestID-->>Recover: \n    Recover-->>Client: \n```\n\n### 권장 순서와 이유\n\n| 순서 | 미들웨어 | Spring 대응 | 위치 이유 |\n|------|---------|------------|----------|\n| 1 | Recover | @ControllerAdvice | 패닉 → 500 변환 |\n| 2 | Request ID | MDC 설정 | 로깅 전 ID 필요 |\n| 3 | Logger | AccessLogFilter | 인증 실패도 로깅 |\n| 4 | CORS | CorsFilter | preflight 처리 |\n| 5 | Auth | SecurityFilter | 핸들러 보호 |\n\n### 경험담\n\n빠른 프로토타이핑을 위해 미들웨어 순서를 고려하지 않았으나, 정식 채택 후:\n\n> Recover를 Auth 뒤에 두었더니, 인증 로직 패닉 시 응답 없이 연결 끊김\n\n> Logger를 RequestID 앞에 두었더니, 로그에서 요청 추적 불가\n\n---\n\n## 에러 핸들링\n\n### 일관된 응답 구조\n\n```mermaid\ngraph TD\n    subgraph \"표준 에러 응답\"\n        CODE[\"code: 'VALIDATION_ERROR'\"]\n        MSG[\"message: '이메일 형식 오류'\"]\n        DETAILS[\"details: {...}\"]\n    end\n```\n\n### 도메인 에러 → HTTP 매핑\n\n| Domain Error | HTTP Status | Spring 대응 |\n|--------------|-------------|------------|\n| ErrNotFound | 404 | @ResponseStatus |\n| ErrValidation | 400 | MethodArgumentNotValidException |\n| ErrUnauthorized | 401 | AuthenticationException |\n| 기타 | 500 | @ExceptionHandler |\n\n---\n\n## Graceful Shutdown\n\n### 왜 필요한가?\n\n```mermaid\nsequenceDiagram\n    participant K8s\n    participant Pod\n    participant Client\n    \n    K8s->>Pod: SIGTERM\n    Pod->>Pod: 새 요청 거부\n    Pod->>Client: 진행 중 요청 완료\n    Pod->>K8s: 정상 종료\n```\n\n### Spring vs Go\n\n| 측면 | Spring Boot | Go Echo |\n|------|-------------|---------|\n| 설정 | server.shutdown=graceful | 직접 구현 |\n| 타임아웃 | spring.lifecycle.timeout-per-shutdown-phase | WithTimeout |\n| 시그널 | 자동 처리 | signal.NotifyContext |\n\n### run.Group 패턴 (Prometheus 방식)\n\n서버가 여러 고루틴(웹 서버, 워커, 헬스체크 등)으로 구성될 때:\n\n```go\nimport \"github.com/oklog/run\"\n\nvar g run.Group\n\n// HTTP 서버\ng.Add(func() error {\n    return server.ListenAndServe()\n}, func(err error) {\n    server.Shutdown(ctx)\n})\n\n// 백그라운드 워커\ng.Add(func() error {\n    return worker.Run()\n}, func(err error) {\n    worker.Stop()\n})\n\n// 하나라도 종료되면 전체 종료\nif err := g.Run(); err != nil {\n    log.Error(err)\n}\n```\n\n**장점:** 시그널 처리, 에러 전파, 종료 순서를 한 곳에서 관리\n\n---\n\n## 정리\n\n| 요소 | 핵심 |\n|------|------|\n| 미들웨어 순서 | Recover → RequestID → Logger → Auth |\n| 에러 핸들링 | 도메인 에러 → HTTP 상태 매핑 |\n| Graceful Shutdown | SIGTERM 처리, run.Group 패턴 |\n\n---\n\n## 다음 편 예고\n\n**3편: Context로 요청 생명주기 관리**에서는 Java의 ThreadLocal, Node.js의 AsyncLocalStorage에 대응하는 Context 패턴을 다룹니다.\n\n---\n\n## 참고 자료\n\n- [Echo 공식 문서](https://echo.labstack.com/)\n- [Graceful Shutdown](https://pkg.go.dev/net/http#Server.Shutdown)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Architecture",
      "Backend",
      "Echo",
      "Enterprise",
      "Go",
      "HTTP",
      "Resilience"
    ],
    "readingTime": 3,
    "wordCount": 460,
    "isFeatured": false,
    "isPublic": true,
    "series": "enterprise-go",
    "date": "2026-01-01"
  },
  {
    "id": "enterprise-go-1-project-structure",
    "slug": "enterprise-go-1-project-structure",
    "path": "backend/go",
    "fullPath": "backend/go/enterprise-go-1-project-structure",
    "title": "Enterprise Go 시리즈 #1: 프로젝트 설계와 구조화",
    "excerpt": "Kubernetes, Docker CLI, Prometheus, Hugo의 소스 코드를 분석하여 도출한 Go 프로젝트 구조 베스트 프랙티스를 소개합니다.",
    "content": "# Enterprise Go 시리즈 #1: 프로젝트 설계와 구조화\n\n> **대상 독자**: Java, Node.js 등 다른 생태계에서 충분한 경험을 쌓은 후 Go로 전환하는 시니어 엔지니어\n\n## 시리즈 소개\n\n| # | 주제 | 다른 언어에서의 대응 개념 |\n|---|------|------------------------|\n| **1** | 프로젝트 설계 | Maven 멀티모듈, Gradle 컨벤션 |\n| 2 | HTTP 서버 | Spring MVC, Express |\n| 3 | Context | ThreadLocal, AsyncLocalStorage |\n| 4 | 동시성 | ExecutorService, Worker Threads |\n| 5 | 데이터베이스 | @Transactional, Sequelize |\n| 6 | 외부 통신 | Resilience4j, Polly |\n| 7 | 테스트 | JUnit, Jest |\n| 8 | Observability | Micrometer, Winston |\n| 9 | Makefile | npm scripts, Gradle tasks |\n\n---\n\n## 실제 프로덕션 프로젝트 분석\n\n이론이 아닌 **실제 코드**를 분석했습니다:\n\n| 프로젝트 | 특성 | Main 역할 | 조립 코드 위치 |\n|---------|------|----------|--------------|\n| Kubernetes | 거대 모놀리스 | 최소화 | `cmd/<bin>/app/` |\n| Docker CLI | CLI 도구 | 최소화 | `cli/command/` |\n| Prometheus | 데몬/서비스 | 중간 | `main.go` 내부 |\n| Hugo | 컴파일러 | 최소화 | `commands/` |\n\n---\n\n## 핵심 인사이트: Hollow Main 패턴\n\n> **프로젝트가 성숙할수록 main 함수는 비어간다.**\n\n### Fat Main의 문제\n\n```go\n// ❌ 나쁜 예: Fat Main\nfunc main() {\n    cfg := loadConfig()\n    db := connectDB(cfg)\n    userRepo := NewUserRepo(db)\n    userService := NewUserService(userRepo)\n    handler := NewHandler(userService)\n    http.ListenAndServe(\":8080\", handler)\n}\n```\n\n**문제점:**\n\n- `main`은 테스트 불가\n- `main` 패키지는 다른 곳에서 import 불가\n\n### Hollow Main (권장)\n\n```go\n// ✅ 좋은 예: Hollow Main\npackage main\n\nimport \"myproject/internal/app\"\n\nfunc main() {\n    if err := app.Run(); err != nil {\n        os.Exit(1)\n    }\n}\n```\n\n모든 로직은 `internal/app`으로 이동 → **테스트 가능**.\n\n---\n\n## 권장 디렉토리 구조\n\n```\nproject/\n├── cmd/\n│   └── myapp/\n│       └── main.go         # 텅 비어있음\n│\n├── internal/\n│   ├── app/                # 조립 코드 (Composition Root)\n│   │   ├── app.go          # Run() 함수\n│   │   └── config.go       # 설정 로드\n│   ├── api/                # HTTP 핸들러, gRPC\n│   ├── biz/                # 비즈니스 로직\n│   └── data/               # DB, 외부 API\n│\n├── pkg/                    # 외부 공개 (신중하게)\n├── configs/                # 설정 파일\n├── api/                    # OpenAPI, Protobuf\n└── Makefile\n```\n\n---\n\n## internal/app: 조립의 중심\n\n```go\n// internal/app/app.go\nfunc Run() error {\n    cfg := LoadConfig()\n    \n    // 의존성 조립 (DI)\n    db := data.NewDatabase(cfg.DSN)\n    svc := biz.NewService(db)\n    server := api.NewServer(svc)\n    \n    // 서버 시작\n    return server.Start()\n}\n```\n\n**이 위치의 장점:**\n\n- `cmd/`에서 분리 → 테스트 가능\n- 설정 로드, DI, 라이프사이클 관리 집중\n\n---\n\n## 프로젝트 진화 단계\n\n### 1단계: 프로토타입\n\n```\nproject/\n├── main.go\n└── go.mod\n```\n\n500줄 미만의 PoC. 괜찮습니다.\n\n### 2단계: 모듈화\n\n```\nproject/\n├── cmd/myapp/main.go\n└── internal/\n    ├── app/\n    ├── api/\n    └── data/\n```\n\n1000줄을 넘어가거나, DB 코드와 핸들러가 섞이기 시작하면.\n\n### 3단계: 멀티 바이너리\n\n```\nproject/\n├── cmd/\n│   ├── api/\n│   ├── worker/\n│   └── admin-cli/\n└── internal/        # 공유\n```\n\n웹 서버 + 워커 + CLI가 같은 로직을 공유할 때.\n\n---\n\n## CLI 도구용 구조\n\n웹 서비스가 아닌 CLI 도구라면:\n\n```\ncmd/myapp/\n└── main.go\n\ninternal/cli/\n├── root.go         # 루트 커맨드\n├── serve.go        # serve 서브커맨드\n└── migrate.go      # migrate 서브커맨드\n```\n\n**패턴:** `cmd.Execute()` 한 줄로 위임\n\n---\n\n## internal/ 우선 전략\n\n| 언제 internal/ | 언제 pkg/ |\n|---------------|----------|\n| 기본값 | 외부에서 import 필요할 때 |\n| 리팩토링 자유 | API 안정성 약속 |\n| 초기 개발 | 프로젝트 성숙 후 |\n\n> **경험칙:** 처음부터 pkg/를 쓰지 마세요. 나중에 필요하면 옮기세요.\n\n---\n\n## 정리\n\n| 원칙 | 설명 |\n|------|------|\n| **Hollow Main** | main.go는 텅 비워두세요 |\n| **internal/app** | 조립 코드는 여기에 |\n| **internal/ 우선** | pkg/는 성숙 후에 |\n| **단계별 진화** | 프로토타입 → 모듈화 → 멀티 바이너리 |\n\n---\n\n## 다음 편 예고\n\n**2편: 견고한 HTTP 서버 구축**에서는 Echo 미들웨어 설계와 Graceful Shutdown을 다룹니다.\n\n---\n\n## 참고 자료\n\n- [golang-standards/project-layout](https://github.com/golang-standards/project-layout)\n- [Kubernetes cmd 구조](https://github.com/kubernetes/kubernetes/tree/master/cmd)\n- [Docker CLI 구조](https://github.com/docker/cli/tree/master/cmd)\n- [Google Wire](https://github.com/google/wire)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Architecture",
      "Backend",
      "DI",
      "Enterprise",
      "Go",
      "Tooling"
    ],
    "readingTime": 4,
    "wordCount": 646,
    "isFeatured": false,
    "isPublic": true,
    "series": "enterprise-go",
    "date": "2026-01-01"
  },
  {
    "id": "turf-polygon-operations",
    "slug": "turf-polygon-operations",
    "path": "frontend/maps",
    "fullPath": "frontend/maps/turf-polygon-operations",
    "title": "Turf.js로 폴리곤 공간 연산 수행하기",
    "excerpt": "Turf.js를 사용하여 멀티폴리곤의 합집합, 차집합, 교집합 연산을 수행하는 방법을 알아봅니다.",
    "content": "# Turf.js로 폴리곤 공간 연산 수행하기\n\n## 개요\n\n**Turf.js**는 브라우저와 Node.js에서 실행되는 GeoJSON 기반 공간 분석 라이브러리입니다. v7부터 `polygon-clipping` 엔진으로 성능과 정확도가 크게 개선되었습니다.\n\n![Union, Difference, Intersect 연산 결과 시각화](/images/turf-operations-demo-image.png)\n\n## 설치\n\n```bash\n# 전체 라이브러리\nnpm install @turf/turf\n\n# 또는 개별 모듈\nnpm install @turf/union @turf/difference @turf/intersect @turf/helpers\n```\n\n## 기본 데이터 생성\n\n### 폴리곤 생성\n\n```javascript\nimport { polygon, multiPolygon, featureCollection } from '@turf/helpers';\n\n// 단일 폴리곤\nconst poly1 = polygon([\n    [[0, 0], [2, 0], [2, 2], [0, 2], [0, 0]]\n]);\n\nconst poly2 = polygon([\n    [[1, 1], [3, 1], [3, 3], [1, 3], [1, 1]]\n]);\n\n// 멀티폴리곤\nconst multiPoly = multiPolygon([\n    [[[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]]],\n    [[[2, 2], [3, 2], [3, 3], [2, 3], [2, 2]]]\n]);\n```\n\n## 합집합 (Union)\n\n여러 폴리곤을 하나로 병합합니다.\n\n```javascript\nimport { union } from '@turf/union';\nimport { featureCollection } from '@turf/helpers';\n\n// v7: FeatureCollection 입력\nconst merged = union(featureCollection([poly1, poly2]));\n// 결과: Polygon 또는 MultiPolygon (연결되지 않으면)\n```\n\n### 여러 폴리곤 병합\n\n```javascript\nimport { union } from '@turf/union';\nimport { featureCollection } from '@turf/helpers';\n\nconst polygons = [poly1, poly2, poly3, poly4];\nconst merged = union(featureCollection(polygons));\n```\n\n### 실용 예: 행정구역 병합\n\n```javascript\n// 경기도 시군구 → 경기도 전체 영역\nconst gyeonggiCities = featureCollection([\n    suwonPolygon,\n    seongnamPolygon,\n    yonginPolygon,\n    // ...\n]);\n\nconst gyeonggiProvince = union(gyeonggiCities);\n```\n\n## 차집합 (Difference)\n\n기준 폴리곤에서 다른 폴리곤 영역을 제거합니다.\n\n```javascript\nimport { difference } from '@turf/difference';\nimport { featureCollection } from '@turf/helpers';\n\n// poly1에서 poly2 영역 제거\nconst subtracted = difference(featureCollection([poly1, poly2]));\n// 결과: Polygon, MultiPolygon, 또는 null (완전히 덮이면)\n```\n\n### 여러 영역 제거\n\n```javascript\n// base에서 exclude1, exclude2 영역 모두 제거\nconst result = difference(featureCollection([base, exclude1, exclude2]));\n```\n\n### 실용 예: 특정 구역 제외\n\n```javascript\n// 서울시 전체에서 한강 영역 제외\nconst seoulLandOnly = difference(featureCollection([\n    seoulBoundary,\n    hanRiverPolygon\n]));\n```\n\n## 교집합 (Intersect)\n\n폴리곤들의 겹치는 영역만 추출합니다.\n\n```javascript\nimport { intersect } from '@turf/intersect';\nimport { featureCollection } from '@turf/helpers';\n\nconst intersection = intersect(featureCollection([poly1, poly2]));\n// 결과: Polygon, MultiPolygon, 또는 null (겹치지 않으면)\n```\n\n### 실용 예: 서비스 가능 영역\n\n```javascript\n// 배달 가능 영역과 사용자 위치 주변 영역의 교집합\nconst deliverableArea = intersect(featureCollection([\n    restaurantDeliveryZone,\n    userRadiusCircle\n]));\n\nif (deliverableArea) {\n    console.log('배달 가능한 영역 존재');\n} else {\n    console.log('배달 불가');\n}\n```\n\n## 연산 결과 검증\n\n### Null 처리\n\n교집합이나 차집합 결과가 없을 수 있습니다:\n\n```javascript\nconst result = intersect(featureCollection([polyA, polyB]));\n\nif (result === null) {\n    console.log('겹치는 영역 없음');\n    return;\n}\n\n// 결과 사용\nconsole.log(result.geometry.type); // 'Polygon' or 'MultiPolygon'\n```\n\n### 면적 검증\n\n```javascript\nimport { area } from '@turf/area';\n\nconst merged = union(featureCollection(polygons));\nconst areaM2 = area(merged);\nconsole.log(`면적: ${(areaM2 / 1000000).toFixed(2)} km²`);\n```\n\n## 복합 연산 예시\n\n### 도넛 폴리곤 생성\n\n```javascript\nimport { difference } from '@turf/difference';\nimport { circle } from '@turf/circle';\nimport { featureCollection } from '@turf/helpers';\n\n// 외부 원 (반경 5km)\nconst outer = circle([127.0, 37.5], 5, { units: 'kilometers' });\n\n// 내부 원 (반경 2km)\nconst inner = circle([127.0, 37.5], 2, { units: 'kilometers' });\n\n// 도넛 형태\nconst donut = difference(featureCollection([outer, inner]));\n```\n\n### 여러 조건 조합\n\n```javascript\nimport { union, difference, intersect } from '@turf/turf';\nimport { featureCollection } from '@turf/helpers';\n\n// 1. 모든 서비스 지역 병합\nconst allServiceAreas = union(featureCollection(servicePolygons));\n\n// 2. 금지 구역 제거\nconst allowedAreas = difference(featureCollection([\n    allServiceAreas,\n    ...restrictedZones\n]));\n\n// 3. 사용자 반경과 교차\nconst finalServiceable = intersect(featureCollection([\n    allowedAreas,\n    userRadiusPolygon\n]));\n```\n\n## 성능 고려사항\n\n### 복잡한 폴리곤\n\n```javascript\nimport { simplify } from '@turf/simplify';\n\n// 연산 전 폴리곤 단순화 (정점 수 감소)\nconst simplified = simplify(complexPolygon, { tolerance: 0.001 });\nconst result = union(featureCollection([simplified, otherPolygon]));\n```\n\n### 유효성 검사\n\n```javascript\nimport { kinks } from '@turf/kinks';\n\n// 자기 교차 검사\nconst selfIntersections = kinks(polygon);\nif (selfIntersections.features.length > 0) {\n    console.warn('자기 교차하는 폴리곤');\n}\n```\n\n## API 정리\n\n| 함수 | 입력 | 출력 | 설명 |\n|------|------|------|------|\n| `union` | FeatureCollection | Polygon/MultiPolygon | 모든 폴리곤 병합 |\n| `difference` | FeatureCollection | Polygon/MultiPolygon/null | 첫 번째에서 나머지 제거 |\n| `intersect` | FeatureCollection | Polygon/MultiPolygon/null | 공통 영역 추출 |\n\n## 모범 사례\n\n1. **FeatureCollection 사용**: v7부터 여러 폴리곤은 FeatureCollection으로 전달\n2. **Null 체크 필수**: 교집합/차집합 결과가 없을 수 있음\n3. **단순화 먼저**: 복잡한 폴리곤은 `simplify` 후 연산\n4. **좌표계 통일**: 모든 입력은 동일한 SRID (보통 4326)\n\n## 참고 자료\n\n- [Turf.js Documentation](https://turfjs.org/)\n- [Turf.js v7 Release Notes](https://github.com/Turfjs/turf/releases/tag/v7.0.0)",
    "docType": "original",
    "category": "Frontend",
    "tags": [
      "Geospatial",
      "JavaScript"
    ],
    "readingTime": 4,
    "wordCount": 714,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "mapbox-gl-guide",
    "slug": "mapbox-gl-guide",
    "path": "frontend/maps",
    "fullPath": "frontend/maps/mapbox-gl-guide",
    "title": "Mapbox GL JS로 인터랙티브 지도 구현하기",
    "excerpt": "Mapbox GL JS v3로 Earth2 스타일 격자 그리드와 네이버부동산 스타일 영역 레이어를 구현하는 방법을 알아봅니다.",
    "content": "# Mapbox GL JS로 인터랙티브 지도 구현하기\n\n## 개요\n\n**Mapbox GL JS**는 WebGL 기반 벡터 타일 지도 라이브러리입니다. 이 글에서는 두 가지 실용적인 패턴을 다룹니다:\n\n1. **격자 그리드 레이어**: Earth2 스타일의 바둑판 오버레이\n2. **영역 폴리곤 레이어**: 네이버부동산 스타일의 지역 표시\n\n## 설치 및 초기화\n\n```bash\nnpm install mapbox-gl @turf/turf\n```\n\n```javascript\nimport mapboxgl from 'mapbox-gl';\nimport 'mapbox-gl/dist/mapbox-gl.css';\n\nmapboxgl.accessToken = 'YOUR_ACCESS_TOKEN';\n\nconst map = new mapboxgl.Map({\n    container: 'map',\n    style: 'mapbox://styles/mapbox/dark-v11',  // 격자가 잘 보이는 다크 테마\n    center: [127.0, 37.5],\n    zoom: 12\n});\n```\n\n---\n\n## 예시 1: Earth2 스타일 격자 그리드\n\n전 세계를 일정한 크기의 셀로 나누어 바둑판처럼 표시합니다.\n\n![격자 그리드 데모 - 클릭으로 셀 선택 가능](/images/mapbox-grid-demo-image.png)\n\n### 격자 생성 (Turf.js)\n\n```javascript\nimport { squareGrid, bboxPolygon } from '@turf/turf';\n\n// 현재 뷰포트 기준 격자 생성\nfunction createGrid(bounds, cellSize = 0.01) {\n    const bbox = [bounds.getWest(), bounds.getSouth(), bounds.getEast(), bounds.getNorth()];\n    \n    // cellSize: 도 단위 (0.01도 ≈ 약 1km)\n    const grid = squareGrid(bbox, cellSize, { units: 'degrees' });\n    \n    // 각 셀에 고유 ID 부여\n    grid.features.forEach((cell, i) => {\n        cell.id = i;\n        cell.properties.cellId = `cell_${i}`;\n    });\n    \n    return grid;\n}\n```\n\n### 격자 레이어 추가\n\n```javascript\nmap.on('load', () => {\n    const grid = createGrid(map.getBounds(), 0.005);\n    \n    // 소스 추가\n    map.addSource('grid', {\n        type: 'geojson',\n        data: grid,\n        generateId: true\n    });\n    \n    // 격자 면 (Fill)\n    map.addLayer({\n        id: 'grid-fill',\n        type: 'fill',\n        source: 'grid',\n        paint: {\n            'fill-color': [\n                'case',\n                ['boolean', ['feature-state', 'selected'], false], '#00ff88',\n                ['boolean', ['feature-state', 'hover'], false], '#ffffff',\n                'transparent'\n            ],\n            'fill-opacity': [\n                'case',\n                ['boolean', ['feature-state', 'selected'], false], 0.6,\n                ['boolean', ['feature-state', 'hover'], false], 0.2,\n                0\n            ]\n        }\n    });\n    \n    // 격자 선 (Line)\n    map.addLayer({\n        id: 'grid-line',\n        type: 'line',\n        source: 'grid',\n        paint: {\n            'line-color': '#00ff88',\n            'line-width': 0.5,\n            'line-opacity': 0.5\n        }\n    });\n});\n```\n\n### 격자 인터랙션\n\n```javascript\nlet hoveredCellId = null;\nconst selectedCells = new Set();\n\n// 호버\nmap.on('mousemove', 'grid-fill', (e) => {\n    if (e.features.length > 0) {\n        if (hoveredCellId !== null) {\n            map.setFeatureState({ source: 'grid', id: hoveredCellId }, { hover: false });\n        }\n        hoveredCellId = e.features[0].id;\n        map.setFeatureState({ source: 'grid', id: hoveredCellId }, { hover: true });\n    }\n});\n\nmap.on('mouseleave', 'grid-fill', () => {\n    if (hoveredCellId !== null) {\n        map.setFeatureState({ source: 'grid', id: hoveredCellId }, { hover: false });\n    }\n    hoveredCellId = null;\n});\n\n// 클릭으로 셀 선택/해제\nmap.on('click', 'grid-fill', (e) => {\n    const cellId = e.features[0].id;\n    const isSelected = selectedCells.has(cellId);\n    \n    if (isSelected) {\n        selectedCells.delete(cellId);\n        map.setFeatureState({ source: 'grid', id: cellId }, { selected: false });\n    } else {\n        selectedCells.add(cellId);\n        map.setFeatureState({ source: 'grid', id: cellId }, { selected: true });\n    }\n    \n    console.log('선택된 셀:', [...selectedCells]);\n});\n```\n\n### 뷰포트 변경 시 격자 갱신\n\n```javascript\nmap.on('moveend', () => {\n    const newGrid = createGrid(map.getBounds(), 0.005);\n    map.getSource('grid').setData(newGrid);\n});\n```\n\n---\n\n## 예시 2: 네이버부동산 스타일 영역 레이어\n\n행정구역별로 색상과 라벨을 표시하고, 클릭 시 상세 정보를 보여줍니다.\n\n![부동산 스타일 영역 레이어 - 가격별 색상과 라벨](/images/mapbox-district-demo-image.png)\n\n### GeoJSON 데이터 구조\n\n```javascript\nconst districtsData = {\n    type: 'FeatureCollection',\n    features: [\n        {\n            type: 'Feature',\n            id: 1,\n            properties: {\n                name: '강남구',\n                avgPrice: 15000000,  // 평당 가격\n                changeRate: 2.5      // 변동률 %\n            },\n            geometry: {\n                type: 'Polygon',\n                coordinates: [[[127.0, 37.5], [127.1, 37.5], [127.1, 37.52], [127.0, 37.52], [127.0, 37.5]]]\n            }\n        },\n        // ...\n    ]\n};\n```\n\n### 영역 레이어 추가\n\n```javascript\nmap.on('load', () => {\n    map.addSource('districts', {\n        type: 'geojson',\n        data: districtsData,\n        generateId: true\n    });\n    \n    // 영역 면 - 가격에 따른 색상\n    map.addLayer({\n        id: 'districts-fill',\n        type: 'fill',\n        source: 'districts',\n        paint: {\n            'fill-color': [\n                'interpolate',\n                ['linear'],\n                ['get', 'avgPrice'],\n                5000000, '#fef0d9',    // 낮은 가격: 연한 색\n                10000000, '#fdbb84',\n                15000000, '#fc8d59',\n                20000000, '#d7301f'    // 높은 가격: 진한 색\n            ],\n            'fill-opacity': [\n                'case',\n                ['boolean', ['feature-state', 'hover'], false], 0.9,\n                0.6\n            ]\n        }\n    });\n    \n    // 영역 경계선\n    map.addLayer({\n        id: 'districts-outline',\n        type: 'line',\n        source: 'districts',\n        paint: {\n            'line-color': '#ffffff',\n            'line-width': [\n                'case',\n                ['boolean', ['feature-state', 'hover'], false], 3,\n                1\n            ]\n        }\n    });\n    \n    // 라벨 (구 이름 + 가격)\n    map.addLayer({\n        id: 'districts-label',\n        type: 'symbol',\n        source: 'districts',\n        layout: {\n            'text-field': [\n                'format',\n                ['get', 'name'], { 'font-scale': 1.2 },\n                '\\n', {},\n                ['concat', ['to-string', ['/', ['get', 'avgPrice'], 10000]], '만원'], { 'font-scale': 0.9 }\n            ],\n            'text-font': ['DIN Pro Medium', 'Arial Unicode MS Bold'],\n            'text-size': 12,\n            'text-anchor': 'center'\n        },\n        paint: {\n            'text-color': '#ffffff',\n            'text-halo-color': '#000000',\n            'text-halo-width': 1\n        }\n    });\n});\n```\n\n### 호버 및 팝업\n\n```javascript\nlet hoveredDistrictId = null;\n\nmap.on('mousemove', 'districts-fill', (e) => {\n    map.getCanvas().style.cursor = 'pointer';\n    \n    if (e.features.length > 0) {\n        if (hoveredDistrictId !== null) {\n            map.setFeatureState({ source: 'districts', id: hoveredDistrictId }, { hover: false });\n        }\n        hoveredDistrictId = e.features[0].id;\n        map.setFeatureState({ source: 'districts', id: hoveredDistrictId }, { hover: true });\n    }\n});\n\nmap.on('mouseleave', 'districts-fill', () => {\n    map.getCanvas().style.cursor = '';\n    if (hoveredDistrictId !== null) {\n        map.setFeatureState({ source: 'districts', id: hoveredDistrictId }, { hover: false });\n    }\n    hoveredDistrictId = null;\n});\n\n// 클릭 시 상세 팝업\nmap.on('click', 'districts-fill', (e) => {\n    const props = e.features[0].properties;\n    const changeColor = props.changeRate >= 0 ? '#ff4444' : '#4444ff';\n    const changeSign = props.changeRate >= 0 ? '+' : '';\n    \n    new mapboxgl.Popup()\n        .setLngLat(e.lngLat)\n        .setHTML(`\n            <div style=\"padding: 8px;\">\n                <h3 style=\"margin: 0 0 8px 0;\">${props.name}</h3>\n                <p style=\"margin: 4px 0;\">평당 가격: <b>${(props.avgPrice / 10000).toFixed(0)}만원</b></p>\n                <p style=\"margin: 4px 0; color: ${changeColor};\">\n                    변동률: ${changeSign}${props.changeRate}%\n                </p>\n            </div>\n        `)\n        .addTo(map);\n});\n```\n\n### 범례 추가\n\n```html\n<div id=\"legend\" style=\"position: absolute; bottom: 20px; left: 20px; background: white; padding: 10px; border-radius: 4px;\">\n    <h4>평당 가격</h4>\n    <div><span style=\"background: #fef0d9; width: 20px; height: 10px; display: inline-block;\"></span> 500만 이하</div>\n    <div><span style=\"background: #fdbb84; width: 20px; height: 10px; display: inline-block;\"></span> 500-1000만</div>\n    <div><span style=\"background: #fc8d59; width: 20px; height: 10px; display: inline-block;\"></span> 1000-1500만</div>\n    <div><span style=\"background: #d7301f; width: 20px; height: 10px; display: inline-block;\"></span> 1500만 이상</div>\n</div>\n```\n\n---\n\n## 공통: 동적 데이터 로드\n\n### API에서 GeoJSON 로드\n\n```javascript\nasync function loadDistricts() {\n    const response = await fetch('/api/districts?city=seoul');\n    const geojson = await response.json();\n    \n    if (map.getSource('districts')) {\n        map.getSource('districts').setData(geojson);\n    } else {\n        map.addSource('districts', { type: 'geojson', data: geojson, generateId: true });\n        // 레이어 추가...\n    }\n}\n```\n\n### 필터링\n\n```javascript\n// 특정 조건만 표시\nmap.setFilter('districts-fill', ['>', ['get', 'avgPrice'], 10000000]);\n\n// 필터 해제\nmap.setFilter('districts-fill', null);\n```\n\n## 모범 사례\n\n1. **generateId 필수**: Feature State 활용 시 필요\n2. **load 이벤트 후 작업**: 소스/레이어는 반드시 `map.on('load', ...)` 내에서\n3. **호버 상태 정리**: `mouseleave`에서 상태 초기화 필수\n4. **성능**: 격자는 뷰포트 범위로 제한, 전체 로드 금지\n5. **메모리**: 컴포넌트 언마운트 시 `map.remove()` 호출\n\n## 참고 자료\n\n- [Mapbox GL JS v3 Documentation](https://docs.mapbox.com/mapbox-gl-js/)\n- [Turf.js squareGrid](https://turfjs.org/docs/#squareGrid)",
    "docType": "original",
    "category": "Frontend",
    "tags": [
      "Geospatial",
      "JavaScript",
      "Visualization"
    ],
    "readingTime": 5,
    "wordCount": 998,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "streams-audit-pipeline",
    "slug": "streams-audit-pipeline",
    "path": "database/redis",
    "fullPath": "database/redis/streams-audit-pipeline",
    "title": "Redis Streams 기반 비동기 감사 파이프라인 구축",
    "excerpt": "Redis Streams와 Consumer Group을 활용하여 At-least-once 전달과 Dead Letter 처리를 지원하는 비동기 감사 파이프라인을 구축하는 방법을 알아봅니다.",
    "content": "# Redis Streams 기반 비동기 감사 파이프라인 구축\n\n## 개요\n\n데이터 변경 이벤트를 안정적으로 처리해야 하는 감사(Audit) 시스템에서 **Redis Streams**는 강력한 선택입니다. 이 글에서는 메시지 유실 없는 감사 파이프라인을 설계하고, 특히 처리 실패 시의 복구 메커니즘을 포함한 구현 방법을 다룹니다.\n\n## 왜 Redis Streams인가?\n\n### 장점\n\n| 특성 | 설명 |\n|------|------|\n| **At-least-once 전달** | ACK 메커니즘과 PEL을 통한 메시지 유실 방지 |\n| **순서 보장** | 스트림 내 메시지 인입 순서 유지 |\n| **고성능** | 인메모리 기반 높은 처리량 |\n| **영속성** | AOF/RDB 설정으로 데이터 지속성 보장 |\n| **Consumer Group** | 여러 워커 간 수평 확장 및 부하 분산 |\n\n### 단점\n\n| 특성 | 설명 |\n|------|------|\n| **메모리 압박** | 데이터가 누적되므로 `XTRIM` 등을 통한 관리가 필수적임 |\n| **상태 관리 복잡성** | Pending 메시지(PEL) 및 재시도 로직을 직접 제어해야 함 |\n\n## 아키텍처 설계\n\n### 파이프라인 흐름\n\n```mermaid\ngraph TD\n    %% Nodes\n    API[\"API Server\"]\n    RS[\"Redis Stream<br/>(audit-events)\"]\n    AW[\"Audit Worker<br/>(Consumer)\"]\n    DLS[\"Dead Letter Stream\"]\n    DLW[\"Dead Letter Worker\"]\n\n    %% Flow\n    API --> RS\n    RS --> AW\n    \n    %% Error Flow\n    AW -- \"실패 시\" --> DLS\n    RS -. \"파싱 실패 시\" .-> DLS\n    \n    DLS --> DLW\n\n    %% Styling\n    style API fill:#f9f9f9,stroke:#333\n    style RS fill:#e1f5fe,stroke:#01579b\n    style AW fill:#e1f5fe,stroke:#01579b\n    style DLS fill:#fff3e0,stroke:#e65100\n    style DLW fill:#fff3e0,stroke:#e65100\n```\n\n### 메시지 구조\n\nRedis Streams의 메시지는 불변(Immutable)입니다. 따라서 재시도 횟수 같은 상태값은 메시지 내부에 담기보다 Redis가 제공하는 metadata(delivery count)를 활용하는 것이 효율적입니다.\n\n```go\npackage audit\n\nimport \"time\"\n\n// EventMessage는 감사 이벤트 메시지입니다.\ntype EventMessage struct {\n    ID            string                 `json:\"id\"`\n    Collection    string                 `json:\"collection\"`\n    DocumentURI   string                 `json:\"document_uri\"`\n    Action        string                 `json:\"action\"` // CREATE, UPDATE, DELETE\n    Version       int32                  `json:\"version\"`\n    Payload       map[string]interface{} `json:\"payload\"`\n    Timestamp     time.Time              `json:\"timestamp\"`\n}\n\n```\n\n## 핵심 구현\n\n### 메시지 발행자 (Producer)\n\n```go\npackage audit\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"fmt\"\n    \n    \"[github.com/redis/go-redis/v9](https://github.com/redis/go-redis/v9)\"\n)\n\nconst (\n    AuditStreamKey = \"audit-events-stream\"\n)\n\ntype EventProducer struct {\n    client redis.UniversalClient\n}\n\nfunc NewEventProducer(client redis.UniversalClient) *EventProducer {\n    return &EventProducer{client: client}\n}\n\n// Publish는 이벤트를 스트림에 발행합니다.\nfunc (p *EventProducer) Publish(ctx context.Context, event *EventMessage) (string, error) {\n    payload, err := json.Marshal(event)\n    if err != nil {\n        return \"\", fmt.Errorf(\"marshal event: %w\", err)\n    }\n    \n    messageID, err := p.client.XAdd(ctx, &redis.XAddArgs{\n        Stream: AuditStreamKey,\n        Values: map[string]interface{}{\n            \"payload\": payload,\n        },\n    }).Result()\n    \n    if err != nil {\n        return \"\", fmt.Errorf(\"xadd: %w\", err)\n    }\n    \n    return messageID, nil\n}\n\n```\n\n### 워커 (Consumer)\n\nRedis Streams에서 `>` ID는 새 메시지만을 의미합니다. 프로세스 재시작 시 처리되지 못한 메시지를 복구하려면 `0` ID를 통해 PEL(Pending Entries List)을 먼저 확인해야 합니다.\n\n```go\n// processMessages는 새 메시지와 Pending 메시지를 함께 처리합니다.\nfunc (w *StreamWorker) processMessages(ctx context.Context) {\n    // 전략: PEL(Pending Entries List)이 비워질 때까지 우선 처리한 후 새 메시지로 이동\n    \n    for {\n        // 1. 본인의 Pending 메시지(\"0\") 확인\n        msgs, err := w.readBatch(ctx, \"0\")\n        if err != nil || len(msgs) == 0 {\n            break // 더 이상 처리할 Pending 메시지가 없으면 탈출\n        }\n        \n        for _, m := range msgs {\n            w.handleMessage(ctx, m)\n        }\n        \n        // BatchSize보다 적게 가져왔다면 PEL이 거의 비어있다는 뜻이므로 다음으로 진행\n        if int64(len(msgs)) < w.config.BatchSize {\n            break\n        }\n    }\n\n    // 2. 이제 새 메시지(\">\") 처리\n    msgs, _ := w.readBatch(ctx, \">\")\n    for _, m := range msgs {\n        w.handleMessage(ctx, m)\n    }\n}\n\n// 공통 읽기 로직 분리\nfunc (w *StreamWorker) readBatch(ctx context.Context, id string) ([]redis.XMessage, error) {\n    streams, err := w.client.XReadGroup(ctx, &redis.XReadGroupArgs{\n        Group:    w.config.Group,\n        Consumer: w.config.Consumer,\n        Streams:  []string{w.config.Stream, id},\n        Count:    w.config.BatchSize,\n        Block:    w.config.PollInterval,\n    }).Result()\n    \n    if err != nil || len(streams) == 0 {\n        return nil, err\n    }\n    return streams[0].Messages, nil\n}\n\nfunc (w *StreamWorker) handleMessage(ctx context.Context, msg redis.XMessage) {\n    var event EventMessage\n    \n    // 안전한 타입 단언\n    payloadRaw, ok := msg.Values[\"payload\"].(string)\n    if !ok {\n        w.moveToDeadLetter(ctx, msg, fmt.Errorf(\"invalid payload type\"))\n        return\n    }\n\n    if err := json.Unmarshal([]byte(payloadRaw), &event); err != nil {\n        w.moveToDeadLetter(ctx, msg, err)\n        return\n    }\n\n    // 핸들러 호출\n    errs := w.handler.Handle(ctx, []*EventMessage{&event})\n    \n    if len(errs) > 0 && errs[0] != nil {\n        w.handleFailure(ctx, msg, errs[0])\n        return\n    }\n    \n    // 성공 시 ACK를 보내 PEL에서 제거\n    w.client.XAck(ctx, w.config.Stream, w.config.Group, msg.ID)\n}\n\nfunc (w *StreamWorker) handleFailure(ctx context.Context, msg redis.XMessage, err error) {\n    // XPENDING으로 현재 메시지의 전달 횟수(delivery count) 확인\n    pends, _ := w.client.XPendingExt(ctx, &redis.XPendingExtArgs{\n        Stream: w.config.Stream,\n        Group:  w.config.Group,\n        Start:  msg.ID,\n        End:    msg.ID,\n        Count:  1,\n    }).Result()\n\n    if len(pends) > 0 && int(pends[0].Count) >= w.config.MaxRetries {\n        // 최대 재시도 초과 시 Dead Letter 이동 후 ACK\n        w.moveToDeadLetter(ctx, msg, err)\n        w.client.XAck(ctx, w.config.Stream, w.config.Group, msg.ID)\n        return\n    }\n    \n    // ACK를 하지 않으면 다음 \"0\" ID 조회 시 재전달됨\n}\n\nfunc (w *StreamWorker) moveToDeadLetter(ctx context.Context, msg redis.XMessage, reason error) {\n    w.client.XAdd(ctx, &redis.XAddArgs{\n        Stream: w.config.DeadLetterStream,\n        Values: map[string]interface{}{\n            \"original_id\": msg.ID,\n            \"payload\":     msg.Values[\"payload\"],\n            \"error\":       reason.Error(),\n            \"failed_at\":   time.Now().Format(time.RFC3339),\n        },\n    })\n}\n\n```\n\n## 모범 사례\n\n1. **ID \"0\"과 \">\"의 조합**: `>`만 사용하면 장애 발생 시 처리 중이던 메시지가 영원히 Pending 상태로 남게 됩니다. 반드시 `0` ID 조회를 병행하세요.\n2. **ACK 신중히**: 로직이 완전히 성공한 후에만 `XACK`를 호출해야 At-least-once를 보장할 수 있습니다.\n3. **타입 안전성**: Redis 데이터 유효성을 믿지 마세요. 타입 단언 시 반드시 `ok` 패턴을 사용해 패닉을 방지해야 합니다.\n4. **XTRIM 정기 실행**: 감사 로그는 양이 매우 많으므로 스트림 생성 시 혹은 주기적으로 `XTRIM`을 수행해 메모리를 관리하세요.\n\n## 참고 자료\n\n* [Redis Streams 공식 문서](https://redis.io/docs/data-types/streams/)\n* [go-redis 라이브러리](https://github.com/redis/go-redis)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Architecture",
      "Database",
      "Go",
      "Kafka",
      "Redis",
      "Streaming"
    ],
    "readingTime": 5,
    "wordCount": 819,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "postgis-geojson-guide",
    "slug": "postgis-geojson-guide",
    "path": "database/postgresql",
    "fullPath": "database/postgresql/postgis-geojson-guide",
    "title": "PostGIS와 GeoJSON을 활용한 공간 데이터 관리",
    "excerpt": "PostGIS에서 GeoJSON 형식의 공간 데이터를 저장, 변환, 조회하는 핵심 함수와 인덱싱 전략을 알아봅니다.",
    "content": "# PostGIS와 GeoJSON을 활용한 공간 데이터 관리\n\n## 개요\n\n**PostGIS**는 PostgreSQL의 공간 확장으로, 지리 데이터를 저장하고 쿼리할 수 있습니다. **GeoJSON**은 지리 정보를 표현하는 JSON 기반 표준(RFC 7946)입니다.\n\n## 테이블 설계\n\n### 기본 공간 테이블\n\n```sql\nCREATE EXTENSION IF NOT EXISTS postgis;\n\nCREATE TABLE regions (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    -- SRID 4326 = WGS84 (위경도)\n    geom GEOMETRY(MultiPolygon, 4326)\n);\n\n-- 공간 인덱스 필수\nCREATE INDEX idx_regions_geom ON regions USING GIST(geom);\n```\n\n### SRID 선택\n\n| SRID | 좌표계 | 용도 |\n|------|--------|------|\n| 4326 | WGS84 위경도 | GeoJSON 표준, GPS |\n| 3857 | Web Mercator | 웹 지도 타일 |\n| 5186 | Korea TM | 한국 측량 |\n\n## GeoJSON → PostGIS 저장\n\n### ST_GeomFromGeoJSON\n\nGeoJSON 문자열을 Geometry로 변환:\n\n```sql\n-- 단일 폴리곤 삽입\nINSERT INTO regions (name, geom)\nVALUES (\n    '서울시',\n    ST_GeomFromGeoJSON('{\n        \"type\": \"Polygon\",\n        \"coordinates\": [[[126.9, 37.5], [127.1, 37.5], [127.1, 37.6], [126.9, 37.6], [126.9, 37.5]]]\n    }')\n);\n\n-- SRID 명시 (PostGIS 3.0+는 기본 4326)\nINSERT INTO regions (name, geom)\nVALUES (\n    '부산시',\n    ST_SetSRID(ST_GeomFromGeoJSON('{\"type\": \"Polygon\", \"coordinates\": ...}'), 4326)\n);\n```\n\n### 배치 삽입\n\n```sql\n-- JSON 배열에서 일괄 삽입\nINSERT INTO regions (name, geom)\nSELECT \n    feature->>'name',\n    ST_GeomFromGeoJSON(feature->'geometry')\nFROM jsonb_array_elements(:geojson_features::jsonb) AS feature;\n```\n\n## PostGIS → GeoJSON 조회\n\n### ST_AsGeoJSON\n\nGeometry를 GeoJSON 문자열로 변환:\n\n```sql\n-- 기본 변환\nSELECT id, name, ST_AsGeoJSON(geom) AS geojson\nFROM regions;\n\n-- 좌표 소수점 6자리로 제한 (위경도 약 0.1m 정밀도)\nSELECT ST_AsGeoJSON(geom, 6) FROM regions;\n\n-- BBox 포함 (옵션 1)\nSELECT ST_AsGeoJSON(geom, 6, 1) FROM regions;\n```\n\n### FeatureCollection 생성\n\n클라이언트에 전달할 완전한 GeoJSON 생성:\n\n```sql\nSELECT json_build_object(\n    'type', 'FeatureCollection',\n    'features', json_agg(\n        json_build_object(\n            'type', 'Feature',\n            'id', id,\n            'geometry', ST_AsGeoJSON(geom, 6)::json,\n            'properties', json_build_object('name', name)\n        )\n    )\n) AS geojson\nFROM regions;\n```\n\n## 공간 쿼리\n\n### 포함 관계\n\n```sql\n-- 특정 좌표가 어느 지역에 속하는지\nSELECT name FROM regions\nWHERE ST_Contains(geom, ST_SetSRID(ST_Point(127.0, 37.5), 4326));\n```\n\n### 반경 검색\n\n```sql\n-- 반경 10km 내 지역 (Geography 타입 활용)\nSELECT name, ST_Distance(geom::geography, ST_Point(127.0, 37.5)::geography) AS distance_m\nFROM regions\nWHERE ST_DWithin(geom::geography, ST_Point(127.0, 37.5)::geography, 10000)\nORDER BY distance_m;\n```\n\n### 교차 영역\n\n```sql\n-- 두 지역의 교집합\nSELECT ST_AsGeoJSON(ST_Intersection(a.geom, b.geom))\nFROM regions a, regions b\nWHERE a.name = '서울시' AND b.name = '경기도';\n```\n\n## 인덱스 전략\n\n### GIST 인덱스\n\n공간 쿼리 성능의 핵심:\n\n```sql\n-- 기본 공간 인덱스\nCREATE INDEX idx_geom ON regions USING GIST(geom);\n\n-- Geography 인덱스 (거리 계산용)\nCREATE INDEX idx_geom_geog ON regions USING GIST((geom::geography));\n```\n\n### 클러스터링\n\n자주 함께 조회되는 데이터를 물리적으로 인접 배치:\n\n```sql\nCLUSTER regions USING idx_regions_geom;\n```\n\n## 좌표계 변환\n\n```sql\n-- WGS84 → Web Mercator\nSELECT ST_Transform(geom, 3857) FROM regions;\n\n-- 면적 계산 (정확한 계산을 위해 적절한 투영 사용)\nSELECT name, ST_Area(ST_Transform(geom, 5186)) AS area_m2\nFROM regions;\n```\n\n## 폴리곤 유효성\n\n```sql\n-- 유효성 검사\nSELECT name, ST_IsValid(geom), ST_IsValidReason(geom)\nFROM regions\nWHERE NOT ST_IsValid(geom);\n\n-- 자동 수정\nUPDATE regions SET geom = ST_MakeValid(geom) WHERE NOT ST_IsValid(geom);\n\n-- GeoJSON 표준 준수 (Right-Hand Rule)\nUPDATE regions SET geom = ST_ForcePolygonCCW(geom);\n```\n\n## 모범 사례\n\n1. **SRID 4326**: GeoJSON과 호환, 웹 서비스 표준\n2. **GIST 인덱스 필수**: 공간 쿼리 성능 결정\n3. **좌표 정밀도 제한**: `ST_AsGeoJSON(geom, 6)`으로 불필요한 정밀도 제거\n4. **유효성 검사**: 삽입 전 `ST_IsValid` 확인\n5. **Geography 타입**: 거리 계산이 중요하면 Geography 사용\n\n## 참고 자료\n\n- [PostGIS 3.4 Documentation](https://postgis.net/docs/)\n- [GeoJSON RFC 7946](https://datatracker.ietf.org/doc/html/rfc7946)",
    "docType": "original",
    "category": "Database",
    "tags": [
      "Database",
      "Geospatial",
      "PostgreSQL"
    ],
    "readingTime": 3,
    "wordCount": 538,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "wiredtiger-storage-engine",
    "slug": "wiredtiger-storage-engine",
    "path": "database/mongodb",
    "fullPath": "database/mongodb/wiredtiger-storage-engine",
    "title": "MongoDB WiredTiger 스토리지 엔진 이해하기",
    "excerpt": "MongoDB의 기본 스토리지 엔진인 WiredTiger의 아키텍처, 캐시, 체크포인트, Lock-Free 알고리즘까지 깊이 있게 알아봅니다.",
    "content": "# MongoDB WiredTiger 스토리지 엔진 이해하기\n\n## 개요\n\n**WiredTiger**는 MongoDB 3.2부터 기본 스토리지 엔진으로 채택된 고성능 스토리지 엔진입니다. 문서 수준 동시성 제어와 압축을 지원하며, 대부분의 워크로드에서 뛰어난 성능을 제공합니다.\n\n## 핵심 아키텍처\n\n### 문서 수준 잠금 (Document-Level Locking)\n\nWiredTiger의 가장 큰 장점은 **문서 수준의 동시성 제어**입니다.\n\n| 스토리지 엔진 | 잠금 수준 | 동시성 |\n|-------------|---------|-------|\n| MMAPv1 (레거시) | 컬렉션 수준 | 낮음 |\n| WiredTiger | 문서 수준 | 높음 |\n\n```javascript\n// 서로 다른 문서에 대한 동시 쓰기가 병렬로 처리됨\ndb.users.updateOne({ _id: 1 }, { $set: { name: \"Alice\" } })\ndb.users.updateOne({ _id: 2 }, { $set: { name: \"Bob\" } })  // 블로킹 없음\n```\n\n### MVCC (Multi-Version Concurrency Control)\n\nWiredTiger는 **MVCC**를 사용하여 읽기와 쓰기 작업이 서로를 차단하지 않습니다.\n\n- 읽기 작업: 시작 시점의 스냅샷을 참조\n- 쓰기 작업: 새 버전 생성\n- 충돌 시: 자동 재시도 메커니즘\n\n---\n\n## 데이터 적용 순서\n\nMongoDB에서 데이터가 저장되는 순서를 이해하는 것이 중요합니다:\n\n```\n1. WAL (Write-Ahead Log) 기록\n2. Data Memory 적용 (공유 캐시)\n3. OpLog 기록 (Replica Set용)\n4. Disk Flush (체크포인트)\n```\n\n이 순서 덕분에 장애 시에도 WAL을 통해 데이터를 복구할 수 있습니다.\n\n---\n\n## 공유 캐시 (Shared Cache)\n\nWiredTiger의 공유 캐시는 MySQL의 Buffer Pool과 유사한 역할을 합니다.\n\n### MySQL Buffer Pool과의 차이\n\n| 특성 | MySQL Buffer Pool | WiredTiger Cache |\n|-----|-------------------|------------------|\n| 캐싱 방식 | B-Tree 상의 주소 사용 | 메모리 주소로 변환하여 적재 |\n| 캐싱 속도 | 빠름 | 상대적으로 느림 (변환 과정) |\n| 읽기 성능 | 보통 | 캐싱 후 더 빠름 |\n\nWiredTiger는 데이터를 메모리에 적합한 트리 형태로 **재구성**하여 적재합니다. 이 변환 과정으로 인해 초기 캐싱은 느릴 수 있지만, 일단 캐싱되면 RDB보다 빠른 읽기 성능을 제공합니다.\n\n### 캐시 설정\n\n```yaml\n# mongod.conf\nstorage:\n  wiredTiger:\n    engineConfig:\n      cacheSizeGB: 4  # 권장: (RAM - 1GB) / 2\n```\n\n**기본 캐시 크기 계산:**\n\n- (RAM - 1GB) × 50%\n- 또는 256MB 중 큰 값\n\n---\n\n## Lock-Free 알고리즘\n\nWiredTiger는 높은 동시성을 위해 **Lock-Free 알고리즘**을 사용합니다.\n\n### Hazard Pointer\n\n- 현재 데이터를 참조하고 있는 메모리 주소를 Hazard Pointer에 등록\n- 캐시에서 제거(Eviction) 시 Hazard Pointer에 등록된 데이터는 보호\n- Disk Flush 여부도 Hazard Pointer를 통해 결정\n\n### Skip List\n\n- Undo Log를 Skip List로 관리\n- 레코드 변경 시 이전 버전을 Skip List에 추가\n- MVCC 구현의 핵심 자료구조\n\n---\n\n## Cache Eviction\n\n캐시의 빈 공간을 적절히 유지하여 새 데이터를 적재할 수 있도록 합니다.\n\n### 동작 방식\n\n- **백그라운드 스레드**: 평상시 Eviction 처리\n- **포그라운드 스레드**: 백그라운드가 공간 확보 실패 시 직접 수행 (성능 저하 발생)\n\n### 튜닝 파라미터\n\n| 파라미터 | 설명 | 기본값 |\n|---------|------|-------|\n| `threads_max` | Eviction 스레드 최대 개수 | 4 (1~20) |\n| `threads_min` | Eviction 스레드 최소 개수 | 1 (1~20) |\n| `eviction_dirty_target` | 더티 페이지 비율 유지 목표 | 5% |\n| `eviction_target` | 전체 캐시 사용률 목표 | 80% |\n\n```yaml\n# 고급 Eviction 설정\nstorage:\n  wiredTiger:\n    engineConfig:\n      eviction:\n        threads_max: 8\n        threads_min: 4\n```\n\n> **주의**: Eviction이 포그라운드에서 실행되면 쓰기 성능이 급격히 저하됩니다. `cache eviction` 관련 지표를 모니터링하세요.\n\n---\n\n## 데이터 압축\n\nWiredTiger는 두 수준의 압축을 지원합니다:\n\n### 컬렉션 데이터 압축\n\n```javascript\n// 컬렉션 생성 시 압축 알고리즘 지정\ndb.createCollection(\"logs\", {\n  storageEngine: {\n    wiredTiger: {\n      configString: \"block_compressor=zstd\"\n    }\n  }\n})\n```\n\n| 알고리즘 | 압축률 | 속도 | 용도 |\n|---------|-------|-----|------|\n| `snappy` (기본) | 보통 | 빠름 | 범용 |\n| `zlib` | 높음 | 느림 | 아카이브 |\n| `zstd` | 높음 | 빠름 | **권장** (4.2+) |\n| `none` | - | - | 실시간 처리 |\n\n> **zstd 권장 이유**: 무손실 압축이면서 압축/해제 속도가 준수함\n\n### 인덱스 프리픽스 압축\n\n인덱스는 기본적으로 **프리픽스 압축**이 적용됩니다:\n\n```javascript\n// 인덱스 압축 비활성화 예시\ndb.collection.createIndex(\n  { field: 1 },\n  { storageEngine: { wiredTiger: { configString: \"prefix_compression=false\" } } }\n)\n```\n\n---\n\n## 체크포인트 (Checkpoint)\n\n체크포인트는 **데이터 파일과 트랜잭션 로그가 동기화되는 시점**입니다. DB 장애 시 복구 시점을 결정하는 기준이 됩니다.\n\n### Sharp Checkpoint\n\nMongoDB는 **Sharp Checkpoint** 방식을 사용합니다:\n\n- 체크포인트 실행 시점에 더티 페이지를 **한 번에 모아서** 디스크에 내려씀\n- Fuzzy Checkpoint(점진적 방식)와 대비되는 개념\n\n### 체크포인트 트리거\n\n기본적으로 다음 조건에서 체크포인트가 발생합니다:\n\n- **60초** 경과\n- **2GB** 저널 데이터 누적\n\n### 체크포인트 옵션\n\n| 옵션 | 설명 | 기본값 |\n|-----|------|-------|\n| `log_size` | 이 크기만큼 트랜잭션 로그 쓰면 체크포인트 실행 | 0 (자동) |\n| `wait` | 주기적 체크포인트 간격 (초) | 0 (자동) |\n\n```yaml\n# mongod.conf\nstorage:\n  wiredTiger:\n    engineConfig:\n      checkpointSizeMB: 1024\n```\n\n---\n\n## 저널링 (Journaling)\n\nWrite-Ahead Logging(WAL)으로 데이터 내구성을 보장합니다.\n\n- Journal Log는 데이터 디렉토리 하위 `journal/` 폴더에 저장\n- 장애 발생 시 Journal Log를 사용해 데이터 복구\n\n```yaml\nstorage:\n  journal:\n    enabled: true\n    commitIntervalMs: 100  # 기본값\n```\n\n---\n\n## 성능 튜닝\n\n### 프로덕션 권장 설정\n\n```yaml\n# mongod.conf\nstorage:\n  wiredTiger:\n    engineConfig:\n      cacheSizeGB: 8\n      journalCompressor: snappy\n    collectionConfig:\n      blockCompressor: zstd\n    indexConfig:\n      prefixCompressionEnabled: true\n```\n\n### 모니터링 명령어\n\n```javascript\n// WiredTiger 전체 통계\ndb.serverStatus().wiredTiger\n\n// 캐시 상태\ndb.serverStatus().wiredTiger.cache\n\n// 컬렉션별 통계\ndb.collection.stats().wiredTiger\n```\n\n---\n\n## 주의사항\n\n1. **캐시 크기**: 시스템 RAM의 50% 이하 권장\n2. **압축**: CPU 오버헤드와 저장 공간 트레이드오프 고려\n3. **저널링**: 비활성화 시 데이터 손실 위험\n4. **Eviction 모니터링**: 포그라운드 Eviction 발생 시 성능 저하\n5. **체크포인트**: Sharp Checkpoint로 인한 일시적 I/O 스파이크 고려\n\n## 참고 자료\n\n- [MongoDB WiredTiger 공식 문서](https://www.mongodb.com/docs/manual/core/wiredtiger/)\n- [WiredTiger GitHub](https://github.com/wiredtiger/wiredtiger)\n- [MongoDB 총 정리 (liltdevs)](https://liltdevs.tistory.com/216)",
    "docType": "original",
    "category": "Database",
    "tags": [
      "Database",
      "MongoDB",
      "WiredTiger"
    ],
    "readingTime": 5,
    "wordCount": 865,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "sharding-guide",
    "slug": "sharding-guide",
    "path": "database/mongodb",
    "fullPath": "database/mongodb/sharding-guide",
    "title": "MongoDB 샤딩(Sharding) 완벽 가이드",
    "excerpt": "MongoDB 샤딩의 개념부터 샤드 키 선택, 클러스터 구성까지 실전 가이드를 제공합니다.",
    "content": "# MongoDB 샤딩(Sharding) 완벽 가이드\n\n## 샤딩이란?\n\n**샤딩**은 대용량 데이터를 여러 서버에 분산 저장하는 수평 확장(Horizontal Scaling) 방식입니다. MongoDB는 자동 샤딩을 지원하여 데이터가 증가해도 성능을 유지할 수 있습니다.\n\n## 샤딩 아키텍처\n\n```\n┌─────────────────────────────────────────────────────────┐\n│                      Application                         │\n└─────────────────────────────────────────────────────────┘\n                            │\n                            ▼\n┌─────────────────────────────────────────────────────────┐\n│                       mongos                             │\n│                   (Query Router)                         │\n└─────────────────────────────────────────────────────────┘\n                            │\n        ┌───────────────────┼───────────────────┐\n        ▼                   ▼                   ▼\n┌───────────────┐   ┌───────────────┐   ┌───────────────┐\n│   Shard 1     │   │   Shard 2     │   │   Shard 3     │\n│  (Replica Set)│   │  (Replica Set)│   │  (Replica Set)│\n└───────────────┘   └───────────────┘   └───────────────┘\n                            │\n                            ▼\n┌─────────────────────────────────────────────────────────┐\n│              Config Servers (Replica Set)                │\n│                  (메타데이터 저장)                        │\n└─────────────────────────────────────────────────────────┘\n```\n\n### 구성 요소\n\n| 구성 요소 | 역할 |\n|---------|------|\n| **mongos** | 클라이언트 요청을 적절한 샤드로 라우팅 |\n| **Shard** | 실제 데이터를 저장하는 레플리카 셋 |\n| **Config Server** | 클러스터 메타데이터 및 청크 정보 저장 |\n\n## 샤드 키 (Shard Key)\n\n샤드 키는 데이터를 분산하는 기준이 되는 필드입니다. **샤드 키 선택은 성능에 직접적인 영향**을 미칩니다.\n\n### 좋은 샤드 키의 조건\n\n1. **높은 카디널리티**: 다양한 값을 가져야 함\n2. **균등한 분포**: 데이터가 골고루 분산되어야 함\n3. **쿼리 패턴 부합**: 자주 사용되는 쿼리 조건과 일치\n\n### 샤드 키 예시\n\n```javascript\n// 좋은 예: 높은 카디널리티 + 균등 분포\nsh.shardCollection(\"mydb.orders\", { orderId: \"hashed\" })\n\n// 범위 샤딩: 범위 쿼리에 유리\nsh.shardCollection(\"mydb.logs\", { timestamp: 1 })\n\n// 복합 샤드 키: 더 세밀한 분산\nsh.shardCollection(\"mydb.users\", { country: 1, createdAt: 1 })\n```\n\n### 샤드 키 전략\n\n| 전략 | 장점 | 단점 | 적합한 경우 |\n|-----|------|------|-----------|\n| **Hashed** | 균등 분산 | 범위 쿼리 비효율 | 랜덤 액세스 |\n| **Ranged** | 범위 쿼리 효율적 | 핫스팟 위험 | 시계열 데이터 |\n| **Compound** | 유연성 | 설계 복잡 | 복잡한 쿼리 |\n\n## 청크 (Chunk)\n\n데이터는 **청크** 단위로 샤드에 분산됩니다.\n\n```javascript\n// 청크 크기 확인\nuse config\ndb.settings.find({ _id: \"chunksize\" })\n\n// 청크 크기 변경 (MB 단위, 기본 128MB)\ndb.settings.updateOne(\n  { _id: \"chunksize\" },\n  { $set: { value: 64 } },\n  { upsert: true }\n)\n```\n\n### 청크 밸런싱\n\nMongoDB는 **밸런서**를 통해 청크를 자동으로 균형 분배합니다:\n\n```javascript\n// 밸런서 상태 확인\nsh.getBalancerState()\n\n// 밸런서 활성화/비활성화\nsh.startBalancer()\nsh.stopBalancer()\n\n// 특정 시간대에만 밸런싱\ndb.settings.updateOne(\n  { _id: \"balancer\" },\n  { $set: { activeWindow: { start: \"02:00\", stop: \"06:00\" } } }\n)\n```\n\n## 샤딩 클러스터 구성\n\n### 1. Config Server 설정\n\n```yaml\n# config-server.conf\nsharding:\n  clusterRole: configsvr\nreplication:\n  replSetName: configReplSet\nnet:\n  port: 27019\n```\n\n### 2. Shard 설정\n\n```yaml\n# shard.conf\nsharding:\n  clusterRole: shardsvr\nreplication:\n  replSetName: shard1ReplSet\nnet:\n  port: 27018\n```\n\n### 3. mongos 설정\n\n```yaml\n# mongos.conf\nsharding:\n  configDB: configReplSet/config1:27019,config2:27019,config3:27019\nnet:\n  port: 27017\n```\n\n### 4. 샤드 추가\n\n```javascript\n// mongos에 접속하여 샤드 추가\nsh.addShard(\"shard1ReplSet/shard1-1:27018,shard1-2:27018\")\nsh.addShard(\"shard2ReplSet/shard2-1:27018,shard2-2:27018\")\n\n// 샤딩 활성화\nsh.enableSharding(\"mydb\")\n\n// 컬렉션 샤딩\nsh.shardCollection(\"mydb.orders\", { customerId: \"hashed\" })\n```\n\n## 쿼리 라우팅\n\n### Targeted Query (효율적)\n\n샤드 키를 포함한 쿼리는 특정 샤드로만 전달됩니다:\n\n```javascript\n// customerId가 샤드 키일 때 - 특정 샤드만 조회\ndb.orders.find({ customerId: \"user123\" })\n```\n\n### Scatter-Gather Query (비효율적)\n\n샤드 키가 없으면 모든 샤드에 쿼리 전송:\n\n```javascript\n// 모든 샤드에 쿼리 전송 후 결과 병합\ndb.orders.find({ status: \"pending\" })\n```\n\n## 모니터링\n\n```javascript\n// 샤딩 상태 확인\nsh.status()\n\n// 각 샤드별 데이터 분포\ndb.orders.getShardDistribution()\n\n// 청크 정보\nuse config\ndb.chunks.find({ ns: \"mydb.orders\" }).pretty()\n```\n\n## 주의사항\n\n1. **샤드 키는 변경 불가**: 설계 단계에서 신중히 선택\n2. **샤드 키 값은 불변**: 한번 설정된 문서의 샤드 키 변경 불가\n3. **트랜잭션 제한**: 다중 샤드 트랜잭션은 성능 저하 가능\n4. **인덱스 필수**: 샤드 키에는 반드시 인덱스 존재해야 함\n\n## 참고 자료\n\n- [MongoDB Sharding 공식 문서](https://www.mongodb.com/docs/manual/sharding/)\n- [샤드 키 선택 가이드](https://www.mongodb.com/docs/manual/core/sharding-choose-a-shard-key/)",
    "docType": "original",
    "category": "Database",
    "tags": [
      "Database",
      "Distributed Systems",
      "MongoDB",
      "Scaling"
    ],
    "readingTime": 3,
    "wordCount": 577,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "merkle-trie-integrity",
    "slug": "merkle-trie-integrity",
    "path": "blockchain/ethereum",
    "fullPath": "blockchain/ethereum/merkle-trie-integrity",
    "title": "go-ethereum Merkle Trie를 활용한 데이터 무결성 검증",
    "excerpt": "go-ethereum의 Merkle Trie 구현을 활용하여 대량 데이터의 무결성을 효율적으로 검증하는 방법을 알아봅니다.",
    "content": "# go-ethereum Merkle Trie를 활용한 데이터 무결성 검증\n\n## 개요\n\n**Merkle Trie**는 대량의 데이터 무결성을 단일 해시값(Merkle Root)으로 증명할 수 있는 자료구조입니다. 이 글에서는 go-ethereum의 Trie 패키지를 활용하여 데이터 무결성 검증 시스템을 구축하는 방법을 다룹니다.\n\n## Merkle Trie란?\n\n### 핵심 개념\n\n```mermaid\ngraph TD\n    %% 노드 정의\n    Root[\"Merkle Root (H_root)\"]\n    HAB[\"H(AB)\"]\n    HCD[\"H(CD)\"]\n    HA[\"H(A)\"]\n    HB[\"H(B)\"]\n    HC[\"H(C)\"]\n    HD[\"H(D)\"]\n    A[\"A\"]\n    B[\"B\"]\n    C[\"C\"]\n    D[\"D\"]\n\n    %% 연결 정의 (해시 과정: 아래에서 위로)\n    HAB --> Root\n    HCD --> Root\n    HA --> HAB\n    HB --> HAB\n    HC --> HCD\n    HD --> HCD\n    A --> HA\n    B --> HB\n    C --> HC\n    D --> HD\n\n    %% 스타일링 (선택사항: 루트와 리프 노드 강조)\n    style Root fill:#f9f,stroke:#333,stroke-width:2px,color:white\n    style A fill:#fff,stroke:#333,stroke-dasharray: 5 5\n    style B fill:#fff,stroke:#333,stroke-dasharray: 5 5\n    style C fill:#fff,stroke:#333,stroke-dasharray: 5 5\n    style D fill:#fff,stroke:#333,stroke-dasharray: 5 5\n```\n\n- **Leaf 노드**: 원본 데이터의 해시\n- **내부 노드**: 자식 노드들의 해시 조합\n- **Root 노드**: 전체 데이터를 대표하는 단일 해시\n\n### 왜 Merkle Trie인가?\n\n| 장점 | 설명 |\n|------|------|\n| **효율적 검증** | O(log n) 복잡도로 특정 데이터 포함 증명 |\n| **배치 검증** | 수천 개 데이터를 단일 루트로 검증 |\n| **변조 감지** | 하나라도 변경되면 루트 해시 변경 |\n| **블록체인 호환** | 대부분의 블록체인이 사용하는 표준 구조 |\n\n## go-ethereum Trie 패키지\n\n### 설치\n\n```bash\ngo get github.com/ethereum/go-ethereum\n```\n\n### 핵심 인터페이스\n\n```go\npackage integrity\n\nimport (\n    \"github.com/ethereum/go-ethereum/common\"\n    \"github.com/ethereum/go-ethereum/core/rawdb\"\n    ethtrie \"github.com/ethereum/go-ethereum/trie\"\n    \"github.com/ethereum/go-ethereum/triedb\"\n)\n```\n\n## 핵심 구현\n\n### Trie 래퍼\n\n```go\npackage integrity\n\nimport (\n    \"errors\"\n    \"fmt\"\n    \"sync\"\n    \n    \"github.com/ethereum/go-ethereum/common\"\n    \"github.com/ethereum/go-ethereum/core/rawdb\"\n    ethtrie \"github.com/ethereum/go-ethereum/trie\"\n    \"github.com/ethereum/go-ethereum/triedb\"\n)\n\nvar (\n    ErrNodeNotFound = errors.New(\"node not found in trie\")\n    ErrInvalidProof = errors.New(\"invalid merkle proof\")\n)\n\n// LeafNode는 Trie의 리프 노드입니다.\ntype LeafNode struct {\n    key   []byte\n    value []byte\n}\n\nfunc NewLeafNode(key, value []byte) *LeafNode {\n    return &LeafNode{key: key, value: value}\n}\n\nfunc (n *LeafNode) Key() []byte   { return n.key }\nfunc (n *LeafNode) Value() []byte { return n.value }\nfunc (n *LeafNode) Hash() []byte  { return n.value }\n\n// IntegrityTrie는 데이터 무결성 검증을 위한 Merkle Trie입니다.\ntype IntegrityTrie struct {\n    trie *ethtrie.Trie\n    db   *triedb.Database\n    mu   sync.RWMutex\n}\n\n// NewIntegrityTrie는 새 Trie를 생성합니다.\nfunc NewIntegrityTrie(seedNodes []*LeafNode) (*IntegrityTrie, error) {\n    // 인메모리 데이터베이스 생성\n    memDB := rawdb.NewMemoryDatabase()\n    trieDB := triedb.NewDatabase(memDB, nil)\n    \n    // 빈 Trie 생성\n    t := ethtrie.NewEmpty(trieDB)\n    \n    integrityTrie := &IntegrityTrie{\n        trie: t,\n        db:   trieDB,\n    }\n    \n    // 초기 노드 삽입\n    for _, node := range seedNodes {\n        if err := integrityTrie.Insert(node.Key(), node.Value()); err != nil {\n            return nil, fmt.Errorf(\"insert seed node: %w\", err)\n        }\n    }\n    \n    return integrityTrie, nil\n}\n\n// Insert는 키-값 쌍을 Trie에 추가합니다.\nfunc (t *IntegrityTrie) Insert(key, value []byte) error {\n    t.mu.Lock()\n    defer t.mu.Unlock()\n    \n    return t.trie.Update(key, value)\n}\n\n// RootHash는 현재 Merkle Root를 반환합니다.\nfunc (t *IntegrityTrie) RootHash() common.Hash {\n    t.mu.RLock()\n    defer t.mu.RUnlock()\n    \n    return t.trie.Hash()\n}\n\n// Commit은 Trie 상태를 저장하고 Root를 반환합니다.\nfunc (t *IntegrityTrie) Commit() (common.Hash, error) {\n    t.mu.Lock()\n    defer t.mu.Unlock()\n    \n    // Trie 커밋\n    rootHash, _, err := t.trie.Commit(false)\n    if err != nil {\n        return common.Hash{}, fmt.Errorf(\"trie commit: %w\", err)\n    }\n    \n    // 데이터베이스에 저장\n    if err := t.db.Commit(rootHash, false); err != nil {\n        return common.Hash{}, fmt.Errorf(\"db commit: %w\", err)\n    }\n    \n    return rootHash, nil\n}\n```\n\n### 배치 데이터 처리\n\n감사 로그 등 대량 데이터를 배치로 처리:\n\n```go\n// BatchData는 배치 처리할 데이터 항목입니다.\ntype BatchData struct {\n    ID      string\n    Content []byte\n}\n\n// ComputeBatchRoot는 배치 데이터의 Merkle Root를 계산합니다.\nfunc ComputeBatchRoot(items []BatchData) (common.Hash, error) {\n    nodes := make([]*LeafNode, len(items))\n    \n    for i, item := range items {\n        // 키: 고유 ID의 해시\n        key := crypto.Keccak256([]byte(item.ID))\n        // 값: 콘텐츠의 해시\n        value := crypto.Keccak256(item.Content)\n        \n        nodes[i] = NewLeafNode(key, value)\n    }\n    \n    trie, err := NewIntegrityTrie(nodes)\n    if err != nil {\n        return common.Hash{}, err\n    }\n    \n    return trie.Commit()\n}\n```\n\n### 감사 로그 통합 예시\n\n```go\npackage audit\n\nimport (\n    \"context\"\n    \"time\"\n    \"encoding/json\"\n)\n\n// AuditBatch는 감사 로그 배치입니다.\ntype AuditBatch struct {\n    ID        string       `json:\"id\"`\n    Logs      []AuditLog   `json:\"logs\"`\n    CreatedAt time.Time    `json:\"created_at\"`\n}\n\n// AuditLog는 개별 감사 로그입니다.\ntype AuditLog struct {\n    DocumentID string `json:\"document_id\"`\n    Action     string `json:\"action\"`\n    Version    int32  `json:\"version\"`\n    Hash       string `json:\"hash\"`\n}\n\n// AnchoringService는 블록체인 앵커링 서비스입니다.\ntype AnchoringService struct {\n    blockchain BlockchainClient\n}\n\n// ProcessBatch는 배치 감사 로그를 처리하고 앵커링합니다.\nfunc (s *AnchoringService) ProcessBatch(ctx context.Context, logs []AuditLog) (*AnchorResult, error) {\n    // 1. 각 로그를 Merkle Trie 노드로 변환\n    items := make([]BatchData, len(logs))\n    for i, log := range logs {\n        content, _ := json.Marshal(log)\n        items[i] = BatchData{\n            ID:      log.DocumentID + \"_\" + fmt.Sprint(log.Version),\n            Content: content,\n        }\n    }\n    \n    // 2. Merkle Root 계산\n    merkleRoot, err := ComputeBatchRoot(items)\n    if err != nil {\n        return nil, fmt.Errorf(\"compute merkle root: %w\", err)\n    }\n    \n    // 3. 블록체인에 앵커링\n    txHash, err := s.blockchain.SubmitAnchor(ctx, merkleRoot.Hex())\n    if err != nil {\n        return nil, fmt.Errorf(\"submit anchor: %w\", err)\n    }\n    \n    return &AnchorResult{\n        MerkleRoot:      merkleRoot.Hex(),\n        TransactionHash: txHash,\n        AnchoredAt:      time.Now(),\n        LogCount:        len(logs),\n    }, nil\n}\n\ntype AnchorResult struct {\n    MerkleRoot      string    `json:\"merkle_root\"`\n    TransactionHash string    `json:\"transaction_hash\"`\n    AnchoredAt      time.Time `json:\"anchored_at\"`\n    LogCount        int       `json:\"log_count\"`\n}\n```\n\n## Merkle Proof 생성 및 검증\n\n특정 데이터가 Merkle Root에 포함되어 있음을 증명:\n\n```go\nimport (\n    \"github.com/ethereum/go-ethereum/trie\"\n)\n\n// GenerateProof는 특정 키에 대한 Merkle Proof를 생성합니다.\nfunc (t *IntegrityTrie) GenerateProof(key []byte) ([][]byte, error) {\n    t.mu.RLock()\n    defer t.mu.RUnlock()\n    \n    // Proof 노드들을 담을 메모리 DB\n    proofDB := rawdb.NewMemoryDatabase()\n    \n    // Proof 생성\n    if err := t.trie.Prove(key, proofDB); err != nil {\n        return nil, fmt.Errorf(\"generate proof: %w\", err)\n    }\n    \n    // Proof 데이터 추출\n    var proofNodes [][]byte\n    it := proofDB.NewIterator(nil, nil)\n    defer it.Release()\n    \n    for it.Next() {\n        proofNodes = append(proofNodes, common.CopyBytes(it.Value()))\n    }\n    \n    return proofNodes, nil\n}\n\n// VerifyProof는 Merkle Proof를 검증합니다.\nfunc VerifyProof(rootHash common.Hash, key []byte, proofNodes [][]byte) ([]byte, error) {\n    // Proof DB 구성\n    proofDB := rawdb.NewMemoryDatabase()\n    for _, node := range proofNodes {\n        hash := crypto.Keccak256(node)\n        proofDB.Put(hash, node)\n    }\n    \n    // Proof 검증 및 값 반환\n    value, err := trie.VerifyProof(rootHash, key, proofDB)\n    if err != nil {\n        return nil, ErrInvalidProof\n    }\n    \n    return value, nil\n}\n```\n\n### 사용 예시\n\n```go\nfunc ExampleMerkleProof() {\n    // 1. 데이터 준비\n    items := []BatchData{\n        {ID: \"doc-001\", Content: []byte(`{\"action\":\"CREATE\"}`)},\n        {ID: \"doc-002\", Content: []byte(`{\"action\":\"UPDATE\"}`)},\n        {ID: \"doc-003\", Content: []byte(`{\"action\":\"DELETE\"}`)},\n    }\n    \n    nodes := make([]*LeafNode, len(items))\n    for i, item := range items {\n        key := crypto.Keccak256([]byte(item.ID))\n        value := crypto.Keccak256(item.Content)\n        nodes[i] = NewLeafNode(key, value)\n    }\n    \n    // 2. Trie 생성 및 Root 계산\n    trie, _ := NewIntegrityTrie(nodes)\n    rootHash, _ := trie.Commit()\n    \n    fmt.Printf(\"Merkle Root: %s\\n\", rootHash.Hex())\n    \n    // 3. 특정 문서의 Proof 생성\n    targetKey := crypto.Keccak256([]byte(\"doc-002\"))\n    proof, _ := trie.GenerateProof(targetKey)\n    \n    fmt.Printf(\"Proof nodes: %d\\n\", len(proof))\n    \n    // 4. Proof 검증 (다른 시스템에서)\n    expectedValue := crypto.Keccak256([]byte(`{\"action\":\"UPDATE\"}`))\n    verifiedValue, err := VerifyProof(rootHash, targetKey, proof)\n    \n    if err != nil {\n        fmt.Println(\"Proof invalid!\")\n    } else if bytes.Equal(verifiedValue, expectedValue) {\n        fmt.Println(\"Proof verified! Data integrity confirmed.\")\n    }\n}\n```\n\n## 무결성 검증 API\n\n외부에서 데이터 무결성을 검증할 수 있는 API:\n\n```go\npackage api\n\ntype IntegrityVerification struct {\n    DocumentID   string `json:\"document_id\"`\n    Version      int32  `json:\"version\"`\n    ContentHash  string `json:\"content_hash\"`\n    MerkleRoot   string `json:\"merkle_root\"`\n    BlockchainTx string `json:\"blockchain_tx\"`\n    Verified     bool   `json:\"verified\"`\n    VerifiedAt   string `json:\"verified_at\"`\n}\n\nfunc (s *APIServer) VerifyIntegrity(ctx context.Context, req VerifyRequest) (*IntegrityVerification, error) {\n    // 1. 문서 조회\n    doc, err := s.docService.FindByVersion(ctx, req.Collection, req.URI, req.Version)\n    if err != nil {\n        return nil, err\n    }\n    \n    // 2. 문서의 앵커링 메타데이터 확인\n    anchor := doc.AnchoringMetadata\n    if anchor.Status != \"COMPLETED\" {\n        return nil, errors.New(\"document not yet anchored\")\n    }\n    \n    // 3. 블록체인에서 Merkle Root 조회\n    blockchainRoot, err := s.blockchain.GetAnchor(ctx, anchor.TransactionHash)\n    if err != nil {\n        return nil, err\n    }\n    \n    // 4. 현재 데이터로 해시 재계산\n    currentHash := computeDocumentHash(doc)\n    \n    // 5. 저장된 Merkle Root와 블록체인 Root 비교\n    verified := anchor.MerkleRoot == blockchainRoot\n    \n    return &IntegrityVerification{\n        DocumentID:   doc.URI,\n        Version:      doc.Version,\n        ContentHash:  currentHash,\n        MerkleRoot:   anchor.MerkleRoot,\n        BlockchainTx: anchor.TransactionHash,\n        Verified:     verified,\n        VerifiedAt:   time.Now().Format(time.RFC3339),\n    }, nil\n}\n```\n\n## 성능 최적화\n\n### 배치 크기 조정\n\n```go\nconst (\n    OptimalBatchSize = 1000 // Trie 연산에 최적화된 배치 크기\n    MaxBatchSize     = 10000\n)\n\nfunc ProcessWithOptimalBatch(items []BatchData) ([]common.Hash, error) {\n    var roots []common.Hash\n    \n    for i := 0; i < len(items); i += OptimalBatchSize {\n        end := i + OptimalBatchSize\n        if end > len(items) {\n            end = len(items)\n        }\n        \n        batch := items[i:end]\n        root, err := ComputeBatchRoot(batch)\n        if err != nil {\n            return nil, err\n        }\n        \n        roots = append(roots, root)\n    }\n    \n    return roots, nil\n}\n```\n\n### 캐싱\n\n```go\ntype TrieCache struct {\n    mu    sync.RWMutex\n    roots map[string]common.Hash // batchID -> rootHash\n}\n\nfunc (c *TrieCache) Get(batchID string) (common.Hash, bool) {\n    c.mu.RLock()\n    defer c.mu.RUnlock()\n    root, ok := c.roots[batchID]\n    return root, ok\n}\n\nfunc (c *TrieCache) Set(batchID string, root common.Hash) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    c.roots[batchID] = root\n}\n```\n\n## 모범 사례\n\n1. **배치 처리**: 개별 데이터마다 Trie 생성하지 말고 배치 단위로 처리\n2. **Root 저장**: 계산된 Merkle Root는 영구 저장소에 저장\n3. **Proof 캐싱**: 자주 조회되는 데이터의 Proof는 캐싱\n4. **해시 일관성**: 동일한 해시 함수(Keccak256) 일관되게 사용\n5. **동시성 제어**: Trie 접근 시 뮤텍스로 보호\n\n## 참고 자료\n\n- [go-ethereum Trie 패키지](https://pkg.go.dev/github.com/ethereum/go-ethereum/trie)\n- [Merkle Tree 위키피디아](https://en.wikipedia.org/wiki/Merkle_tree)\n- [Ethereum Yellow Paper](https://ethereum.github.io/yellowpaper/paper.pdf)",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain",
      "Go",
      "MerkleTrie",
      "Transaction"
    ],
    "readingTime": 8,
    "wordCount": 1409,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "functional-options-pattern",
    "slug": "functional-options-pattern",
    "path": "backend/patterns",
    "fullPath": "backend/patterns/functional-options-pattern",
    "title": "Functional Options 패턴을 활용한 Go 설정 관리",
    "excerpt": "Go의 Functional Options 패턴을 활용하여 필수 파라미터와 선택적 설정을 명확히 분리하고, 합리적인 기본값 위에 유연한 커스터마이징을 제공하는 방법을 알아봅니다.",
    "content": "# Functional Options 패턴을 활용한 Go 설정 관리\n\n## 개요\n\n**Functional Options 패턴**은 Go에서 객체 생성 시 **필수 파라미터**와 **선택적 설정**을 명확히 분리하는 관용적인 패턴입니다.\n\n핵심 원칙:\n\n- **필수값**: 생성자의 명시적 파라미터로 전달 (없으면 컴파일 에러)\n- **선택값**: 합리적인 기본값을 정의하고, `With...` 옵션 함수로 오버라이드\n\n## 패턴의 핵심: 필수 vs 선택\n\n### 설계 의도\n\n```\nNewWorker(client, handler, opts...)\n         ↑       ↑        ↑\n      필수값   필수값   선택적 옵션들\n      \n생성자 시그니처가 \"무엇이 필수인지\"를 명확히 선언합니다.\n옵션을 전달하지 않아도 기본값으로 동작합니다.\n```\n\n| 구분 | 전달 방식 | 특징 |\n|------|----------|------|\n| **필수 파라미터** | 생성자의 명시적 인자 | 누락 시 컴파일 에러, 기본값 없음 |\n| **선택적 설정** | `...WorkerOption` 가변 인자 | 기본값 존재, 필요 시 오버라이드 |\n\n### 왜 이 구분이 중요한가?\n\n```go\n// ❌ 나쁜 예: 모든 것이 옵션\n// client가 nil이어도 컴파일은 통과 → 런타임 에러\nworker := NewWorker(\n    WithClient(client),     // 필수인데 옵션처럼 보임\n    WithHandler(handler),   // 필수인데 옵션처럼 보임\n    WithBatchSize(100),\n)\n\n// ✅ 좋은 예: 필수값은 명시적 파라미터\n// client나 handler 누락 시 컴파일 에러\nworker := NewWorker(\n    client,                 // 필수: 첫 번째 인자\n    handler,                // 필수: 두 번째 인자  \n    WithBatchSize(100),     // 선택: 기본값 10을 100으로 오버라이드\n)\n```\n\n## 기본 구현\n\n### 1. 내부 설정 구조체 (기본값 정의)\n\n```go\npackage worker\n\nimport \"time\"\n\n// workerConfig는 Worker의 \"선택적\" 설정을 담습니다.\n// 필수값(client, handler)은 여기에 포함되지 않습니다.\ntype workerConfig struct {\n    batchSize        int\n    pollInterval     time.Duration\n    maxRetries       int\n    shutdownTimeout  time.Duration\n    deadLetterStream string\n}\n\n// defaultConfig는 합리적인 기본값을 반환합니다.\n// 옵션을 전혀 전달하지 않아도 이 값으로 동작합니다.\nfunc defaultConfig() workerConfig {\n    return workerConfig{\n        batchSize:        10,                    // 기본: 10개씩 처리\n        pollInterval:     100 * time.Millisecond, // 기본: 100ms 폴링\n        maxRetries:       3,                     // 기본: 3회 재시도\n        shutdownTimeout:  30 * time.Second,      // 기본: 30초 종료 대기\n        deadLetterStream: \"dead-letter-stream\",  // 기본 DLQ 이름\n    }\n}\n```\n\n### 2. 옵션 타입과 With 함수들 (기본값 오버라이드)\n\n```go\n// WorkerOption은 기본 설정을 변경하는 함수 타입입니다.\ntype WorkerOption func(*workerConfig)\n\n// WithBatchSize는 기본값(10)을 오버라이드합니다.\nfunc WithBatchSize(size int) WorkerOption {\n    return func(c *workerConfig) {\n        if size > 0 {\n            c.batchSize = size\n        }\n    }\n}\n\n// WithPollInterval은 기본값(100ms)을 오버라이드합니다.\nfunc WithPollInterval(interval time.Duration) WorkerOption {\n    return func(c *workerConfig) {\n        if interval > 0 {\n            c.pollInterval = interval\n        }\n    }\n}\n\n// WithMaxRetries는 기본값(3)을 오버라이드합니다.\nfunc WithMaxRetries(retries int) WorkerOption {\n    return func(c *workerConfig) {\n        c.maxRetries = retries\n    }\n}\n\n// WithShutdownTimeout은 기본값(30s)을 오버라이드합니다.\nfunc WithShutdownTimeout(timeout time.Duration) WorkerOption {\n    return func(c *workerConfig) {\n        c.shutdownTimeout = timeout\n    }\n}\n\n// WithDeadLetterStream은 기본값을 오버라이드합니다.\nfunc WithDeadLetterStream(stream string) WorkerOption {\n    return func(c *workerConfig) {\n        c.deadLetterStream = stream\n    }\n}\n```\n\n### 3. 생성자 (필수 파라미터 + 선택적 옵션)\n\n```go\n// StreamWorker는 메시지 스트림을 처리하는 워커입니다.\ntype StreamWorker struct {\n    // 필수 의존성 (생성자 파라미터로 주입)\n    client  redis.UniversalClient\n    handler MessageHandler\n    \n    // 선택적 설정 (기본값 + 옵션으로 구성)\n    config  workerConfig\n    \n    stopCh  chan struct{}\n}\n\n// NewStreamWorker는 새 워커를 생성합니다.\n//\n// 파라미터:\n//   - client: Redis 클라이언트 (필수)\n//   - handler: 메시지 핸들러 (필수)\n//   - opts: 선택적 설정 (기본값 존재, 필요 시 오버라이드)\nfunc NewStreamWorker(\n    client redis.UniversalClient,   // 필수: 없으면 컴파일 에러\n    handler MessageHandler,          // 필수: 없으면 컴파일 에러\n    opts ...WorkerOption,            // 선택: 없어도 기본값으로 동작\n) *StreamWorker {\n    // 1. 기본값으로 시작\n    cfg := defaultConfig()\n    \n    // 2. 전달된 옵션들로 기본값 오버라이드\n    for _, opt := range opts {\n        opt(&cfg)\n    }\n    \n    return &StreamWorker{\n        client:  client,\n        handler: handler,\n        config:  cfg,\n        stopCh:  make(chan struct{}),\n    }\n}\n```\n\n### 4. 사용 예시\n\n```go\nfunc main() {\n    // 필수 의존성 준비\n    client := redis.NewClusterClient(&redis.ClusterOptions{\n        Addrs: []string{\"localhost:7001\"},\n    })\n    handler := &AuditHandler{}\n    \n    // Case 1: 기본값 그대로 사용 (옵션 없음)\n    // batchSize=10, pollInterval=100ms, maxRetries=3 ...\n    worker1 := NewStreamWorker(client, handler)\n    \n    // Case 2: 일부 설정만 오버라이드\n    // batchSize=100 (오버라이드), 나머지는 기본값\n    worker2 := NewStreamWorker(client, handler,\n        WithBatchSize(100),\n    )\n    \n    // Case 3: 여러 설정 오버라이드\n    worker3 := NewStreamWorker(client, handler,\n        WithBatchSize(100),              // 10 → 100\n        WithPollInterval(50*time.Millisecond), // 100ms → 50ms\n        WithMaxRetries(5),               // 3 → 5\n    )\n    \n    worker3.Start(context.Background())\n}\n```\n\n## 실제 활용 예시\n\n### 데이터베이스 클라이언트\n\n```go\ntype dbConfig struct {\n    // 선택적 설정 (기본값 존재)\n    host       string\n    port       int\n    poolSize   int\n    timeout    time.Duration\n}\n\nfunc defaultDBConfig() dbConfig {\n    return dbConfig{\n        host:     \"localhost\",\n        port:     5432,\n        poolSize: 10,\n        timeout:  5 * time.Second,\n    }\n}\n\ntype DBOption func(*dbConfig)\n\nfunc WithHost(host string) DBOption {\n    return func(c *dbConfig) { c.host = host }\n}\n\nfunc WithPort(port int) DBOption {\n    return func(c *dbConfig) { c.port = port }\n}\n\nfunc WithPoolSize(size int) DBOption {\n    return func(c *dbConfig) { c.poolSize = size }\n}\n\n// NewDBClient: database는 필수, 나머지는 선택\nfunc NewDBClient(database string, opts ...DBOption) (*DBClient, error) {\n    // database: 필수 파라미터 (기본값 없음)\n    if database == \"\" {\n        return nil, errors.New(\"database name is required\")\n    }\n    \n    // 선택적 설정: 기본값 + 옵션 오버라이드\n    cfg := defaultDBConfig()\n    for _, opt := range opts {\n        opt(&cfg)\n    }\n    \n    return &DBClient{\n        database: database,  // 필수\n        config:   cfg,       // 선택 (기본값 + 오버라이드)\n    }, nil\n}\n\n// 사용\ndb, _ := NewDBClient(\"mydb\")                           // 기본값 사용\ndb, _ := NewDBClient(\"mydb\", WithHost(\"db.prod.com\"))  // host만 오버라이드\ndb, _ := NewDBClient(\"mydb\", WithHost(\"db.prod.com\"), WithPoolSize(50))\n```\n\n## 응용 패턴\n\n### 옵션 프리셋 (자주 쓰는 설정 조합)\n\n```go\n// ProductionOptions는 프로덕션 환경용 설정 프리셋입니다.\nfunc ProductionOptions() []WorkerOption {\n    return []WorkerOption{\n        WithBatchSize(100),\n        WithMaxRetries(5),\n        WithShutdownTimeout(60 * time.Second),\n    }\n}\n\n// 프리셋 사용\nworker := NewStreamWorker(client, handler, ProductionOptions()...)\n\n// 프리셋 + 추가 오버라이드\nopts := append(ProductionOptions(), WithBatchSize(200))\nworker := NewStreamWorker(client, handler, opts...)\n```\n\n### 검증 포함 옵션\n\n```go\nfunc WithBatchSize(size int) WorkerOption {\n    return func(c *workerConfig) {\n        // 검증: 범위 제한\n        if size < 1 {\n            size = 1\n        }\n        if size > 1000 {\n            size = 1000\n        }\n        c.batchSize = size\n    }\n}\n```\n\n### 에러 반환 옵션 (고급)\n\n```go\ntype WorkerOptionWithError func(*workerConfig) error\n\nfunc NewStreamWorkerSafe(\n    client redis.UniversalClient,\n    handler MessageHandler,\n    opts ...WorkerOptionWithError,\n) (*StreamWorker, error) {\n    cfg := defaultConfig()\n    \n    for _, opt := range opts {\n        if err := opt(&cfg); err != nil {\n            return nil, err\n        }\n    }\n    \n    return &StreamWorker{config: cfg}, nil\n}\n\nfunc WithStreamFromEnv(key string) WorkerOptionWithError {\n    return func(c *workerConfig) error {\n        value := os.Getenv(key)\n        if value == \"\" {\n            return fmt.Errorf(\"env %s is not set\", key)\n        }\n        // 설정 적용\n        return nil\n    }\n}\n```\n\n## 테스트 용이성\n\n옵션 패턴은 테스트에서 특히 유용합니다:\n\n```go\nfunc TestWorker_ProcessMessages(t *testing.T) {\n    // 테스트용 짧은 타임아웃으로 오버라이드\n    worker := NewStreamWorker(\n        mockClient,\n        mockHandler,\n        WithBatchSize(1),                        // 빠른 테스트를 위해 1개씩\n        WithPollInterval(10*time.Millisecond),   // 빠른 폴링\n        WithMaxRetries(1),                       // 재시도 최소화\n        WithShutdownTimeout(100*time.Millisecond),\n    )\n    \n    // 테스트 로직...\n}\n```\n\n## 핵심 정리\n\n```mermaid\nflowchart TD\n    subgraph Constructor [\"NewStreamWorker(client, handler, opts...)\"]\n        direction LR\n        P1[\"client\"] --> |\"필수\"| REQ[\"없으면<br/>컴파일 에러\"]\n        P2[\"handler\"] --> |\"필수\"| REQ\n        P3[\"opts...\"] --> |\"선택\"| OPT[\"없으면<br/>기본값 사용\"]\n    end\n\n    subgraph Internal [\"내부 동작\"]\n        direction TB\n        S1[\"1. cfg := defaultConfig()\"] --> |\"기본값 초기화\"| S2\n        S2[\"2. for opt := range opts\"] --> |\"오버라이드\"| S3\n        S3[\"opt(&cfg)\"]\n    end\n\n    Constructor --> Internal\n\n    style REQ fill:#ffebee,stroke:#c62828\n    style OPT fill:#e8f5e9,stroke:#2e7d32\n```\n\n## 모범 사례\n\n1. **필수/선택 분리**: 없으면 안 되는 것은 파라미터, 기본값이 있는 것은 옵션\n2. **기본값 명시**: `defaultConfig()` 함수로 합리적인 기본값 정의\n3. **With 접두사**: 옵션 함수는 `With...` 네이밍 컨벤션 준수\n4. **검증 포함**: 옵션 함수 내에서 값 검증\n5. **불변성**: 내부 config 구조체는 외부에 노출하지 않음\n\n## 참고 자료\n\n- [Dave Cheney - Functional Options](https://dave.cheney.net/2014/10/17/functional-options-for-friendly-apis)\n- [Rob Pike - Self-referential functions](https://commandcenter.blogspot.com/2014/01/self-referential-functions-and-design.html)\n- [Uber Go Style Guide](https://github.com/uber-go/guide/blob/master/style.md#functional-options)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Architecture",
      "Design Patterns",
      "Go"
    ],
    "readingTime": 6,
    "wordCount": 1163,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "append-only-versioning",
    "slug": "append-only-versioning",
    "path": "backend/patterns",
    "fullPath": "backend/patterns/append-only-versioning",
    "title": "Append-Only 문서 버저닝 설계 및 구현",
    "excerpt": "데이터 불변성과 완전한 감사 추적을 보장하는 Append-Only 문서 버저닝 아키텍처의 설계 원칙과 Go 구현 방법을 알아봅니다.",
    "content": "# Append-Only 문서 버저닝 설계 및 구현\n\n## 개요\n\n데이터의 불변성(Immutability)과 완전한 감사 추적(Audit Trail)이 요구되는 시스템에서는 Append-Only 아키텍처가 효과적입니다. 이 글에서는 문서 버저닝 시스템을 설계하고 구현하는 방법을 다룹니다.\n\n## 왜 Append-Only인가?\n\n### 장점\n\n| 특성 | 설명 |\n|------|------|\n| **불변성 보장** | 기존 데이터를 수정하지 않아 데이터 무결성 유지 |\n| **감사 추적** | 모든 변경 이력이 자동으로 보존 |\n| **충돌 방지** | 동시 수정으로 인한 데이터 손실 위험 감소 |\n| **복구 용이** | 특정 시점으로 롤백 가능 |\n| **규정 준수** | 금융/의료 등 규제 산업의 데이터 보존 요건 충족 |\n\n### 단점\n\n| 특성 | 설명 |\n|------|------|\n| **저장 공간** | 버전마다 전체 스냅샷 저장으로 용량 증가 |\n| **쿼리 복잡도** | 최신 버전 조회 시 추가 로직 필요 |\n| **삭제 처리** | 물리적 삭제 대신 소프트 삭제 필요 |\n\n## 아키텍처 설계\n\n### 듀얼 컬렉션 구조\n\n```mermaid\ngraph LR\n    subgraph StorageLayer [Storage Layer]\n        direction TB\n        subgraph AllVersions [\"&lt;collection&gt; (All Version Snapshots)\"]\n            direction TB\n            v1[\"doc_uri: 'user-001'<br/>version: 1<br/>created_at: ...\"]\n            v2[\"doc_uri: 'user-001'<br/>version: 2\"]\n            v3[\"doc_uri: 'user-001'<br/>version: 3\"]\n        end\n\n        subgraph Current [\"current_&lt;collection&gt; (Latest Only)\"]\n            direction TB\n            latest[\"doc_uri: 'user-001'<br/>version: 3 (latest)<br/>updated_at: ...\"]\n        end\n    end\n\n    %% 스타일링\n    style StorageLayer fill:#f9f9f9,stroke:#333,stroke-dasharray: 5 5\n    style AllVersions fill:#fff,stroke:#007bff\n    style Current fill:#fff,stroke:#28a745\n```\n\n**두 컬렉션을 사용하는 이유:**\n\n- `<collection>`: 히스토리 보존, 감사 추적, 규정 준수\n- `current_<collection>`: 최신 데이터 빠른 조회, 인덱스 최적화\n\n### 문서 상태 관리\n\n```go\ntype DocumentState string\n\nconst (\n    DocStateActive  DocumentState = \"ACTIVE\"\n    DocStateDeleted DocumentState = \"DELETED\"\n    DocStatePending DocumentState = \"PENDING\"\n)\n```\n\n## 핵심 구현\n\n### 문서 모델\n\n```go\npackage model\n\nimport (\n    \"time\"\n)\n\n// VersionedDocument는 버전 관리되는 문서의 기본 구조입니다.\ntype VersionedDocument struct {\n    ID        string                 `bson:\"_id,omitempty\"`\n    URI       string                 `bson:\"uri\"`\n    Version   int32                  `bson:\"version\"`\n    DocStatus DocumentState          `bson:\"doc_status\"`\n    Fields    map[string]interface{} `bson:\"fields\"`\n    CreatedAt time.Time              `bson:\"created_at\"`\n    UpdatedAt time.Time              `bson:\"updated_at\"`\n}\n\n// AuditEntry는 변경 감사 로그 항목입니다.\ntype AuditEntry struct {\n    ID            string                 `bson:\"_id,omitempty\"`\n    Collection    string                 `bson:\"collection\"`\n    DocumentURI   string                 `bson:\"document_uri\"`\n    Action        string                 `bson:\"action\"` // CREATE, UPDATE, DELETE\n    ChangedBy     string                 `bson:\"changed_by\"`\n    ChangedAt     time.Time              `bson:\"changed_at\"`\n    PreviousData  map[string]interface{} `bson:\"previous_data,omitempty\"`\n    NewData       map[string]interface{} `bson:\"new_data,omitempty\"`\n    ChangedFields []string               `bson:\"changed_fields,omitempty\"`\n}\n```\n\n### 서비스 인터페이스\n\n```go\npackage document\n\nimport (\n    \"context\"\n)\n\n// VersionedService는 버전 관리 문서 서비스 인터페이스입니다.\ntype VersionedService interface {\n    // Create는 버전 1의 새 문서를 생성합니다.\n    Create(ctx context.Context, collection string, data BsonDocument) (*BsonDocument, error)\n    \n    // FindLatest는 URI의 최신 버전 문서를 조회합니다.\n    FindLatest(ctx context.Context, collection, uri string) (*BsonDocument, error)\n    \n    // FindByVersion는 특정 버전의 문서를 조회합니다.\n    FindByVersion(ctx context.Context, collection, uri string, version int32) (*BsonDocument, error)\n    \n    // SoftUpdate는 새 버전을 생성하여 문서를 업데이트합니다.\n    SoftUpdate(ctx context.Context, collection, uri string, updates BsonDocument) (*BsonDocument, error)\n    \n    // SoftDelete는 삭제 상태의 새 버전을 생성합니다.\n    SoftDelete(ctx context.Context, collection, uri string) (*BsonDocument, error)\n    \n    // GetHistory는 문서의 전체 버전 히스토리를 조회합니다.\n    GetHistory(ctx context.Context, collection, uri string) ([]BsonDocument, error)\n}\n```\n\n### 문서 생성\n\n```go\nfunc (s *versionService) Create(ctx context.Context, collection string, data BsonDocument) (*BsonDocument, error) {\n    now := time.Now()\n    \n    doc := &VersionedDocument{\n        URI:       data.URI,\n        Version:   1, // 첫 버전\n        DocStatus: DocStateActive,\n        Fields:    data.Fields,\n        CreatedAt: now,\n        UpdatedAt: now,\n    }\n    \n    // 트랜잭션으로 두 컬렉션에 동시 저장\n    _, err := s.WithTransaction(ctx, func(sessCtx context.Context) (interface{}, error) {\n        // 1. 버전 히스토리 컬렉션에 저장\n        if _, err := s.collection(collection).InsertOne(sessCtx, doc); err != nil {\n            return nil, err\n        }\n        \n        // 2. 최신 버전 컬렉션에도 저장\n        currentColl := s.currentCollection(collection)\n        if _, err := currentColl.InsertOne(sessCtx, doc); err != nil {\n            return nil, err\n        }\n        \n        // 3. 감사 로그 생성\n        if err := s.createAuditLog(sessCtx, collection, doc, \"CREATE\", nil); err != nil {\n            return nil, err\n        }\n        \n        return doc, nil\n    })\n    \n    if err != nil {\n        return nil, err\n    }\n    \n    return doc, nil\n}\n```\n\n### 소프트 업데이트\n\n기존 버전은 그대로 두고 새 버전을 추가합니다:\n\n```go\nfunc (s *versionService) SoftUpdate(ctx context.Context, collection, uri string, updates BsonDocument) (*BsonDocument, error) {\n    // 현재 최신 버전 조회\n    current, err := s.FindLatest(ctx, collection, uri)\n    if err != nil {\n        return nil, err\n    }\n    \n    if current.DocStatus == DocStateDeleted {\n        return nil, ErrDocumentDeleted\n    }\n    \n    now := time.Now()\n    \n    // 새 버전 문서 생성 (기존 필드 + 업데이트 필드)\n    newFields := mergeFields(current.Fields, updates.Fields)\n    changedFields := detectChangedFields(current.Fields, newFields)\n    \n    newDoc := &VersionedDocument{\n        URI:       uri,\n        Version:   current.Version + 1, // 버전 증가\n        DocStatus: DocStateActive,\n        Fields:    newFields,\n        CreatedAt: current.CreatedAt, // 원본 생성 시간 유지\n        UpdatedAt: now,\n    }\n    \n    _, err = s.WithTransaction(ctx, func(sessCtx context.Context) (interface{}, error) {\n        // 1. 새 버전을 히스토리 컬렉션에 추가 (Append)\n        if _, err := s.collection(collection).InsertOne(sessCtx, newDoc); err != nil {\n            return nil, err\n        }\n        \n        // 2. 최신 버전 컬렉션 업데이트 (Replace)\n        filter := bson.M{\"uri\": uri}\n        if _, err := s.currentCollection(collection).ReplaceOne(sessCtx, filter, newDoc); err != nil {\n            return nil, err\n        }\n        \n        // 3. 감사 로그\n        auditLog := &AuditEntry{\n            Collection:    collection,\n            DocumentURI:   uri,\n            Action:        \"UPDATE\",\n            ChangedAt:     now,\n            PreviousData:  current.Fields,\n            NewData:       newFields,\n            ChangedFields: changedFields,\n        }\n        if err := s.insertAuditLog(sessCtx, auditLog); err != nil {\n            return nil, err\n        }\n        \n        return newDoc, nil\n    })\n    \n    if err != nil {\n        return nil, err\n    }\n    \n    return newDoc, nil\n}\n```\n\n### 소프트 삭제\n\n물리적 삭제 대신 삭제 상태의 새 버전을 생성합니다:\n\n```go\nfunc (s *versionService) SoftDelete(ctx context.Context, collection, uri string) (*BsonDocument, error) {\n    current, err := s.FindLatest(ctx, collection, uri)\n    if err != nil {\n        return nil, err\n    }\n    \n    now := time.Now()\n    \n    // 삭제 상태의 새 버전 생성\n    deletedDoc := &VersionedDocument{\n        URI:       uri,\n        Version:   current.Version + 1,\n        DocStatus: DocStateDeleted, // DELETED 상태\n        Fields:    current.Fields,  // 마지막 데이터 보존\n        CreatedAt: current.CreatedAt,\n        UpdatedAt: now,\n    }\n    \n    _, err = s.WithTransaction(ctx, func(sessCtx context.Context) (interface{}, error) {\n        // 1. 삭제 버전 히스토리에 추가\n        if _, err := s.collection(collection).InsertOne(sessCtx, deletedDoc); err != nil {\n            return nil, err\n        }\n        \n        // 2. 최신 버전 컬렉션도 삭제 상태로 업데이트\n        filter := bson.M{\"uri\": uri}\n        if _, err := s.currentCollection(collection).ReplaceOne(sessCtx, filter, deletedDoc); err != nil {\n            return nil, err\n        }\n        \n        // 3. 감사 로그\n        if err := s.createAuditLog(sessCtx, collection, deletedDoc, \"DELETE\", current.Fields); err != nil {\n            return nil, err\n        }\n        \n        return deletedDoc, nil\n    })\n    \n    return deletedDoc, err\n}\n```\n\n### 변경 필드 감지\n\n```go\nfunc detectChangedFields(previous, current map[string]interface{}) []string {\n    var changed []string\n    \n    for key, newVal := range current {\n        oldVal, exists := previous[key]\n        if !exists || !reflect.DeepEqual(oldVal, newVal) {\n            changed = append(changed, key)\n        }\n    }\n    \n    // 삭제된 필드 감지\n    for key := range previous {\n        if _, exists := current[key]; !exists {\n            changed = append(changed, key)\n        }\n    }\n    \n    return changed\n}\n```\n\n## Optimistic Locking\n\n동시 업데이트 충돌 방지를 위한 낙관적 잠금:\n\n```go\nfunc (s *versionService) SoftUpdateWithLock(\n    ctx context.Context, \n    collection, uri string, \n    expectedVersion int32,\n    updates BsonDocument,\n) (*BsonDocument, error) {\n    current, err := s.FindLatest(ctx, collection, uri)\n    if err != nil {\n        return nil, err\n    }\n    \n    // 버전 불일치 시 충돌 에러\n    if current.Version != expectedVersion {\n        return nil, fmt.Errorf(\n            \"version conflict: expected %d, actual %d\",\n            expectedVersion, current.Version,\n        )\n    }\n    \n    return s.SoftUpdate(ctx, collection, uri, updates)\n}\n```\n\n## 쿼리 패턴\n\n### 최신 버전 조회 (빠름)\n\n```go\n// current_<collection>에서 직접 조회\nfunc (s *versionService) FindLatest(ctx context.Context, collection, uri string) (*BsonDocument, error) {\n    filter := bson.M{\n        \"uri\":        uri,\n        \"doc_status\": bson.M{\"$ne\": DocStateDeleted},\n    }\n    \n    var doc VersionedDocument\n    err := s.currentCollection(collection).FindOne(ctx, filter).Decode(&doc)\n    if err != nil {\n        return nil, err\n    }\n    \n    return &doc, nil\n}\n```\n\n### 특정 버전 조회\n\n```go\nfunc (s *versionService) FindByVersion(ctx context.Context, collection, uri string, version int32) (*BsonDocument, error) {\n    filter := bson.M{\n        \"uri\":     uri,\n        \"version\": version,\n    }\n    \n    var doc VersionedDocument\n    err := s.collection(collection).FindOne(ctx, filter).Decode(&doc)\n    if err != nil {\n        return nil, err\n    }\n    \n    return &doc, nil\n}\n```\n\n### 히스토리 조회\n\n```go\nfunc (s *versionService) GetHistory(ctx context.Context, collection, uri string) ([]BsonDocument, error) {\n    filter := bson.M{\"uri\": uri}\n    opts := options.Find().SetSort(bson.D{{Key: \"version\", Value: 1}})\n    \n    cursor, err := s.collection(collection).Find(ctx, filter, opts)\n    if err != nil {\n        return nil, err\n    }\n    defer cursor.Close(ctx)\n    \n    var docs []VersionedDocument\n    if err := cursor.All(ctx, &docs); err != nil {\n        return nil, err\n    }\n    \n    return docs, nil\n}\n```\n\n## 인덱스 전략\n\n```go\n// 히스토리 컬렉션 인덱스\nindexes := []mongo.IndexModel{\n    {\n        Keys: bson.D{{Key: \"uri\", Value: 1}, {Key: \"version\", Value: -1}},\n        Options: options.Index().SetUnique(true),\n    },\n    {\n        Keys: bson.D{{Key: \"created_at\", Value: -1}},\n    },\n}\n\n// 최신 버전 컬렉션 인덱스\ncurrentIndexes := []mongo.IndexModel{\n    {\n        Keys: bson.D{{Key: \"uri\", Value: 1}},\n        Options: options.Index().SetUnique(true),\n    },\n    {\n        Keys: bson.D{{Key: \"doc_status\", Value: 1}},\n    },\n}\n```\n\n## 스토리지 최적화\n\n버전 누적으로 인한 저장 공간 증가 대응:\n\n### 1. 필드 압축\n\n```go\n// 변경된 필드만 저장하는 Delta 방식 (선택적)\ntype DeltaDocument struct {\n    URI         string                 `bson:\"uri\"`\n    Version     int32                  `bson:\"version\"`\n    BaseVersion int32                  `bson:\"base_version,omitempty\"`\n    Delta       map[string]interface{} `bson:\"delta,omitempty\"`\n    IsSnapshot  bool                   `bson:\"is_snapshot\"`\n}\n```\n\n### 2. TTL 정책\n\n```go\n// 오래된 버전 자동 정리 (최신 N개 유지)\nindexes := mongo.IndexModel{\n    Keys:    bson.D{{Key: \"created_at\", Value: 1}},\n    Options: options.Index().SetExpireAfterSeconds(86400 * 365), // 1년\n}\n```\n\n## 모범 사례\n\n1. **트랜잭션 필수**: 두 컬렉션 동시 업데이트 시 원자성 보장\n2. **URI 불변성**: 문서 식별자(URI)는 생성 후 변경 금지\n3. **감사 로그 분리**: 감사 로그는 별도 컬렉션에 저장하여 독립적 접근\n4. **인덱스 최적화**: 최신 버전 조회 패턴에 맞는 인덱스 설계\n5. **버전 상한 설정**: 무한 버전 증가 방지 정책 고려\n\n## 참고 자료\n\n- [Event Sourcing Pattern](https://martinfowler.com/eaaDev/EventSourcing.html)\n- [MongoDB Transactions](https://www.mongodb.com/docs/manual/core/transactions/)\n- [Immutable Data Patterns](https://www.cidrdb.org/cidr2015/Papers/CIDR15_Paper16.pdf)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Architecture",
      "Audit",
      "Go",
      "MongoDB"
    ],
    "readingTime": 8,
    "wordCount": 1447,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "grpc-gateway-dual-protocol",
    "slug": "grpc-gateway-dual-protocol",
    "path": "backend/http",
    "fullPath": "backend/http/grpc-gateway-dual-protocol",
    "title": "gRPC-Gateway로 단일 API 듀얼 프로토콜 지원",
    "excerpt": "하나의 Proto 정의로 gRPC와 RESTful HTTP API를 동시에 제공하는 gRPC-Gateway 구현 방법을 알아봅니다.",
    "content": "# gRPC-Gateway로 단일 API 듀얼 프로토콜 지원\n\n## 개요\n\n**gRPC-Gateway**는 gRPC 서비스에 RESTful HTTP/JSON 인터페이스를 자동으로 추가해주는 리버스 프록시입니다. 하나의 Proto 정의로 두 프로토콜을 모두 지원할 수 있습니다.\n\n## 왜 듀얼 프로토콜인가?\n\n### 장점\n\n| 프로토콜 | 사용 시나리오 |\n|----------|--------------|\n| **gRPC** | 마이크로서비스 간 통신, 고성능 필요, 스트리밍 |\n| **HTTP/JSON** | 웹 브라우저, 외부 API 클라이언트, 디버깅 |\n\n단일 코드베이스로 두 니즈를 모두 충족할 수 있습니다.\n\n### 단점\n\n| 특성 | 설명 |\n|------|------|\n| 추가 레이어 | HTTP → gRPC 변환 오버헤드 |\n| 기능 제한 | HTTP에서 gRPC 스트리밍 일부 지원 제한 |\n| 복잡성 | 두 프로토콜의 에러 처리 매핑 필요 |\n\n## 아키텍처\n\n```mermaid\ngraph LR\n    %% 외부 클라이언트 정의\n    subgraph Clients [\"Clients\"]\n        HTTP_Client[\"HTTP Client<br/>(Browser, etc)\"]\n        gRPC_Client[\"gRPC Client<br/>(Microservice)\"]\n    end\n\n    %% API 서버 내부 구조 정의\n    subgraph APIServer [\"API Server\"]\n        direction TB\n        \n        subgraph Ports [\"Incoming Ports\"]\n            HTTP_Port[\":8080 HTTP\"]\n            gRPC_Port[\":9090 gRPC\"]\n        end\n\n        Mux[\"gRPC-Gateway Mux\"]\n        gRPC_Srv[\"gRPC Server<br/>(Core Logic)\"]\n        \n        subgraph Endpoints [\"Misc Endpoints\"]\n            OpenAPI[\"OpenAPI<br/>/openapi\"]\n            Health[\"Health<br/>/ready\"]\n        end\n\n        %% 내부 흐름\n        HTTP_Port --> Mux\n        Mux --> gRPC_Srv\n        gRPC_Port --> gRPC_Srv\n    end\n\n    %% 클라이언트 연결\n    HTTP_Client -- \"JSON/REST\" --> HTTP_Port\n    gRPC_Client -- \"Protobuf\" --> gRPC_Port\n\n    %% 스타일링\n    style APIServer fill:#f5f5f5,stroke:#333,stroke-width:2px\n    style Mux fill:#e1f5fe,stroke:#01579b\n    style gRPC_Srv fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px\n    style HTTP_Client fill:#fff,stroke:#333\n    style gRPC_Client fill:#fff,stroke:#333\n```\n\n## Proto 정의\n\n### HTTP 어노테이션 추가\n\n```protobuf\nsyntax = \"proto3\";\n\npackage v1beta;\n\nimport \"google/api/annotations.proto\";\nimport \"google/protobuf/struct.proto\";\n\nservice DocumentService {\n  // POST /v1beta/collections/{collection}/documents\n  rpc CreateDocument(CreateDocumentRequest) returns (CreateDocumentResponse) {\n    option (google.api.http) = {\n      post: \"/v1beta/collections/{collection}/documents\"\n      body: \"*\"\n    };\n  }\n  \n  // GET /v1beta/collections/{collection}/documents/{uri}\n  rpc GetDocument(GetDocumentRequest) returns (GetDocumentResponse) {\n    option (google.api.http) = {\n      get: \"/v1beta/collections/{collection}/documents/{uri}\"\n    };\n  }\n  \n  // PATCH /v1beta/collections/{collection}/documents/{uri}\n  rpc UpdateDocument(UpdateDocumentRequest) returns (UpdateDocumentResponse) {\n    option (google.api.http) = {\n      patch: \"/v1beta/collections/{collection}/documents/{uri}\"\n      body: \"*\"\n    };\n  }\n  \n  // DELETE /v1beta/collections/{collection}/documents/{uri}\n  rpc DeleteDocument(DeleteDocumentRequest) returns (DeleteDocumentResponse) {\n    option (google.api.http) = {\n      delete: \"/v1beta/collections/{collection}/documents/{uri}\"\n    };\n  }\n  \n  // POST로 쿼리 (URL 길이 제한 회피)\n  // POST /v1beta/collections/{collection}/documents:query\n  rpc QueryDocuments(QueryDocumentsRequest) returns (QueryDocumentsResponse) {\n    option (google.api.http) = {\n      post: \"/v1beta/collections/{collection}/documents:query\"\n      body: \"*\"\n    };\n  }\n  \n  // 배치 작업\n  // POST /v1beta/collections/{collection}/documents:batchCreate\n  rpc BatchCreateDocuments(BatchCreateRequest) returns (BatchCreateResponse) {\n    option (google.api.http) = {\n      post: \"/v1beta/collections/{collection}/documents:batchCreate\"\n      body: \"*\"\n    };\n  }\n}\n\nmessage CreateDocumentRequest {\n  // URL 경로 파라미터로 추출됨\n  string collection = 1;\n  DocumentInput document = 2;\n}\n\nmessage GetDocumentRequest {\n  string collection = 1;\n  string uri = 2;\n  // 쿼리 파라미터: ?version=3\n  optional int32 version = 3;\n  // 쿼리 파라미터: ?skip_validation=true\n  optional bool skip_validation = 4;\n}\n\n// ... 나머지 메시지 정의\n```\n\n### URL 매핑 규칙\n\n| gRPC 필드 위치 | HTTP 위치 |\n|---------------|-----------|\n| `{field}` in path | URL 경로 파라미터 |\n| `body: \"*\"` | 나머지 필드는 JSON body |\n| `body: \"field\"` | 특정 필드만 body |\n| 그 외 필드 | 쿼리 파라미터 |\n\n## 코드 생성 설정\n\n### buf.gen.yaml\n\n```yaml\nversion: v2\nplugins:\n  # gRPC 서버/클라이언트\n  - remote: buf.build/grpc/go:v1.5.1\n    out: generated/go/proto\n    opt: paths=source_relative\n  \n  # gRPC-Gateway 핸들러\n  - remote: buf.build/grpc-ecosystem/gateway:v2.25.1\n    out: generated/go/proto\n    opt:\n      - paths=source_relative\n      - standalone=true       # 독립 파일로 생성\n      - generate_unbound_methods=true\n```\n\n### 생성 실행\n\n```bash\nbuf generate\n```\n\n생성된 파일:\n\n- `api_grpc.pb.go`: gRPC 서버/클라이언트\n- `api.pb.gw.go`: HTTP Gateway 핸들러\n\n## 서버 구현\n\n### 통합 서버\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"net\"\n    \"net/http\"\n    \"sync\"\n    \n    \"github.com/grpc-ecosystem/grpc-gateway/v2/runtime\"\n    \"google.golang.org/grpc\"\n    \"google.golang.org/grpc/credentials/insecure\"\n    \n    pb \"github.com/myorg/myservice/generated/go/proto/v1beta\"\n)\n\ntype Server struct {\n    grpcServer *grpc.Server\n    httpServer *http.Server\n    \n    grpcPort string\n    httpPort string\n}\n\nfunc NewServer(service DocumentService) *Server {\n    // gRPC 서버 설정\n    grpcServer := grpc.NewServer(\n        grpc.UnaryInterceptor(loggingInterceptor),\n    )\n    pb.RegisterDocumentServiceServer(grpcServer, &documentHandler{service})\n    \n    return &Server{\n        grpcServer: grpcServer,\n        grpcPort:   \":9090\",\n        httpPort:   \":8080\",\n    }\n}\n\nfunc (s *Server) Start(ctx context.Context) error {\n    var wg sync.WaitGroup\n    errCh := make(chan error, 2)\n    \n    // 1. gRPC 서버 시작\n    wg.Add(1)\n    go func() {\n        defer wg.Done()\n        lis, err := net.Listen(\"tcp\", s.grpcPort)\n        if err != nil {\n            errCh <- err\n            return\n        }\n        if err := s.grpcServer.Serve(lis); err != nil {\n            errCh <- err\n        }\n    }()\n    \n    // 2. HTTP Gateway 시작\n    wg.Add(1)\n    go func() {\n        defer wg.Done()\n        if err := s.startHTTPGateway(ctx); err != nil {\n            errCh <- err\n        }\n    }()\n    \n    select {\n    case err := <-errCh:\n        return err\n    case <-ctx.Done():\n        s.Shutdown()\n        return ctx.Err()\n    }\n}\n\nfunc (s *Server) startHTTPGateway(ctx context.Context) error {\n    mux := runtime.NewServeMux(\n        // JSON 필드 이름 설정\n        runtime.WithMarshalerOption(runtime.MIMEWildcard, &runtime.JSONPb{\n            MarshalOptions: protojson.MarshalOptions{\n                UseProtoNames:   true,\n                EmitUnpopulated: true,\n            },\n            UnmarshalOptions: protojson.UnmarshalOptions{\n                DiscardUnknown: true,\n            },\n        }),\n        // 에러 핸들링 커스터마이즈\n        runtime.WithErrorHandler(customErrorHandler),\n    )\n    \n    opts := []grpc.DialOption{grpc.WithTransportCredentials(insecure.NewCredentials())}\n    \n    // gRPC 서버에 연결\n    err := pb.RegisterDocumentServiceHandlerFromEndpoint(ctx, mux, \"localhost\"+s.grpcPort, opts)\n    if err != nil {\n        return err\n    }\n    \n    // 추가 HTTP 엔드포인트\n    handler := http.NewServeMux()\n    handler.Handle(\"/\", mux)\n    handler.HandleFunc(\"/openapi.yaml\", serveOpenAPI)\n    handler.HandleFunc(\"/ready\", healthCheck)\n    \n    s.httpServer = &http.Server{\n        Addr:    s.httpPort,\n        Handler: handler,\n    }\n    \n    return s.httpServer.ListenAndServe()\n}\n\nfunc (s *Server) Shutdown() {\n    s.grpcServer.GracefulStop()\n    if s.httpServer != nil {\n        s.httpServer.Shutdown(context.Background())\n    }\n}\n```\n\n### 에러 핸들링\n\ngRPC 상태 코드를 HTTP 상태 코드로 매핑:\n\n```go\nfunc customErrorHandler(\n    ctx context.Context,\n    mux *runtime.ServeMux,\n    marshaler runtime.Marshaler,\n    w http.ResponseWriter,\n    r *http.Request,\n    err error,\n) {\n    // gRPC 에러에서 상태 추출\n    st, _ := status.FromError(err)\n    \n    // HTTP 상태 코드 매핑\n    httpStatus := runtime.HTTPStatusFromCode(st.Code())\n    \n    // 커스텀 에러 응답\n    response := map[string]interface{}{\n        \"code\":    st.Code().String(),\n        \"message\": st.Message(),\n    }\n    \n    if details := st.Details(); len(details) > 0 {\n        response[\"details\"] = details\n    }\n    \n    w.Header().Set(\"Content-Type\", \"application/json\")\n    w.WriteHeader(httpStatus)\n    json.NewEncoder(w).Encode(response)\n}\n```\n\n## 인-프로세스 vs 네트워크 연결\n\n### 네트워크 연결 (기본)\n\n```go\n// gRPC 서버에 네트워크로 연결\npb.RegisterDocumentServiceHandlerFromEndpoint(ctx, mux, \"localhost:9090\", opts)\n```\n\n### 인-프로세스 연결 (권장)\n\n네트워크 오버헤드 없이 직접 연결:\n\n```go\nfunc (s *Server) startHTTPGatewayInProcess(ctx context.Context) error {\n    mux := runtime.NewServeMux()\n    \n    // 서버 구현체를 직접 등록 (네트워크 경유 없음)\n    err := pb.RegisterDocumentServiceHandlerServer(ctx, mux, s.documentHandler)\n    if err != nil {\n        return err\n    }\n    \n    s.httpServer = &http.Server{\n        Addr:    s.httpPort,\n        Handler: mux,\n    }\n    \n    return s.httpServer.ListenAndServe()\n}\n```\n\n## 미들웨어\n\n### HTTP 미들웨어 체인\n\n```go\nfunc (s *Server) buildHTTPHandler(mux *runtime.ServeMux) http.Handler {\n    // 미들웨어 체인\n    handler := corsMiddleware(mux)\n    handler = loggingMiddleware(handler)\n    handler = metricsMiddleware(handler)\n    \n    return handler\n}\n\nfunc corsMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        w.Header().Set(\"Access-Control-Allow-Origin\", \"*\")\n        w.Header().Set(\"Access-Control-Allow-Methods\", \"GET, POST, PUT, PATCH, DELETE, OPTIONS\")\n        w.Header().Set(\"Access-Control-Allow-Headers\", \"Content-Type, Authorization\")\n        \n        if r.Method == \"OPTIONS\" {\n            w.WriteHeader(http.StatusNoContent)\n            return\n        }\n        \n        next.ServeHTTP(w, r)\n    })\n}\n\nfunc loggingMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        start := time.Now()\n        \n        wrapped := &responseWriter{ResponseWriter: w, status: 200}\n        next.ServeHTTP(wrapped, r)\n        \n        log.Printf(\"%s %s %d %v\", r.Method, r.URL.Path, wrapped.status, time.Since(start))\n    })\n}\n```\n\n### gRPC 인터셉터\n\n```go\nfunc loggingInterceptor(\n    ctx context.Context,\n    req interface{},\n    info *grpc.UnaryServerInfo,\n    handler grpc.UnaryHandler,\n) (interface{}, error) {\n    start := time.Now()\n    \n    resp, err := handler(ctx, req)\n    \n    log.Printf(\"gRPC %s %v err=%v\", info.FullMethod, time.Since(start), err)\n    \n    return resp, err\n}\n```\n\n## OpenAPI 스펙 제공\n\n### 자동 생성된 스펙 제공\n\n```go\n//go:embed generated/docs/openapi.yaml\nvar openAPISpec []byte\n\nfunc serveOpenAPI(w http.ResponseWriter, r *http.Request) {\n    w.Header().Set(\"Content-Type\", \"application/x-yaml\")\n    w.Write(openAPISpec)\n}\n```\n\n### Stoplight Elements 통합\n\n정적 HTML과 Stoplight Elements 웹 컴포넌트로 인터랙티브 API 문서를 제공합니다:\n\n```html\n<!-- docs/openapi.html -->\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"utf-8\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n    <title>API Documentation</title>\n    <script src=\"https://unpkg.com/@stoplight/elements/web-components.min.js\"></script>\n    <link rel=\"stylesheet\" href=\"https://unpkg.com/@stoplight/elements/styles.min.css\" />\n  </head>\n  <body>\n    <elements-api\n      apiDescriptionUrl=\"openapi.swagger.json\"\n      router=\"hash\"\n      layout=\"sidebar\"\n      hideInternal=\"true\"\n    />\n  </body>\n</html>\n```\n\nProto 프로젝트에서 embed.FS로 정적 파일 제공:\n\n```go\n// generated/go/docs/embed.go\npackage docs\n\nimport \"embed\"\n\n//go:embed openapi.html openapi.swagger.json\nvar StaticFiles embed.FS\n```\n\ngRPC-Gateway Mux에 직접 경로 등록:\n\n```go\nimport (\n    \"io/fs\"\n    \"net/http\"\n    \n    \"github.com/grpc-ecosystem/grpc-gateway/v2/runtime\"\n    \"myproject/generated/go/docs\"\n)\n\nfunc setupAPIRoutes(gatewayMux *runtime.ServeMux) {\n    // 파일 서빙 헬퍼\n    serveEmbeddedFile := func(w http.ResponseWriter, r *http.Request, fsys fs.FS, name string) {\n        fileServer := http.FileServer(http.FS(fsys))\n        r.URL.Path = name\n        fileServer.ServeHTTP(w, r)\n    }\n    \n    // OpenAPI JSON 스펙\n    gatewayMux.HandlePath(\"GET\", \"/openapi.swagger.json\", \n        func(w http.ResponseWriter, r *http.Request, _ map[string]string) {\n            serveEmbeddedFile(w, r, docs.StaticFiles, \"openapi.swagger.json\")\n        })\n    \n    // Stoplight Elements UI (HTML)\n    gatewayMux.HandlePath(\"GET\", \"/openapi.html\", \n        func(w http.ResponseWriter, r *http.Request, _ map[string]string) {\n            serveEmbeddedFile(w, r, docs.StaticFiles, \"openapi.html\")\n        })\n    \n    // 루트 접속 시 문서로 리다이렉트\n    gatewayMux.HandlePath(\"GET\", \"/\", \n        func(w http.ResponseWriter, r *http.Request, _ map[string]string) {\n            http.Redirect(w, r, \"/openapi.html\", http.StatusFound)\n        })\n}\n```\n\n## 요청/응답 예시\n\n### HTTP 요청\n\n```bash\n# 문서 생성\ncurl -X POST http://localhost:8080/v1beta/collections/users/documents \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"document\": {\n      \"uri\": \"user-001\",\n      \"fields\": {\n        \"name\": \"John Doe\",\n        \"email\": \"john@example.com\"\n      }\n    }\n  }'\n\n# 문서 조회\ncurl http://localhost:8080/v1beta/collections/users/documents/user-001?version=1\n\n# 문서 업데이트\ncurl -X PATCH http://localhost:8080/v1beta/collections/users/documents/user-001 \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"fields\": {\"name\": \"Jane Doe\"}}'\n```\n\n### gRPC 요청 (grpcurl)\n\n```bash\n# 문서 생성\ngrpcurl -plaintext -d '{\n  \"collection\": \"users\",\n  \"document\": {\n    \"uri\": \"user-001\",\n    \"fields\": {\"name\": \"John Doe\"}\n  }\n}' localhost:9090 v1beta.DocumentService/CreateDocument\n\n# 문서 조회\ngrpcurl -plaintext -d '{\n  \"collection\": \"users\",\n  \"uri\": \"user-001\"\n}' localhost:9090 v1beta.DocumentService/GetDocument\n```\n\n## 모범 사례\n\n1. **인-프로세스 연결**: 가능하면 `RegisterHandlerServer` 사용\n2. **일관된 에러 처리**: gRPC/HTTP 에러 매핑 통일\n3. **OpenAPI 통합**: 자동 생성 스펙으로 문서화\n4. **미들웨어 분리**: HTTP/gRPC 각각 적절한 미들웨어 적용\n5. **Health Check**: `/ready` 엔드포인트로 헬스체크 분리\n\n## 참고 자료\n\n- [gRPC-Gateway 공식 문서](https://grpc-ecosystem.github.io/grpc-gateway/)\n- [google.api.http 어노테이션](https://cloud.google.com/endpoints/docs/grpc/transcoding)\n- [runtime.ServeMux 옵션](https://pkg.go.dev/github.com/grpc-ecosystem/grpc-gateway/v2/runtime)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Architecture",
      "Backend",
      "Go",
      "HTTP",
      "gRPC"
    ],
    "readingTime": 7,
    "wordCount": 1375,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "wire-dependency-injection",
    "slug": "wire-dependency-injection",
    "path": "backend/go",
    "fullPath": "backend/go/wire-dependency-injection",
    "title": "Wire를 활용한 Go 의존성 주입(DI) 구현",
    "excerpt": "Google의 Wire를 사용하여 Go 애플리케이션에서 컴파일 타임 의존성 주입을 구현하는 방법을 알아봅니다.",
    "content": "# Wire를 활용한 Go 의존성 주입(DI) 구현\n\n## 개요\n\n**Wire**는 Google에서 개발한 Go용 컴파일 타임 의존성 주입 도구입니다. 리플렉션 없이 코드 생성을 통해 DI를 구현하므로 런타임 오버헤드가 없습니다.\n\n## 왜 Wire인가?\n\n| 도구 | 방식 | 장점 | 단점 |\n|-----|------|------|------|\n| 수동 DI | 직접 구성 | 단순, 명시적 | 보일러플레이트 |\n| dig/fx | 런타임 리플렉션 | 유연 | 런타임 오버헤드, 디버깅 어려움 |\n| **Wire** | 컴파일 타임 코드 생성 | 타입 안전, 오버헤드 없음 | 초기 설정 필요 |\n\n## 설치\n\n```bash\ngo install github.com/google/wire/cmd/wire@latest\n```\n\n## 기본 개념\n\n### Provider\n\n**Provider**는 의존성을 생성하는 함수입니다:\n\n```go\n// providers.go\npackage main\n\ntype Config struct {\n    DBHost string\n    DBPort int\n}\n\nfunc NewConfig() *Config {\n    return &Config{\n        DBHost: \"localhost\",\n        DBPort: 5432,\n    }\n}\n\ntype Database struct {\n    config *Config\n}\n\nfunc NewDatabase(cfg *Config) (*Database, error) {\n    return &Database{config: cfg}, nil\n}\n\ntype UserRepository struct {\n    db *Database\n}\n\nfunc NewUserRepository(db *Database) *UserRepository {\n    return &UserRepository{db: db}\n}\n\ntype UserService struct {\n    repo *UserRepository\n}\n\nfunc NewUserService(repo *UserRepository) *UserService {\n    return &UserService{repo: repo}\n}\n```\n\n### Injector\n\n**Injector**는 의존성 그래프를 정의하는 함수입니다:\n\n```go\n// wire.go\n//go:build wireinject\n\npackage main\n\nimport \"github.com/google/wire\"\n\nfunc InitializeUserService() (*UserService, error) {\n    wire.Build(\n        NewConfig,\n        NewDatabase,\n        NewUserRepository,\n        NewUserService,\n    )\n    return nil, nil // Wire가 이 부분을 생성된 코드로 대체\n}\n```\n\n### 코드 생성\n\n```bash\nwire ./...\n\n# 생성된 파일: wire_gen.go\n```\n\n생성된 `wire_gen.go`:\n\n```go\n// Code generated by Wire. DO NOT EDIT.\n//go:build !wireinject\n\npackage main\n\nfunc InitializeUserService() (*UserService, error) {\n    config := NewConfig()\n    database, err := NewDatabase(config)\n    if err != nil {\n        return nil, err\n    }\n    userRepository := NewUserRepository(database)\n    userService := NewUserService(userRepository)\n    return userService, nil\n}\n```\n\n## Provider Set\n\n관련 Provider들을 그룹화할 수 있습니다:\n\n```go\n// infrastructure/wire.go\npackage infrastructure\n\nimport \"github.com/google/wire\"\n\nvar InfraSet = wire.NewSet(\n    NewConfig,\n    NewDatabase,\n    NewRedisClient,\n    NewLogger,\n)\n```\n\n```go\n// repository/wire.go\npackage repository\n\nimport \"github.com/google/wire\"\n\nvar RepositorySet = wire.NewSet(\n    NewUserRepository,\n    NewOrderRepository,\n    NewProductRepository,\n)\n```\n\n```go\n// service/wire.go\npackage service\n\nimport \"github.com/google/wire\"\n\nvar ServiceSet = wire.NewSet(\n    NewUserService,\n    NewOrderService,\n    NewProductService,\n)\n```\n\n```go\n// wire.go\npackage main\n\nimport \"github.com/google/wire\"\n\nfunc InitializeApp() (*App, error) {\n    wire.Build(\n        infrastructure.InfraSet,\n        repository.RepositorySet,\n        service.ServiceSet,\n        NewApp,\n    )\n    return nil, nil\n}\n```\n\n## 인터페이스 바인딩\n\n인터페이스에 구현체를 바인딩할 수 있습니다:\n\n```go\n// 인터페이스 정의\ntype UserRepository interface {\n    FindByID(id string) (*User, error)\n    Save(user *User) error\n}\n\n// 구현체\ntype userRepositoryImpl struct {\n    db *Database\n}\n\nfunc NewUserRepository(db *Database) *userRepositoryImpl {\n    return &userRepositoryImpl{db: db}\n}\n\n// Wire 바인딩\nvar RepositorySet = wire.NewSet(\n    NewUserRepository,\n    wire.Bind(new(UserRepository), new(*userRepositoryImpl)),\n)\n```\n\n## Struct Provider\n\n구조체 필드를 직접 주입할 수 있습니다:\n\n```go\ntype App struct {\n    UserService  *UserService\n    OrderService *OrderService\n    Logger       *Logger\n}\n\nvar AppSet = wire.NewSet(\n    wire.Struct(new(App), \"*\"), // 모든 필드 주입\n    // 또는 특정 필드만\n    // wire.Struct(new(App), \"UserService\", \"Logger\"),\n)\n```\n\n## Value Provider\n\n정적 값을 제공할 수 있습니다:\n\n```go\nfunc InitializeApp(cfg *Config) (*App, error) {\n    wire.Build(\n        wire.Value(&Config{DBHost: \"localhost\"}), // 정적 값\n        NewDatabase,\n        NewApp,\n    )\n    return nil, nil\n}\n```\n\n## 실전 프로젝트 구조\n\n```\nmyapp/\n├── cmd/\n│   └── api/\n│       ├── main.go\n│       ├── wire.go          # Injector 정의\n│       └── wire_gen.go      # 생성된 코드\n├── internal/\n│   ├── config/\n│   │   └── config.go\n│   ├── infrastructure/\n│   │   ├── database.go\n│   │   ├── redis.go\n│   │   └── wire.go          # Provider Set\n│   ├── repository/\n│   │   ├── user_repository.go\n│   │   └── wire.go\n│   ├── service/\n│   │   ├── user_service.go\n│   │   └── wire.go\n│   └── handler/\n│       ├── user_handler.go\n│       └── wire.go\n└── go.mod\n```\n\n### cmd/api/wire.go\n\n```go\n//go:build wireinject\n\npackage main\n\nimport (\n    \"github.com/google/wire\"\n    \"myapp/internal/config\"\n    \"myapp/internal/infrastructure\"\n    \"myapp/internal/repository\"\n    \"myapp/internal/service\"\n    \"myapp/internal/handler\"\n)\n\nfunc InitializeServer(cfg *config.Config) (*Server, error) {\n    wire.Build(\n        infrastructure.InfraSet,\n        repository.RepositorySet,\n        service.ServiceSet,\n        handler.HandlerSet,\n        NewServer,\n    )\n    return nil, nil\n}\n```\n\n### cmd/api/main.go\n\n```go\npackage main\n\nfunc main() {\n    cfg := config.Load()\n    \n    server, err := InitializeServer(cfg)\n    if err != nil {\n        log.Fatal(err)\n    }\n    \n    server.Run()\n}\n```\n\n## 테스트에서 Wire 활용\n\n```go\n// wire_test.go\n//go:build wireinject\n\npackage main\n\nimport \"github.com/google/wire\"\n\nfunc InitializeTestUserService(mockDB *MockDatabase) *UserService {\n    wire.Build(\n        NewUserRepository,\n        NewUserService,\n    )\n    return nil\n}\n```\n\n## Makefile 통합\n\n```makefile\n.PHONY: generate wire\n\nwire:\n wire ./...\n\ngenerate: wire\n go generate ./...\n\nbuild: generate\n go build -o bin/app ./cmd/api\n```\n\n## 주의사항\n\n1. **빌드 태그 필수**: `//go:build wireinject` 잊지 말 것\n2. **wire_gen.go 커밋**: 생성된 코드는 버전 관리에 포함\n3. **순환 의존성 불가**: Wire가 에러로 감지\n4. **에러 처리**: Provider가 error를 반환하면 자동 전파\n\n## 참고 자료\n\n- [Wire GitHub](https://github.com/google/wire)\n- [Wire Tutorial](https://github.com/google/wire/blob/main/docs/guide.md)\n- [Wire Best Practices](https://github.com/google/wire/blob/main/docs/best-practices.md)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Architecture",
      "Backend",
      "DI",
      "Go"
    ],
    "readingTime": 4,
    "wordCount": 720,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "pprof-profiling-guide",
    "slug": "pprof-profiling-guide",
    "path": "backend/go",
    "fullPath": "backend/go/pprof-profiling-guide",
    "title": "Go pprof를 활용한 성능 프로파일링 가이드",
    "excerpt": "Go의 내장 프로파일링 도구 pprof를 활용하여 CPU, 메모리, 고루틴 병목을 분석하는 방법을 알아봅니다.",
    "content": "# Go pprof를 활용한 성능 프로파일링 가이드\n\n## 개요\n\n**pprof**는 Go에 내장된 프로파일링 도구로, CPU 사용량, 메모리 할당, 고루틴 상태 등을 분석할 수 있습니다. 프로덕션 환경에서도 안전하게 사용할 수 있어 성능 최적화에 필수적입니다.\n\n## 기본 설정\n\n### HTTP 서버에 pprof 엔드포인트 추가\n\n```go\npackage main\n\nimport (\n    \"net/http\"\n    _ \"net/http/pprof\" // 자동으로 /debug/pprof/* 엔드포인트 등록\n)\n\nfunc main() {\n    go func() {\n        // 별도 포트에서 pprof 서버 실행 (보안상 권장)\n        http.ListenAndServe(\"localhost:6060\", nil)\n    }()\n    \n    // 메인 애플리케이션 로직\n    // ...\n}\n```\n\n### 제공되는 엔드포인트\n\n| 엔드포인트 | 설명 |\n|----------|------|\n| `/debug/pprof/` | 프로파일 인덱스 페이지 |\n| `/debug/pprof/heap` | 메모리 할당 프로파일 |\n| `/debug/pprof/goroutine` | 고루틴 스택 트레이스 |\n| `/debug/pprof/profile` | CPU 프로파일 (30초) |\n| `/debug/pprof/trace` | 실행 트레이스 |\n\n## CPU 프로파일링\n\n### 프로파일 수집\n\n```bash\n# 30초간 CPU 프로파일 수집\ngo tool pprof http://localhost:6060/debug/pprof/profile?seconds=30\n\n# 또는 파일로 저장\ncurl -o cpu.prof http://localhost:6060/debug/pprof/profile?seconds=30\ngo tool pprof cpu.prof\n```\n\n### 주요 명령어\n\n```bash\n# pprof 인터랙티브 모드에서\n(pprof) top10          # 상위 10개 함수\n(pprof) list funcName  # 특정 함수의 라인별 분석\n(pprof) web            # 그래프 시각화 (브라우저)\n(pprof) pdf            # PDF로 저장\n```\n\n### 출력 예시\n\n```\n      flat  flat%   sum%        cum   cum%\n     2.50s 25.00% 25.00%      4.00s 40.00%  main.processData\n     1.50s 15.00% 40.00%      1.50s 15.00%  runtime.mallocgc\n     1.00s 10.00% 50.00%      3.00s 30.00%  encoding/json.Marshal\n```\n\n| 컬럼 | 설명 |\n|-----|------|\n| `flat` | 해당 함수 자체 실행 시간 |\n| `cum` | 해당 함수 + 호출한 함수 합계 시간 |\n\n## 메모리 프로파일링\n\n### 힙 프로파일 수집\n\n```bash\n# 현재 힙 상태\ngo tool pprof http://localhost:6060/debug/pprof/heap\n\n# 할당된 객체 수 기준\ngo tool pprof -alloc_objects http://localhost:6060/debug/pprof/heap\n\n# 할당된 메모리 크기 기준\ngo tool pprof -alloc_space http://localhost:6060/debug/pprof/heap\n```\n\n### 메모리 누수 탐지\n\n```go\n// runtime.MemStats 활용\nimport \"runtime\"\n\nfunc printMemStats() {\n    var m runtime.MemStats\n    runtime.ReadMemStats(&m)\n    \n    fmt.Printf(\"Alloc = %v MiB\\n\", m.Alloc / 1024 / 1024)\n    fmt.Printf(\"TotalAlloc = %v MiB\\n\", m.TotalAlloc / 1024 / 1024)\n    fmt.Printf(\"Sys = %v MiB\\n\", m.Sys / 1024 / 1024)\n    fmt.Printf(\"NumGC = %v\\n\", m.NumGC)\n}\n```\n\n## 고루틴 프로파일링\n\n```bash\n# 현재 고루틴 상태 확인\ngo tool pprof http://localhost:6060/debug/pprof/goroutine\n\n# 덤프 파일로 저장\ncurl http://localhost:6060/debug/pprof/goroutine?debug=2 > goroutines.txt\n```\n\n고루틴 누수 징후:\n\n- 고루틴 수가 지속적으로 증가\n- 특정 함수에서 대기 중인 고루틴이 많음\n\n## 웹 UI로 시각화\n\n**pprof 웹 인터페이스** (Go 1.10+):\n\n```bash\ngo tool pprof -http=:8080 cpu.prof\n```\n\n브라우저에서 `http://localhost:8080`으로 접속하면:\n\n- **Top**: 함수별 CPU/메모리 사용량\n- **Graph**: 호출 그래프\n- **Flame Graph**: 플레임 차트\n- **Source**: 소스 코드 레벨 분석\n\n## 프로덕션 환경 고도화\n\n### 1. 보안 설정\n\n```go\npackage main\n\nimport (\n    \"net/http\"\n    \"net/http/pprof\"\n)\n\nfunc main() {\n    // 별도 서버에서 인증 추가\n    pprofMux := http.NewServeMux()\n    pprofMux.HandleFunc(\"/debug/pprof/\", pprof.Index)\n    pprofMux.HandleFunc(\"/debug/pprof/cmdline\", pprof.Cmdline)\n    pprofMux.HandleFunc(\"/debug/pprof/profile\", pprof.Profile)\n    pprofMux.HandleFunc(\"/debug/pprof/symbol\", pprof.Symbol)\n    pprofMux.HandleFunc(\"/debug/pprof/trace\", pprof.Trace)\n    \n    // 인증 미들웨어 적용\n    go http.ListenAndServe(\"localhost:6060\", authMiddleware(pprofMux))\n}\n```\n\n### 2. 연속 프로파일링 (Continuous Profiling)\n\n```go\nimport (\n    \"os\"\n    \"runtime/pprof\"\n    \"time\"\n)\n\nfunc startContinuousProfiling() {\n    ticker := time.NewTicker(10 * time.Minute)\n    for range ticker.C {\n        f, _ := os.Create(fmt.Sprintf(\"heap_%s.prof\", time.Now().Format(\"20060102_150405\")))\n        pprof.WriteHeapProfile(f)\n        f.Close()\n    }\n}\n```\n\n### 3. 외부 서비스 연동\n\n- **Pyroscope**: 오픈소스 연속 프로파일링\n- **Datadog Continuous Profiler**: 상용 APM\n- **Google Cloud Profiler**: GCP 통합\n\n```go\n// Pyroscope 예시\nimport \"github.com/grafana/pyroscope-go\"\n\nfunc main() {\n    pyroscope.Start(pyroscope.Config{\n        ApplicationName: \"my-app\",\n        ServerAddress:   \"http://pyroscope:4040\",\n    })\n}\n```\n\n## 벤치마크와 함께 사용\n\n```bash\n# 벤치마크 실행 + CPU 프로파일\ngo test -bench=. -cpuprofile=cpu.prof\n\n# 벤치마크 실행 + 메모리 프로파일\ngo test -bench=. -memprofile=mem.prof\n\n# 분석\ngo tool pprof cpu.prof\n```\n\n## 주의사항\n\n1. **프로덕션 포트 분리**: pprof 엔드포인트는 별도 포트에서 실행\n2. **인증 필수**: 외부 노출 시 반드시 인증 적용\n3. **오버헤드**: CPU 프로파일링은 약 5% 오버헤드 발생\n4. **샘플링 주기**: `runtime.SetCPUProfileRate()`로 조절 가능\n\n## 참고 자료\n\n- [Go pprof 공식 문서](https://pkg.go.dev/net/http/pprof)\n- [Profiling Go Programs](https://go.dev/blog/pprof)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Go",
      "Performance"
    ],
    "readingTime": 4,
    "wordCount": 603,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "ginkgo-bdd-testing",
    "slug": "ginkgo-bdd-testing",
    "path": "backend/go",
    "fullPath": "backend/go/ginkgo-bdd-testing",
    "title": "Ginkgo와 Testcontainers를 활용한 통합 테스트 전략",
    "excerpt": "Ginkgo BDD 프레임워크와 Testcontainers를 결합하여 실제 데이터베이스를 사용하는 신뢰성 높은 통합 테스트를 구축하는 방법을 알아봅니다.",
    "content": "# Ginkgo와 Testcontainers를 활용한 통합 테스트 전략\n\n## 개요\n\n**Ginkgo**는 Go의 BDD 테스트 프레임워크이며, **Testcontainers**는 테스트에서 Docker 컨테이너를 프로그래매틱하게 관리합니다. 이 조합으로 Mock 없이 실제 데이터베이스를 사용하는 통합 테스트를 구축할 수 있습니다.\n\n## 왜 이 조합인가?\n\n### 장점\n\n| 특성 | 설명 |\n|------|------|\n| **실제 환경** | Mock 대신 실제 DB로 테스트 → 높은 신뢰도 |\n| **격리성** | 테스트마다 깨끗한 컨테이너 환경 |\n| **BDD 가독성** | Describe/Context/It으로 의도 명확히 표현 |\n| **병렬 실행** | 컨테이너 격리로 안전한 병렬 테스트 |\n\n### 단점\n\n| 특성 | 설명 |\n|------|------|\n| **속도** | 컨테이너 시작 시간으로 유닛 테스트보다 느림 |\n| **리소스** | Docker 실행 필요, CI에서 추가 설정 필요 |\n| **복잡성** | 컨테이너 라이프사이클 관리 필요 |\n\n## 설치\n\n```bash\n# Ginkgo CLI 및 라이브러리\ngo install github.com/onsi/ginkgo/v2/ginkgo@latest\ngo get github.com/onsi/gomega/...\n\n# Testcontainers\ngo get github.com/testcontainers/testcontainers-go\ngo get github.com/testcontainers/testcontainers-go/modules/mongodb\ngo get github.com/testcontainers/testcontainers-go/modules/redis\n```\n\n## 테스트 환경 구조\n\n### 테스트 헬퍼\n\n```go\n// testutils/mongodb.go\npackage testutils\n\nimport (\n    \"context\"\n    \"os\"\n    \n    \"github.com/testcontainers/testcontainers-go\"\n    \"github.com/testcontainers/testcontainers-go/modules/mongodb\"\n    \"go.mongodb.org/mongo-driver/mongo\"\n    \"go.mongodb.org/mongo-driver/mongo/options\"\n)\n\n// MongoDBFixture는 MongoDB 테스트 환경을 관리합니다.\ntype MongoDBFixture struct {\n    container *mongodb.MongoDBContainer\n    client    *mongo.Client\n    cleanup   func()\n}\n\nfunc (f *MongoDBFixture) GetConnectionString() (string, error) {\n    return f.container.ConnectionString(context.Background())\n}\n\nfunc (f *MongoDBFixture) GetClient() *mongo.Client {\n    return f.client\n}\n\nfunc (f *MongoDBFixture) Cleanup() {\n    f.cleanup()\n}\n\n// GetMongoDBFixture는 MongoDB 테스트 Fixture를 생성합니다.\n// packageName을 전달하면 동일 이름의 컨테이너를 재사용하여 테스트 속도를 높입니다.\nfunc GetMongoDBFixture(ctx context.Context, packageName string) (*MongoDBFixture, error) {\n    // Ryuk(리소스 정리 컨테이너) 비활성화 - CI 환경에서 권장\n    os.Setenv(\"TESTCONTAINERS_RYUK_DISABLED\", \"true\")\n    \n    // ReplicaSet 활성화 - 트랜잭션 테스트에 필요\n    replicaSetName := \"rs0\"\n    \n    mongoContainer, err := mongodb.Run(ctx,\n        \"public.ecr.aws/docker/library/mongo:8\",  // 공식 ECR 이미지\n        mongodb.WithReplicaSet(replicaSetName),   // 트랜잭션 지원\n        testcontainers.WithReuseByName(packageName), // 컨테이너 재사용으로 속도 향상\n    )\n    if err != nil {\n        return nil, err\n    }\n    \n    // 연결 문자열 가져오기\n    connString, err := mongoContainer.ConnectionString(ctx)\n    if err != nil {\n        return nil, err\n    }\n    \n    // ReplicaSet 사용 시 Direct 연결 필요\n    clientOpts := options.Client().ApplyURI(connString)\n    clientOpts.SetDirect(true)\n    \n    mongoClient, err := mongo.Connect(ctx, clientOpts)\n    if err != nil {\n        return nil, err\n    }\n    \n    // 연결 확인\n    if err := mongoClient.Ping(ctx, nil); err != nil {\n        return nil, err\n    }\n    \n    // cleanup 클로저 - 컨테이너와 클라이언트 정리\n    cleanup := func(client *mongo.Client, container *mongodb.MongoDBContainer) func() {\n        return func() {\n            client.Disconnect(ctx)\n            container.Terminate(ctx)\n        }\n    }(mongoClient, mongoContainer)\n    \n    return &MongoDBFixture{\n        container: mongoContainer,\n        client:    mongoClient,\n        cleanup:   cleanup,\n    }, nil\n}\n```\n\n### Redis Fixture\n\n```go\n// testutils/redis.go\npackage testutils\n\nimport (\n    \"context\"\n    \"os\"\n    \n    goredis \"github.com/redis/go-redis/v9\"\n    \"github.com/testcontainers/testcontainers-go/modules/redis\"\n)\n\ntype RedisFixture struct {\n    container *redis.RedisContainer\n    client    *goredis.Client\n    cleanup   func()\n}\n\nfunc (f *RedisFixture) GetClient() *goredis.Client {\n    return f.client\n}\n\nfunc (f *RedisFixture) Cleanup() {\n    f.cleanup()\n}\n\nfunc (f *RedisFixture) GetConnectionString() (string, error) {\n    return f.container.Endpoint(context.Background(), \"\")\n}\n\nfunc GetRedisFixture(ctx context.Context, packageName string) (*RedisFixture, error) {\n    os.Setenv(\"TESTCONTAINERS_RYUK_DISABLED\", \"true\")\n    \n    redisContainer, err := redis.Run(ctx,\n        \"public.ecr.aws/docker/library/redis:alpine\",\n    )\n    if err != nil {\n        return nil, err\n    }\n    \n    // Endpoint로 host:port 형태의 주소 획득\n    redisAddr, err := redisContainer.Endpoint(ctx, \"\")\n    if err != nil {\n        return nil, err\n    }\n    \n    // Options에 직접 Addr 설정\n    redisClient := goredis.NewClient(&goredis.Options{\n        Addr: redisAddr,\n    })\n    \n    if err := redisClient.Ping(ctx).Err(); err != nil {\n        return nil, err\n    }\n    \n    cleanup := func(client *goredis.Client, container *redis.RedisContainer) func() {\n        return func() {\n            client.Close()\n            container.Terminate(ctx)\n        }\n    }(redisClient, redisContainer)\n    \n    return &RedisFixture{\n        container: redisContainer,\n        client:    redisClient,\n        cleanup:   cleanup,\n    }, nil\n}\n```\n\n### 테스트 스위트 설정\n\n```go\n// internal/document/document_suite_test.go\npackage document_test\n\nimport (\n    \"context\"\n    \"testing\"\n    \n    . \"github.com/onsi/ginkgo/v2\"\n    . \"github.com/onsi/gomega\"\n    \n    \"myapp/testutil\"\n)\n\nvar (\n    testEnv *testutil.TestEnvironment\n    testCtx context.Context\n)\n\nfunc TestDocument(t *testing.T) {\n    RegisterFailHandler(Fail)\n    RunSpecs(t, \"Document Suite\")\n}\n\nvar _ = BeforeSuite(func() {\n    var err error\n    testCtx = context.Background()\n    \n    By(\"테스트 환경 초기화\")\n    testEnv, err = testutil.NewTestEnvironment(testCtx)\n    Expect(err).NotTo(HaveOccurred())\n    \n    By(\"MongoDB 연결 확인\")\n    // ... 연결 테스트\n})\n\nvar _ = AfterSuite(func() {\n    By(\"테스트 환경 정리\")\n    testEnv.Cleanup(testCtx)\n})\n```\n\n## 테스트 작성\n\n### 문서 서비스 통합 테스트\n\n```go\n// internal/document/service_integration_test.go\npackage document_test\n\nimport (\n    \"context\"\n    \n    . \"github.com/onsi/ginkgo/v2\"\n    . \"github.com/onsi/gomega\"\n    \n    \"go.mongodb.org/mongo-driver/mongo\"\n    \"go.mongodb.org/mongo-driver/mongo/options\"\n    \n    \"myapp/internal/document\"\n)\n\nvar _ = Describe(\"DocumentService 통합 테스트\", Label(\"integration\"), func() {\n    var (\n        service    document.Service\n        collection *mongo.Collection\n        ctx        context.Context\n    )\n    \n    BeforeEach(func() {\n        ctx = context.Background()\n        \n        // 테스트용 MongoDB 클라이언트\n        client, err := mongo.Connect(ctx, options.Client().ApplyURI(testEnv.MongoURI))\n        Expect(err).NotTo(HaveOccurred())\n        \n        // 테스트마다 새 컬렉션 사용\n        dbName := \"test_db\"\n        collName := \"test_collection_\" + GinkgoParallelProcess()\n        collection = client.Database(dbName).Collection(collName)\n        \n        // 서비스 생성\n        service = document.NewService(client, dbName)\n    })\n    \n    AfterEach(func() {\n        // 테스트 데이터 정리\n        collection.Drop(ctx)\n    })\n    \n    Describe(\"Create\", func() {\n        Context(\"유효한 문서가 주어졌을 때\", func() {\n            It(\"버전 1의 새 문서를 생성한다\", func() {\n                input := document.CreateInput{\n                    URI: \"doc-001\",\n                    Fields: map[string]interface{}{\n                        \"name\":  \"Test Document\",\n                        \"value\": 42,\n                    },\n                }\n                \n                doc, err := service.Create(ctx, \"test_collection\", input)\n                \n                Expect(err).NotTo(HaveOccurred())\n                Expect(doc.URI).To(Equal(\"doc-001\"))\n                Expect(doc.Version).To(Equal(int32(1)))\n                Expect(doc.Fields[\"name\"]).To(Equal(\"Test Document\"))\n            })\n        })\n        \n        Context(\"중복 URI가 주어졌을 때\", func() {\n            BeforeEach(func() {\n                _, err := service.Create(ctx, \"test_collection\", document.CreateInput{\n                    URI:    \"doc-001\",\n                    Fields: map[string]interface{}{},\n                })\n                Expect(err).NotTo(HaveOccurred())\n            })\n            \n            It(\"에러를 반환한다\", func() {\n                _, err := service.Create(ctx, \"test_collection\", document.CreateInput{\n                    URI:    \"doc-001\",\n                    Fields: map[string]interface{}{},\n                })\n                \n                Expect(err).To(HaveOccurred())\n                Expect(err).To(MatchError(ContainSubstring(\"duplicate\")))\n            })\n        })\n    })\n    \n    Describe(\"SoftUpdate\", func() {\n        var existingDoc *document.Document\n        \n        BeforeEach(func() {\n            var err error\n            existingDoc, err = service.Create(ctx, \"test_collection\", document.CreateInput{\n                URI:    \"doc-update-test\",\n                Fields: map[string]interface{}{\"name\": \"Original\"},\n            })\n            Expect(err).NotTo(HaveOccurred())\n        })\n        \n        Context(\"정상적인 업데이트 요청일 때\", func() {\n            It(\"새 버전을 생성하고 기존 버전을 보존한다\", func() {\n                updated, err := service.SoftUpdate(ctx, \"test_collection\", \"doc-update-test\", \n                    document.UpdateInput{\n                        Fields: map[string]interface{}{\"name\": \"Updated\"},\n                    },\n                )\n                \n                Expect(err).NotTo(HaveOccurred())\n                Expect(updated.Version).To(Equal(int32(2)))\n                Expect(updated.Fields[\"name\"]).To(Equal(\"Updated\"))\n                \n                // 이전 버전이 보존되는지 확인\n                history, err := service.GetHistory(ctx, \"test_collection\", \"doc-update-test\")\n                Expect(err).NotTo(HaveOccurred())\n                Expect(history).To(HaveLen(2))\n                Expect(history[0].Version).To(Equal(int32(1)))\n                Expect(history[1].Version).To(Equal(int32(2)))\n            })\n        })\n    })\n    \n    Describe(\"SoftDelete\", func() {\n        BeforeEach(func() {\n            _, err := service.Create(ctx, \"test_collection\", document.CreateInput{\n                URI:    \"doc-delete-test\",\n                Fields: map[string]interface{}{\"data\": \"value\"},\n            })\n            Expect(err).NotTo(HaveOccurred())\n        })\n        \n        It(\"문서를 DELETED 상태로 마킹한다\", func() {\n            deleted, err := service.SoftDelete(ctx, \"test_collection\", \"doc-delete-test\")\n            \n            Expect(err).NotTo(HaveOccurred())\n            Expect(deleted.Status).To(Equal(document.StatusDeleted))\n            \n            // 최신 버전 조회 시 찾을 수 없음\n            _, err = service.FindLatest(ctx, \"test_collection\", \"doc-delete-test\")\n            Expect(err).To(MatchError(document.ErrNotFound))\n        })\n    })\n})\n```\n\n### Redis 통합 테스트\n\n```go\n// internal/worker/worker_integration_test.go\npackage worker_test\n\nimport (\n    \"context\"\n    \"time\"\n    \n    . \"github.com/onsi/ginkgo/v2\"\n    . \"github.com/onsi/gomega\"\n    \n    \"github.com/redis/go-redis/v9\"\n    \n    \"myapp/internal/worker\"\n)\n\nvar _ = Describe(\"StreamWorker 통합 테스트\", Label(\"integration\"), func() {\n    var (\n        redisClient redis.UniversalClient\n        ctx         context.Context\n    )\n    \n    BeforeEach(func() {\n        ctx = context.Background()\n        \n        redisClient = redis.NewClient(&redis.Options{\n            Addr: testEnv.RedisAddr,\n        })\n        \n        // 이전 테스트 데이터 정리\n        redisClient.FlushAll(ctx)\n    })\n    \n    AfterEach(func() {\n        redisClient.Close()\n    })\n    \n    Describe(\"메시지 처리\", func() {\n        Context(\"정상 메시지가 발행되었을 때\", func() {\n            It(\"핸들러가 호출되고 ACK 처리된다\", func() {\n                processed := make(chan string, 1)\n                \n                handler := &testHandler{\n                    onHandle: func(msgs []*worker.Message) []error {\n                        for _, m := range msgs {\n                            processed <- m.ID\n                        }\n                        return nil\n                    },\n                }\n                \n                w := worker.NewStreamWorker(\n                    redisClient,\n                    handler,\n                    worker.WithStream(\"test-stream\"),\n                    worker.WithGroup(\"test-group\"),\n                    worker.WithBatchSize(1),\n                    worker.WithPollInterval(50*time.Millisecond),\n                )\n                \n                w.Start(ctx)\n                defer w.Stop()\n                \n                // 메시지 발행\n                redisClient.XAdd(ctx, &redis.XAddArgs{\n                    Stream: \"test-stream\",\n                    Values: map[string]interface{}{\"data\": \"test\"},\n                })\n                \n                // 처리 확인\n                Eventually(processed).Should(Receive())\n                \n                // ACK 확인 (Pending 없음)\n                pending, _ := redisClient.XPending(ctx, \"test-stream\", \"test-group\").Result()\n                Expect(pending.Count).To(BeZero())\n            })\n        })\n        \n        Context(\"처리 실패 시\", func() {\n            It(\"Dead Letter 스트림으로 이동한다\", func() {\n                handler := &testHandler{\n                    onHandle: func(msgs []*worker.Message) []error {\n                        return []error{errors.New(\"processing failed\")}\n                    },\n                }\n                \n                w := worker.NewStreamWorker(\n                    redisClient,\n                    handler,\n                    worker.WithStream(\"test-stream\"),\n                    worker.WithGroup(\"test-group\"),\n                    worker.WithMaxRetries(1),\n                    worker.WithDeadLetterStream(\"dead-letters\"),\n                )\n                \n                w.Start(ctx)\n                defer w.Stop()\n                \n                // 메시지 발행\n                redisClient.XAdd(ctx, &redis.XAddArgs{\n                    Stream: \"test-stream\",\n                    Values: map[string]interface{}{\"data\": \"fail\"},\n                })\n                \n                // Dead Letter 확인\n                Eventually(func() int64 {\n                    len, _ := redisClient.XLen(ctx, \"dead-letters\").Result()\n                    return len\n                }).Should(BeNumerically(\">\", 0))\n            })\n        })\n    })\n})\n\ntype testHandler struct {\n    onHandle func([]*worker.Message) []error\n}\n\nfunc (h *testHandler) Handle(ctx context.Context, msgs []*worker.Message) []error {\n    return h.onHandle(msgs)\n}\n```\n\n## 테이블 드리븐 테스트\n\nGinkgo의 `DescribeTable`로 다양한 케이스 커버:\n\n```go\nvar _ = Describe(\"스키마 검증\", func() {\n    DescribeTable(\"유효한 문서\",\n        func(fields map[string]interface{}, expectValid bool) {\n            err := validator.Validate(schema, fields)\n            \n            if expectValid {\n                Expect(err).NotTo(HaveOccurred())\n            } else {\n                Expect(err).To(HaveOccurred())\n            }\n        },\n        Entry(\"모든 필수 필드 존재\", map[string]interface{}{\n            \"name\": \"test\", \"email\": \"test@example.com\",\n        }, true),\n        Entry(\"필수 필드 누락\", map[string]interface{}{\n            \"name\": \"test\",\n        }, false),\n        Entry(\"잘못된 타입\", map[string]interface{}{\n            \"name\": 123, \"email\": \"test@example.com\",\n        }, false),\n    )\n})\n```\n\n## 테스트 실행\n\n### 기본 실행\n\n```bash\n# 모든 테스트\nginkgo ./...\n\n# 상세 출력\nginkgo -v ./...\n\n# 통합 테스트만\nginkgo --label-filter=\"integration\" ./...\n\n# 유닛 테스트만 (통합 제외)\nginkgo --label-filter=\"!integration\" ./...\n```\n\n### 병렬 실행\n\n```bash\n# 프로세스 자동 결정\nginkgo -p ./...\n\n# 프로세스 수 지정\nginkgo -procs=4 ./...\n```\n\n### 커버리지\n\n```bash\nginkgo -cover -coverprofile=coverage.out ./...\ngo tool cover -html=coverage.out -o coverage.html\n```\n\n## Makefile 통합\n\n```makefile\n# 기본 테스트 플래그 + 추가 인자는 TESTFLAGS로 전달\nTEST_FLAGS = --skip-package \"./deps\"\nTEST_FLAGS += $(TESTFLAGS)\nTEST_TIMEOUT = 30m\nCOVERAGE_OUT = coverage.out\n\n.PHONY: test test-verbose unit-test integration-test coverage-test cov-html\n\ntest:\n ginkgo -r $(TEST_FLAGS) --timeout=$(TEST_TIMEOUT)\n\ntest-verbose:\n ginkgo -r $(TEST_FLAGS) --timeout=$(TEST_TIMEOUT) -v\n\nunit-test:\n ginkgo -r $(TEST_FLAGS) --label-filter=\"!integration\" --junit-report=unit-test-report.xml --timeout=$(TEST_TIMEOUT)\n\nintegration-test:\n ginkgo -r $(TEST_FLAGS) --label-filter=\"integration\" --junit-report=integration-test-report.xml --timeout=$(TEST_TIMEOUT)\n\ncoverage-test:\n ginkgo -r -cover --coverprofile=$(COVERAGE_OUT) --timeout=$(TEST_TIMEOUT)\n\ncov-html: coverage-test\n go tool cover -html=$(COVERAGE_OUT) -o coverage.html\n```\n\n### 사용 예시\n\n```bash\n# 전체 테스트\nmake test\n\n# 상세 출력\nmake test-verbose\n\n# 유닛 테스트만\nmake unit-test\n\n# 통합 테스트만\nmake integration-test\n\n# 커버리지 HTML 리포트\nmake cov-html\n\n# 추가 인자 전달 (특정 패키지, focus 등)\nmake test TESTFLAGS=\"./internal/document/...\"\nmake test TESTFLAGS='--focus=\"CreateDocument\"'\nmake test-verbose TESTFLAGS=\"-p\"\n```\n\n## CI 설정\n\n```yaml\n# .github/workflows/test.yml\nname: Tests\n\non: [push, pull_request]\n\njobs:\n  unit-tests:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-go@v5\n        with:\n          go-version: '1.24'\n      \n      - name: Install Ginkgo\n        run: go install github.com/onsi/ginkgo/v2/ginkgo@latest\n      \n      - name: Run Unit Tests\n        run: ginkgo -r --label-filter=\"!integration\"\n  \n  integration-tests:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-go@v5\n        with:\n          go-version: '1.24'\n      \n      - name: Install Ginkgo\n        run: go install github.com/onsi/ginkgo/v2/ginkgo@latest\n      \n      - name: Run Integration Tests\n        run: ginkgo -r --label-filter=\"integration\" --timeout=30m\n```\n\n## 모범 사례\n\n1. **Label 사용**: `integration` 라벨로 유닛/통합 테스트 분리\n2. **병렬 안전**: `GinkgoParallelProcess()`로 리소스 이름 분리\n3. **정리 철저**: `AfterEach`로 테스트 데이터 반드시 정리\n4. **타임아웃 설정**: 통합 테스트는 충분한 타임아웃 설정\n5. **실패 격리**: 한 테스트 실패가 다른 테스트에 영향 없도록\n\n## 참고 자료\n\n- [Ginkgo 공식 문서](https://onsi.github.io/ginkgo/)\n- [Gomega 공식 문서](https://onsi.github.io/gomega/)\n- [Testcontainers Go](https://golang.testcontainers.org/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Docker",
      "Go",
      "MongoDB",
      "Redis",
      "Testing"
    ],
    "readingTime": 8,
    "wordCount": 1520,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "gc-tuning-experience",
    "slug": "gc-tuning-experience",
    "path": "backend/go",
    "fullPath": "backend/go/gc-tuning-experience",
    "title": "Go 가비지 컬렉터(GC) 이해와 튜닝 경험",
    "excerpt": "Go의 가비지 컬렉터 동작 원리와 프로덕션 환경에서의 GC 튜닝 경험을 공유합니다.",
    "content": "# Go 가비지 컬렉터(GC) 이해와 튜닝 경험\n\n## 개요\n\nGo는 **Concurrent Mark-and-Sweep** 방식의 가비지 컬렉터를 사용합니다. Go 1.5 이후 STW(Stop-The-World) 시간을 최소화하는 방향으로 지속적으로 개선되어, 대부분의 경우 1ms 이하의 pause time을 달성합니다.\n\n## GC 동작 원리\n\n### Tricolor Mark-and-Sweep\n\nGo GC는 **삼색 마킹 알고리즘**을 사용합니다:\n\n| 색상 | 의미 |\n|-----|------|\n| **White** | 아직 스캔되지 않음 (수집 대상 후보) |\n| **Gray** | 스캔 중, 참조 객체 확인 필요 |\n| **Black** | 스캔 완료, 참조 객체 모두 확인됨 |\n\n### GC 단계\n\n```\n1. Mark Setup (STW)     → 0.1ms 미만\n2. Concurrent Marking   → 백그라운드에서 실행\n3. Mark Termination (STW) → 0.1ms 미만\n4. Concurrent Sweeping  → 백그라운드에서 실행\n```\n\n> Go 1.8+부터 대부분의 STW 시간이 **sub-millisecond** 수준입니다.\n\n## GC 모니터링\n\n### runtime 패키지 활용\n\n```go\npackage main\n\nimport (\n    \"fmt\"\n    \"runtime\"\n    \"time\"\n)\n\nfunc printGCStats() {\n    var stats runtime.MemStats\n    runtime.ReadMemStats(&stats)\n    \n    fmt.Printf(\"Alloc = %v MiB\\n\", stats.Alloc/1024/1024)\n    fmt.Printf(\"HeapAlloc = %v MiB\\n\", stats.HeapAlloc/1024/1024)\n    fmt.Printf(\"HeapSys = %v MiB\\n\", stats.HeapSys/1024/1024)\n    fmt.Printf(\"NumGC = %v\\n\", stats.NumGC)\n    fmt.Printf(\"PauseTotalNs = %v ms\\n\", stats.PauseTotalNs/1e6)\n    fmt.Printf(\"LastGC = %v\\n\", time.Unix(0, int64(stats.LastGC)))\n}\n```\n\n### GODEBUG 환경변수\n\n```bash\n# GC 트레이스 활성화\nGODEBUG=gctrace=1 ./myapp\n\n# 출력 예시:\n# gc 1 @0.012s 2%: 0.018+1.2+0.014 ms clock, 0.14+0.8/1.0/0+0.11 ms cpu, 4->4->1 MB, 5 MB goal, 8 P\n```\n\n| 필드 | 의미 |\n|-----|------|\n| `gc 1` | GC 번호 |\n| `2%` | CPU 사용률 |\n| `0.018+1.2+0.014 ms` | STW + concurrent + STW 시간 |\n| `4->4->1 MB` | 힙: 시작 → 종료 → 라이브 |\n| `5 MB goal` | 다음 GC 목표 힙 크기 |\n\n## GOGC 튜닝\n\n### GOGC 환경변수\n\n**GOGC**는 GC 트리거 임계값을 조절합니다:\n\n```bash\n# 기본값: 100 (힙이 2배가 되면 GC)\nGOGC=100 ./myapp\n\n# 더 자주 GC (메모리 절약, CPU 증가)\nGOGC=50 ./myapp\n\n# 덜 자주 GC (메모리 증가, CPU 절약)\nGOGC=200 ./myapp\n\n# GC 비활성화 (극단적 케이스)\nGOGC=off ./myapp\n```\n\n### 런타임에서 조절\n\n```go\nimport \"runtime/debug\"\n\n// GOGC 값 변경\ndebug.SetGCPercent(50)\n\n// 현재 값 확인 및 변경\nold := debug.SetGCPercent(200)\nfmt.Printf(\"Previous GOGC: %d\\n\", old)\n```\n\n## 메모리 제한 (Go 1.19+)\n\n### GOMEMLIMIT\n\nGo 1.19에서 도입된 **소프트 메모리 제한**:\n\n```bash\n# 최대 4GB 힙 제한\nGOMEMLIMIT=4GiB ./myapp\n```\n\n```go\nimport \"runtime/debug\"\n\n// 런타임에서 설정\ndebug.SetMemoryLimit(4 * 1024 * 1024 * 1024) // 4GB\n```\n\n### GOGC + GOMEMLIMIT 조합\n\n```bash\n# 권장: GOGC=off + GOMEMLIMIT\n# 메모리 제한에 도달하면 자동으로 GC\nGOGC=off GOMEMLIMIT=2GiB ./myapp\n```\n\n## 프로덕션 튜닝 경험\n\n### Case 1: 고빈도 할당 서비스\n\n**문제**: 초당 10만 건 요청 처리, GC pause가 p99 레이턴시에 영향\n\n**해결**:\n\n```go\n// sync.Pool로 객체 재사용\nvar bufferPool = sync.Pool{\n    New: func() interface{} {\n        return make([]byte, 4096)\n    },\n}\n\nfunc handleRequest() {\n    buf := bufferPool.Get().([]byte)\n    defer bufferPool.Put(buf)\n    \n    // buf 사용\n}\n```\n\n**결과**: 할당량 70% 감소, GC 빈도 50% 감소\n\n### Case 2: 대용량 캐시 서비스\n\n**문제**: 32GB 힙, GC marking 시간이 길어짐\n\n**해결**:\n\n```bash\n# 메모리 제한 설정으로 예측 가능한 GC\nGOMEMLIMIT=30GiB GOGC=100 ./cache-server\n```\n\n또한 **외부 캐시**(Redis, Memcached)로 대용량 데이터 오프로드\n\n### Case 3: 배치 처리 워커\n\n**문제**: 배치 처리 중 GC가 처리 시간에 영향\n\n**해결**:\n\n```go\nfunc processBatch(items []Item) {\n    // 배치 처리 전 GC 강제 실행\n    runtime.GC()\n    \n    // 처리 중 GC 비활성화\n    debug.SetGCPercent(-1)\n    defer debug.SetGCPercent(100)\n    \n    for _, item := range items {\n        process(item)\n    }\n}\n```\n\n## 메모리 할당 최적화\n\n### 1. 사전 할당\n\n```go\n// Bad\nvar result []int\nfor i := 0; i < 1000; i++ {\n    result = append(result, i)\n}\n\n// Good\nresult := make([]int, 0, 1000)\nfor i := 0; i < 1000; i++ {\n    result = append(result, i)\n}\n```\n\n### 2. 포인터 회피\n\n```go\n// 힙 할당 유발\ntype Bad struct {\n    data *int\n}\n\n// 스택 할당 가능\ntype Good struct {\n    data int\n}\n```\n\n### 3. Escape Analysis 활용\n\n```bash\n# 이스케이프 분석 결과 확인\ngo build -gcflags=\"-m\" ./...\n```\n\n## Ballast 기법 (레거시)\n\n> **Note**: Go 1.19+ GOMEMLIMIT 도입 이후 ballast 기법은 권장되지 않습니다.\n\n```go\n// 레거시: 큰 배열로 힙 크기 유지\nvar ballast = make([]byte, 1<<30) // 1GB\n\nfunc main() {\n    _ = ballast // 변수 유지\n    // ...\n}\n```\n\n## 모니터링 지표\n\n프로덕션에서 추적해야 할 GC 관련 지표:\n\n| 지표 | 설명 | 임계값 |\n|-----|------|-------|\n| `go_gc_duration_seconds` | GC pause 시간 | p99 < 10ms |\n| `go_memstats_heap_alloc_bytes` | 현재 힙 사용량 | GOMEMLIMIT의 80% |\n| `go_memstats_gc_cpu_fraction` | GC CPU 사용률 | < 5% |\n\n## 참고 자료\n\n- [Go GC Guide](https://tip.golang.org/doc/gc-guide)\n- [Go 1.19 Memory Limit](https://go.dev/blog/go1.19)\n- [Getting to Go: The Journey of Go's Garbage Collector](https://go.dev/blog/ismmkeynote)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Go",
      "Performance"
    ],
    "readingTime": 4,
    "wordCount": 740,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "buf-protobuf-management",
    "slug": "buf-protobuf-management",
    "path": "backend/go",
    "fullPath": "backend/go/buf-protobuf-management",
    "title": "Buf v2 기반 Proto 관리 및 코드 자동 생성",
    "excerpt": "Buf v2를 활용하여 Protobuf 스키마를 체계적으로 관리하고, gRPC 서버/클라이언트, HTTP Gateway, OpenAPI 스펙을 자동 생성하는 방법을 알아봅니다.",
    "content": "# Buf v2 기반 Proto 관리 및 코드 자동 생성\n\n## 개요\n\n대규모 gRPC 서비스에서 **Protobuf 스키마**는 서비스 계약의 핵심입니다. **Buf v2**는 Proto 파일의 린트, Breaking Change 감지, 다중 언어 코드 생성을 통합 관리합니다.\n\n## 왜 Buf인가?\n\n### protoc 대비 장점\n\n| 기능 | protoc | buf v2 |\n|------|--------|--------|\n| 의존성 관리 | 수동 (include 경로) | 자동 (BSR/deps) |\n| 린팅 | 별도 도구 필요 | 내장 + 커스텀 규칙 |\n| Breaking Change | 없음 | 자동 감지 |\n| 플러그인 관리 | 로컬 설치 필수 | Remote Plugins 지원 |\n| 설정 | 복잡한 CLI 플래그 | YAML 설정 파일 |\n\n### 단점\n\n| 특성 | 설명 |\n|------|------|\n| 학습 곡선 | 신규 설정 체계 이해 필요 |\n| BSR 의존성 | 일부 기능은 Buf Schema Registry 필요 |\n| 네트워크 | Remote Plugins 사용 시 인터넷 연결 필요 |\n\n## 프로젝트 구조\n\n### Proto 전용 레포지토리\n\n```\nproto-service/\n├── buf.yaml              # 모듈 설정\n├── buf.gen.yaml          # 코드 생성 설정\n├── buf.lock              # 의존성 락 파일\n├── deps/                 # 로컬 의존성 (선택)\n│   └── custom/\n│       └── options.proto\n└── proto/\n    └── v1beta/\n        └── api.proto     # 서비스 정의\n```\n\n## 설정 파일\n\n### buf.yaml (모듈 설정)\n\n```yaml\n# buf.yaml\nversion: v2\nmodules:\n  - path: proto/v1beta\n    name: buf.build/myorg/myservice\n  - path: deps/custom  # 로컬 의존성 모듈\ndeps:\n  # 외부 의존성 (Google APIs, gRPC-Gateway 등)\n  - buf.build/googleapis/googleapis\n  - buf.build/grpc-ecosystem/grpc-gateway\n  - buf.build/gnostic/gnostic\nlint:\n  use:\n    - STANDARD\n  except:\n    - FIELD_NOT_REQUIRED\n    - PACKAGE_NO_IMPORT_CYCLE\n  disallow_comment_ignores: true\nbreaking:\n  use:\n    - FILE\n  except:\n    - EXTENSION_NO_DELETE\n    - FIELD_SAME_DEFAULT\n```\n\n### buf.gen.yaml (코드 생성 설정)\n\n```yaml\n# buf.gen.yaml\nversion: v2\nmanaged:\n  enabled: true\n  disable:\n    - module: buf.build/googleapis/googleapis\n    - module: buf.build/grpc-ecosystem/grpc-gateway\n    - module: buf.build/gnostic/gnostic\n  override:\n    - file_option: go_package_prefix\n      value: github.com/myorg/myservice/generated/go/proto/\nplugins:\n  # Go Protobuf 메시지\n  - remote: buf.build/protocolbuffers/go:v1.36.2\n    out: generated/go/proto\n    opt: paths=source_relative\n  \n  # gRPC Go 서버/클라이언트\n  - remote: buf.build/grpc/go:v1.5.1\n    out: generated/go/proto\n    opt: paths=source_relative\n  \n  # gRPC-Gateway (HTTP 핸들러)\n  - remote: buf.build/grpc-ecosystem/gateway:v2.25.1\n    out: generated/go/proto/gateway\n    opt:\n      - paths=source_relative\n      - standalone=true\n  \n  # OpenAPI 스펙 자동 생성\n  - remote: buf.build/community/google-gnostic-openapi:v0.7.0\n    out: generated/docs\n    opt: paths=source_relative\n\ninputs:\n  - directory: proto/v1beta\n  - proto_file: deps/custom/options.proto  # 특정 파일만 포함\n```\n\n## Proto 작성 예시\n\n### 서비스 정의\n\n```protobuf\n// proto/v1beta/api.proto\nsyntax = \"proto3\";\n\npackage v1beta;\n\nimport \"gnostic/openapi/v3/annotations.proto\";\nimport \"google/api/annotations.proto\";\nimport \"google/protobuf/struct.proto\";\nimport \"google/protobuf/timestamp.proto\";\n\n// DocumentService는 버전 관리 문서를 관리합니다.\nservice DocumentService {\n  // 새 문서를 생성합니다. 버전 1로 시작됩니다.\n  rpc CreateDocument(CreateDocumentRequest) returns (CreateDocumentResponse) {\n    option (google.api.http) = {\n      post: \"/v1beta/collections/{collection}/documents\"\n      body: \"*\"\n    };\n  }\n  \n  // URI로 문서를 조회합니다. 버전 미지정 시 최신 버전을 반환합니다.\n  rpc GetDocument(GetDocumentRequest) returns (GetDocumentResponse) {\n    option (google.api.http) = {\n      get: \"/v1beta/collections/{collection}/documents/{uri}\"\n    };\n  }\n  \n  // 문서를 업데이트합니다. 새 버전이 생성됩니다.\n  rpc UpdateDocument(UpdateDocumentRequest) returns (UpdateDocumentResponse) {\n    option (google.api.http) = {\n      patch: \"/v1beta/collections/{collection}/documents/{uri}\"\n      body: \"*\"\n    };\n  }\n  \n  // 문서를 삭제합니다. 소프트 삭제로 처리됩니다.\n  rpc DeleteDocument(DeleteDocumentRequest) returns (DeleteDocumentResponse) {\n    option (google.api.http) = {\n      delete: \"/v1beta/collections/{collection}/documents/{uri}\"\n    };\n  }\n}\n\n// 문서 상태\nenum DocumentStatus {\n  DOCUMENT_STATUS_UNSPECIFIED = 0;\n  DOCUMENT_STATUS_ACTIVE = 1;\n  DOCUMENT_STATUS_DELETED = 2;\n}\n\n// 문서 모델\nmessage Document {\n  // MongoDB ObjectID 문자열\n  optional string id = 1 [(gnostic.openapi.v3.property) = {\n    description: \"문서의 데이터베이스 ID\"\n    nullable: true\n  }];\n  \n  // 논리적 문서 식별자 (버전 전체에서 공유)\n  string uri = 2 [(gnostic.openapi.v3.property) = {\n    description: \"문서의 고유 식별자\"\n    nullable: false\n  }];\n  \n  // JSON 스키마를 준수하는 문서 데이터\n  google.protobuf.Struct fields = 3 [(gnostic.openapi.v3.property) = {\n    description: \"문서 필드 데이터\"\n    nullable: false\n  }];\n  \n  // 버전 번호 (1부터 시작, 업데이트마다 증가)\n  int32 version = 4;\n  \n  // 문서 상태\n  DocumentStatus status = 5;\n  \n  // 생성 시각\n  google.protobuf.Timestamp created_at = 6;\n  \n  // 업데이트 시각\n  google.protobuf.Timestamp updated_at = 7;\n}\n\n// 요청/응답 메시지들\nmessage CreateDocumentRequest {\n  string collection = 1;\n  DocumentInput document = 2;\n}\n\nmessage DocumentInput {\n  string uri = 1;\n  google.protobuf.Struct fields = 2;\n}\n\nmessage CreateDocumentResponse {\n  Document document = 1;\n}\n\nmessage GetDocumentRequest {\n  string collection = 1;\n  string uri = 2;\n  optional int32 version = 3;  // 미지정 시 최신 버전\n}\n\nmessage GetDocumentResponse {\n  Document document = 1;\n}\n\nmessage UpdateDocumentRequest {\n  string collection = 1;\n  string uri = 2;\n  google.protobuf.Struct fields = 3;\n  optional int32 expected_version = 4;  // Optimistic Locking\n}\n\nmessage UpdateDocumentResponse {\n  Document document = 1;\n}\n\nmessage DeleteDocumentRequest {\n  string collection = 1;\n  string uri = 2;\n}\n\nmessage DeleteDocumentResponse {\n  bool success = 1;\n}\n```\n\n## 코드 생성\n\n### 기본 생성\n\n```bash\n# 의존성 업데이트\nbuf mod update\n\n# 코드 생성\nbuf generate\n\n# 생성 구조\ngenerated/\n├── go/\n│   └── proto/\n│       ├── v1beta/\n│       │   ├── api.pb.go         # 메시지 정의\n│       │   └── api_grpc.pb.go    # gRPC 서버/클라이언트\n│       └── gateway/\n│           └── v1beta/\n│               └── api.pb.gw.go  # HTTP Gateway\n└── docs/\n    └── v1beta/\n        └── openapi.yaml          # OpenAPI 스펙\n```\n\n### 특정 경로만 생성\n\n```bash\nbuf generate --path proto/v1beta/api.proto\n```\n\n## 생성된 코드 활용\n\n### gRPC 서버\n\n```go\npackage main\n\nimport (\n    \"net\"\n    \n    \"google.golang.org/grpc\"\n    pb \"github.com/myorg/myservice/generated/go/proto/v1beta\"\n)\n\ntype documentServer struct {\n    pb.UnimplementedDocumentServiceServer\n    service DocumentService\n}\n\nfunc (s *documentServer) CreateDocument(ctx context.Context, req *pb.CreateDocumentRequest) (*pb.CreateDocumentResponse, error) {\n    doc, err := s.service.Create(ctx, req.Collection, req.Document)\n    if err != nil {\n        return nil, err\n    }\n    return &pb.CreateDocumentResponse{Document: doc}, nil\n}\n\nfunc main() {\n    lis, _ := net.Listen(\"tcp\", \":9090\")\n    \n    grpcServer := grpc.NewServer()\n    pb.RegisterDocumentServiceServer(grpcServer, &documentServer{})\n    \n    grpcServer.Serve(lis)\n}\n```\n\n### HTTP Gateway\n\n```go\npackage main\n\nimport (\n    \"net/http\"\n    \n    \"github.com/grpc-ecosystem/grpc-gateway/v2/runtime\"\n    gw \"github.com/myorg/myservice/generated/go/proto/gateway/v1beta\"\n)\n\nfunc main() {\n    ctx := context.Background()\n    mux := runtime.NewServeMux()\n    \n    // gRPC 서버에 연결하여 HTTP 요청 프록시\n    opts := []grpc.DialOption{grpc.WithTransportCredentials(insecure.NewCredentials())}\n    err := gw.RegisterDocumentServiceHandlerFromEndpoint(ctx, mux, \"localhost:9090\", opts)\n    if err != nil {\n        panic(err)\n    }\n    \n    // HTTP 서버 시작\n    http.ListenAndServe(\":8080\", mux)\n}\n```\n\n### OpenAPI 스펙\n\n생성된 `openapi.yaml`을 Swagger UI와 함께 제공:\n\n```go\nfunc main() {\n    // ... Gateway 설정 ...\n    \n    // OpenAPI 스펙 제공\n    http.HandleFunc(\"/openapi.yaml\", func(w http.ResponseWriter, r *http.Request) {\n        http.ServeFile(w, r, \"generated/docs/v1beta/openapi.yaml\")\n    })\n}\n```\n\n## 린팅 및 Breaking Change 감지\n\n### 린트 실행\n\n```bash\nbuf lint\n\n# 특정 파일만\nbuf lint --path proto/v1beta/api.proto\n\n# 에러 출력 예시:\n# proto/v1beta/api.proto:15:3:Field \"id\" should be marked as optional.\n```\n\n### Breaking Change 감지\n\n```bash\n# 현재 브랜치 vs main\nbuf breaking --against '.git#branch=main'\n\n# 현재 vs 이전 커밋\nbuf breaking --against '.git#ref=HEAD~1'\n\n# 현재 vs BSR 최신 버전\nbuf breaking --against 'buf.build/myorg/myservice'\n```\n\n## CI/CD 통합\n\n### GitHub Actions\n\n```yaml\n# .github/workflows/proto.yml\nname: Proto CI\n\non:\n  push:\n    paths: ['proto/**', 'buf.*']\n  pull_request:\n    paths: ['proto/**', 'buf.*']\n\njobs:\n  lint-and-check:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n      - uses: bufbuild/buf-setup-action@v1\n        with:\n          version: latest\n      \n      - name: Lint\n        run: buf lint\n      \n      - name: Breaking Change Check\n        run: buf breaking --against 'https://github.com/${{ github.repository }}.git#branch=main'\n  \n  generate:\n    needs: lint-and-check\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n      - uses: bufbuild/buf-setup-action@v1\n      \n      - name: Generate Code\n        run: buf generate\n      \n      - name: Check for uncommitted changes\n        run: |\n          if [[ -n $(git status --porcelain generated/) ]]; then\n            echo \"Generated code is out of sync!\"\n            git diff generated/\n            exit 1\n          fi\n```\n\n## Makefile 통합\n\n```makefile\n.PHONY: proto-deps proto-lint proto-breaking proto-gen proto-clean\n\n# 의존성 업데이트\nproto-deps:\n buf mod update\n\n# 린트\nproto-lint:\n buf lint\n\n# Breaking Change 검사\nproto-breaking:\n buf breaking --against '.git#branch=main'\n\n# 코드 생성\nproto-gen:\n buf generate\n\n# 정리\nproto-clean:\n rm -rf generated/\n\n# 전체 빌드\nproto-all: proto-deps proto-lint proto-gen\n```\n\n## 모범 사례\n\n1. **버전 네이밍**: 패키지에 `v1`, `v1beta` 등 버전 포함\n2. **Breaking Change CI**: PR마다 자동 검사\n3. **생성 코드 커밋**: `.gitignore`에 `generated/` 추가 권장\n4. **Proto 주석**: 서비스/메시지 주석은 생성 코드와 OpenAPI에 반영됨\n5. **Optional 명시**: Proto3에서 `optional` 키워드로 nullable 명확히 표현\n\n## 참고 자료\n\n- [Buf 공식 문서](https://buf.build/docs/)\n- [Buf Schema Registry](https://buf.build/docs/bsr/introduction)\n- [gRPC-Gateway](https://grpc-ecosystem.github.io/grpc-gateway/)\n- [gnostic OpenAPI](https://github.com/google/gnostic)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Go",
      "OpenAPI",
      "buf",
      "gRPC"
    ],
    "readingTime": 7,
    "wordCount": 1210,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "스파이크-트래픽-대응-전략",
    "slug": "seupaikeu-teuraepig-daeeung-jeonryag",
    "path": "backend/devops",
    "fullPath": "backend/devops/seupaikeu-teuraepig-daeeung-jeonryag",
    "title": "스파이크 트래픽 대응 전략",
    "excerpt": "티켓팅, 이벤트 응모, 플래시 세일 등 순간적으로 폭발하는 쓰기 요청을 처리하기 위한 아키텍처 전략을 다룹니다.",
    "content": "# 스파이크 트래픽 대응 전략\n\n일반적인 서비스 스케일링 전략은 **읽기 중심**의 점진적인 성장을 가정합니다. 하지만 다음과 같은 서비스는 완전히 다른 패턴을 보입니다:\n\n- **티켓팅 시스템**: 콘서트 티켓 오픈 시 수십만 명이 동시에 구매 시도\n- **이벤트 응모**: 선착순 쿠폰 발급\n- **플래시 세일**: 특정 시간에 할인 상품 판매\n- **실시간 투표/설문**: 방송 중 시청자 투표\n\n이런 서비스의 특징은 **예측 가능한 시점에 폭발적인 쓰기 요청**이 발생한다는 것입니다.\n\n## 문제: 왜 일반적인 스케일링으로 부족한가?\n\n### 읽기 vs 쓰기 확장의 차이\n\n```mermaid\nflowchart TB\n    subgraph 읽기확장[\"읽기 확장 (쉬움)\"]\n        Cache[Redis Cache]\n        R1[(Read Replica 1)]\n        R2[(Read Replica 2)]\n        R3[(Read Replica 3)]\n    end\n    \n    subgraph 쓰기확장[\"쓰기 확장 (어려움)\"]\n        Master[(Master DB)]\n        Lock[Row Lock]\n        Constraint[Unique Constraint]\n    end\n```\n\n**읽기 확장**은 캐시와 복제본으로 거의 무한대로 확장할 수 있습니다. 반면 **쓰기 확장**은:\n\n- 모든 쓰기가 Master DB로 집중\n- 동시성 제어를 위한 락(Lock) 경합\n- 데이터 정합성 보장 필요\n\n### 실제 병목 사례\n\n```python\n# 선착순 100명 쿠폰 발급 - 문제가 있는 코드\ndef issue_coupon(user_id):\n    coupon = db.query(\"SELECT * FROM coupons WHERE issued = false LIMIT 1 FOR UPDATE\")\n    if coupon:\n        db.execute(\"UPDATE coupons SET issued = true, user_id = ? WHERE id = ?\", \n                   user_id, coupon.id)\n        return coupon\n    return None\n```\n\n10만 명이 동시에 요청하면:\n\n- `FOR UPDATE` 락으로 인한 대기열 형성\n- DB 연결 풀 고갈\n- 타임아웃 발생\n- 서비스 전체 마비\n\n## 전략 1: 메시지 큐로 부하 분산\n\n가장 기본적이면서 효과적인 전략입니다. 동기 처리를 비동기로 전환합니다.\n\n```mermaid\nflowchart LR\n    Client[Client] --> API[API Server]\n    API -->|즉시 응답| Client\n    API -->|Enqueue| Queue[(Message Queue)]\n    Queue --> Worker1[Worker 1]\n    Queue --> Worker2[Worker 2]\n    Queue --> Worker3[Worker 3]\n    Worker1 --> DB[(DB)]\n    Worker2 --> DB\n    Worker3 --> DB\n```\n\n### 구현 예시\n\n```python\n# API 레이어 - 즉시 응답\nasync def apply_coupon(user_id: str):\n    request_id = uuid.uuid4()\n    await redis.lpush(\"coupon:queue\", json.dumps({\n        \"request_id\": request_id,\n        \"user_id\": user_id,\n        \"timestamp\": time.time()\n    }))\n    return {\"status\": \"queued\", \"request_id\": request_id}\n\n# Worker - 순차 처리\nasync def process_coupon_queue():\n    while True:\n        request = await redis.brpop(\"coupon:queue\")\n        data = json.loads(request)\n        \n        # 이미 발급받았는지 확인\n        if await redis.sismember(\"coupon:issued_users\", data[\"user_id\"]):\n            continue\n            \n        # 재고 확인 및 발급 (원자적 연산)\n        remaining = await redis.decr(\"coupon:remaining\")\n        if remaining >= 0:\n            await redis.sadd(\"coupon:issued_users\", data[\"user_id\"])\n            await notify_user(data[\"user_id\"], \"쿠폰 발급 완료!\")\n        else:\n            await redis.incr(\"coupon:remaining\")  # 롤백\n            await notify_user(data[\"user_id\"], \"쿠폰이 소진되었습니다\")\n```\n\n**장점:**\n\n- API 서버는 즉시 응답 → 타임아웃 방지\n- 처리량을 Worker 수로 제어 가능\n- DB 부하 예측 가능\n\n**단점:**\n\n- 사용자에게 즉시 결과를 알려주기 어려움\n- 추가 인프라(큐, Worker) 필요\n\n## 전략 2: 요청 제한 (Rate Limiting + 가상 대기열)\n\n사용자 경험을 개선하면서 시스템을 보호하는 전략입니다.\n\n```mermaid\nflowchart TB\n    Users[10만 명 동시 접속] --> Gate[Rate Limiter]\n    Gate -->|초당 1000명| Queue[가상 대기열]\n    Gate -->|나머지| Waiting[대기 화면]\n    Queue --> API[API Server]\n    API --> DB[(DB)]\n    \n    subgraph 사용자화면[\"사용자 화면\"]\n        Waiting -->|10초마다 폴링| Position[대기 순번 표시]\n    end\n```\n\n### 가상 대기열 구현\n\n```python\n# 입장 토큰 발급\nasync def enter_queue(user_id: str):\n    queue_position = await redis.incr(\"queue:position\")\n    token = jwt.encode({\n        \"user_id\": user_id,\n        \"position\": queue_position,\n        \"exp\": time.time() + 600  # 10분 유효\n    }, SECRET_KEY)\n    return {\"token\": token, \"position\": queue_position}\n\n# 현재 처리 중인 순번 확인\nasync def get_current_position():\n    return await redis.get(\"queue:current\") or 0\n\n# 실제 구매 페이지 진입 허용 여부\nasync def can_enter(token: str):\n    data = jwt.decode(token, SECRET_KEY)\n    current = await get_current_position()\n    \n    # 내 순번이 현재 처리 순번 + 100 이내면 진입 허용\n    if data[\"position\"] <= current + 100:\n        return True\n    return False\n```\n\n**장점:**\n\n- 사용자에게 대기 상황 투명하게 공개\n- 시스템 과부하 방지\n- 공정한 순서 보장\n\n## 전략 3: 사전 재고 할당 (Quota Pre-allocation)\n\n재고 데이터를 미리 분할하여 락 경합을 제거합니다.\n\n```mermaid\nflowchart TB\n    subgraph 사전할당[\"사전 재고 할당\"]\n        Total[총 재고: 1000개]\n        Total --> S1[Server 1: 200개]\n        Total --> S2[Server 2: 200개]\n        Total --> S3[Server 3: 200개]\n        Total --> S4[Server 4: 200개]\n        Total --> S5[Server 5: 200개]\n    end\n    \n    User1[User] --> S1\n    User2[User] --> S2\n    User3[User] --> S3\n```\n\n### 구현 예시\n\n```python\n# 서버 시작 시 재고 할당\nSERVER_ID = os.environ.get(\"SERVER_ID\")\nLOCAL_QUOTA_KEY = f\"quota:{SERVER_ID}\"\n\nasync def initialize_quota():\n    # Coordinator에서 할당받은 수량\n    allocated = await coordinator.allocate_quota(SERVER_ID, 200)\n    await redis.set(LOCAL_QUOTA_KEY, allocated)\n\n# 구매 처리 - 로컬 Redis만 사용\nasync def purchase(user_id: str):\n    remaining = await redis.decr(LOCAL_QUOTA_KEY)\n    if remaining >= 0:\n        await redis.sadd(\"purchased_users\", user_id)\n        return {\"success\": True}\n    else:\n        await redis.incr(LOCAL_QUOTA_KEY)  # 롤백\n        \n        # 다른 서버에 재고가 남았는지 확인\n        for server_id in await get_other_servers():\n            result = await try_other_server(server_id, user_id)\n            if result:\n                return result\n        \n        return {\"success\": False, \"message\": \"품절\"}\n```\n\n**장점:**\n\n- DB 락 경합 완전 제거\n- 각 서버가 독립적으로 처리 → 수평 확장 용이\n- 초당 수만 건 처리 가능\n\n**단점:**\n\n- 재고 불균형 발생 가능\n- 구현 복잡도 증가\n\n## 전략 4: 이벤트 소싱 + CQRS\n\n복잡한 비즈니스 로직이 필요한 경우, 쓰기와 읽기를 완전히 분리합니다.\n\n```mermaid\nflowchart LR\n    subgraph Command[\"Command (쓰기)\"]\n        API[API] --> EventStore[(Event Store)]\n        EventStore --> Processor[Event Processor]\n    end\n    \n    subgraph Query[\"Query (읽기)\"]\n        Processor --> ReadDB[(Read DB)]\n        ReadDB --> QueryAPI[Query API]\n    end\n    \n    Client[Client] --> API\n    Client --> QueryAPI\n```\n\n### 핵심 개념\n\n```python\n# 이벤트 저장 (Append Only)\nasync def apply_for_event(user_id: str, event_id: str):\n    event = {\n        \"type\": \"APPLICATION_SUBMITTED\",\n        \"user_id\": user_id,\n        \"event_id\": event_id,\n        \"timestamp\": datetime.utcnow().isoformat(),\n        \"sequence\": await redis.incr(f\"event:{event_id}:seq\")\n    }\n    \n    # Kafka, EventStore 등에 저장\n    await kafka.send(\"applications\", event)\n    return {\"status\": \"accepted\", \"sequence\": event[\"sequence\"]}\n\n# 이벤트 처리 (비동기)\nasync def process_application_event(event):\n    if event[\"type\"] == \"APPLICATION_SUBMITTED\":\n        # 중복 체크, 자격 확인 등 비즈니스 로직\n        if await is_eligible(event[\"user_id\"], event[\"event_id\"]):\n            await update_read_model(event)\n```\n\n**장점:**\n\n- 쓰기 처리량 극대화 (Append Only)\n- 완벽한 감사 로그\n- 시간 여행(특정 시점 상태 복원) 가능\n\n**단점:**\n\n- 최종 일관성(Eventual Consistency)\n- 학습 곡선이 높음\n\n## 전략 5: 하드웨어 레벨 최적화\n\n소프트웨어 아키텍처 외에도 하드웨어 선택이 중요합니다.\n\n### 데이터베이스 선택\n\n| 요구사항 | 권장 DB |\n|----------|---------|\n| 단순 카운터/재고 | Redis (INCR/DECR) |\n| 트랜잭션 필수 | PostgreSQL, MySQL |\n| 대용량 쓰기 | ScyllaDB, Cassandra |\n| 이벤트 로그 | Kafka, Apache Pulsar |\n\n### 네트워크 최적화\n\n```mermaid\nflowchart LR\n    User[User] --> CDN[CDN Edge]\n    CDN -->|Static| Static[정적 자산]\n    CDN -->|Dynamic| Origin[Origin Server]\n    \n    subgraph Region[\"Same Region\"]\n        Origin --> Redis[(Redis)]\n        Origin --> DB[(DB)]\n    end\n```\n\n- **같은 리전에 모든 인프라 배치**: 네트워크 지연 최소화\n- **Connection Pooling**: DB 연결 재사용\n- **Keep-Alive**: HTTP 연결 재사용\n\n## 실전 체크리스트\n\n트래픽 스파이크에 대비한 체크리스트:\n\n| 항목 | 체크 |\n|------|------|\n| Rate Limiting 설정 | ☐ |\n| 메시지 큐 도입 | ☐ |\n| 캐시 워밍업 완료 | ☐ |\n| DB 커넥션 풀 크기 조정 | ☐ |\n| 오토스케일링 테스트 | ☐ |\n| 부하 테스트 수행 | ☐ |\n| 장애 발생 시 Fallback 준비 | ☐ |\n| 모니터링/알림 설정 | ☐ |\n\n## 정리\n\n스파이크성 쓰기 트래픽 대응의 핵심 원칙:\n\n1. **동기 → 비동기 전환**: 즉시 응답하고 나중에 처리\n2. **락 경합 최소화**: 사전 할당, 낙관적 락 활용\n3. **사용자 경험 고려**: 대기열 순번 표시, 진행 상황 알림\n4. **Fail-safe 설계**: 과부하 시 우아한 성능 저하(Graceful Degradation)\n\n---\n\n## Related Posts\n\n- [서비스 규모에 따른 스케일링 전략](/blog/backend/devops/서비스-규모에-따른-스케일링-전략) - 일반적인 읽기 중심 서비스의 스케일링\n\n## References\n\n- [Designing Data-Intensive Applications - Martin Kleppmann](https://dataintensive.net/)\n- [The Twelve-Factor App](https://12factor.net/)",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [
      "Architecture",
      "Backend",
      "DevOps",
      "Scaling",
      "Testing",
      "queue"
    ],
    "readingTime": 6,
    "wordCount": 1122,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "locust-e2e-testing",
    "slug": "locust-e2e-testing",
    "path": "backend/devops",
    "fullPath": "backend/devops/locust-e2e-testing",
    "title": "Locust 기반 환경별 E2E 테스트 자동화",
    "excerpt": "Locust를 활용하여 개발부터 프로덕션까지 환경별 E2E 테스트를 자동화하고 Kubernetes에서 실행하는 방법을 알아봅니다.",
    "content": "# Locust 기반 환경별 E2E 테스트 자동화\n\n## 개요\n\n**Locust**는 Python 기반의 오픈소스 부하 테스트 도구입니다. 코드로 테스트 시나리오를 작성하고, 다양한 환경(INT/STAGE/PROD)에서 일관된 E2E 테스트를 실행할 수 있습니다.\n\n## 왜 Locust인가?\n\n### 장점\n\n| 특성 | 설명 |\n|------|------|\n| **코드 기반** | Python으로 복잡한 시나리오 작성 |\n| **분산 실행** | 여러 워커로 대규모 부하 생성 |\n| **실시간 모니터링** | Web UI로 실시간 메트릭 확인 |\n| **유연성** | 다양한 프로토콜 지원 (HTTP, gRPC 등) |\n| **Kubernetes 친화** | Job/Pod으로 쉽게 배포 |\n\n### 단점\n\n| 특성 | 설명 |\n|------|------|\n| **Python 의존** | Python 환경 필요 |\n| **초기 설정** | 복잡한 시나리오는 코드 작성 필요 |\n\n## 프로젝트 구조\n\n```\ne2e/\n├── .python-version          # Python 버전\n├── pyproject.toml           # 의존성 정의\n├── Makefile                 # 실행 스크립트\n├── config/\n│   ├── local.yaml           # 로컬 환경 설정\n│   ├── int.yaml             # 통합 환경 설정\n│   ├── stage.yaml           # 스테이지 설정\n│   └── prod.yaml            # 프로덕션 설정\n├── suites/\n│   ├── smoke.py             # 스모크 테스트\n│   ├── functional.py        # 기능 테스트\n│   ├── performance.py       # 성능 테스트\n│   └── stress.py            # 스트레스 테스트\n├── utils/\n│   ├── client.py            # API 클라이언트\n│   └── data_generator.py    # 테스트 데이터 생성\n└── locustfile.py            # 메인 진입점\n```\n\n## 설정 파일\n\n### pyproject.toml\n\n```toml\n[project]\nname = \"e2e-tests\"\nversion = \"1.0.0\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"locust>=2.20.0\",\n    \"pyyaml>=6.0\",\n    \"grpcio>=1.60.0\",\n    \"grpcio-tools>=1.60.0\",\n]\n\n[tool.uv]\ndev-dependencies = [\n    \"pytest>=8.0.0\",\n]\n```\n\n### 환경별 설정\n\n```yaml\n# config/int.yaml\nenvironment: int\nbase_url: https://api-int.example.com\ngrpc_host: grpc-int.example.com:443\n\nsettings:\n  default_timeout: 30\n  max_retries: 3\n\ntest_data:\n  collection_prefix: \"e2e_test_\"\n  cleanup_after: true\n```\n\n## 테스트 스위트 구현\n\n### 기본 클라이언트\n\n```python\n# utils/client.py\nfrom typing import Any\nimport grpc\nfrom locust import events\n\nclass APIClient:\n    def __init__(self, base_url: str, timeout: int = 30):\n        self.base_url = base_url\n        self.timeout = timeout\n    \n    def create_document(self, collection: str, uri: str, fields: dict) -> dict:\n        \"\"\"문서를 생성합니다.\"\"\"\n        response = self._post(\n            f\"/v1beta/collections/{collection}/documents\",\n            json={\n                \"document\": {\n                    \"uri\": uri,\n                    \"fields\": fields\n                }\n            }\n        )\n        return response.json()\n    \n    def get_document(self, collection: str, uri: str) -> dict:\n        \"\"\"문서를 조회합니다.\"\"\"\n        response = self._get(f\"/v1beta/collections/{collection}/documents/{uri}\")\n        return response.json()\n    \n    def update_document(self, collection: str, uri: str, fields: dict) -> dict:\n        \"\"\"문서를 업데이트합니다.\"\"\"\n        response = self._patch(\n            f\"/v1beta/collections/{collection}/documents/{uri}\",\n            json={\"fields\": fields}\n        )\n        return response.json()\n    \n    def delete_document(self, collection: str, uri: str) -> bool:\n        \"\"\"문서를 삭제합니다.\"\"\"\n        response = self._delete(f\"/v1beta/collections/{collection}/documents/{uri}\")\n        return response.json().get(\"success\", False)\n```\n\n### 스모크 테스트\n\n빠른 헬스체크 및 기본 기능 확인:\n\n```python\n# suites/smoke.py\nfrom locust import HttpUser, task, between\nimport uuid\n\nclass SmokeTestUser(HttpUser):\n    \"\"\"30초 내 핵심 기능 검증\"\"\"\n    \n    wait_time = between(0.5, 1)\n    \n    def on_start(self):\n        \"\"\"테스트 시작 전 초기화\"\"\"\n        self.collection = f\"smoke_test_{uuid.uuid4().hex[:8]}\"\n        self.created_docs = []\n    \n    @task(3)\n    def create_and_get_document(self):\n        \"\"\"문서 생성 및 조회 테스트\"\"\"\n        doc_uri = f\"doc-{uuid.uuid4().hex[:8]}\"\n        \n        # 생성\n        with self.client.post(\n            f\"/v1beta/collections/{self.collection}/documents\",\n            json={\n                \"document\": {\n                    \"uri\": doc_uri,\n                    \"fields\": {\"test\": True, \"timestamp\": str(time.time())}\n                }\n            },\n            catch_response=True\n        ) as response:\n            if response.status_code == 200:\n                self.created_docs.append(doc_uri)\n                response.success()\n            else:\n                response.failure(f\"Create failed: {response.text}\")\n        \n        # 조회\n        with self.client.get(\n            f\"/v1beta/collections/{self.collection}/documents/{doc_uri}\",\n            catch_response=True\n        ) as response:\n            if response.status_code == 200:\n                data = response.json()\n                if data.get(\"document\", {}).get(\"uri\") == doc_uri:\n                    response.success()\n                else:\n                    response.failure(\"URI mismatch\")\n            else:\n                response.failure(f\"Get failed: {response.text}\")\n    \n    @task(1)\n    def health_check(self):\n        \"\"\"헬스체크\"\"\"\n        self.client.get(\"/ready\")\n    \n    def on_stop(self):\n        \"\"\"테스트 종료 후 정리\"\"\"\n        for uri in self.created_docs:\n            self.client.delete(f\"/v1beta/collections/{self.collection}/documents/{uri}\")\n```\n\n### 기능 테스트\n\nCRUD 전체 흐름 및 엣지 케이스:\n\n```python\n# suites/functional.py\nfrom locust import HttpUser, task, between, SequentialTaskSet\nimport uuid\n\nclass DocumentCRUDFlow(SequentialTaskSet):\n    \"\"\"순차적 CRUD 플로우 테스트\"\"\"\n    \n    def on_start(self):\n        self.doc_uri = f\"crud-test-{uuid.uuid4().hex[:8]}\"\n        self.version = 0\n    \n    @task\n    def step1_create(self):\n        \"\"\"1. 문서 생성\"\"\"\n        response = self.client.post(\n            f\"/v1beta/collections/functional_test/documents\",\n            json={\n                \"document\": {\n                    \"uri\": self.doc_uri,\n                    \"fields\": {\"step\": 1, \"status\": \"created\"}\n                }\n            }\n        )\n        if response.status_code == 200:\n            self.version = response.json()[\"document\"][\"version\"]\n    \n    @task\n    def step2_read(self):\n        \"\"\"2. 문서 조회\"\"\"\n        response = self.client.get(\n            f\"/v1beta/collections/functional_test/documents/{self.doc_uri}\"\n        )\n        assert response.json()[\"document\"][\"version\"] == self.version\n    \n    @task\n    def step3_update(self):\n        \"\"\"3. 문서 업데이트\"\"\"\n        response = self.client.patch(\n            f\"/v1beta/collections/functional_test/documents/{self.doc_uri}\",\n            json={\n                \"fields\": {\"step\": 2, \"status\": \"updated\"},\n                \"expected_version\": self.version\n            }\n        )\n        if response.status_code == 200:\n            self.version = response.json()[\"document\"][\"version\"]\n    \n    @task\n    def step4_verify_history(self):\n        \"\"\"4. 히스토리 확인\"\"\"\n        response = self.client.get(\n            f\"/v1beta/collections/functional_test/documents/{self.doc_uri}/history\"\n        )\n        history = response.json()[\"documents\"]\n        assert len(history) == 2  # 버전 1, 2\n    \n    @task\n    def step5_delete(self):\n        \"\"\"5. 문서 삭제\"\"\"\n        response = self.client.delete(\n            f\"/v1beta/collections/functional_test/documents/{self.doc_uri}\"\n        )\n        assert response.json()[\"success\"] == True\n        self.interrupt()  # 플로우 종료\n\n\nclass FunctionalTestUser(HttpUser):\n    wait_time = between(1, 3)\n    tasks = [DocumentCRUDFlow]\n```\n\n### 성능/스트레스 테스트\n\n```python\n# suites/performance.py\nfrom locust import HttpUser, task, between, LoadTestShape\nimport uuid\n\nclass PerformanceTestUser(HttpUser):\n    \"\"\"고부하 성능 테스트\"\"\"\n    \n    wait_time = between(0.1, 0.5)  # 빠른 요청\n    \n    @task(5)\n    def batch_create(self):\n        \"\"\"배치 생성\"\"\"\n        docs = [\n            {\"uri\": f\"perf-{uuid.uuid4().hex[:8]}\", \"fields\": {\"batch\": True}}\n            for _ in range(10)\n        ]\n        self.client.post(\n            \"/v1beta/collections/perf_test/documents:batchCreate\",\n            json={\"documents\": docs}\n        )\n    \n    @task(10)\n    def query_documents(self):\n        \"\"\"쿼리 테스트\"\"\"\n        self.client.post(\n            \"/v1beta/collections/perf_test/documents:query\",\n            json={\n                \"query\": {\"filter\": {}},\n                \"page_size\": 100\n            }\n        )\n\n\nclass StressTestShape(LoadTestShape):\n    \"\"\"점진적 부하 증가 테스트\"\"\"\n    \n    stages = [\n        {\"duration\": 60, \"users\": 10, \"spawn_rate\": 2},    # 램프업\n        {\"duration\": 120, \"users\": 50, \"spawn_rate\": 5},   # 유지\n        {\"duration\": 60, \"users\": 100, \"spawn_rate\": 10},  # 피크\n        {\"duration\": 60, \"users\": 50, \"spawn_rate\": 10},   # 다운\n    ]\n    \n    def tick(self):\n        run_time = self.get_run_time()\n        \n        for stage in self.stages:\n            if run_time < stage[\"duration\"]:\n                return (stage[\"users\"], stage[\"spawn_rate\"])\n            run_time -= stage[\"duration\"]\n        \n        return None  # 테스트 종료\n```\n\n### 메인 진입점\n\n```python\n# locustfile.py\nimport os\nimport yaml\nfrom locust import events\n\nfrom suites.smoke import SmokeTestUser\nfrom suites.functional import FunctionalTestUser\nfrom suites.performance import PerformanceTestUser\n\n# 환경 설정 로드\ndef load_config():\n    env = os.getenv(\"TEST_ENV\", \"local\")\n    config_path = f\"config/{env}.yaml\"\n    \n    with open(config_path) as f:\n        return yaml.safe_load(f)\n\nCONFIG = load_config()\n\n@events.init.add_listener\ndef on_locust_init(environment, **kwargs):\n    \"\"\"테스트 초기화\"\"\"\n    environment.host = CONFIG[\"base_url\"]\n    print(f\"Testing against: {CONFIG['environment']}\")\n\n# 테스트 모드에 따른 User 클래스 선택\nTEST_MODE = os.getenv(\"TEST_MODE\", \"smoke\")\n\nif TEST_MODE == \"smoke\":\n    class User(SmokeTestUser):\n        pass\nelif TEST_MODE == \"functional\":\n    class User(FunctionalTestUser):\n        pass\nelif TEST_MODE == \"performance\":\n    class User(PerformanceTestUser):\n        pass\n```\n\n## Makefile\n\n```makefile\n# e2e/Makefile\n\n.PHONY: install smoke functional performance stress\n\nLOCUST_FLAGS = --headless --only-summary\n\ninstall:\n uv sync\n\n# 스모크 테스트 (30초, 1 사용자)\nsmoke:\n TEST_MODE=smoke TEST_ENV=$(ENV) locust \\\n  $(LOCUST_FLAGS) \\\n  -u 1 -r 1 -t 30s\n\n# 기능 테스트 (5분, 10 사용자)\nfunctional:\n TEST_MODE=functional TEST_ENV=$(ENV) locust \\\n  $(LOCUST_FLAGS) \\\n  -u 10 -r 2 -t 300s\n\n# 성능 테스트 (10분, 50 사용자)\nperformance:\n TEST_MODE=performance TEST_ENV=$(ENV) locust \\\n  $(LOCUST_FLAGS) \\\n  -u 50 -r 5 -t 600s\n\n# 스트레스 테스트 (10분, 100 사용자)\nstress:\n TEST_MODE=stress TEST_ENV=$(ENV) locust \\\n  $(LOCUST_FLAGS) \\\n  -u 100 -r 10 -t 600s\n\n# 인터랙티브 모드 (Web UI)\ninteractive:\n TEST_MODE=$(MODE) TEST_ENV=$(ENV) locust\n\n# 환경별 실행\ntest-local:\n $(MAKE) smoke ENV=local\n\ntest-int:\n $(MAKE) functional ENV=int\n\ntest-stage:\n $(MAKE) performance ENV=stage\n```\n\n## Kubernetes 배포\n\n### ConfigMap\n\n```yaml\n# k8s/tests/locust/configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: locust-test-config\ndata:\n  test_mode: \"smoke\"\n  test_env: \"int\"\n  users: \"10\"\n  spawn_rate: \"2\"\n  duration: \"300s\"\n```\n\n### Job\n\n```yaml\n# k8s/tests/locust/job.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: locust-e2e-test\nspec:\n  ttlSecondsAfterFinished: 86400\n  template:\n    spec:\n      containers:\n        - name: locust\n          image: myregistry/e2e-tests:latest\n          command:\n            - locust\n            - --headless\n            - --only-summary\n            - -u\n            - $(USERS)\n            - -r\n            - $(SPAWN_RATE)\n            - -t\n            - $(DURATION)\n          env:\n            - name: TEST_MODE\n              valueFrom:\n                configMapKeyRef:\n                  name: locust-test-config\n                  key: test_mode\n            - name: TEST_ENV\n              valueFrom:\n                configMapKeyRef:\n                  name: locust-test-config\n                  key: test_env\n            - name: USERS\n              valueFrom:\n                configMapKeyRef:\n                  name: locust-test-config\n                  key: users\n            - name: SPAWN_RATE\n              valueFrom:\n                configMapKeyRef:\n                  name: locust-test-config\n                  key: spawn_rate\n            - name: DURATION\n              valueFrom:\n                configMapKeyRef:\n                  name: locust-test-config\n                  key: duration\n      restartPolicy: Never\n  backoffLimit: 0\n```\n\n### 환경별 실행\n\n```bash\n# INT 환경 스모크 테스트\nkubectl -n testing patch configmap locust-test-config \\\n  --type=merge -p '{\"data\":{\"test_mode\":\"smoke\",\"test_env\":\"int\",\"users\":\"1\",\"duration\":\"30s\"}}'\nkubectl -n testing apply -f k8s/tests/locust/job.yaml\n\n# STAGE 환경 성능 테스트\nkubectl -n testing patch configmap locust-test-config \\\n  --type=merge -p '{\"data\":{\"test_mode\":\"performance\",\"test_env\":\"stage\",\"users\":\"50\",\"duration\":\"600s\"}}'\nkubectl delete job locust-e2e-test -n testing --ignore-not-found\nkubectl -n testing apply -f k8s/tests/locust/job.yaml\n\n# 로그 확인\nkubectl -n testing logs -f job/locust-e2e-test\n```\n\n## 프로덕션 테스트 안전 장치\n\n```makefile\n# 프로덕션 테스트 (극도로 제한된 설정)\ntest-prod:\n @echo \"⚠️  WARNING: Production test!\"\n @read -p \"Type 'I understand the risks': \" confirm && \\\n  [ \"$$confirm\" = \"I understand the risks\" ] || (echo \"Cancelled.\" && exit 1)\n TEST_MODE=smoke TEST_ENV=prod locust \\\n  $(LOCUST_FLAGS) \\\n  -u 1 -r 1 -t 60s  # 1명, 1분만\n```\n\n## 모범 사례\n\n1. **환경 분리**: 환경별 설정 파일로 엔드포인트/설정 관리\n2. **테스트 데이터 정리**: `on_stop`에서 생성한 데이터 삭제\n3. **점진적 부하**: `LoadTestShape`로 급격한 부하 방지\n4. **프로덕션 보호**: 프로덕션 테스트는 극도로 제한\n5. **결과 저장**: `--csv` 옵션으로 결과 기록\n\n## 참고 자료\n\n- [Locust 공식 문서](https://docs.locust.io/)\n- [Locust Kubernetes 배포](https://docs.locust.io/en/stable/running-distributed.html)\n- [LoadTestShape 가이드](https://docs.locust.io/en/stable/custom-load-shape.html)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "Kubernetes",
      "Performance",
      "Python",
      "Testing"
    ],
    "readingTime": 7,
    "wordCount": 1308,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "linux-ulimit-guide",
    "slug": "linux-ulimit-guide",
    "path": "backend/devops",
    "fullPath": "backend/devops/linux-ulimit-guide",
    "title": "Linux 파일 디스크립터 제한 (ulimit) 완벽 가이드",
    "excerpt": "대용량 트래픽 서버에서 'Too many open files' 오류를 해결하기 위한 ulimit 설정 방법을 알아봅니다.",
    "content": "# Linux 파일 디스크립터 제한 (ulimit) 완벽 가이드\n\n## 개요\n\n고트래픽 서버를 운영하다 보면 `Too many open files` 에러를 마주치게 됩니다. 이는 Linux의 **파일 디스크립터(File Descriptor)** 제한 때문입니다. 이 글에서는 ulimit의 개념과 실전 설정 방법을 다룹니다.\n\n## 파일 디스크립터란?\n\n파일 디스크립터는 Linux에서 열린 파일, 소켓, 파이프 등을 나타내는 정수 값입니다. 모든 I/O 작업은 파일 디스크립터를 통해 이루어집니다.\n\n```bash\n# 현재 프로세스의 열린 파일 디스크립터 확인\nls -la /proc/self/fd/\n```\n\n## 제한 종류\n\n### 시스템 전체 제한\n\n```bash\n# 시스템 전체 최대 파일 디스크립터 수\ncat /proc/sys/fs/file-max\n\n# 현재 사용 중인 파일 디스크립터 수\ncat /proc/sys/fs/file-nr\n```\n\n### 프로세스별 제한\n\n```bash\n# 현재 쉘의 제한 확인\nulimit -n       # soft limit\nulimit -Hn      # hard limit\n```\n\n| 구분 | 설명 |\n|-----|------|\n| **Soft Limit** | 실제 적용되는 제한, 사용자가 변경 가능 |\n| **Hard Limit** | 최대 상한선, root만 증가 가능 |\n\n## 일시적 변경 (현재 세션만)\n\n```bash\n# Soft limit 변경 (hard limit 범위 내에서)\nulimit -n 65535\n\n# Hard limit 변경 (root 권한 필요)\nsudo ulimit -Hn 100000\n```\n\n## 영구적 변경\n\n### 1. limits.conf 설정\n\n`/etc/security/limits.conf` 파일을 수정합니다:\n\n```bash\n# /etc/security/limits.conf\n# <domain>  <type>  <item>  <value>\n\n*           soft    nofile  65535\n*           hard    nofile  100000\nroot        soft    nofile  65535\nroot        hard    nofile  100000\n```\n\n| 필드 | 설명 | 예시 |\n|-----|------|------|\n| domain | 적용 대상 | `*` (모든 사용자), `root`, `@group` |\n| type | 제한 유형 | `soft`, `hard`, `-` (둘 다) |\n| item | 제한 항목 | `nofile` (파일 수), `nproc` (프로세스 수) |\n| value | 제한 값 | 숫자 또는 `unlimited` |\n\n### 2. systemd 서비스 설정\n\nsystemd로 관리되는 서비스는 별도 설정이 필요합니다:\n\n```ini\n# /etc/systemd/system/myapp.service\n[Service]\nLimitNOFILE=65535\nLimitNPROC=65535\n```\n\n또는 전역 설정:\n\n```ini\n# /etc/systemd/system.conf\nDefaultLimitNOFILE=65535\n```\n\n설정 후 재시작:\n\n```bash\nsudo systemctl daemon-reload\nsudo systemctl restart myapp\n```\n\n### 3. sysctl로 시스템 전체 제한 변경\n\n```bash\n# /etc/sysctl.conf\nfs.file-max = 2097152\nfs.nr_open = 2097152\n\n# 적용\nsudo sysctl -p\n```\n\n## 실전 예시: Nginx 설정\n\n```nginx\n# /etc/nginx/nginx.conf\nworker_rlimit_nofile 65535;\n\nevents {\n    worker_connections 65535;\n}\n```\n\n## 문제 해결\n\n### 설정이 적용되지 않을 때\n\n```bash\n# PAM 모듈 확인\ngrep pam_limits /etc/pam.d/common-session\n# session required pam_limits.so 가 있어야 함\n```\n\n### 현재 프로세스의 제한 확인\n\n```bash\n# 특정 프로세스의 제한 확인\ncat /proc/<PID>/limits\n```\n\n## 권장 설정값\n\n| 서버 용도 | nofile 권장값 |\n|---------|--------------|\n| 일반 웹 서버 | 65,535 |\n| 고트래픽 API 서버 | 100,000+ |\n| 데이터베이스 | 65,535 ~ 100,000 |\n| 메시지 브로커 | 500,000+ |\n\n## 주의사항\n\n- **과도한 값 설정 금지**: 메모리 오버헤드 발생 가능\n- **soft ≤ hard**: soft limit은 hard limit을 초과할 수 없음\n- **재부팅 후 확인**: 영구 설정 적용 여부 검증 필수\n\n## 참고 자료\n\n- [Linux man page: limits.conf](https://man7.org/linux/man-pages/man5/limits.conf.5.html)\n- [systemd LimitNOFILE](https://www.freedesktop.org/software/systemd/man/systemd.exec.html)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "Infrastructure",
      "Performance"
    ],
    "readingTime": 3,
    "wordCount": 458,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "helm-environment-management",
    "slug": "helm-environment-management",
    "path": "backend/devops",
    "fullPath": "backend/devops/helm-environment-management",
    "title": "Helm 환경별 Values 오버라이드 전략",
    "excerpt": "Helm Chart에서 공통 설정과 환경별 설정을 분리하여 GitOps 방식으로 관리하는 방법을 알아봅니다.",
    "content": "# Helm 환경별 Values 오버라이드 전략\n\n## 개요\n\nKubernetes 환경에서 동일한 애플리케이션을 INT/STAGE/REAL 등 여러 환경에 배포할 때, **공통 설정**과 **환경별 설정**을 분리하면 유지보수성이 크게 향상됩니다.\n\n## 핵심 개념\n\n```mermaid\nflowchart LR\n    subgraph Common [\"공통 설정\"]\n        VApp[\"values.application.yaml\"]\n        VInfra[\"values.infra.yaml\"]\n    end\n    \n    subgraph EnvOverride [\"환경별 오버라이드\"]\n        Int[\"int/values.application.yaml\"]\n        Stage[\"stage/values.application.yaml\"]\n        Real[\"real/values.application.yaml\"]\n    end\n    \n    VApp --> Int\n    VApp --> Stage\n    VApp --> Real\n    \n    Int --> Merge[\"helm template 병합\"]\n    Stage --> Merge\n    Real --> Merge\n```\n\n- **공통 설정**: 모든 환경에서 동일한 기본값\n- **환경별 오버라이드**: 해당 환경에만 다른 값을 덮어씌움\n\n## 디렉토리 구조\n\n```\nk8s/manifests/my-service/helm/\n├── values.application.yaml    # 공통 애플리케이션 설정\n├── values.infra.yaml          # 공통 인프라 설정\n├── dev/\n│   ├── Chart.yaml\n│   ├── values.application.yaml  # dev 환경 오버라이드\n│   └── values.infra.yaml\n├── int/\n│   ├── Chart.yaml\n│   ├── values.application.yaml  # int 환경 오버라이드\n│   └── values.infra.yaml\n├── stage/\n│   ├── Chart.yaml\n│   ├── values.application.yaml  # stage 환경 오버라이드\n│   └── values.infra.yaml\n└── real/\n    ├── Chart.yaml\n    ├── values.application.yaml  # real(production) 환경 오버라이드\n    └── values.infra.yaml\n```\n\n## 공통 설정\n\n### values.application.yaml (공통)\n\n모든 환경에서 동일한 기본값을 정의:\n\n```yaml\n# 공통 설정 - 모든 환경에서 동일\napp:\n  regionCode: \"apn2\"\n  enableProbe: \"true\"\n  \n  args: [\"run\", \"--config\", \"/etc/my-service/config.yaml\"]\n  \n  readinessProbe:\n    grpc:\n      port: 9090\n    initialDelaySeconds: 15\n    periodSeconds: 20\n    \n  livenessProbe:\n    grpc:\n      port: 9090\n    initialDelaySeconds: 15\n    periodSeconds: 20\n  \n  replicaCount: 1  # 기본값, 환경별로 오버라이드\n  \n  service:\n    ports:\n      - name: http\n        port: 8080\n        protocol: TCP\n      - name: grpc\n        port: 9090\n        protocol: TCP\n```\n\n## 환경별 오버라이드\n\n### int/values.application.yaml\n\nINT 환경에만 다른 설정:\n\n```yaml\n# int 환경 오버라이드 - 공통 값을 덮어씀\n\napp:\n  serviceAccount:\n    annotations:\n      eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789:role/int-role\"\n    create: true\n    name: \"my-service-api\"\n  \n  # INT 환경 시크릿\n  secretsStores:\n    - path: /common/int/secrets\n      type: secretsmanager\n      keys:\n        - MONGO_CONNECTION_STRING\n        - REDIS_CONNECTION_STRING\n  \n  # INT 환경 ConfigMap\n  configmaps:\n    - name: my-service-config\n      mount:\n        enabled: true\n        path: /etc/my-service\n        filename: config.yaml\n      value: |\n        database:\n          mongo_connection_string: \"get_secret_from_secret_manager\"\n          redis_connection_strings:\n            - \"get_secret_from_secret_manager\"\n        stellar:\n          url: https://api.int.example.com\n  \n  # INT 환경 환경변수\n  env:\n    - name: ENV\n      value: \"int\"\n```\n\n### real/values.application.yaml\n\nREAL(Production) 환경:\n\n```yaml\n# real(production) 환경 오버라이드\n\napp:\n  replicaCount: 3  # 프로덕션은 3개 레플리카\n  \n  resources:\n    requests:\n      cpu: 500m\n      memory: 2Gi\n    limits:\n      cpu: 1000m\n      memory: 2Gi\n  \n  serviceAccount:\n    annotations:\n      eks.amazonaws.com/role-arn: \"arn:aws:iam::987654321:role/real-role\"\n    create: true\n    name: \"my-service-api\"\n  \n  secretsStores:\n    - path: /common/real/secrets\n      type: secretsmanager\n      keys:\n        - MONGO_CONNECTION_STRING\n        - REDIS_CONNECTION_STRING\n  \n  configmaps:\n    - name: my-service-config\n      mount:\n        enabled: true\n        path: /etc/my-service\n        filename: config.yaml\n      value: |\n        database:\n          mongo_connection_string: \"get_secret_from_secret_manager\"\n        stellar:\n          url: https://api.example.com\n  \n  env:\n    - name: ENV\n      value: \"real\"\n```\n\n## 로컬 테스트\n\n배포 전 템플릿 렌더링으로 검증:\n\n```bash\n# 의존성 업데이트\nhelm dependency update int\n\n# 템플릿 렌더링 (공통 + 환경별 병합)\nhelm template my-service \\\n  -f values.infra.yaml \\\n  -f values.application.yaml \\\n  -f int/values.infra.yaml \\\n  -f int/values.application.yaml \\\n  ./int | yq\n```\n\n순서가 중요합니다:\n\n1. 공통 infra → 2. 공통 application → 3. 환경별 infra → 4. 환경별 application\n\n뒤에 오는 파일이 앞의 값을 오버라이드합니다.\n\n## ArgoCD GitOps 배포\n\nArgoCD는 각 환경별 디렉토리를 별도 Application으로 등록:\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: my-service-int\n  namespace: argocd\nspec:\n  source:\n    repoURL: https://github.com/myorg/k8s-manifests.git\n    path: k8s/manifests/my-service/helm/int\n    helm:\n      valueFiles:\n        - ../values.infra.yaml\n        - ../values.application.yaml\n        - values.infra.yaml\n        - values.application.yaml\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: my-namespace-int\n```\n\n## 핵심 정리\n\n| 파일 | 위치 | 역할 |\n|------|------|------|\n| `values.application.yaml` | 루트 | 공통 애플리케이션 설정 |\n| `values.infra.yaml` | 루트 | 공통 인프라 설정 |\n| `{env}/values.application.yaml` | 환경 폴더 | 환경별 오버라이드 |\n| `{env}/Chart.yaml` | 환경 폴더 | 의존성 차트 정의 |\n\n## 모범 사례\n\n1. **공통 최대화**: 최대한 공통 설정에 넣고, 환경별은 최소화\n2. **시크릿 분리**: 민감 정보는 Secrets Manager/Vault 사용\n3. **리소스 차등**: 프로덕션은 더 높은 리소스 설정\n4. **로컬 검증**: 배포 전 `helm template`로 렌더링 확인\n5. **레플리카 차등**: dev=1, int=1~2, real=3+\n\n## 참고 자료\n\n- [Helm Values Files](https://helm.sh/docs/chart_template_guide/values_files/)\n- [ArgoCD Helm](https://argo-cd.readthedocs.io/en/stable/user-guide/helm/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "Kubernetes"
    ],
    "readingTime": 3,
    "wordCount": 587,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "mac-os-finder에서-사용자-루트-디렉토리로-이동하는-법",
    "slug": "mac-os-findereseo-sayongja-ruteu-diregtoriro-idonghaneun-beob",
    "path": "misc",
    "fullPath": "misc/mac-os-findereseo-sayongja-ruteu-diregtoriro-idonghaneun-beob",
    "title": "Mac OS Finder에서 사용자 루트 디렉토리로 이동하는 법",
    "excerpt": "Mac OS Finder에서 사용자 루트 디렉토리로 이동하는 법 를 눌러보면 순간이동한다!...",
    "content": "# Mac OS Finder에서 사용자 루트 디렉토리로 이동하는 법\n\n`CMD + SHIFT + H`를 눌러보면 순간이동한다!",
    "docType": "original",
    "category": "Research",
    "tags": [],
    "readingTime": 1,
    "wordCount": 17,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "typescript-5-0-rc-발표에서-데코레이터-부분-요약",
    "slug": "typescript-5-0-rc-balpyoeseo-dekoreiteo-bubun-yoyag",
    "path": "languages/typescript",
    "fullPath": "languages/typescript/typescript-5-0-rc-balpyoeseo-dekoreiteo-bubun-yoyag",
    "title": "Typescript 5.0 RC 발표에서 데코레이터 부분 요약",
    "excerpt": "Typescript 5.0 RC 발표에서 데코레이터 부분 요약 설치 방법 Decorators 데코레이터는 ECMAScript에 곧 추가되는 기능으로, 클래스와 멤버를 재사용 가능...",
    "content": "# Typescript 5.0 RC 발표에서 데코레이터 부분 요약\n\n## 설치 방법\n\n```shell\nnpm install typescript@rc\n```\n\n## Decorators\n\n데코레이터는 ECMAScript에 곧 추가되는 기능으로, 클래스와 멤버를 재사용 가능한 방식으로 사용자화 할 수 있도록 해줍니다.\n\n다음 코드를 고려해봅시다.\n\n```typescript\nclass Person {\n    name: string;\n\n    constructor(name: string) {\n        this.name = name;\n    }\n\n    greet() {\n        console.log(`Hello, my name is ${this.name}`);\n    }\n}\n\nconst p = new Person(\"Ray\");\np.greet();\n```\n\n`greet`은 매우 간단하게 작성되었으나, 좀 더 복잡한 경우를 상상해봅시다. 비동기 논리 흐름이나 재귀호출, 또는 예기치 못한 부작용 등 여러가지가 있을 수 있습니다. 어떤 것을 상상하든 간에, 우리는 한번\n디버깅 로그를 찍어봅시다.\n\n```typescript\nclass Person {\n    name: string;\n\n    constructor(name: string) {\n        this.name = name;\n    }\n\n    greet() {\n        console.log(\"LOG: Entering method.\");\n        console.log(`Hello, my name is ${this.name}`);\n        console.log(\"LOG: Exiting method.\");\n    }\n}\n```\n\n이러한 패턴은 꽤나 일반적입니다. 사실 모든 메소드에 적용해도 좋을만 하죠!\n여기서 데코레이터가 등장합니다. 우리는 `loggedMethod`라는 함수를 다음과 같이 작성해봅니다.\n\n```typescript\nfunction loggedMethod(originalMethod: any, _context: any) {\n    function replacementMethod(this: any, ...args: any[]) {\n        console.log(\"LOG: Entering method.\");\n        const result = originalMethod.call(this, args);\n        console.log(\"LOG: Exiting method.\");\n        return result;\n    }\n\n    return replacementMethod;\n}\n```\n\n\"대체 왜 `any`로 떡칠한거야, `any`script야?\"\n\n인내심을 가져보세요. 당장은 우리가 이 함수의 동작을 보는 것에 집중하기 위해 다른 것을 단순화 했습니다.\n`loggedMethod`가 원본 메소드를 매개변수로 받고, 원본 메소드의 동작 앞 뒤로 로그를 찍은 뒤, 원본 메소드의 결과값을 반환하는 것을 눈치 채셨나요?\n\n이제 우리는 `loggedmethod`로 `greet`메소드를 `decorate`할 수 있습니다.\n\n```typescript\nclass Person {\n    name: string;\n\n    constructor(name: string) {\n        this.name = name;\n    }\n\n    @loggedMethod\n    greet() {\n        console.log(`Hello, my name is ${this.name}.`);\n    }\n}\n\nconst p = new Person(\"Ray\");\np.greet();\n\n// Output:\n//\n//   LOG: Entering method.\n//   Hello, my name is Ray.\n//   LOG: Exiting method.\n```\n\n우리는 `loggedMethod`를 단순히 `greet`위에 `@`를 붙여서 올려놓았습니다. 이렇게 하니 매개변수로는 *target*과 *context* 개체가 넘어옵니다.\n`loggedMethod`가 새로운 함수를 반환하기에 원래 정의된 `greet`는 반환되는 새로운 함수로 대체됩니다.\n\n언급하지는 않았지만 `loggedMethod`에는 \"context object\"라는 두번째 매개변수가 있습니다. 이것은 decorated된 메소드가 어떻게 선언되었는지에 대한 유용한 정보를 가지고 있습니다. 정보에는\n그것이 `#private`이나 `static` 멤버인지, 또는 메소드의 이름은 무엇인지 등이 있죠. 이를 활용해 `decorated`된 메소드 이름을 출력해 보겠습니다.\n\n```typescript\nfunction loggedMethod(originalMethod: any, context: ClassMethodDecoratorContext) {\n    const methodName = String(context.name);\n\n    function replacementMethod(this: any, ...args: any[]) {\n        console.log(`LOG: Entering method '${methodName}'.`)\n        const result = originalMethod.call(this, ...args);\n        console.log(`LOG: Exiting method '${methodName}'.`)\n        return result;\n    }\n\n    return replacementMethod;\n}\n```\n\n드디어 `loggedMethod`에서 `any`를 하나 지웠습니다. 타입스크립트는 `ClassMethodDecoratorContext`라는 타입을 제공하는데, 이것은 데코레이터가 붙은 메소드의 context\nobject를 유형화합니다.\n\n메타데이터와 별개로, 메소드를 위한 context object는 `addInitializer`라는 유용한 함수를 제공합니다. 이는 생성자가 호출되거나 스태틱 메소드 호출시 클래스가 초기화 될 때 연결하는\n방법입니다.)\n\n예를 들어, 자바스크립트에서는 다음과 같은 방식이 일반적입니다.\n\n```typescript\nclass Person {\n    name: string;\n\n    constructor(name: string) {\n        this.name = name;\n        this.greet = this.greet.bind(this);\n    }\n\n    greet() {\n        console.log(`Hello, my name is ${this.name}`);\n    }\n}\n```\n\n또는 `greet`를 화살표 함수로 선언이고 속성으로 선언했을 수도 있습니다.\n\n```typescript\nclass Person {\n    name: string;\n\n    constructor(name: string) {\n        this.name = name;\n    }\n\n    greet = () => {\n        console.log(`Hello, my name is ${this.name}.`);\n    }\n}\n```\n\n이 코드는 `greet`이 독립 실행형 함수로 호출되거나 콜백으로 전달되는 경우 *this*가 다시 바인딩 되지 않도록 작성되었습니다.\n\n```typescript\nconst greet = new Person(\"Ray\").greet;\n// We don't want this to fail!\ngreet();\n```\n\n우리는 `addInitializer`로 생성자에 `bind`하도록 하는 데코레이터를 작성할 수 있습니다.\n\n```typescript\nfunction bound(originalMethod: any, context: ClassMethodDecoratorContext) {\n    const methodName = context.name;\n    if (context.private) {\n        throw new Error(`'bound' cannot decorate private properties like ${methodName as string}.`);\n    }\n    context.addInitializer(function () {\n        this[methodName] = this[methodName].bind(this);\n    });\n}\n```\n\n`bound`는 아무것도 반환하지 않고 오버라이딩 하지도 않으므로 이 로직은 초기화시에만 실행될 겁니다.\n\n```typescript\nclass Person {\n    name: string;\n\n    constructor(name: string) {\n        this.name = name;\n    }\n\n    @bound\n    @loggedMethod\n    greet() {\n        console.log(`Hello, my name is ${this.name}.`);\n    }\n}\n\nconst p = new Person(\"Ray\");\nconst greet = p.greet;\n\n// Works!\ngreet();\n```\n\n데코레이터가 두개 이상일 땐 역순으로 실행됩니다. 그러모르 위 경우에선 `loggedMethod`가 `greet`을 감싸고 새 함수를 반환하며, 그 새 함수를 bound가 감싸게 되겠네요. 지금 경우에선 문제가\n안되지만 특정 순서가 중요한 구조에서는 문제가 되므로 주의해야 합니다.\n\n여기에 약간의 기술을 더하면 데코레이터를 반환하는 함수를 만들 수도 있습니다.\n\n```typescript\nfunction loggedMethod(headMessage = \"LOG:\") {\n    return function actualDecorator(originalMethod: any, context: ClassMethodDecoratorContext) {\n        const methodName = String(context.name);\n\n        function replacementMethod(this: any, ...args: any[]) {\n            console.log(`${headMessage} Entering method '${methodName}'.`)\n            const result = originalMethod.call(this, ...args);\n            console.log(`${headMessage} Exiting method '${methodName}'.`)\n            return result;\n        }\n\n        return replacementMethod;\n    }\n}\n```\n\n이때 우리는 반드시 `loggedMethod`를 메소드 전에 호출해야 하고, 필요시 파라미터도 넘겨줘야 합니다.\n\n```typescript\nclass Person {\n    name: string;\n\n    constructor(name: string) {\n        this.name = name;\n    }\n\n    @loggedMethod(\"\")\n    greet() {\n        console.log(`Hello, my name is ${this.name}.`);\n    }\n}\n\nconst p = new Person(\"Ray\");\np.greet();\n\n// Output:\n//\n//    Entering method 'greet'.\n//   Hello, my name is Ray.\n//    Exiting method 'greet'.\n```\n\n데코레이터는 메소드 뿐만 아니라 프로퍼티, 필드, 게터, 세터, 자동접근자(`auto-accessor`)까지도 사용할 수 있습니다. 심지어는 클래스 스스로도 서브클래싱이나 등록으로 데코레이팅 될 수 있습니다.\n데코레이터를 더 깊게 공부하고 싶다면 [Axel Rauschmayer’s extensive summary.](https://2ality.com/2022/10/javascript-decorators.html)을 읽어\n보세요.\n포함된 변경사항에 대해 더 많은 정보를 알고 싶다면 [원본 풀리퀘스트](https://github.com/microsoft/TypeScript/pull/50820)를 확인해 보세요.",
    "docType": "original",
    "category": "Research",
    "tags": [
      "TypeScript"
    ],
    "readingTime": 4,
    "wordCount": 788,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "mocha의-node-env는",
    "slug": "mochayi-node-envneun",
    "path": "languages/typescript",
    "fullPath": "languages/typescript/mochayi-node-envneun",
    "title": "Mocha의 NODE_ENV는?",
    "excerpt": "Mocha의 NODE_ENV는? 참조 https://github.com/mochajs/mocha/issues/185 23년 1월 24일 기준 확인해본 결과 `\"mocha\": \"^10.2.0\" 에서 undefined이다....",
    "content": "# Mocha의 NODE_ENV는?\n\n> 참조 https://github.com/mochajs/mocha/issues/185\n\n23년 1월 24일 기준 확인해본 결과 \n`\"mocha\": \"^10.2.0\" 에서 undefined이다.",
    "docType": "original",
    "category": "Research",
    "tags": [
      "TypeScript"
    ],
    "readingTime": 1,
    "wordCount": 18,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "enum-대신-사용할-수-있는-as-const",
    "slug": "enum-daesin-sayonghal-su-issneun-as-const",
    "path": "languages/typescript",
    "fullPath": "languages/typescript/enum-daesin-sayonghal-su-issneun-as-const",
    "title": "Enum 대신 사용할 수 있는 `as const`",
    "excerpt": "Enum 대신 사용할 수 있는 자세한 내용은 우아한형제들 기술 블로그 참고 부탁드립니다. 링크 ```typescript / const NodeEnv...",
    "content": "# Enum 대신 사용할 수 있는 `as const`\n\n> 자세한 내용은 우아한형제들 기술 블로그 참고 부탁드립니다. [링크](https://techblog.woowahan.com/9804/#toc-1)\n\n```typescript\n/**\n * const NodeEnvMap: {  \n *   readonly Local: \"local\";  \n *   readonly Dev: \"dev\";  \n *   readonly Prod: \"prod\";  \n *   readonly Test: \"test\";  \n * }\n */\nexport const NodeEnvMap = {\n\tLocal: 'local',\n\tDev: 'dev',\n\tProd: 'prod',\n\tTest: 'test',\n} as const;\n  \n// type NodeEnvMapType = \"local\" | \"dev\" | \"prod\" | \"test\"\nexport type NodeEnvMapType = typeof NodeEnvMap[keyof typeof NodeEnvMap];\n```\n\n`as const`를 안해주면 아래와 같이 `string`!\n\n```typescript\n/**\n* const NodeEnvMap: {\n*   Local: string;\n*   Dev: string;\n*   Prod: string;\n*   Test: string;\n* }\n*/\nexport const NodeEnvMap = {\n\tLocal: 'local',\n\tDev: 'dev',\n\tProd: 'prod',\n\tTest: 'test',\n} \n```",
    "docType": "original",
    "category": "Research",
    "tags": [
      "TypeScript"
    ],
    "readingTime": 1,
    "wordCount": 122,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "rust에서-os-cpu-갯수-찾기",
    "slug": "rusteseo-os-cpu-gaessu-cajgi",
    "path": "languages/rust",
    "fullPath": "languages/rust/rusteseo-os-cpu-gaessu-cajgi",
    "title": "Rust에서 OS CPU 갯수 찾기",
    "excerpt": "참고 Rust에서 OS CPU 갯수 찾기 태초의 방식 (deprecated) ```rs extern crate num_cpus; let num = num_cpus::get(...",
    "content": "> 참고\n\n# Rust에서 OS CPU 갯수 찾기\n\n## 태초의 방식 (deprecated)\n\n```ini\n[dependencies]\nnum_cpus = \"1.0\"\n```\n\n```rs\nextern crate num_cpus;\nlet num = num_cpus::get();\n```\n\n## 그 다음 방식 (deprecated)\n\n```rs\nfn main() {\n    println!(\"{}\", std::os::num_cpus());\n}\n```\n\n## 현재 방식\n\n```rs\n// rustc 1.67.0 (fc594f156 2023-01-24)\nstd::thread::available_parallelism().unwrap().get()\n```",
    "docType": "original",
    "category": "Research",
    "tags": [
      "Rust"
    ],
    "readingTime": 1,
    "wordCount": 52,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "비트-연산-활용",
    "slug": "biteu-yeonsan-hwalyong",
    "path": "languages/others",
    "fullPath": "languages/others/biteu-yeonsan-hwalyong",
    "title": "비트 연산 활용",
    "excerpt": "비트 연산 활용 종류 비트 연산자 설명 &amp;</...",
    "content": "# 비트 연산 활용\n\n## 종류\n\n<table class=\"tb-2\" >\n\t<thead>\n\t\t<tr class=\"bg\">\n\t\t\t<th>비트 연산자</th>\n\t\t\t<th>설명</th>\n\t\t</tr>\n\t</thead>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td>&amp;</td>\n\t\t\t<td>대응되는 비트가 모두 1이면 1을 반환함. (비트 AND 연산)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>|</td>\n\t\t\t<td>대응되는 비트 중에서 하나라도 1이면 1을 반환함. (비트 OR 연산)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>^</td>\n\t\t\t<td>대응되는 비트가 서로 다르면 1을 반환함. (비트 XOR 연산)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>~</td>\n\t\t\t<td>비트를 1이면 0으로, 0이면 1로 반전시킴. (비트 NOT 연산, 1의 보수)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>&lt;&lt;</td>\n\t\t\t<td>명시된 수만큼 비트들을 전부 왼쪽으로 이동시킴. (left shift 연산)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>&gt;&gt;</td>\n\t\t\t<td>부호를 유지하면서 지정한 수만큼 비트를 전부 오른쪽으로 이동시킴. (right shift 연산)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>&gt;&gt;&gt;</td>\n\t\t\t<td>지정한 수만큼 비트를 전부 오른쪽으로 이동시키며, 새로운 비트는 전부 0이 됨.</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n## 종류 별 예시\n\n![](http://www.tcpschool.com/lectures/img_php_bitwise_and.png)\n![](http://www.tcpschool.com/lectures/img_php_bitwise_or.png)\n![](http://www.tcpschool.com/lectures/img_php_bitwise_xor.png)\n![](http://www.tcpschool.com/lectures/img_php_bitwise_not.png)\n\n## 비트마스크\n\n<table>\n<thead>\n<tr>\n<td>\n    연산\n</td>\n<td>\n    사용 예시\n</td>\n</tr>\n</thead>\n<tbody>\n<tr>\n</tr>\n<tr>\n<td>\n    i번째 요소 조회하기\n</td>\n<td >\n\n```\n// 10 & (1 << 2)\n// 1010 & 0100\n// => 0000\n// 결과값의 idx=3 요소는 i번째 요소의 존재여부를 나타낸다 \nn & (1 << i);\n```\n\n</td>\n</tr>\n<tr>\n<td>\n    변경(삽입)\n</td>\n<td>\n\n```\n// 10 | (1 << 2)\n// 1010 | 0100\n// => 1110\n// 결과값의 idx=2 요소를 1로 만들었다\nn | (1 << i)\n```\n\n</td>\n</tr>\n<tr>\n<td>\n    삭제\n</td>\n<td>\n\n```\n// 15 & ~(1 << 2) \n// 1111 & ~0100\n// 1111 & 1011\n// => 1011\n// 결과값의 idx=2 요소를 0으로 만들었다\nn & ~(1 << i)\n```\n\n</td>\n</tr>\n<tr>\n<td>\n공집합\n</td>\n<td>\n\n```\nint result = 0;\n```\n\n</td>\n</tr>\n<tr>\n<td>\n꽉 찬 집합\n</td>\n<td>\n\n```\n// A개의 원소를 가진 집합의 종류\n// 점화식으로는 (2**n) - 1\nint result = ((1 << A) - 1);\n```\n\n</td>\n</tr>\n<tr>\n<td>\n최소 원소 찾기\n</td>\n<td>\n\n```\nint firstBit = b & -b;\n```\n</td>\n</tr>\n<tr>\n<td>\n최소 원소 지우기\n</td>\n<td>\n\n```\nint removed = origin & (origin-1);\n```\n</td>\n</tr>\n<tr>\n<td>\n부분 집합 순회\n</td>\n<td>\n\n```\n집합 A의 부분집합 순회\nfor (int i = A;; i = ((i - 1) & A)) {\n    ...\n}\n```\n</td>\n</tr>\n</tbody>\n</table>\n\n## 참고 문서\n\n- [비트마스크](https://hongjuzzang.github.io/bitmask/bit_mask/#-%EB%B9%84%ED%8A%B8%EB%A7%88%EC%8A%A4%ED%81%AC)",
    "docType": "original",
    "category": "Research",
    "tags": [],
    "readingTime": 2,
    "wordCount": 359,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "c-overview",
    "slug": "c-overview",
    "path": "languages/others",
    "fullPath": "languages/others/c-overview",
    "title": "C# Overview",
    "excerpt": "C Overview Summary of the data types in C with example code snippets Value types bool: represents a Boolean value (true or false) ```cs b...",
    "content": "# C# Overview\n\nSummary of the data types in C# with example code snippets\n\n## Value types\n\n- bool: represents a Boolean value (true or false)\n\n```cs\nbool isCorrect = true;\n```\n\n- char: represents a single Unicode character\n\n```cs\nchar letter = 'a';\n```\n\n- byte: represents an 8-bit unsigned integer\n\n```cs\nbyte b = 255;\n```\n\n- short: represents a 16-bit signed integer\n\n```cs\nshort s = -32768;\n```\n\n- int: represents a 32-bit signed integer\n\n```cs\nint i = 42;\n```\n\n- long: represents a 64-bit signed integer\n\n```cs\nlong l = 1000000000L;\n```\n\n- float: represents a single-precision floating-point number\n\n```cs\nfloat f = 3.1415927f;\n```\n\n- double: represents a double-precision floating-point number\n\n```cs\ndouble d = 3.141592653589793;\n```\n\n- decimal: represents a decimal number with up to 28 significant digits\n\n```cs\ndecimal price = 9.99M;\n```\n\n## Reference types\n\n- string: represents a sequence of Unicode characters\n\n```cs\nstring greeting = \"Hello, world!\";\n```\n\n- object: represents an instance of any type\n\n```cs\nobject obj = new object();\n```\n\ndynamic: represents a type that is determined at runtime\n\n```cs\ndynamic dynamicVar = \"hello\";\nConsole.WriteLine(dynamicVar.GetType()); // System.String\ndynamicVar = 42;\nConsole.WriteLine(dynamicVar.GetType()); // System.Int32\n```\n\n- array: represents a collection of elements of the same type\n\n```cs\nint[] numbers = { 1, 2, 3, 4, 5 };\n```\n\nclass: represents a blueprint for creating objects\n\n```cs\npublic class Person\n{\npublic string Name { get; set; }\npublic int Age { get; set; }\n}\n\nPerson person = new Person { Name = \"Alice\", Age = 30 };\n```\n\n- interface: represents a contract for implementing functionality\n\n```csharp\npublic interface IShape\n{\ndouble GetArea();\n}\n\npublic class Rectangle : IShape\n{\npublic double Width { get; set; }\npublic double Height { get; set; }\n\n    public double GetArea()\n    {\n        return Width * Height;\n    }\n\n}\n\nIShape shape = new Rectangle { Width = 10, Height = 20 };\ndouble area = shape.GetArea();\n```",
    "docType": "original",
    "category": "Research",
    "tags": [],
    "readingTime": 2,
    "wordCount": 327,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "dart-programming-language",
    "slug": "dart-programming-language",
    "path": "languages/dart",
    "fullPath": "languages/dart/dart-programming-language",
    "title": "Dart Programming Language",
    "excerpt": "Dart Programming Language Hello world Variables The Var keywor...",
    "content": "# Dart Programming Language\n\n## Hello world\n\n```dart\nvoid main() {\n  print('hello world');\n}\n\ndart run main.dart\n```\n\n## Variables\n\n### The Var keyword\n\n```dart\nvoid main() {\n  // Compiler automatically infers type of value\n  var name = 'seogyugim';\n\n  // Declare obviously type of value\n  String strname = 'seogyugim';\n}\n```\n\n### Dynamic Type\n\n```dart\nvoid main() {\n  dynamic name;\n  if (name is String) {\n    // Compiler already knows type of name\n    name.isEmpty;\n  }\n\n  // And dart support optional chaining\n  name?.isEmpty;\n}\n```\n\n### Null Safety\n\n```dart\nbool isEmpty(String s) => s.length == 0;\n\nvoid main() {\n  // It'll throw NoSuchMethodError\n  isEmpty(null);\n\n  // It's not okay\n  String name1 = 'seogyugim';\n  // Because name2 must be not null\n  if (name1 != null) {\n    nico.isNotEmpty;\n  }\n\n  // It's not okay\n  String? name2 = 'seogyugim';\n  name = null;\n  // Because name could be a null\n  name.isNotEmpty\n  if (name2 != null) {\n    nico.isNotEmpty;\n  }\n\n  // It's okay\n  String? name = 'seogyugim';\n  name = null;\n  name?.isNotEmpty;\n}\n```\n\n### Final\n\n```dart\nvoid main() {\n  // As same as val of kotlin, const of javascript\n  final name = 'seogyugim';\n}\n```\n\n### Late Variables\n\n```dart\nvoid main() {\n  // We can create variable without data with late keyword\n  late final String name;\n  late var name2;\n\n  // It will throw Error because it is not definitely assigned\n  print(name);\n}\n```\n\n### Constant Variables\n\n```dart\nvoid main() {\n  // compile-time constant\n  const name = 'seogyugim';\n  // Error\n  name = ''\n\n  // OK\n  const API_KEY = '123123';\n  // Error, because compiler don't know when compile-time.\n  const apiRes = fetchApi();\n  // OK\n  final apiRes = fetchApi();\n}\n```\n\n## Data Types\n\n### Basics\n\n```dart\nvoid main() {\n  String name = 'seogyugim';\n  bool isExist = true;\n  int age = 30;\n  double money = 0.01;\n\n // father class of int and double\n // abstract class int extends num { ...\n  num x = 12;\n}\n```\n\n### Lists\n\n```dart\nvoid main() {\n // Type:  List<int>\n var numbers = [1, 2, 3, 4];\n\n // abstract class List<E> implements ...\n List<int> nums = [1, 2, 3, 4];\n numbs.first;\n numbs.last;\n numbs.add(3);\n numbs.contains(9);\n\n var giveMeFive = true;\n var nums = [\n  1,\n  2,\n  3,\n  // Collection If\n  if (giveMeFive) 5,\n ];\n}\n```\n\n### String Interpolation\n\n```dart\nvoid main() {\n var age = 10;\n var hello = \"Hello everyone! my name is $name, and I\\'m ${age + 1} Nice to meet you!\";\n print(hello);\n}\n```\n\n### Collection For\n\n```dart\nvoid main() {\n var oldFriends = [\"nico\", \"lynn\"];\n var newFriends = [\n  \"seogyugim\",\n  \"kyoungseo\",\n  for (var f in oldFriends) \"Hi $f\",\n ];\n}\n```\n\n### Maps\n\n```dart\nvoid main() {\n // Object in Dart is as same as 'any' type in Typescript\n var p = {\n  \"name\": 'seogyugim',\n  \"xp\": 100,\n  \"po\": 100,\n };\n Map<int, bool> existanceTable = {\n  1: true,\n  2: false,\n  3: true,\n };\n}\n```\n\n### Sets\n\n```dart\nvoid main() {\n var numbers = {1, 2, 3, 4};\n Set<int> nums = {1, 2, 3, 4};\n}\n```\n\n## Functions\n\n### How to define\n\n```dart\nvoid sayHello(String name) {\n print(\"Hello $name, nice to meet you!\");\n}\n\nvoid retHello(String name) => \"Hello $name, nice to meet you!\";\n\nvoid main() {\n sayHello('seogyugim');\n print(rethello('seogyugim'));\n}\n```\n\n### Named Parameters\n\n```dart\nString hello(\n String name,\n int age,\n String country,\n) {\n return \"$name, $age, $country\";\n}\n\nString namedDefaultHello({\n String name = 'anonymous',\n int age = 50,\n String country = 'Korea',\n}) {\n return \"$name, $age, $country\";\n}\n\nString namedRequiredHello({\n required String name,\n required int age,\n required String country,\n}) {\n return \"$name, $age, $country\";\n}\n\nvoid main() {\n print(hello(\n  'seogyugim',\n  30,\n  'Korea',\n ));\n\n print(namedDefaultHello());\n\n print(namedRequiredHello(\n  name: 'seogyugim',\n  age: 30,\n  country: 'Korea',\n ));\n}\n```\n\n### Optional Positional Parameters\n\n```dart\nString sayHello(String name, int age, [String? country = \"Hello\"]) {\n return \"$name, $age, $country\";\n}\n\nvoid main(List<String> args) {\n print(sayHello(\"Hello\",31,));\n}\n```\n\n### Question Question Operator\n\n```dart\nString getName([String? name]) => name?.toUpperCase() ?? \"Kim Seogyu\";\n\nvoid main(List<String> args) {\n String name = getName();\n String? name2;\n name2 ??= \"Example\";\n print(name);\n print(name2);\n}\n```\n\n### Typedef\n\n```dart\ntypedef ListOfInts = List<int>;\n\nListOfInts reverseListOfNumbers(ListOfInts list) {\n var reversed = list.reversed;\n return reversed.toList();\n}\n\nvoid main() {\n reverseListOfNumbers([1,2,3]);\n}\n```\n\n## Classes\n\n### Constructors\n\n### Named Constructor Parameters\n\n### Named Constructors\n\n### Cascade Notations\n\n### Enums\n\n### Abstract Classes\n\n### Inheritance\n\n### Mixins",
    "docType": "original",
    "category": "Research",
    "tags": [],
    "readingTime": 4,
    "wordCount": 709,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "minio의-힐링-healing-메커니즘",
    "slug": "minioyi-hilring-healing-mekeonijeum",
    "path": "distributed-systems/minio",
    "fullPath": "distributed-systems/minio/minioyi-hilring-healing-mekeonijeum",
    "title": "MinIO의 힐링(Healing) 메커니즘",
    "excerpt": "MinIO의 힐링(Healing) 메커니즘 MinIO의 힐링 메커니즘은 분산 스토리지 환경에서 데이터 내구성을 보장하기 위한 핵심 기능입니다. 이 메커니즘은 디스크 장애, 손상된 메타데이터, 불완전한 쓰기 등을 감지하고 자동으로 복구합니다. 힐링 프...",
    "content": "# MinIO의 힐링(Healing) 메커니즘\n\nMinIO의 힐링 메커니즘은 분산 스토리지 환경에서 데이터 내구성을 보장하기 위한 핵심 기능입니다. 이 메커니즘은 디스크 장애, 손상된 메타데이터, 불완전한 쓰기 등을 감지하고 자동으로 복구합니다.\n\n## 1. 힐링 프로세스 개요\n\n힐링은 다음과 같은 단계로 진행됩니다:\n\n```go\nfunc (er *erasureObjects) healObject(ctx context.Context, bucket, object, versionID string, opts madmin.HealOpts) (result madmin.HealResultItem, err error) {\n    // 1. 객체 메타데이터 수집\n    // 2. 객체 손상 여부 확인\n    // 3. 필요시 데이터 복구\n    // 4. 복구된 데이터 재배포\n}\n```\n\n## 2. 손상 감지 메커니즘\n\n### 디스크 상태 모니터링\n\nMinIO는 지속적으로 디스크 상태를 확인합니다:\n\n```go\nfunc diskErrToDriveState(err error) (state string) {\n    if err == nil {\n        return madmin.DriveStateOk\n    }\n    switch {\n    case errors.Is(err, errDiskNotFound):\n        return madmin.DriveStateOffline\n    case errors.Is(err, errCorruptedFormat):\n        return madmin.DriveStateCorrupt\n    // ... 기타 상태 확인\n    }\n    return madmin.DriveStateUnknown\n}\n```\n\n### 객체 무결성 확인\n\n`checkObjectWithAllParts` 함수는 객체의 모든 부분이 올바르게 존재하는지 확인합니다:\n\n```go\nfunc checkObjectWithAllParts(ctx context.Context, onlineDisks []StorageAPI, partsMetadata []FileInfo,\n    errs []error, latestMeta FileInfo, filterByETag bool, bucket, object string,\n    scanMode madmin.HealScanMode) (dataErrsByDisk map[int][]int, dataErrsByPart map[int][]int) {\n    // 각 디스크에서 객체 부분 확인\n    // 누락되거나 손상된 부분 식별\n    // 디스크별, 부분별 오류 매핑\n}\n```\n\n## 3. 힐링 결정 로직\n\nMinIO는 다음 조건에 따라 힐링이 필요한지 결정합니다:\n\n```go\nfunc shouldHealObjectOnDisk(erErr error, partsErrs []int, meta FileInfo, latestMeta FileInfo) (bool, bool, error) {\n    switch {\n    case erErr != nil:\n        // 디스크 오류 발생 시 힐링 필요\n        return true, false, nil\n    case !meta.IsValid():\n        // 메타데이터가 유효하지 않은 경우 힐링 필요\n        return true, false, nil\n    case meta.XLV1:\n        // 구 버전 형식의 경우 업그레이드 필요\n        return true, false, nil\n    case meta.ModTime.Before(latestMeta.ModTime):\n        // 메타데이터가 최신이 아닌 경우 힐링 필요\n        return true, false, nil\n    // ... 기타 조건\n    }\n    \n    // 데이터 부분 손상 확인\n    for _, err := range partsErrs {\n        if err != 0 {\n            return true, false, nil\n        }\n    }\n    \n    return false, false, nil\n}\n```\n\n## 4. 데이터 복구 과정\n\n### 쿼럼 기반 데이터 복구\n\nMinIO는 충분한 수의 정상 디스크가 있을 때 손상된 데이터를 복구합니다:\n\n```go\nfunc (e Erasure) Heal(ctx context.Context, writers []io.Writer, readers []io.ReaderAt, totalLength int64, prefer []bool) (derr error) {\n    // 병렬로 데이터 블록 읽기\n    // 리드-솔로몬 알고리즘으로 누락/손상된 블록 복구\n    // 복구된 데이터를 해당 디스크에 쓰기\n}\n```\n\n### 메타데이터 복구\n\n객체 메타데이터 복구는 별도로 처리됩니다:\n\n```go\nfunc writeAllMetadata(ctx context.Context, disks []StorageAPI, origbucket, bucket, prefix string, files []FileInfo, quorum int) ([]StorageAPI, error) {\n    // 모든 디스크에 메타데이터 쓰기 시도\n    // 쿼럼 충족 확인\n}\n```\n\n## 5. 자동 힐링 스캐너\n\nMinIO는 백그라운드에서 스캐너를 실행하여 손상된 객체를 식별합니다:\n\n```go\nfunc (er erasureObjects) nsScanner(ctx context.Context, buckets []BucketInfo, wantCycle uint32, updates chan<- dataUsageCache, healScanMode madmin.HealScanMode) error {\n    // 네임스페이스 스캔\n    // 객체 상태 확인\n    // 필요시 힐링 큐에 추가\n}\n```\n\n## 6. 댕글링(Dangling) 객체 처리\n\n댕글링 객체는 메타데이터는 있지만 실제 데이터가 없거나 불완전한 객체입니다:\n\n```go\nfunc isObjectDangling(metaArr []FileInfo, errs []error, dataErrsByPart map[int][]int) (validMeta FileInfo, ok bool) {\n    // 메타데이터와 실제 데이터 상태 비교\n    // 불일치 발견 시 댕글링 객체로 판단\n}\n```\n\nMinIO는 댕글링 객체를 감지하면 자동으로 삭제하거나 복구합니다:\n\n```go\nfunc (er erasureObjects) deleteIfDangling(ctx context.Context, bucket, object string, metaArr []FileInfo, errs []error, dataErrsByPart map[int][]int, opts ObjectOptions) (FileInfo, error) {\n    // 댕글링 객체 감지\n    // 복구 가능성 평가\n    // 복구 불가능하면 안전하게 제거\n}\n```\n\n## 7. 성능 최적화\n\n### 병렬 힐링\n\n여러 객체와 디스크를 동시에 힐링하여 성능을 최적화합니다:\n\n```go\nfunc (z *erasureServerPools) HealObjects(ctx context.Context, bucket, prefix string, opts madmin.HealOpts, healObjectFn HealObjectFn) error {\n    // 병렬로 객체 스캔\n    // 동시에 여러 객체 힐링\n}\n```\n\n### 힐링 추적 및 메트릭\n\n```go\nfunc healTrace(funcName healingMetric, startTime time.Time, bucket, object string, opts *madmin.HealOpts, err error, result *madmin.HealResultItem) {\n    // 힐링 작업 추적\n    // 성능 및 결과 메트릭 수집\n}\n```\n\n## 결론\n\nMinIO의 힐링 메커니즘은 분산 환경에서 데이터 일관성과 내구성을 보장하는 핵심 기능입니다. 디스크 오류, 데이터 손상, 메타데이터 불일치 등 다양한 장애 상황을 감지하고, 리드-솔로몬 이레이저 코딩을 통해 자동으로 복구함으로써 데이터 손실 없이 시스템이 지속적으로 작동하도록 합니다.",
    "docType": "original",
    "category": "Distributed Systems",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 4,
    "wordCount": 631,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "minio의-이레이저-코딩-구현",
    "slug": "minioyi-ireijeo-koding-guhyeon",
    "path": "distributed-systems/minio",
    "fullPath": "distributed-systems/minio/minioyi-ireijeo-koding-guhyeon",
    "title": "MinIO의 이레이저 코딩 구현",
    "excerpt": "MinIO의 이레이저 코딩 구현 MinIO는 분산 객체 스토리지 시스템으로, 데이터 내구성과 가용성을 위해 이레이저 코딩(Erasure Coding)을 구현했습니다. 이레이저 코딩은 데이터를 여러 조각으로 나누고 패리티 조각을 추가하여 일부 디스크 손실에도 데이...",
    "content": "# MinIO의 이레이저 코딩 구현\n\nMinIO는 분산 객체 스토리지 시스템으로, 데이터 내구성과 가용성을 위해 이레이저 코딩(Erasure Coding)을 구현했습니다. 이레이저 코딩은 데이터를 여러 조각으로 나누고 패리티 조각을 추가하여 일부 디스크 손실에도 데이터를 복구할 수 있게 합니다.\n\n## 핵심 구조\n\n### 1. 구조체 계층\n\n- **Erasure**: 실제 인코딩/디코딩을 수행하는 기본 구조체 (`erasure-coding.go`)\n  ```go\n  type Erasure struct {\n    encoder                  func() reedsolomon.Encoder\n    dataBlocks, parityBlocks int\n    blockSize                int64\n  }\n  ```\n\n- **erasureObjects**: 객체 스토리지 작업을 처리하는 구조체 (`erasure.go`)\n  ```go\n  type erasureObjects struct {\n    setDriveCount      int\n    defaultParityCount int\n    setIndex           int\n    poolIndex          int\n    getDisks           func() []StorageAPI\n    // ...기타 필드\n  }\n  ```\n\n- **erasureSets**: 여러 erasureObjects 세트를 관리 (`erasure-sets.go`)\n\n- **erasureServerPools**: 여러 erasureSets 풀을 관리하는 최상위 계층 (`erasure-server-pool.go`)\n\n### 2. 코딩 메커니즘\n\nMinIO는 Reed-Solomon 알고리즘을 사용하여 이레이저 코딩을 구현합니다:\n\n```go\nfunc NewErasure(ctx context.Context, dataBlocks, parityBlocks int, blockSize int64) (e Erasure, err error) {\n  e = Erasure{\n    dataBlocks:   dataBlocks,\n    parityBlocks: parityBlocks,\n    blockSize:    blockSize,\n  }\n  e.encoder = func() reedsolomon.Encoder {\n    // Reed-Solomon 인코더 초기화\n    return encoder\n  }\n  return e, nil\n}\n```\n\n## 데이터 흐름\n\n### 1. 데이터 인코딩 (쓰기)\n\n`erasure-encode.go`의 `Encode` 메서드는 데이터를 다음과 같이 처리합니다:\n\n1. 객체 데이터를 청크로 분할\n2. 각 청크를 Reed-Solomon 알고리즘으로 인코딩하여 데이터 블록과 패리티 블록 생성\n3. 데이터와 패리티 블록을 여러 디스크에 분산 저장\n\n```go\nfunc (e *Erasure) Encode(ctx context.Context, src io.Reader, writers []io.Writer, buf []byte, quorum int) (total int64, err error) {\n  // 데이터 읽기 및 인코딩\n  blocks, err := e.EncodeData(ctx, buf[:n])\n  // 인코딩된 블록을 여러 디스크에 쓰기\n  err = writer.Write(ctx, blocks)\n}\n```\n\n### 2. 데이터 디코딩 (읽기)\n\n`erasure-decode.go`의 `Decode` 메서드는 다음과 같이 데이터를 복원합니다:\n\n1. 여러 디스크에서 데이터와 패리티 블록 읽기\n2. 일부 블록이 손상되거나 누락되었을 경우 Reed-Solomon 알고리즘으로 복구\n3. 원본 데이터 재구성\n\n```go\nfunc (e Erasure) Decode(ctx context.Context, writer io.Writer, readers []io.ReaderAt, offset, length, totalLength int64, prefer []bool) (written int64, err error) {\n  // 병렬 읽기로 데이터 블록 수집\n  // 필요시 데이터 복구\n  // 원본 데이터 재구성하여 writer에 쓰기\n}\n```\n\n## 내구성 메커니즘\n\n### 1. 쿼럼 기반 작업\n\nMinIO는 쿼럼 기반 접근 방식을 사용하여 읽기/쓰기 작업의 내구성을 보장합니다:\n\n- **읽기 쿼럼(Read Quorum)**: 데이터 블록 수보다 크거나 같은 디스크에서 읽기 성공 필요\n  ```go\n  func (er erasureObjects) defaultRQuorum() int {\n    return er.setDriveCount - er.defaultParityCount\n  }\n  ```\n\n- **쓰기 쿼럼(Write Quorum)**: 데이터 블록 + 패리티 블록 수에서 패리티 블록 수를 뺀 것\n  ```go\n  func (er erasureObjects) defaultWQuorum() int {\n    return er.setDriveCount - er.defaultParityCount\n  }\n  ```\n\n### 2. 힐링(Healing) 메커니즘\n\nMinIO는 자동으로 손상된 데이터를 감지하고 복구하는 힐링 메커니즘을 제공합니다:\n\n```go\nfunc (er *erasureObjects) healObject(ctx context.Context, bucket, object, versionID string, opts madmin.HealOpts) (result madmin.HealResultItem, err error) {\n  // 객체 상태 확인\n  // 손상된 부분 감지\n  // Reed-Solomon 알고리즘 사용하여 복구\n  // 복구된 데이터를 다시 분산 저장\n}\n```\n\n## 고급 기능\n\n### 1. 멀티파트 업로드\n\n대용량 객체를 효율적으로 업로드하기 위한 멀티파트 업로드 지원:\n\n```go\nfunc (er erasureObjects) PutObjectPart(ctx context.Context, bucket, object, uploadID string, partID int, r *PutObjReader, opts ObjectOptions) (pi PartInfo, err error) {\n  // 파트 데이터를 이레이저 코딩으로 인코딩\n  // 인코딩된 조각을 디스크에 저장\n}\n```\n\n### 2. 디스크 풀 리밸런싱 및 디커미셔닝\n\n- **리밸런싱**: 디스크 간 데이터 재분배\n- **디커미셔닝**: 풀에서 디스크 안전하게 제거\n\n## 성능 최적화\n\n1. **병렬 I/O 작업**: 동시에 여러 디스크에서 읽기/쓰기 수행\n2. **버퍼 풀링**: 메모리 사용 최적화\n3. **비트맵 기반 디스크 상태 추적**: 빠른 디스크 상태 확인\n\n## 결론\n\nMinIO의 이레이저 코딩 구현은 Reed-Solomon 알고리즘을 기반으로 하며, 계층적 구조(serverPools > sets > objects)를 통해 확장성을 제공합니다. 데이터 블록과 패리티 블록의 분산 저장, 쿼럼 기반 작업, 자동 힐링 기능으로 높은 내구성과 가용성을 보장합니다.",
    "docType": "original",
    "category": "Distributed Systems",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 3,
    "wordCount": 569,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "minio의-손상-감지-알고리즘",
    "slug": "minioyi-sonsang-gamji-algorijeum",
    "path": "distributed-systems/minio",
    "fullPath": "distributed-systems/minio/minioyi-sonsang-gamji-algorijeum",
    "title": "MinIO의 손상 감지 알고리즘",
    "excerpt": "MinIO의 손상 감지 알고리즘 MinIO는 분산 시스템에서 데이터 손상을 감지하기 위해 여러 계층의 검증 메커니즘을 구현하고 있습니다. 이 알고리즘들은 메타데이터부터 실제 데이터 블록까지 다양한 수준에서 작동합니다. 메타데이터 검증 File...",
    "content": "# MinIO의 손상 감지 알고리즘\n\nMinIO는 분산 시스템에서 데이터 손상을 감지하기 위해 여러 계층의 검증 메커니즘을 구현하고 있습니다. 이 알고리즘들은 메타데이터부터 실제 데이터 블록까지 다양한 수준에서 작동합니다.\n\n## 1. 메타데이터 검증\n\n### FileInfo 유효성 검증\n\n```go\nfunc (fi FileInfo) IsValid() bool {\n    if fi.Erasure.DataBlocks == 0 || fi.Erasure.ParityBlocks == 0 {\n        return false\n    }\n    if len(fi.Erasure.Distribution) != (fi.Erasure.DataBlocks + fi.Erasure.ParityBlocks) {\n        return false\n    }\n    for _, checksum := range fi.Parts {\n        if checksum.ETag == \"\" {\n            return false\n        }\n    }\n    return true\n}\n```\n\n이 함수는 객체의 메타데이터가 유효한지 검사합니다:\n- 데이터 블록과 패리티 블록 수가 올바른지\n- 분산 패턴이 전체 블록 수와 일치하는지\n- 모든 부분(파트)이 체크섬을 가지고 있는지\n\n### 버전 일관성 검사\n\n```go\nfunc findFileInfoInQuorum(ctx context.Context, metaArr []FileInfo, modTime time.Time, etag string, quorum int) (FileInfo, error) {\n    // 메타데이터 배열에서 쿼럼을 만족하는 일관된 버전 찾기\n    // 시간 기반 그룹화 및 버전 비교\n}\n```\n\n이 함수는 여러 디스크에서 수집한 메타데이터를 비교하여 쿼럼을 만족하는 정확한 버전을 찾습니다.\n\n## 2. 데이터 블록 검증\n\n### 체크섬 검증\n\nMinIO는 각 데이터 부분에 대해 ETag(MD5 체크섬)를 저장하고 이를 사용하여 데이터 무결성을 검증합니다:\n\n```go\nfunc (e ErasureInfo) GetChecksumInfo(partNumber int) (ckSum ChecksumInfo) {\n    // 지정된 파트 번호에 대한 체크섬 정보 검색\n}\n```\n\n데이터를 읽을 때, 계산된 체크섬과 저장된 체크섬을 비교하여 손상 여부를 감지합니다.\n\n### 비트롯 검증\n\n비트롯(Bitrot)은 시간이 지남에 따라 발생하는 데이터 손상을 의미합니다. MinIO는 이를 감지하기 위해 추가적인 해시(예: SHA-256, HighwayHash)를 사용합니다:\n\n```go\n// BitrotVerifier 인터페이스는 비트롯 감지를 위한 검증 메커니즘을 제공\ntype BitrotVerifier interface {\n    // 데이터 검증을 위한 메서드\n    Verify(buf []byte) error\n}\n```\n\n## 3. 객체 부분 검증\n\n실제 데이터 블록의 존재와 무결성을 검증하는 과정:\n\n```go\nfunc checkObjectWithAllParts(ctx context.Context, onlineDisks []StorageAPI, partsMetadata []FileInfo,\n    errs []error, latestMeta FileInfo, filterByETag bool, bucket, object string,\n    scanMode madmin.HealScanMode) (dataErrsByDisk map[int][]int, dataErrsByPart map[int][]int) {\n    \n    // 결과 맵 초기화\n    dataErrsByDisk = make(map[int][]int)\n    dataErrsByPart = make(map[int][]int)\n    \n    // 각 파트에 대한 상태 확인\n    for partIdx, partInfo := range latestMeta.Parts {\n        // 각 디스크에서 파트 데이터 확인\n        for diskIdx, disk := range onlineDisks {\n            if disk == nil {\n                // 디스크 오프라인\n                continue\n            }\n            \n            // 파트 데이터 상태 확인\n            partPath := filepath.Join(bucket, object, partInfo.ETag)\n            err := disk.CheckParts(ctx, partPath)\n            \n            if err != nil {\n                // 오류 기록\n                dataErrsByDisk[diskIdx] = append(dataErrsByDisk[diskIdx], partIdx)\n                dataErrsByPart[partIdx] = append(dataErrsByPart[partIdx], diskIdx)\n            }\n        }\n    }\n    \n    return dataErrsByDisk, dataErrsByPart\n}\n```\n\n이 함수는:\n1. 각 디스크에서 객체의 모든 부분을 확인\n2. 누락되거나 손상된 부분을 식별\n3. 디스크별, 부분별로 오류를 맵핑하여 상세한 손상 정보 제공\n\n## 4. 쿼럼 기반 손상 감지\n\nMinIO는 쿼럼 메커니즘을 사용하여 다수결 원칙으로 손상을 감지합니다:\n\n```go\nfunc reduceReadQuorumErrs(ctx context.Context, errs []error, ignoredErrs []error, readQuorum int) (maxErr error) {\n    // 오류 유형별 카운팅\n    errCount := make(map[error]int)\n    for _, err := range errs {\n        if err != nil {\n            errCount[err]++\n        }\n    }\n    \n    // 읽기 쿼럼이 충족되는지 확인\n    if len(errs) - len(errCount) >= readQuorum {\n        return nil // 쿼럼 충족\n    }\n    \n    // 가장 많이 발생한 오류 반환\n    maxCount := 0\n    for err, count := range errCount {\n        if count > maxCount {\n            maxCount = count\n            maxErr = err\n        }\n    }\n    \n    return maxErr\n}\n```\n\n이 접근 방식은:\n1. 필요한 최소 쿼럼 수의 디스크가 동일한 데이터를 가질 때 해당 데이터가 정확하다고 판단\n2. 쿼럼에 미달하는 경우 손상으로 간주하고 복구 시도\n\n## 5. 댕글링 객체 감지\n\n일관성 없는 상태의 객체를 감지하는 알고리즘:\n\n```go\nfunc isObjectDangling(metaArr []FileInfo, errs []error, dataErrsByPart map[int][]int) (validMeta FileInfo, ok bool) {\n    // 유효한 메타데이터 찾기\n    for _, meta := range metaArr {\n        if meta.IsValid() {\n            validMeta = meta\n            break\n        }\n    }\n    \n    if !validMeta.IsValid() {\n        return FileInfo{}, false // 유효한 메타데이터 없음\n    }\n    \n    // 데이터 파트의 상태 확인\n    for partIdx, errDisks := range dataErrsByPart {\n        // 손상된 디스크 수 계산\n        notFoundCount, nonActionableCount := danglingPartErrsCount(errDisks)\n        \n        // 임계값 초과 시 댕글링으로 간주\n        if notFoundCount > (len(metaArr) / 2) {\n            return validMeta, true\n        }\n    }\n    \n    return FileInfo{}, false\n}\n```\n\n이 함수는:\n1. 유효한 메타데이터 존재 여부 확인\n2. 실제 데이터 파트의 상태와 메타데이터 일치 여부 확인\n3. 대다수의 디스크에서 데이터가 누락된 경우 댕글링 객체로 판단\n\n## 6. 디스크 상태 감지\n\n디스크 자체의 상태를 모니터링하는 알고리즘:\n\n```go\nfunc diskErrToDriveState(err error) (state string) {\n    if err == nil {\n        return madmin.DriveStateOk\n    }\n    \n    switch {\n    case errors.Is(err, errDiskNotFound):\n        return madmin.DriveStateOffline\n    case errors.Is(err, errCorruptedFormat):\n        return madmin.DriveStateCorrupt\n    case errors.Is(err, errUnformattedDisk):\n        return madmin.DriveStateUnformatted\n    case errors.Is(err, errDiskAccessDenied):\n        return madmin.DriveStatePermission\n    case errors.Is(err, errFaultyDisk):\n        return madmin.DriveStateFaulty\n    case errors.Is(err, errDiskFull):\n        return madmin.DriveStateFull\n    }\n    \n    return madmin.DriveStateUnknown\n}\n```\n\n이 함수는 디스크 접근 시 발생하는 오류 유형을 분석하여 디스크 상태를 판단합니다.\n\n## 결론\n\nMinIO의 손상 감지 알고리즘은 여러 계층에서 작동합니다:\n\n1. **메타데이터 수준**: 구조 유효성, 버전 일관성 검증\n2. **데이터 블록 수준**: 체크섬, 비트롯 검증\n3. **객체 부분 수준**: 각 부분의 존재 및 무결성 확인\n4. **쿼럼 기반 검증**: 다수결 원칙으로 손상 여부 판단\n5. **댕글링 객체 감지**: 메타데이터와 실제 데이터 간 일관성 확인\n6. **디스크 상태 모니터링**: 디스크 자체의 건강 상태 평가\n\n이러한 다층적 접근 방식으로 MinIO는 분산 환경에서 발생할 수 있는 다양한 유형의 데이터 손상을 효과적으로 감지하고 복구할 수 있습니다.",
    "docType": "original",
    "category": "Distributed Systems",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 5,
    "wordCount": 802,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "주요-구현-내용",
    "slug": "juyo-guhyeon-naeyong",
    "path": "distributed-systems/erasure-coding",
    "fullPath": "distributed-systems/erasure-coding/juyo-guhyeon-naeyong",
    "title": "주요 구현 내용",
    "excerpt": "Ceph erasure-code 주요 구현 내용 핵심 인터페이스 (ErasureCodeInterface) 목적: 모든 이레이저 코드 구현의 표준화된 API 제공 주요 메서드: : 프로파일에 따라 코드 초기...",
    "content": "## Ceph erasure-code 주요 구현 내용\n\n### 1. 핵심 인터페이스 (ErasureCodeInterface)\n- **목적**: 모든 이레이저 코드 구현의 표준화된 API 제공\n- **주요 메서드**:\n  - `init()`: 프로파일에 따라 코드 초기화\n  - `encode()`: 데이터를 청크로 인코딩\n  - `decode()`: 청크에서 원본 데이터 복구\n  - `minimum_to_decode_with_cost()`: 최소 비용으로 복구 가능한 청크 세트 결정\n\n### 2. 기본 구현 (ErasureCode)\n- **역할**: 공통 기능을 구현하는 추상 클래스\n- **기능**:\n  - CRUSH 규칙 생성\n  - 청크 매핑 관리\n  - 디코딩을 위한 최소 청크 세트 계산\n  - 기본적인 인코딩/디코딩 워크플로우\n\n### 3. 플러그인 시스템 (ErasureCodePlugin)\n- **구조**: 동적 로딩 가능한 라이브러리 구조\n- **관리**: `ErasureCodePluginRegistry`가 플러그인 로딩, 초기화, 관리\n- **확장성**: 새로운 알고리즘 쉽게 추가 가능\n\n### 4. 주요 알고리즘 구현\n\n#### Jerasure\n- **특징**: 다양한 이레이저 코드 알고리즘 구현(Reed-Solomon, Cauchy 등)\n- **최적화**: SSE, NEON 등 하드웨어 가속 지원\n- **하위 라이브러리**: \n  - `jerasure`: 코어 이레이저 코딩 기능\n  - `gf-complete`: 유한체(Galois Field) 연산 최적화\n\n#### LRC (Local Reconstruction Codes)\n- **목적**: 지역적 복구를 통한 성능 향상\n- **동작**: \n  - 전역 패리티와 지역 패리티 모두 생성\n  - 단일 디스크 오류는 로컬 패리티만으로 빠르게 복구\n  - 심각한 오류는 전역 패리티로 복구\n\n#### SHEC (Shingled Erasure Code)\n- **특징**: 복구 대역폭 최적화\n- **구현**: 중첩된(shingled) 패리티 구조로 더 효율적인 복구\n\n#### ISA (Intel Storage Acceleration)\n- **특징**: 인텔 ISA-L 라이브러리 사용\n- **최적화**: AVX, AVX2 명령어 활용한 고성능 구현\n- **조건부 컴파일**: 하드웨어 지원 여부에 따라 컴파일 타임에 결정\n\n#### CLAY (Coupled-Layer)\n- **특징**: 최소 네트워크 사용으로 최적 복구\n- **구현**: 계층적 인코딩 구조\n\n### 5. 구현 세부사항\n- **청크 정렬**: SIMD 연산을 위한 메모리 정렬 (SIMD_ALIGN = 64바이트)\n- **프로파일 관리**: 키-값 형태로 코딩 파라미터 관리\n- **시스템 통합**: CRUSH 맵과 통합하여 데이터 배치 관리\n- **성능 최적화**: \n  - 하위 청크(sub-chunks) 지원으로 세밀한 데이터 배치\n  - 비용 기반 청크 선택 알고리즘\n\n### 6. 빌드 시스템 (CMakeLists.txt)\n- **플러그인 구성**: 각 알고리즘은 별도 라이브러리로 빌드\n- **조건부 컴파일**: 하드웨어 기능(SIMD, AVX 등)에 따른 최적화 버전 선택\n- **종속성 관리**: 외부 라이브러리 통합",
    "docType": "original",
    "category": "Distributed Systems",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 2,
    "wordCount": 324,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "용어-정리",
    "slug": "yongeo-jeongri",
    "path": "distributed-systems/erasure-coding",
    "fullPath": "distributed-systems/erasure-coding/yongeo-jeongri",
    "title": "용어 정리",
    "excerpt": "이레이저 코딩 용어 정리 기본 개념: 이레이저 코딩(Erasure Coding): 데이터 중 일부가 손실되더라도 복구할 수 있게 해주는 기술 청크(Chunk): 데이터를 분할한 단위, 각 청크는 별도의 저장소에 분산 저장됨 -...",
    "content": "## 이레이저 코딩 용어 정리\n\n**기본 개념:**\n- **이레이저 코딩(Erasure Coding)**: 데이터 중 일부가 손실되더라도 복구할 수 있게 해주는 기술\n- **청크(Chunk)**: 데이터를 분할한 단위, 각 청크는 별도의 저장소에 분산 저장됨\n- **데이터 청크(Data Chunk)**: 원본 데이터를 저장하는 청크\n- **코딩 청크/패리티 청크(Coding/Parity Chunk)**: 데이터 복구에 사용되는 추가 정보를 담은 청크\n\n**코딩 매개변수:**\n- **K**: 데이터 청크의 수\n- **M**: 코딩/패리티 청크의 수\n- **K+M**: 총 청크 수로, 시스템이 최대 M개의 청크 손실까지 견딜 수 있음\n\n**알고리즘:**\n- **Reed-Solomon**: 널리 사용되는 이레이저 코딩 알고리즘, 어떤 K개의 청크로도 원본 데이터 복구 가능\n- **LRC(Local Reconstruction Codes)**: 일부 데이터는 로컬 패리티로 빠르게 복구하도록 최적화된 코드\n- **SHEC(Shingled Erasure Code)**: 복구 성능을 개선한 코드\n- **CLAY(Coupled-Layer)**: 복구 시 네트워크 사용량을 최소화하는 최적 복구 코드\n\n**Ceph 관련 용어:**\n- **플러그인(Plugin)**: 다양한 이레이저 코딩 알고리즘을 구현한 모듈\n- **CRUSH**: Ceph의 데이터 배치 알고리즘으로, 데이터를 물리적 장치에 분산 배치\n- **CRUSH 규칙(Rule)**: 데이터 배치 정책을 정의\n- **실패 도메인(Failure Domain)**: 함께 실패할 수 있는 구성 요소 집합(예: 랙, 호스트)\n\n**성능 관련 용어:**\n- **인코딩(Encoding)**: 원본 데이터를 데이터 청크와 코딩 청크로 변환하는 과정\n- **디코딩(Decoding)**: 사용 가능한 청크에서 원본 데이터를 복구하는 과정\n- **스트라이프(Stripe)**: 함께 인코딩되는 데이터 집합\n- **최소 복구 집합(Minimum Recovery Set)**: 데이터 복구에 필요한 최소 청크 집합",
    "docType": "original",
    "category": "Distributed Systems",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 2,
    "wordCount": 205,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "리드-솔로몬-코드의-수학적-원리",
    "slug": "rideu-solromon-kodeuyi-suhagjeog-weonri",
    "path": "distributed-systems/erasure-coding",
    "fullPath": "distributed-systems/erasure-coding/rideu-solromon-kodeuyi-suhagjeog-weonri",
    "title": "리드-솔로몬 코드의 수학적 원리",
    "excerpt": "리드-솔로몬 코드의 수학적 원리 개요 리드-솔로몬(Reed-Solomon) 코드는 데이터 무결성을 보장하고 오류 정정을 가능하게 하는 강력한 오류 수정 코드(Error Correction Code, ECC)이다. 본 문서에서는 리드-솔로몬 코드의 핵심...",
    "content": "# 리드-솔로몬 코드의 수학적 원리\n\n## 1. 개요\n리드-솔로몬(Reed-Solomon) 코드는 데이터 무결성을 보장하고 오류 정정을 가능하게 하는 강력한 오류 수정 코드(Error Correction Code, ECC)이다. 본 문서에서는 리드-솔로몬 코드의 핵심 원리인 다항식 표현과 갈루아 필드(Galois Field, GF)를 활용한 연산 과정을 설명한다.\n\n---\n\n## 2. 다항식을 이용한 데이터 표현\n리드-솔로몬 코드는 데이터를 다항식(polynomial)의 계수로 변환하여 저장한다. 이는 일부 데이터가 손실되더라도 남아 있는 데이터로 원본 다항식을 복구할 수 있도록 하기 위함이다.\n\n### 2.1 다항식 표현\n주어진 데이터 $d_0, d_1, \\dots, d_{k-1}$ 를 계수로 하는 다항식 $P(x)$ 를 정의할 수 있다.\n\n$$\nP(x) = d_0 + d_1 x + d_2 x^2 + \\dots + d_{k-1} x^{k-1}\n$$\n\n이 다항식을 통해 특정한 $x$ 값에서 평가(evaluation)한 값이 데이터 조각이 된다.\n\n### 2.2 데이터 샘플링\n데이터 샘플링 과정에서는 특정한 $x$ 값에서 다항식을 평가하여 데이터를 생성한다. 일반적으로, $x$ 값은 서로 다른 정수 또는 유한체 값으로 설정된다.\n\n샘플링한 데이터 포인트는 다음과 같이 표현될 수 있다:\n\n$$\n(x_0, P(x_0)), (x_1, P(x_1)), \\dots, (x_{k-1}, P(x_{k-1)})\n$$\n\n여기서 각 $P(x_i)$ 값이 원본 데이터의 조각이 된다.\n\n---\n\n## 3. 갈루아 필드(Galois Field, GF)\n리드-솔로몬 코드에서는 정수 또는 실수 연산이 아니라 **유한체(Galois Field, GF)** 위에서 연산을 수행한다. 유한체를 사용하면 데이터 크기를 일정하게 유지하면서도 오류 정정을 효과적으로 수행할 수 있다.\n\n### 3.1 $GF(2^m)$ 연산\n리드-솔로몬 코드에서 일반적으로 사용하는 유한체는 $GF(2^m)$ 이다. 예를 들어, $GF(2^8)$ 는 256개의 원소(0부터 255까지의 숫자)로 구성되며, 8비트 연산을 수행할 수 있다. $GF(2^m)$ 에서는 덧셈과 곱셈 연산이 모듈러 연산을 기반으로 수행된다.\n\n- **덧셈**: $GF(2^m)$에서는 비트 단위 XOR 연산을 수행한다.\n- **곱셈**: 다항식 곱셈을 수행한 후 특정한 **원시 다항식(primitive polynomial)** 로 나눈다.\n\n이러한 연산을 사용하면, 유한체 내에서 항상 일정한 크기의 숫자를 유지하면서도 오류 정정을 수행할 수 있다.\n\n---\n\n## 4. 패리티 데이터 생성\n리드-솔로몬 코드에서는 원본 데이터 $k$ 개를 기반으로 $r$ 개의 패리티 데이터를 생성하여 총 $n = k + r$ 개의 데이터 조각을 저장한다.\n\n### 4.1 패리티 생성 방식\n새로운 패리티 데이터를 만들기 위해 기존 데이터 포인트를 바탕으로 새로운 $x$ 위치에서 다항식을 평가한다.\n\n1. $k$ 개의 원본 데이터 포인트를 이용하여 다항식 $P(x)$ 를 생성한다.\n2. 새로운 위치 $x_k, x_{k+1}, \\dots, x_{k+r-1}$ 에서 다항식을 평가하여 패리티 데이터를 생성한다.\n3. 생성된 패리티 데이터를 원본 데이터와 함께 저장한다.\n\n패리티 데이터는 다음과 같이 나타낼 수 있다:\n\n$$\nP(x_k), P(x_{k+1}), \\dots, P(x_{k+r-1})\n$$\n\n이를 통해 일부 데이터가 손실되더라도 다항식 복원을 통해 원본 데이터를 재구성할 수 있다.\n\n---\n\n## 5. 손실 데이터 복구\n데이터 일부가 손실되었을 때, 리드-솔로몬 코드에서는 **라그랑주 보간법(Lagrange Interpolation)** 을 이용하여 원래 다항식을 복구할 수 있다.\n\n### 5.1 라그랑주 보간법을 이용한 복구\n남아 있는 $k$ 개 이상의 데이터 포인트를 이용하여 원래 다항식을 재구성할 수 있다. 라그랑주 보간법을 사용하면 다음과 같이 원본 다항식을 복구할 수 있다.\n\n$$\nP(x) = \\sum_{i=0}^{k-1} P(x_i) \\cdot l_i(x)\n$$\n\n여기서 $l_i(x)$ 는 라그랑주 기본 다항식으로 정의된다:\n\n$$\nl_i(x) = \\prod_{j \\neq i} \\frac{x - x_j}{x_i - x_j}\n$$\n\n이 보간법을 통해 손실된 데이터 포인트를 복구할 수 있다. 즉, $k$ 개 이상의 데이터 조각이 남아 있다면 손실된 데이터도 복구가 가능하다.\n\n---\n\n## 6. 결론\n리드-솔로몬 코드는 다항식 표현과 갈루아 필드 연산을 기반으로 데이터 무결성을 보장하는 오류 정정 기법이다. 이를 통해 다음과 같은 장점이 제공된다:\n\n1. **데이터 손실 복구 가능**: 일부 데이터가 손실되더라도 남은 데이터를 이용하여 원래 데이터를 복원할 수 있음.\n2. **유연한 저장 시스템 지원**: RAID 6, 클라우드 스토리지, 위성 통신 등 다양한 환경에서 사용됨.\n3. **수학적으로 강력한 보장**: 갈루아 필드 연산을 사용하여 데이터를 효율적으로 보호하고 정정 가능.\n\n리드-솔로몬 코드는 단순한 오류 검출을 넘어 **데이터 복구까지 가능한 강력한 알고리즘**이며, 현대의 데이터 저장 및 통신 시스템에서 중요한 역할을 하고 있다.",
    "docType": "original",
    "category": "Research",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 3,
    "wordCount": 558,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "데이터-원본을-전체적으로-리드솔로몬-인코딩-후-샤드-분배-vs-2-스트라이핑-후-개별-블록-단위로-리드솔로몬-인코딩-후-샤드-분배-비교",
    "slug": "deiteo-weonboneul-jeoncejeogeuro-rideusolromon-inkoding-hu-syadeu-bunbae-vs-2-seuteuraiping-hu-gaebyeol-beulrog-danwiro-rideusolromon-inkoding-hu-syadeu-bunbae-bigyo",
    "path": "distributed-systems/erasure-coding",
    "fullPath": "distributed-systems/erasure-coding/deiteo-weonboneul-jeoncejeogeuro-rideusolromon-inkoding-hu-syadeu-bunbae-vs-2-seuteuraiping-hu-gaebyeol-beulrog-danwiro-rideusolromon-inkoding-hu-syadeu-bunbae-bigyo",
    "title": "데이터 원본을 전체적으로 리드솔로몬 인코딩 후 샤드 분배 vs. 2. 스트라이핑 후 개별 블록 단위로 리드솔로몬 인코딩 후 샤드 분배 비교",
    "excerpt": "--- 🔹 1. 데이터 원본을 리드솔로몬 인코딩 후 노드마다 샤드를 나누는 방식 🛠️ 개요 • 원본 데이터를 한 번에 큰 단위로 리드솔로몬(Erasure Coding, EC) 인코딩을 수행한 뒤, 생성된 데이터 블록(원본)과 패리티...",
    "content": "---\n\n**🔹 1. 데이터 원본을 리드솔로몬 인코딩 후 노드마다 샤드를 나누는 방식**\n\n  \n\n**🛠️ 개요**\n\n• 원본 데이터를 한 번에 큰 단위로 리드솔로몬(Erasure Coding, EC) 인코딩을 수행한 뒤, 생성된 **데이터 블록(원본)과 패리티 블록(복구용)을 여러 노드에 분배**하는 방식입니다.\n\n• 예를 들어, **(6,3) Reed-Solomon 코드**를 사용하면, 원본 데이터를 6개로 나누고, 3개의 패리티 블록을 추가 생성하여 **총 9개 블록을 9개 노드에 분배**합니다.\n\n  \n\n**✅ 장점**\n\n1. **쓰기 성능이 상대적으로 좋음**\n\n• 원본 데이터를 한 번에 인코딩 후 분배하기 때문에, **추가적인 스트라이핑 연산이 필요 없음**.\n\n• 네트워크 전송량이 예측 가능하고 일정함.\n\n2. **복구 효율성이 높음 (특히 전체 노드 장애 시)**\n\n• 데이터 블록과 패리티 블록이 일정한 방식으로 저장되므로, **특정 노드 손실 시 패리티 블록을 이용해 빠르게 재구성 가능**.\n\n3. **CPU 오버헤드가 비교적 낮음**\n\n• 리드솔로몬 인코딩을 한 번만 수행하면 되므로, CPU 연산량이 줄어듦.\n\n  \n\n**❌ 단점**\n\n1. **데이터가 노드에 균등하게 저장되지 않을 수 있음**\n\n• 하나의 원본 데이터를 여러 노드에 나누어 저장하기 때문에, **특정 노드가 자주 사용될 가능성**이 있음.\n\n• 네트워크 트래픽이 특정 노드에 집중될 가능성이 있음.\n\n2. **조각난 블록만 읽고 싶어도 전체 데이터의 일부를 복원해야 할 수도 있음**\n\n• 작은 단위 데이터 접근 시에도, 원본 데이터가 인코딩된 상태이므로 **원본 블록을 직접 읽는 것이 어려울 수 있음**.\n\n---\n\n**🔹 2. 데이터를 스트라이핑한 후 개별 블록 단위로 리드솔로몬 인코딩 후 샤드 분배**\n\n  \n\n**🛠️ 개요**\n\n• 원본 데이터를 일정 크기로 **스트라이핑(Striping)** 한 뒤, 각 블록 단위로 리드솔로몬 인코딩을 적용하여 샤드를 생성하고 분배하는 방식입니다.\n\n• 예를 들어, 1GB 파일을 64MB 단위로 나누어 저장할 경우, 각각의 64MB 블록을 개별적으로 리드솔로몬 인코딩하여 분산 저장.\n\n  \n\n**✅ 장점**\n\n1. **작은 단위 데이터 접근이 효율적**\n\n• 특정 블록만 필요할 경우, 해당 블록만 읽고 패리티 블록을 활용하여 복구 가능.\n\n• **Hadoop HDFS의 Erasure Coding 방식**이나 **MinIO의 Parity Striping** 방식에서 사용됨.\n\n2. **노드 간 부하가 균등하게 분산됨**\n\n• 데이터가 작은 단위로 쪼개져 분산되므로, **특정 노드에 트래픽이 집중되는 문제를 줄일 수 있음**.\n\n3. **병렬 I/O 성능 향상**\n\n• 여러 개의 작은 블록이 동시에 다른 노드에서 읽히므로, **대규모 분산 스토리지에서 높은 병렬성을 가질 수 있음**.\n\n  \n\n**❌ 단점**\n\n1. **쓰기 성능이 낮을 수 있음**\n\n• 데이터 단위마다 개별적으로 인코딩을 수행해야 하므로, CPU 연산량 증가 및 추가적인 네트워크 비용 발생.\n\n• 네트워크에서 더 많은 작은 패킷이 이동해야 하므로, 지연(latency)이 증가할 가능성이 있음.\n\n2. **복구 시 오버헤드 증가**\n\n• 블록 단위로 분산되었기 때문에, 특정 노드에서 복구해야 할 블록이 많으면 복구 작업이 병목이 될 수 있음.\n\n---\n\n**🔹 최종 비교**\n\n|**비교 항목**|**1. 원본 데이터 단위로 리드솔로몬 인코딩 후 샤드 분배**|**2. 스트라이핑 후 개별 블록 단위로 리드솔로몬 인코딩**|\n|---|---|---|\n|**쓰기 성능**|상대적으로 빠름 (한 번만 인코딩)|상대적으로 느림 (블록마다 개별 인코딩 필요)|\n|**읽기 성능**|전체 데이터의 일부를 복구해야 할 수도 있음|특정 블록만 읽는 것이 가능|\n|**복구 효율성**|전체 원본을 유지하기 쉬움|개별 블록 복구 시 오버헤드 증가 가능|\n|**노드 부하**|일부 노드에 집중될 가능성이 있음|노드 간 부하가 균등하게 분배됨|\n|**병렬 처리**|병렬성이 상대적으로 낮음|높은 병렬성 가능|\n\n  \n\n---\n\n**🔹 결론: 어떤 방식이 더 적합할까?**\n\n• **대용량 데이터 저장 & 읽기/쓰기 성능이 중요한 경우** → **1번 방식이 유리**\n\n• Ceph RADOS Erasure Coding과 유사한 방식.\n\n• 클러스터 전체의 안정성이 필요하고, 특정 파일을 전체적으로 다루는 경우 적합.\n\n• **데이터 조각 단위로 접근이 많고, 병렬성이 중요한 경우** → **2번 방식이 유리**\n\n• Hadoop HDFS의 EC, MinIO의 스트라이핑 방식과 유사.\n\n• 개별적인 작은 파일이나 블록 단위의 병렬 처리가 중요한 경우 적합.\n\n  \n\n**즉, “1번은 전체 데이터 중심의 안정성을 강조하고, 2번은 개별 블록 접근성과 병렬성을 강조하는 방식”**이라고 볼 수 있습니다. 🚀\n\n  \n\n혹시 더 구체적으로 비교할 부분이 있다면 알려주세요! 😊",
    "docType": "original",
    "category": "Distributed Systems",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 3,
    "wordCount": 543,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "succinct-proofs-of-random-access-spora-번역",
    "slug": "succinct-proofs-of-random-access-spora-beonyeog",
    "path": "distributed-systems/data-availability",
    "fullPath": "distributed-systems/data-availability/succinct-proofs-of-random-access-spora-beonyeog",
    "title": "Succinct Proofs of Random Access (SPoRA) 번역",
    "excerpt": "--- 개요 (Abstract) 이 문서는 Arweave 네트워크의 새로운 합의 메커니즘을 설명한다. 이 메커니즘은 최신 블록위브(blockweave) 상태에서 유추된 과거 데이터 청크를 찾는 경쟁 방식에 기반을 둔다. --- 동기...",
    "content": "---\n\n**1. 개요 (Abstract)**\n\n이 문서는 Arweave 네트워크의 새로운 합의 메커니즘을 설명한다. 이 메커니즘은 **최신 블록위브(blockweave) 상태에서 유추된 과거 데이터 청크를 찾는 경쟁 방식**에 기반을 둔다.\n\n---\n\n**2. 동기 (Motivation)**\n\n현재 Arweave 네트워크에서 사용되는 합의 알고리즘은 **기존 작업 증명(Proof of Work, PoW)** 방식과 유사하지만, 추가적으로 최신 블록위브 상태에서 결정된 **과거 데이터 청크(최대 256KiB)** 를 포함해야 한다. 이 방식은 **네트워크가 과거 데이터를 유지하도록 장려**하는 효과는 있지만, 채굴자가 데이터를 얼마나 빨리 액세스할 수 있어야 하는지에 대한 **제한이 거의 없는 문제**가 있다.\n\n• **문제점 1:**\n\t• 채굴자가 원격 스토리지 풀(remote storage pool)을 활용하여 빠르게 데이터를 검색하는 방식이 가능하다.\n\t• 원격 저장소와 컴퓨팅 풀을 조합하면 **1Gbps 인터넷 링크를 통해 초당 수백만 개의 PoW 입력값을 계산하여 제공**할 수도 있다.\n\t• 실제로 Arweave 네트워크에서는 **공개 노드(public node) 수가 감소하는 동시에, 네트워크 해시파워(hashpower)는 증가**하고 있다.\n\t• 이는 일부 채굴자들이 **공동 저장 및 계산 풀(storage & computation pool)** 을 형성하고 있음을 시사한다.\n\n• **문제점 2:**\n\t• PoW 기반 네트워크는 **에너지 소비량이 매우 크다.**\n\t• 탄소 배출량이 많아 환경에 부정적인 영향을 미칠 수 있다.\n\t• 친환경적인 합의 알고리즘을 도입하는 것은 **Arweave 플랫폼의 장기적인 지속 가능성을 위해 필수적**이다.\n\n---\n\n**SPoRA의 목표**\n\n새로운 합의 알고리즘(SPoRA)은 다음 두 가지 주요 목표를 달성하고자 한다.\n\n1. **데이터를 필요할 때만 불러오는 방식(온디맨드 데이터 검색)을 억제하고, 채굴자들이 데이터를 채굴 기계에 더 가깝게 저장하도록 유도한다.**\n\t• 즉, 데이터를 로컬에 저장하지 않고 네트워크에서 불러오는 채굴자는 불리해지도록 설계한다.\n\n2. **네트워크의 에너지 소비를 줄인다.**\n\t• 기존 PoW의 높은 연산 비용과 전력 소모를 낮춰 보다 효율적인 합의 과정을 만든다.\n\n---\n\n**3. 참조 구현 (Reference Implementation)**\n\n해당 합의 알고리즘의 참조 구현(Reference Implementation)은 아래 링크에서 확인할 수 있다.\n\n🔗 [**ArweaveTeam GitHub 링크**](https://github.com/ArweaveTeam/arweave/pull/269)\n\n---\n\n**4. SPoRA 알고리즘 명세 (Specification)**\n\n**4.1 사전 요구 사항 (Prerequisites)**\n\n**1. 인덱싱된 데이터셋 (Indexed Dataset)**\n\nSPoRA의 핵심은 **과거 데이터 청크를 지속적으로 검색하는 것**이다. 모든 데이터 청크는 **전역 오프셋(Global Offset)** 으로 식별되며, 네트워크 전체에서 모든 바이트가 **동일한 인센티브를 받도록 설계**되어야 한다. 따라서, **블록위브 전체를 인덱싱하여, 특정 청크를 빠르게 검색할 수 있도록 해야 한다.** **Arweave Erlang 클라이언트**는 **버전 2.1부터** 이러한 인덱스를 유지하도록 설계되었다.\n\n---\n\n**2. 느린 해시(Slow Hash)**\n\nSPoRA는 **채굴자가 지속적으로 저장소에 접근하도록 강제**하는 방식으로 동작해야 한다. 즉, **채굴자가 특정 데이터를 빠르게 선택할 수 없도록 만들어야 하며**, 이는 아래 두 가지 이유 때문입니다.\n\n• **위협 1:**\n\t• 채굴자가 데이터 저장 비용을 아끼고, 대신 **빠른 연산으로 PoW를 수행하려는 가능성**이 있다.\n\t• 데이터 저장 없이 빠른 계산만으로 동일한 보상을 받는다면, 저장 인센티브가 사라진다.\n• **위협 2:**\n\t• 현재의 컴퓨팅 기술은 매우 발전하여, 데이터 검색을 하지 않더라도 높은 효율로 연산을 수행할 수 있다.\n\t• 이는 기존 PoW 방식보다도 더 빠르게 채굴이 이루어질 수 있음을 의미한다.\n\n이를 해결하기 위해, Arweave는 **버전 1.7부터** [**RandomX**](https://44jxru4mdgbtd66dlzjlc3huktqmmzufomg5p24jl66zyut562yq.arweave.net/5xN404wZgzH7w15SsWz0VODGZoVzDdfriV-9nFJ99rE)**를 사용**한다. RandomX는 **일반 CPU에 최적화된 PoW 알고리즘**으로, **전문 하드웨어의 채굴 우위를 줄이는 역할**을 한다.\n\n---\n\n**4.2 알고리즘 설명 (Algorithm Description)**\n\n  **채굴자(Miner)의 과정**\n1. 랜덤 논스(nonce)를 생성하고, 현재 블록 상태, 후보 블록, nonce를 포함하는 머클 트리 해시를 생성한다.\n2. 이 해시 값을 기반으로 **특정 Recall Byte(검색해야 할 바이트 위치)** 를 결정한다.\n3. 로컬 저장소에서 해당 바이트가 포함된 **데이터 청크를 검색**한다.\n\t• 데이터를 찾지 못했다면 1번 단계부터 다시 시작한다.\n4. 찾은 데이터 청크와 이전 블록 해시를 조합하여 **빠른 해시(Fast Hash)를 계산**한다.\n5. 계산된 해시 값이 **현재 난이도보다 크다면**, 해당 블록을 네트워크에 전파한다.\n\t• 블록에는 **nonce와 해당 데이터 청크**가 포함된다.\n\n이 과정에서 사용된 **해당 데이터 청크와 머클 증명(Merkle Proofs)** 를 **Succinct Proof of Random Access (SPoRA)** 라고 한다.\n\n이는 새로운 합의 알고리즘의 이름이기도 하다.\n\n---\n\n**검증자(Verifier)의 과정**\n\n• 검증자는 **채굴자가 수행한 과정**을 한 번 실행하여 블록의 유효성을 검증한다.\n• 블록 내의 **nonce와 데이터 청크가 올바른지 확인**하면 된다.\n\n---\n\n**4.3 검색 공간 제한 (Search Space Constraints)**\n\nSPoRA는 **검색 공간(Search Space)을 적절히 설정하여 데이터 저장을 장려**해야 한다.\n\n1. 검색 공간이 **충분히 커야 하는 이유**:\n\t• 채굴자가 온디맨드 방식으로 **전체 검색 공간을 다운로드하는 것이 불가능하도록 하기 위해서**.\n\t• 네트워크 대역폭이 증가함에 따라, 데이터 요청만으로 PoW를 수행하는 것이 점점 쉬워지므로 이를 방지해야 한다.\n\n2. 검색 공간이 **너무 크면 안 되는 이유**:\n\t• 데이터 저장량이 적은 채굴자도 경쟁할 수 있도록 하려면 검색 공간이 너무 크면 안 된다.\n\t• 따라서 SPoRA는 **검색 공간을 블록위브의 10%로 설정**한다.\n\t• 이 경우, 일부 채굴자가 특정 부분을 집중적으로 저장하면 **약 1.2배의 채굴 효율을 얻게 됨**.\n\n---\n\n**5. 관련 연구 (Related Work)**\n\nSPoRA는 **Permacoin: Repurposing Bitcoin Work for Data Preservation** 논문에서 영감을 받았다. Arweave는 이를 **신뢰할 수 있는 중앙화된 데이터 대신, 분산 네트워크 전체에서 데이터를 유지하는 방식**으로 개선했다. SPoRA는 **연산으로 데이터 부족을 보완하는 것이 불가능하도록 설계**되었으며,\n네트워크가 데이터를 고르게 복제하도록 인센티브를 제공한다.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 4,
    "wordCount": 707,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "2024-data-replication-design-spectrum-요약",
    "slug": "2024-data-replication-design-spectrum-yoyag",
    "path": "distributed-systems/data-availability",
    "fullPath": "distributed-systems/data-availability/2024-data-replication-design-spectrum-yoyag",
    "title": "“2024 Data Replication Design Spectrum” 요약",
    "excerpt": "“2024 Data Replication Design Spectrum” 요약 이 글에서는 데이터 복제(Data Replication) 알고리즘의 다양한 설계 방식을 소개하며, 특히 레플리카(replica) 장애를 처리하는 방식에 초점을 맞추고 있...",
    "content": "**“2024 Data Replication Design Spectrum” 요약**\n\n이 글에서는 **데이터 복제(Data Replication) 알고리즘의 다양한 설계 방식**을 소개하며, 특히 **레플리카(replica) 장애를 처리하는 방식**에 초점을 맞추고 있음. 복제 알고리즘들은 **장애 관리 방식**에 따라 분류되며, **리소스 효율성, 가용성, 지연시간(레이턴시)** 등의 측면에서 서로 다른 트레이드오프를 가지게 됨.\n\n**🔹 1. 장애 처리 방식: Failure Masking vs. Failure Detection**\n\n**✅ Failure Masking (장애 마스킹)**\n\n• 일부 레플리카가 장애가 나더라도 **즉각적인 개입 없이 운영 가능**한 방식.\n• **쿼럼 기반(Quorum-based) 리더 없는 복제**가 대표적인 예시.\n• **특징:**\n\t• 다수결(majority) 기반의 동작 → 과반수 이상이 살아있다면 서비스 지속 가능.\n\t• 장애 탐지를 위한 별도 작업 없이 계속 운영 가능하지만, 성능 저하 가능성 존재.\n\n  \n\n**✅ Failure Detection (장애 감지)**\n\n• 장애 발생 시, **명시적으로 감지하고 재구성(reconfiguration)이 필요한 방식**.\n• 레플리카의 상태를 추적하고, 장애가 확인되면 새로운 복제 구조를 설정해야 함.\n• **특징:**\n\t• 장애 감지를 위한 추가적인 오버헤드 존재.\n\t• 장애가 발생하면 즉시 대응이 필요하므로, 복구 과정이 필요함.\n\n**🔹 2. 하이브리드 방식: 리더 기반 복제 (Leader-Based Replication)**\n\n• 리더(Leader)를 두고, 리더가 모든 복제를 관리하는 방식.\n• 대표적인 알고리즘: **Raft, Paxos**\n• **Failure Masking과 Failure Detection의 중간 형태**\n→ 리더가 장애가 나면 새로운 리더를 선출해야 하지만, 운영 중에는 복제를 쉽게 관리할 수 있음.\n\n• **특징:**\n\t• 장애 시 리더를 선출하는 과정에서 지연이 발생할 수 있음.\n\t• 트랜잭션 일관성을 보장하기에 적합.\n\n**🔹 3. 복제 알고리즘 비교 (트레이드오프)**\n\n| **방식**                         | **리소스 효율성** | **가용성(Availability)** | **지연시간(Latency)** |\n| ------------------------------ | ----------- | --------------------- | ----------------- |\n| **Failure Masking (쿼럼 기반)**    | 보통          | 높음                    | 낮음                |\n| **Failure Detection (재구성 필요)** | 낮음          | 중간                    | 높음                |\n| **리더 기반 복제 (Raft 등)**          | 높음          | 중간                    | 중간                |\n\n각 방식은 **특정한 시스템 요구사항에 따라 적합성이 달라지며, 완벽한 방식은 없음**.\n\n**🔹 4. 주요 데이터베이스 시스템의 적용 방식**\n\n  다양한 데이터베이스들이 각각의 목적에 맞는 복제 방식을 선택하고 있어.\n\n• **리더 기반 복제:** MongoDB, Redis Cluster 등\n• **쿼럼 기반 복제:** Cassandra, Riak KV 등\n• **재구성 기반 복제:** Elasticsearch, InfluxDB 등\n\n**🔹 5. 결론: 완벽한 방식은 없음**\n\n• **모든 복제 알고리즘은 트레이드오프가 존재**하며, 시스템이 요구하는 **일관성(Consistency), 가용성(Availability), 성능(Performance)** 을 고려해 선택해야 함.\n• 예를 들어,\n\t• **높은 가용성**이 필요하면 Failure Masking 방식이 유리.\n\t• **리더 기반으로 강한 일관성**을 원하면 Raft 같은 리더 기반 복제가 적합.\n\t• **재구성이 용이한 시스템**이 필요하면 Failure Detection 기반의 접근이 효과적.\n\n  \n\n🔗 원문: [Transactional Blog](https://transactional.blog/blog/2024-data-replication-design-spectrum?utm_source=chatgpt.com)",
    "docType": "original",
    "category": "Research",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 2,
    "wordCount": 378,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "crush-controlled-replication-under-scalable-hashing",
    "slug": "crush-controlled-replication-under-scalable-hashing",
    "path": "distributed-systems/ceph",
    "fullPath": "distributed-systems/ceph/crush-controlled-replication-under-scalable-hashing",
    "title": "CRUSH (Controlled Replication Under Scalable Hashing)",
    "excerpt": "CRUSH (Controlled Replication Under Scalable Hashing) 기본 개념 CRUSH는 Ceph의 핵심 데이터 배치 알고리즘으로, 데이터 객체를 클러스터의 물리적 저장 장치에 분산 배치하는 방법을 결정합니다. 주요 특...",
    "content": "# CRUSH (Controlled Replication Under Scalable Hashing)\n\n## 기본 개념\nCRUSH는 Ceph의 핵심 데이터 배치 알고리즘으로, 데이터 객체를 클러스터의 물리적 저장 장치에 분산 배치하는 방법을 결정합니다.\n\n## 주요 특징\n1. **결정적 배치**: 동일한 입력에 대해 항상 같은 결과 반환 (중앙 조정 없음)\n2. **확장성**: 수천~수만 개의 장치 지원 가능\n3. **자가 관리**: 클러스터 변화에 자동 적응\n4. **장애 도메인 인식**: 하드웨어 장애 시나리오 고려한 배치\n\n## CRUSH 작동 방식\n\n### 1. 계층 구조(CRUSH 맵)\n- **장치(Device)**: 실제 물리적 OSD(Object Storage Daemon)\n- **버킷(Bucket)**: 장치나 다른 버킷을 포함하는 논리적 그룹\n  - **호스트(Host)**: 한 서버의 OSD 그룹\n  - **랙(Rack)**: 여러 호스트 그룹\n  - **로우(Row)**: 여러 랙 그룹\n  - **룸(Room)**: 여러 로우 그룹\n  - **데이터센터(DC)**: 여러 룸 그룹\n  - **루트(Root)**: 최상위 버킷\n\n### 2. 배치 규칙(CRUSH Rule)\n- **목적**: 어떤 방식으로 데이터를 배치할지 정의\n- **구성 요소**:\n  - **규칙 세트(Rule Set)**: 규칙 모음\n  - **규칙 단계(Rule Step)**: 각 규칙의 작업 단위\n  - **실패 도메인(Failure Domain)**: 함께 실패할 수 있는 구성 요소 단위(예: 호스트, 랙)\n  - **타입 지정자(Type Specifier)**: 배치 시 사용할 계층 지정\n\n### 3. 알고리즘 프로세스\n1. **해싱**: 객체 ID를 해시하여 의사 난수 시드 생성\n2. **규칙 적용**: CRUSH 규칙에 따라 배치 위치 결정\n3. **계층 탐색**: 지정된 실패 도메인에서 적절한 장치 선택\n4. **배치 확정**: 선택된 장치에 데이터 배치\n\n## 이레이저 코딩과 CRUSH 통합\n\n### 1. 청크 배치\n- 각 데이터 청크(K)와 코딩 청크(M)는 CRUSH 알고리즘에 의해 서로 다른 장치에 배치\n- 실패 도메인을 고려하여 같은 실패 지점에 여러 청크 배치 방지\n\n### 2. 이레이저 코드별 CRUSH 규칙\n- `ErasureCode::create_rule()`: 각 이레이저 코드 구현은 자신만의 CRUSH 규칙 생성\n- **규칙 매개변수**:\n  - `rule_root`: 최상위 버킷 지정 (기본값: \"default\")\n  - `rule_failure_domain`: 실패 도메인 지정 (기본값: \"host\")\n  - `rule_device_class`: 장치 클래스 제한 (예: SSD만 사용)\n\n### 3. 구현 세부사항\n```cpp\nint ErasureCode::create_rule(const std::string &name,\n                            CrushWrapper &crush,\n                            std::ostream *ss) const {\n  // 최소 필요 장치 수 계산\n  int min_rep = get_chunk_count();\n  // 지정된 실패 도메인에 해당하는 타입 ID 조회\n  int type = crush.get_type_id(rule_failure_domain);\n  // 루트 버킷 ID 조회\n  int rootid = crush.get_item_id(rule_root);\n  // 실제 CRUSH 규칙 생성\n  int rno = crush.add_simple_rule(name, rule_root, rule_failure_domain,\n                                  \"firstn\", pg_pool_t::TYPE_ERASURE,\n                                  min_rep, ss);\n  return rno;\n}\n```\n\n## 실패 복구 시나리오\n\n### 1. 청크 손실 상황\n- 특정 OSD 실패로 일부 청크 손실\n- CRUSH 맵 참조하여 손실된 청크의 위치 파악\n\n### 2. 복구 프로세스\n- `minimum_to_decode_with_cost()`: 최소 비용으로 복구 가능한 청크 세트 계산\n- 남아있는 청크 위치 파악 (CRUSH 맵 이용)\n- 해당 청크들로부터 손실된 청크 복구\n- 새로운 장치에 복구된 청크 재배치 (CRUSH 알고리즘 사용)\n\n## CRUSH의 장점\n1. **확장성**: 중앙 메타데이터 테이블 없이 배치 계산\n2. **복원력**: 클러스터 변화에 동적 적응\n3. **튜닝 가능성**: 다양한 워크로드에 최적화 가능\n4. **하드웨어 인식**: 실제 물리적 토폴로지 반영 가능",
    "docType": "original",
    "category": "Distributed Systems",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 3,
    "wordCount": 441,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ceph-erasure-coding-메타데이터-관리",
    "slug": "ceph-erasure-coding-metadeiteo-gwanri",
    "path": "distributed-systems/ceph",
    "fullPath": "distributed-systems/ceph/ceph-erasure-coding-metadeiteo-gwanri",
    "title": "Ceph Erasure Coding 메타데이터 관리",
    "excerpt": "Ceph Erasure Coding 메타데이터 관리 메타데이터 종류 코딩 프로파일 정보: K, M 값 (데이터 청크 수, 코딩 청크 수) 사용된 이레이저 코딩 알고리즘 유형 특수 파라미터 (예: jerasure...",
    "content": "# Ceph Erasure Coding 메타데이터 관리\n\n## 1. 메타데이터 종류\n1. **코딩 프로파일 정보**:\n   - K, M 값 (데이터 청크 수, 코딩 청크 수)\n   - 사용된 이레이저 코딩 알고리즘 유형\n   - 특수 파라미터 (예: jerasure 기법, LRC 로컬 그룹 크기)\n\n2. **청크 매핑 정보**:\n   - 각 청크의 논리적 인덱스와 물리적 OSD 매핑\n   - `chunk_mapping` 벡터에 저장\n\n3. **객체 레이아웃 정보**:\n   - 청크 크기\n   - 스트라이프 크기\n   - 객체 크기 및 패딩 정보\n\n4. **CRUSH 규칙 메타데이터**:\n   - 실패 도메인 설정\n   - 규칙 ID 및 이름\n   - 디바이스 클래스 제한 정보\n\n## 2. 메타데이터 저장 방식\n\n### 풀(Pool) 수준 메타데이터\n1. **풀 속성**:\n   - `ceph osd pool set {pool-name} erasure_code_profile {profile-name}`\n   - 클러스터 모니터 데이터베이스에 저장\n   - CRUSH 규칙과 연결\n\n2. **프로파일 저장**:\n   ```cpp\n   const ErasureCodeProfile &get_profile() const {\n     return _profile;\n   }\n   ```\n   - 클러스터 모니터의 키-값 저장소에 보관\n   - 각 풀마다 하나의 프로파일 사용\n\n### 객체 수준 메타데이터\n1. **OMAP(Object Map) 활용**:\n   - 객체 헤더에 메타데이터 저장\n   - xattr(확장 속성)으로 청크 정보 저장\n\n2. **객체 속성**:\n   - 객체 크기, 스트라이프 정보\n   - 타임스탬프, 사용자 정의 메타데이터\n\n### PG(Placement Group) 수준 메타데이터\n1. **PG 로그**:\n   - 모든 쓰기 작업 기록\n   - 복구에 필요한 작업 순서 보존\n\n2. **OSDMap**:\n   - 현재 클러스터 상태 반영\n   - 각 PG의 CRUSH 매핑 정보 포함\n\n## 3. 청크 매핑 관리\n\n1. **논리적 매핑**:\n   ```cpp\n   const std::vector<int> &get_chunk_mapping() const;\n   ```\n   - 청크 인덱스(0~K+M-1)와 실제 OSD 매핑\n   - 인코딩/디코딩 시 참조\n\n2. **매핑 초기화**:\n   ```cpp\n   int to_mapping(const ErasureCodeProfile &profile, std::ostream *ss);\n   ```\n   - 프로파일에서 매핑 정보 추출\n   - 필요시 기본 순차 매핑 생성\n\n3. **매핑 활용**:\n   ```cpp\n   int chunk_index(unsigned int i) const;\n   ```\n   - 청크 ID를 실제 저장 위치로 변환\n   - 데이터 조회 시 필요한 위치 계산\n\n## 4. 프로파일 관리\n\n1. **프로파일 구성**:\n   ```cpp\n   typedef std::map<std::string,std::string> ErasureCodeProfile;\n   ```\n   - 키-값 쌍 형태로 설정 저장\n   - 예: `{\"k\":\"4\", \"m\":\"2\", \"technique\":\"reed_sol_van\"}`\n\n2. **프로파일 파싱**:\n   ```cpp\n   int parse(const ErasureCodeProfile &profile, std::ostream *ss);\n   ```\n   - 문자열 형태의 설정을 내부 값으로 변환\n   - 타입 변환 헬퍼 메서드 제공:\n     ```cpp\n     static int to_int(const std::string &name, ErasureCodeProfile &profile, ...);\n     static int to_bool(const std::string &name, ErasureCodeProfile &profile, ...);\n     ```\n\n3. **프로파일 검증**:\n   ```cpp\n   int sanity_check_k_m(int k, int m, std::ostream *ss);\n   ```\n   - K, M 값의 유효성 검사\n   - 최소/최대 값 제한 적용\n\n## 5. 메타데이터 복구 전략\n\n1. **클러스터 맵 동기화**:\n   - 모니터 노드에서 주기적으로 OSDMap 동기화\n   - 변경 사항을 모든 노드에 전파\n\n2. **메타데이터 중복 저장**:\n   - 중요 메타데이터는 여러 모니터 노드에 복제\n   - Paxos 알고리즘으로 일관성 보장\n\n3. **PG 로그 활용**:\n   - 작업 순서대로 기록된 로그로 메타데이터 복구\n   - PG 스크러빙으로 메타데이터 무결성 검증\n\n4. **오류 복구 시나리오**:\n   - OSD 실패: 다른 OSD에서 메타데이터 복구\n   - 모니터 실패: 다른 모니터에서 메타데이터 복제\n   - 전체 메타데이터 손실: 백업 또는 CRUSH 재계산\n\n## 6. 성능 최적화\n\n1. **메타데이터 캐싱**:\n   - 자주 사용되는 프로파일과 매핑 정보 메모리 캐싱\n   - OSD 및 클라이언트 측 캐시 활용\n\n2. **효율적인 조회**:\n   - 인덱스 기반 빠른 청크 위치 조회\n   - 병렬 메타데이터 조회 지원\n\n3. **압축 저장**:\n   - 메타데이터 압축으로 저장 공간 및 네트워크 대역폭 절약\n   - 작은 객체 메타데이터 통합 저장\n\n4. **지연 업데이트**:\n   - 비중요 메타데이터 변경은 지연 기록으로 성능 향상\n   - 일괄 처리(batching)로 디스크 I/O 최소화",
    "docType": "original",
    "category": "Distributed Systems",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 3,
    "wordCount": 521,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ceph-erasure-coding-데이터-조회-흐름",
    "slug": "ceph-erasure-coding-deiteo-johoe-heureum",
    "path": "distributed-systems/ceph",
    "fullPath": "distributed-systems/ceph/ceph-erasure-coding-deiteo-johoe-heureum",
    "title": "Ceph Erasure Coding 데이터 조회 흐름",
    "excerpt": "Ceph Erasure Coding 데이터 조회 흐름 클라이언트 요청 단계 객체 접근 요청: 클라이언트가 특정 객체 ID로 데이터 요청 CRUSH 계산: 클라이언트는 CRUSH 알고리즘을 사용해 객체의 청크들이 저장된 OSD...",
    "content": "# Ceph Erasure Coding 데이터 조회 흐름\n\n## 1. 클라이언트 요청 단계\n1. **객체 접근 요청**: 클라이언트가 특정 객체 ID로 데이터 요청\n2. **CRUSH 계산**: 클라이언트는 CRUSH 알고리즘을 사용해 객체의 청크들이 저장된 OSD 위치 계산\n3. **PG(Placement Group) 결정**: 객체가 속한 배치 그룹 식별\n\n## 2. 데이터 청크 수집 단계\n1. **가용 청크 확인**: \n   - 모든 데이터 청크(K개)가 가용한 경우: 직접 데이터 청크만 읽음\n   - 일부 데이터 청크 누락: 복구를 위한 최소 청크 세트 결정\n\n2. **최적 청크 선택**:\n   ```cpp\n   minimum_to_decode_with_cost(want_to_read, available, &minimum);\n   ```\n   - `want_to_read`: 필요한 청크 ID 집합\n   - `available`: 가용한 청크와 접근 비용 맵\n   - `minimum`: 최소 비용으로 복구 가능한 청크 집합\n\n3. **청크 병렬 요청**: 필요한 모든 청크를 병렬로 요청 (데이터 청크 + 필요한 코딩 청크)\n\n## 3. 데이터 복원 단계\n1. **모든 데이터 청크 가용 시**: \n   - 단순히 필요한 데이터 청크를 연결하여 원본 데이터 구성\n   - 예: K=4인 경우, 4개 데이터 청크 연결\n\n2. **일부 데이터 청크 누락 시**:\n   - `decode()` 메서드 호출로 복원 시작\n   ```cpp\n   decode(want_to_read, chunks, &decoded, chunk_size);\n   ```\n\n3. **복호화 과정**:\n   - 이레이저 코딩 알고리즘 사용하여 누락된 청크 복원\n   - 각 구현별 복호화 방식 차이:\n     - **Jerasure**: Reed-Solomon 기반 행렬 연산으로 복원\n     - **LRC**: 로컬/글로벌 패리티 활용 복원\n     - **ISA**: Intel ISA-L 최적화 라이브러리 사용\n     - **CLAY**: 계층적 복원 방식\n\n4. **하위 청크(Sub-chunk) 처리**:\n   - 일부 구현은 청크를 더 작은 하위 청크로 분할\n   - 필요한 하위 청크만 복원하여 효율성 증가\n\n## 4. 응답 처리 단계\n1. **데이터 재구성**:\n   - 복원된 청크들에서 요청된 데이터 부분 추출\n   - 청크 오프셋 계산: `byte_offset = B % chunk_size`\n   - 청크 인덱스 계산: `chunk_index = B / chunk_size`\n\n2. **버퍼 생성 및 반환**:\n   - 복원된 데이터를 단일 버퍼로 연결\n   - 패딩 제거 (마지막 청크에 추가된 패딩)\n   ```cpp\n   decode_concat(want_to_read, chunks, &decoded);\n   ```\n\n3. **클라이언트 응답**:\n   - 재구성된 원본 데이터를 클라이언트에 반환\n\n## 5. 청크 캐싱 및 최적화\n1. **읽기 캐싱**: 자주 접근하는 청크는 메모리에 캐싱\n2. **부분 읽기**: 필요한 부분만 복원 (전체 객체가 필요 없는 경우)\n3. **지연 복구**: 읽기 요청이 없는 손실 청크는 즉시 복구하지 않고 지연\n\n## 성능 고려사항\n1. **복구 비용**: M(코딩 청크 수)이 클수록 저장 효율성 증가, 복구 비용 증가\n2. **네트워크 트래픽**: 복구 시 필요한 데이터 전송량 (특히 CLAY 알고리즘에서 최적화)\n3. **디코딩 계산 비용**: 알고리즘별 CPU 사용량과 지연 시간 차이",
    "docType": "original",
    "category": "Distributed Systems",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 2,
    "wordCount": 370,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ceph-erasure-coding-데이터-저장-흐름",
    "slug": "ceph-erasure-coding-deiteo-jeojang-heureum",
    "path": "distributed-systems/ceph",
    "fullPath": "distributed-systems/ceph/ceph-erasure-coding-deiteo-jeojang-heureum",
    "title": "Ceph Erasure Coding 데이터 저장 흐름",
    "excerpt": "Ceph Erasure Coding 데이터 저장 흐름 클라이언트 쓰기 요청 단계 객체 쓰기 요청: 클라이언트가 특정 풀에 데이터 쓰기 요청 PG(Placement Group) 결정: 객체 ID를 해시하여 속할 PG 계산 3....",
    "content": "# Ceph Erasure Coding 데이터 저장 흐름\n\n## 1. 클라이언트 쓰기 요청 단계\n1. **객체 쓰기 요청**: 클라이언트가 특정 풀에 데이터 쓰기 요청\n2. **PG(Placement Group) 결정**: 객체 ID를 해시하여 속할 PG 계산\n3. **주 OSD 선택**: 해당 PG의 주(Primary) OSD로 쓰기 요청 전달\n\n## 2. 데이터 인코딩 단계\n1. **청크 크기 계산**:\n   - 객체 크기를 데이터 청크 수(K)로 나누어 각 청크 크기 결정\n   - 필요시 마지막 청크 패딩 추가\n\n2. **인코딩 요청**:\n   ```cpp\n   encode(want_to_encode, in, &encoded);\n   ```\n   - `want_to_encode`: 인코딩할 청크 ID 집합 (일반적으로 모든 K+M 청크)\n   - `in`: 원본 데이터 버퍼\n   - `encoded`: 인코딩된 청크들을 담을 맵\n\n3. **데이터 분할**:\n   - 원본 데이터를 K개의 데이터 청크로 분할\n   - 예: 12KB 데이터, K=4인 경우 각 3KB 청크로 분할\n\n4. **코딩 청크 생성**:\n   - 선택된 이레이저 코딩 알고리즘 실행\n   - 데이터 청크로부터 M개의 코딩 청크 계산\n   - 구현별 계산 방식:\n     - **Jerasure**: Reed-Solomon 행렬 연산\n     - **LRC**: 로컬/글로벌 패리티 계산\n     - **ISA**: 인텔 최적화 라이브러리 사용\n     - **CLAY**: 계층적 인코딩 방식\n\n## 3. 청크 배치 과정\n1. **CRUSH 계산**:\n   - 각 청크의 저장 위치 결정을 위해 CRUSH 알고리즘 실행\n   - 실패 도메인 고려 (예: 다른 호스트에 청크 배치)\n\n2. **배치 규칙 적용**:\n   ```cpp\n   int rule_id = crush.get_rule_id(rule_name);\n   crush.do_rule(rule_id, x, out, placement_count, weights);\n   ```\n   - 생성된 K+M개 청크를 서로 다른 OSD에 배치\n   - 청크 매핑 정보 저장 (각 청크가 어떤 OSD에 있는지)\n\n3. **청크 인덱스 관리**:\n   - `chunk_mapping` 벡터에 청크 인덱스와 실제 OSD 매핑 저장\n   - 이후 데이터 조회 시 이 매핑 정보 사용\n\n## 4. 청크 저장 과정\n1. **병렬 쓰기 작업**:\n   - 각 청크를 해당 OSD에 병렬로 쓰기 요청\n   - 데이터 청크(K개)와 코딩 청크(M개) 모두 저장\n\n2. **원자적 쓰기 보장**:\n   - 모든 청크가 성공적으로 쓰여질 때까지 대기\n   - 일부 실패 시 롤백 메커니즘 활성화\n\n3. **메타데이터 업데이트**:\n   - 객체 속성, 크기, 청크 위치 등 메타데이터 업데이트\n   - OMAP(Object Map)에 추가 속성 저장 가능\n\n## 5. 완료 및 확인 단계\n1. **쿼럼 확인**:\n   - 필요한 최소 수의 OSD가 쓰기 완료 확인\n   - 일반적으로 (K+M)/2 + 1 개의 확인 필요\n\n2. **클라이언트 응답**:\n   - 쓰기 완료 확인 메시지를 클라이언트에 반환\n\n3. **백그라운드 복제**:\n   - 일부 OSD 쓰기 실패 시 백그라운드에서 복제 시도\n   - 자가 복구 메커니즘 시작\n\n## 6. 최적화 기법\n1. **청크 버퍼링**: 작은 쓰기 작업을 버퍼링하여 일괄 처리\n2. **스트라이핑**: 대용량 객체를 여러 스트라이프로 나누어 저장\n3. **비동기 쓰기**: 성능 향상을 위한 비동기 쓰기 작업\n4. **로컬 패리티**: LRC 코드에서 로컬 패리티로 쓰기 성능 최적화\n\n## 7. 저장 효율성\n- **저장 효율**: `(K/(K+M)) * 100%`\n- 예: K=8, M=4 구성에서 저장 효율은 66.7%\n- 일반 복제(replication) 대비 저장 공간 절약 (3x 복제는 33.3% 효율)",
    "docType": "original",
    "category": "Distributed Systems",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 3,
    "wordCount": 427,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "redis-기반-분산-락-가이드",
    "slug": "redis-giban-bunsan-rag-gaideu",
    "path": "database/redis",
    "fullPath": "database/redis/redis-giban-bunsan-rag-gaideu",
    "title": "Redis 기반 분산 락 가이드",
    "excerpt": "분산 시스템에서 Redis를 활용한 분산 락 구현 방법과 Redlock 알고리즘을 알아봅니다.",
    "content": "# Redis 기반 분산 락 가이드\n\n## 개요\n\n분산 시스템에서 여러 노드나 프로세스가 공유 자원에 동시에 접근하는 것을 제어하기 위해 **분산 락(Distributed Lock)**이 필요합니다. Redis는 빠르고 간단한 방식으로 분산 락을 구현할 수 있는 인메모리 저장소로 자주 사용됩니다.\n\n## 기본 개념\n\n- **분산 락**은 여러 프로세스 간 자원 접근을 조율하는 도구\n- Redis는 단일 키의 원자적 조작이 가능하므로 분산 락에 적합\n- 락은 일정 시간 동안만 유효해야 하며, 적절한 TTL 설정이 중요\n\n---\n\n## 기본 구현 방식 (SET NX PX)\n\n### 락 획득\n\n```bash\nSET lock_key unique_value NX PX 3000\n```\n\n| 옵션 | 설명 |\n|-----|------|\n| `NX` | 키가 존재하지 않을 때만 설정 |\n| `PX 3000` | TTL을 3초(3000ms)로 설정 |\n| `unique_value` | 락 주체를 식별하기 위한 UUID |\n\n### 락 해제 (Lua 스크립트)\n\n```lua\n-- 자신이 획득한 락만 해제\nif redis.call(\"GET\", KEYS[1]) == ARGV[1] then\n  return redis.call(\"DEL\", KEYS[1])\nelse\n  return 0\nend\n```\n\n> GET과 DEL을 **원자적**으로 처리하여 다른 프로세스의 락을 실수로 해제하는 것을 방지합니다.\n\n### 주의사항\n\n- 반드시 TTL(만료 시간)을 설정할 것\n- 락을 획득한 주체만 해제할 것\n- Redis 장애 시 락 상태 유실 가능\n\n---\n\n## Redlock 알고리즘 (고가용성)\n\n**Redlock**은 Redis 창시자 Salvatore Sanfilippo가 제안한 고가용성 분산 락 알고리즘입니다.\n\n### 동작 원리\n\n1. 현재 시간을 기록\n2. N개의 Redis 인스턴스에 동시에 락 요청 (`SET NX PX`)\n3. 과반수(N/2 + 1) 이상 성공 시 락 획득\n4. 전체 소요 시간 < TTL일 경우에만 유효\n5. 실패 시 모든 인스턴스에 락 해제\n\n### 장단점\n\n| 장점 | 단점 |\n|-----|------|\n| 단일 노드 장애에 강함 | 구현이 복잡함 |\n| 락 일관성 유지 | 네트워크 지연에 민감 |\n| | 완전한 안전성 보장 불가 (논쟁 있음) |\n\n---\n\n## 사용 권장/비권장\n\n### ✅ 추천 사용처\n\n- 크론잡 중복 실행 방지\n- 공유 리소스 접근 제어\n- 분산 환경 사전 동기화\n\n### ❌ 비권장 사용처\n\n- 강력한 트랜잭션이 필요한 금융 거래\n- 네트워크/시계 오류에 민감한 시스템\n\n---\n\n## 참고 자료\n\n- [Redis 공식 문서: Distributed Locks](https://redis.io/docs/manual/patterns/distributed-locks/)\n- [Redlock Algorithm](https://redis.io/docs/interact/locking/)\n- [Martin Kleppmann의 Redlock 비판](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)",
    "docType": "original",
    "category": "Database",
    "tags": [
      "Concurrency",
      "Database",
      "Distributed Systems",
      "Redis"
    ],
    "readingTime": 2,
    "wordCount": 329,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "aws-elasticache-for-redis-oss-샤딩과-마스터-failover-정리",
    "slug": "aws-elasticache-for-redis-oss-syadinggwa-maseuteo-failover-jeongri",
    "path": "database/redis",
    "fullPath": "database/redis/aws-elasticache-for-redis-oss-syadinggwa-maseuteo-failover-jeongri",
    "title": "AWS ElastiCache for Redis (OSS) - 샤딩과 마스터 Failover 정리",
    "excerpt": "AWS ElastiCache for Redis (OSS) - 샤딩과 마스터 Failover 정리 📌 핵심 개념 요약 Redis Cluster 모드에서는 데이터를 샤딩하여 여러 마스터...",
    "content": "# AWS ElastiCache for Redis (OSS) - 샤딩과 마스터 Failover 정리\n\n![[Pasted image 20250328100819.png]]\n\n## 📌 핵심 개념 요약\n\n- Redis Cluster 모드에서는 데이터를 샤딩하여 여러 마스터 노드에 분산 저장할 수 있다.\n- 그러나 **하나의 키는 반드시 하나의 마스터 노드만이 관리**한다.\n- 이 구조에서 **마스터 노드의 장애(Failover)** 발생 시, 리플리카를 자동 승격시켜 서비스를 지속할 수 있도록 한다.\n\n---\n\n## 🧱 샤딩 구조\n\n### 🔹 해시 슬롯 기반 샤딩\n\n- Redis Cluster는 키를 **0 ~ 16383 해시 슬롯** 중 하나에 매핑한다.\n- 각 해시 슬롯은 **하나의 샤드(= 마스터 노드)** 가 담당한다.\n- 샤드 수가 늘어나면 슬롯이 분산되며, 각 샤드는 자신이 맡은 슬롯 범위의 키만 관리한다.\n\n### 🔹 키 관리\n\n- 하나의 키는 하나의 해시 슬롯에 매핑되므로,\n- **동일한 키에 대해 동시에 여러 마스터가 접근하는 일은 없음**.\n\n---\n\n## 🔁 마스터 노드 Failover 처리\n\n### 🔹 리플리카 구성\n\n- ElastiCache는 **각 마스터 노드에 대해 하나 이상의 리플리카 노드(replica)를 구성**할 수 있도록 지원한다.\n- 리플리카는 마스터의 데이터를 비동기 복제한다.\n\n### 🔹 Failover 시나리오\n\n1. 마스터 노드에 장애 발생\n2. ElastiCache 감시 시스템이 자동 감지\n3. 해당 샤드의 리플리카 노드 중 하나를 **자동으로 마스터로 승격(promote)**\n4. 클러스터 메타데이터가 갱신됨\n5. 클라이언트는 재접속 시 **MOVED 리다이렉션 응답**을 통해 새 마스터에 연결됨\n\n### 🔹 구성 예시\n\n샤드 1: 마스터 A <–> 리플리카 A’\n\n샤드 2: 마스터 B <–> 리플리카 B’\n\n샤드 3: 마스터 C <–> 리플리카 C’\n\n- 마스터 B가 죽으면 → 리플리카 B'가 새 마스터로 승격\n\n---\n\n## ⚠️ 주의사항\n\n- Redis의 복제는 **비동기(asynchronous)** 이므로, 장애 발생 시 **일부 데이터 손실 가능성**이 존재한다.\n- 강력한 데이터 정합성이 요구되는 경우, 사용 시점 및 구조를 신중하게 설계해야 한다.\n- **리플리카가 없으면 Failover가 불가능**하므로, 최소한 샤드당 1개의 리플리카 구성이 권장된다.\n\n---\n\n## ✅ 모범 구성\n\n- **Multi-AZ 배포**로 장애 도메인 분리\n- 각 샤드에 **1개 이상의 리플리카 구성**\n- 클라이언트 라이브러리는 `Cluster-aware` 모드 사용 (예: `lettuce`, `Jedis`, `ioredis` 등)\n\n---\n\n## 📚 참고 자료\n\n- [AWS 공식 문서 - ElastiCache에서 클러스터 관리](https://docs.aws.amazon.com/ko_kr/AmazonElastiCache/latest/dg/Clusters.html)\n-",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [
      "Database"
    ],
    "readingTime": 2,
    "wordCount": 316,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "which-is-best-isolation-level-for-a-common-situation",
    "slug": "which-is-best-isolation-level-for-a-common-situation",
    "path": "database/mysql",
    "fullPath": "database/mysql/which-is-best-isolation-level-for-a-common-situation",
    "title": "Which is best isolation level for a common situation",
    "excerpt": "Which is best isolation level for a common situation MySQL은 데이터베이스 엔진이 동시 트랜잭션을 처리하는 방법을 정의하는 여러 분리 수준을 지원합니다. 일반적인 상황에 가장 적합한 옵션은 응용프로그램의 특정 요구사항에...",
    "content": "# Which is best isolation level for a common situation\n\nMySQL은 데이터베이스 엔진이 동시 트랜잭션을 처리하는 방법을 정의하는 여러 분리 수준을 지원합니다. 일반적인 상황에 가장 적합한 옵션은 응용프로그램의 특정 요구사항에 따라 달라집니다.\n\n일반적으로 REPEATABLE READ 분리 수준은 일관성과 성능 사이에서 균형을 맞추기 때문에 대부분의 응용 프로그램에 적합한 기본 옵션입니다. 그러나 응용프로그램에 더 높은 수준의 일관성이 필요한 경우에는 직렬화를 사용하는 것을 고려할 수 있습니다. 일관성보다 성능을 우선시해야 하는 경우 READ COMMITED 또는 READ UNCOMMITED를 사용하는 것을 고려할 수 있지만 잠재적인 위험과 절충점을 알고 있어야 합니다.\n\n## ISOLATION LEVELS\n\n- READ UNCOMMITTED: 이 격리 수준은 더티 읽기를 허용하며, 이는 트랜잭션이 다른 트랜잭션에 의해 커밋되지 않은 변경 사항을 읽을 수 있음을 의미합니다. 데이터가 일관되지 않을 수 있으므로 대부분의 응용 프로그램에는 권장되지 않습니다.\n\n- READ COMMITTED: 이 격리 수준을 통해 트랜잭션은 커밋된 데이터만 읽을 수 있으므로 더티 읽기를 방지할 수 있습니다. 그러나 다른 트랜잭션이 읽기 간에 변경 사항을 커밋할 수 있기 때문에 계속해서 반복할 수 없는 읽기 및 팬텀 읽기가 발생할 수 있습니다.\n\n- REPEATABLE READ: 이 격리 수준을 사용하면 다른 트랜잭션이 변경되더라도 트랜잭션이 데이터베이스의 일관된 스냅샷을 볼 수 있습니다. 반복할 수 없는 읽기 및 팬텀 읽기를 방지하지만 새 데이터를 트랜잭션에 삽입할 수 있는 작은 창을 허용합니다.\n\n- SERIALIZABLE: 이 분리 수준은 트랜잭션이 완료될 때까지 트랜잭션이 액세스하는 모든 행을 잠궈 최고 수준의 분리를 제공합니다. 이렇게 하면 모든 형태의 동시성 관련 이상 징후를 방지할 수 있지만 동시성 및 성능이 저하될 수도 있습니다.\n\n## REPEATABLE READ의 한계점\n\nMySQL의 REPEATABLE READ 격리 수준은 강력한 수준의 일관성을 제공하지만 애플리케이션의 성능과 기능에 영향을 미칠 수 있는 몇 가지 제한 사항도 있습니다. 다음은 반복 가능한 읽기 분리 수준의 몇 가지 제한 사항입니다:\n\n1. 동시성이 감소: 반복 가능 읽기 분리 수준은 트랜잭션이 완료될 때까지 트랜잭션이 액세스하는 모든 행을 잠급니다. 이로 인해 다른 트랜잭션은 잠긴 행이 해제될 때까지 기다려야 하므로 동시성이 저하될 수 있습니다.\n\n2. 리소스 사용량 증가: 더 많은 잠금을 획득하고 해제해야 하기 때문에 REPEATABLE READ 격리 수준의 잠금 동작은 리소스 사용 증가로 이어질 수도 있습니다.\n\n3. 일관되지 않은 읽기: REPEATABLE READ 분리 수준은 트랜잭션이 데이터베이스의 일관된 스냅샷을 볼 수 있도록 보장하지만, 격리 레벨에서는 여전히 Phantom Read라고 알려진 작은 불일치 창이 허용됩니다. 이 문제는 다른 트랜잭션이 동일한 트랜잭션 내의 첫 번째 쿼리 실행과 두 번째 쿼리 실행 사이에 첫 번째 트랜잭션에 의해 실행된 쿼리의 기준과 일치하는 새 데이터를 삽입할 때 발생할 수 있습니다.\n   예를 들어 다음 두 가지 쿼리를 실행하는 트랜잭션을 생각해 보십시오:\n\n   ```sql\n   SELECT * FROM orders WHERE status = 'processing';\n   -- Some time passes, during which another transaction inserts a new order with status 'processing'\n   SELECT * FROM orders WHERE status = 'processing';\n   ```\n\n   트랜잭션이 REPEATABLE READ에서 실행 중인 경우, 새 주문이 그 사이에 삽입된 경우에도 두 번 모두 동일한 주문 집합을 읽습니다. 데이터베이스의 실제 상태를 반영하지 않을 수 있기 때문에 트랜잭션에서 읽은 데이터에 불일치가 발생할 수 있습니다.\n\n   이 문제를 완화하려면 응용프로그램의 일관성을 보장하기 위해 잠금 또는 낙관적 동시성 제어와 같은 추가 메커니즘을 구현해야 할 수 있습니다. 또한 특정 실행 순서에 의존하지 않도록 쿼리 및 응용 프로그램 로직을 신중하게 설계하거나, 강력한 일관성이 필요한 경우 SERIALIZABLE와 같은 다른 분리 수준을 사용하는 것을 고려해야 할 수도 있습니다.\n\n4. 교착 상태(Deadlocks): 반복 가능한 읽기 격리 수준은 둘 이상의 트랜잭션이 잠긴 리소스를 해제하기 위해 서로 대기하는 교착 상태로 이어질 수도 있습니다.\n\n   이러한 제한을 완화하려면, 교착 상태의 가능성을 최소화하고 필요한 잠금 양을 줄이기 위해 응용프로그램 및 데이터베이스 스키마를 신중하게 설계해야 할 수 있습니다. 또한 응용프로그램의 특정 요구사항 및 성능 특성에 따라 READ COMMITTED 또는 SERIALIZABLE와 같은 다른 분리 수준을 사용하는 것을 고려해야 할 수도 있습니다.",
    "docType": "original",
    "category": "Database",
    "tags": [
      "Database"
    ],
    "readingTime": 3,
    "wordCount": 562,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "mysql-트랜잭션-쿼리-예시",
    "slug": "mysql-teuraenjaegsyeon-kweori-yesi",
    "path": "database/mysql",
    "fullPath": "database/mysql/mysql-teuraenjaegsyeon-kweori-yesi",
    "title": "MySQL 트랜잭션 쿼리 예시 ",
    "excerpt": "MySQL 트랜잭션 쿼리 예시 ```mysql START TRANSACTION; SAVEPOINT A; INSERT INTO user(username, password, salt) VALUES ('testuser', 'testpassword',...",
    "content": "# MySQL 트랜잭션 쿼리 예시 \n\n```mysql\nSTART TRANSACTION;  \n  \nSAVEPOINT A;  \nINSERT INTO user(username, password, salt)  \nVALUES ('testuser', 'testpassword', 'testsalt');  \n  \nSAVEPOINT B;  \nINSERT INTO user(username, password, salt)  \nVALUES ('testuser1', 'testpassword1', 'testsalt1');  \n  \n# B 쿼리 이전, 즉 A - B 사이의 WRITE만 유효  \nROLLBACK TO SAVEPOINT B;  \n  \n# 전체 취소  \n# ROLLBACK;  \n  \n# 전체 적용  \n# COMMIT;\n```",
    "docType": "original",
    "category": "Database",
    "tags": [
      "Database"
    ],
    "readingTime": 1,
    "wordCount": 58,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "upsert-vs-unique-key-constraint",
    "slug": "upsert-vs-unique-key-constraint",
    "path": "database/concepts",
    "fullPath": "database/concepts/upsert-vs-unique-key-constraint",
    "title": "대량 데이터 처리 시 Upsert 패턴 활용법",
    "excerpt": "대량 데이터 인덱싱 작업에서 INSERT와 Upsert 패턴을 비교하고, 효율적인 배치 처리 방법을 알아봅니다.",
    "content": "# 대량 데이터 처리 시 Upsert 패턴 활용법\n\n## Upsert란?\n\nUpsert는 \"Update + Insert\"의 합성어로, 레코드가 존재하면 업데이트하고 없으면 삽입하는 원자적 연산입니다.\n\n### DB별 Upsert 문법\n\n**PostgreSQL**\n\n```sql\nINSERT INTO users (id, name, email)\nVALUES (1, 'Kim', 'kim@example.com')\nON CONFLICT (id) DO UPDATE SET\n  name = EXCLUDED.name,\n  email = EXCLUDED.email;\n```\n\n**MySQL**\n\n```sql\nINSERT INTO users (id, name, email)\nVALUES (1, 'Kim', 'kim@example.com')\nON DUPLICATE KEY UPDATE\n  name = VALUES(name),\n  email = VALUES(email);\n```\n\n## 왜 Upsert가 더 효율적인가?\n\n### 기존 방식의 문제점\n\n```sql\n-- 비효율적인 방식: SELECT 후 INSERT/UPDATE\nSELECT * FROM users WHERE id = 1;\n-- 결과에 따라\nINSERT INTO users ... -- 또는\nUPDATE users SET ... WHERE id = 1;\n```\n\n이 방식은:\n\n1. **2번의 쿼리 실행** (SELECT + INSERT/UPDATE)\n2. **Race Condition 위험** (SELECT와 INSERT 사이에 다른 트랜잭션이 삽입할 수 있음)\n3. **트랜잭션 오버헤드** 증가\n\n### Upsert의 장점\n\n1. **단일 원자적 연산**: DB 엔진이 최적화된 방식으로 처리\n2. **Race Condition 방지**: 동시성 문제 해결\n3. **네트워크 라운드트립 감소**: 한 번의 쿼리로 처리\n\n## 배치 Upsert 패턴\n\n대량 데이터 처리 시 배치 Upsert가 가장 효율적입니다:\n\n**PostgreSQL 배치 Upsert**\n\n```sql\nINSERT INTO transactions (tx_id, amount, status)\nVALUES \n  ('tx_001', 100, 'pending'),\n  ('tx_002', 200, 'confirmed'),\n  ('tx_003', 150, 'pending')\nON CONFLICT (tx_id) DO UPDATE SET\n  amount = EXCLUDED.amount,\n  status = EXCLUDED.status,\n  updated_at = NOW();\n```\n\n## 주의사항\n\n- **인덱스 필수**: Upsert는 고유 제약 조건(UNIQUE) 또는 PK가 있어야 동작\n- **대용량 컬럼**: TEXT/BLOB 등 큰 컬럼이 많으면 UPDATE 비용이 높아질 수 있음\n- **트리거 주의**: INSERT/UPDATE 트리거가 모두 실행될 수 있음\n\n## 결론\n\n대량 데이터 인덱싱 작업에서는 **배치 Upsert 패턴**을 사용하면:\n\n- 쿼리 수 감소\n- 트랜잭션 오버헤드 감소\n- 동시성 문제 해결\n\n로 인해 전체적인 데이터베이스 부하를 크게 줄일 수 있습니다.",
    "docType": "original",
    "category": "Database",
    "tags": [
      "Database",
      "MySQL",
      "Performance",
      "PostgreSQL"
    ],
    "readingTime": 2,
    "wordCount": 283,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "acid",
    "slug": "acid",
    "path": "database/concepts",
    "fullPath": "database/concepts/acid",
    "title": "ACID",
    "excerpt": "ACID 정의 Atomic 트랜잭션의 연산은 모두 정상적으로 실패하거나 모두 실패해야 한다. Consistent 트랜잭션이 성공한 후에 데이터베이스의 제약조건을 포함한 일관성이 지켜져야 한다. Isolation 현재 진행중인...",
    "content": "# ACID\n\n## 정의\n\n- Atomic\n\n  트랜잭션의 연산은 모두 정상적으로 실패하거나 모두 실패해야 한다.\n\n- Consistent\n\n  트랜잭션이 성공한 후에 데이터베이스의 제약조건을 포함한 일관성이 지켜져야 한다.\n\n- Isolation\n\n  현재 진행중인 트랜잭션이 있다면 다른 트랜잭션이 이 트랜잭션에 접근할 수 없다. 각 트랜잭션은 독립적으로 수행되어야 한다.\n\n- Durability\n\n  트랜잭션이 성공되었다면 데이터베이스는 이 결과를 영구적으로 반영해야 한다. 하나라도 손실이 있으면 안된다.\n\n## 더 나아가서\n\nRedis와 함께 데이터베이스 트랜잭션 및 분산 잠금을 사용하는 것은 백엔드에서 동시성 처리의 원자성과 일관성을 보장하는 좋은 방법입니다. 응용프로그램의 특정 요구에 따라 이러한 기술을 적용할 수 있는 추가 방법이 있습니다.\n\n빅테크에서 원자성과 일관성을 보장하는 한 가지 추가적인 방법은 여러 데이터베이스 또는 서비스에 분산 트랜잭션을 사용하는 것입니다. 예를 들어 각 서비스에 자체 데이터베이스가 있는 마이크로서비스 아키텍처가 있는 경우 분산 트랜잭션을 사용하여 특정 트랜잭션과 관련된 모든 서비스가 함께 커밋되거나 롤백되도록 할 수 있습니다.\n\n분산 트랜잭션을 지원하기 위해 2PC(Two-Phase Commit) 프로토콜, X/Open XA 프로토콜, Saga 패턴 등 다양한 도구와 기술을 사용할 수 있습니다. 이러한 기술을 통해 트랜잭션이 완전히 커밋되거나 관련된 모든 서비스에 걸쳐 완전히 롤백되어 시스템이 일관되게 유지됩니다.\n\n그러나 분산 트랜잭션은 구현하기가 복잡하고 성능 오버헤드가 발생할 수 있습니다. 따라서 절충안을 신중하게 고려하고 사용 사례에 필요한지 여부를 결정하는 것이 중요합니다.\n\n분산 트랜잭션 외에도 Big Tech에서 원자성과 일관성을 보장하기 위한 다른 모범 사례로는 데이터 유효성 검사 구현, 최적의 잠금 기술 사용, 정기적인 데이터 백업 및 재해 복구 계획 수행 등이 있습니다. 이러한 모범 사례는 예기치 않은 오류나 오류가 발생한 경우에도 데이터를 일관성 있게 유지하고 사용할 수 있도록 보장하는 데 도움이 됩니다.\n\nUsing database transactions and distributed locks with Redis is a good way to ensure atomicity and consistency in concurrency processing at the backend. There are additional ways to apply these techniques depending on the specific needs of your application.\n\nOne additional way to ensure atomicity and consistency in Big Tech is to use distributed transactions across multiple databases or services. For example, if you have a microservices architecture where each service has its own database, you can use distributed transactions to ensure that all the services involved in a particular transaction either commit or rollback together.\n\nThere are various tools and technologies available to support distributed transactions, such as Two-Phase Commit (2PC) protocol, X/Open XA protocol, and the Saga pattern. These technologies ensure that transactions are either fully committed or fully rolled back across all services involved, ensuring that the system remains consistent.\n\nHowever, it's important to note that distributed transactions can be complex to implement and can introduce performance overhead. Therefore, it's important to carefully consider the trade-offs and determine if they are necessary for your use case.\n\nIn addition to distributed transactions, other best practices to ensure atomicity and consistency in Big Tech include implementing data validation checks, using optimistic locking techniques, and performing regular data backups and disaster recovery planning. These best practices help to ensure that data remains consistent and available even in the face of unexpected failures or errors.\n\n## Ref\n\n- [ACID](https://ko.wikipedia.org/wiki/ACID)",
    "docType": "original",
    "category": "Database",
    "tags": [
      "Database"
    ],
    "readingTime": 3,
    "wordCount": 474,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "solana-sol",
    "slug": "solana-sol",
    "path": "blockchain/solana",
    "fullPath": "blockchain/solana/solana-sol",
    "title": "Solana (SOL)",
    "excerpt": "Solana (SOL) 💡 Cheatsheet of SOL for web3 developers. 요약 중앙화를 벗어나지 못함. 재단에서 네트워크를 껐다 켰다 할 수 있음 노드가 엄청 잘 죽음 (올해만 몇 번 중지...",
    "content": "# Solana (SOL)\n\n<aside>\n💡 Cheatsheet of SOL for web3 developers.\n\n</aside>\n\n# 요약\n\n1. 중앙화를 벗어나지 못함. 재단에서 네트워크를 껐다 켰다 할 수 있음\n2. 노드가 엄청 잘 죽음 (올해만 몇 번 중지되었는지..)\n3. 솔라나와 솔라나 프로그램(컨트랙트) 개발언어가 통일되어 있다는게 매우 매력포인트\n4. 재단에서 제공하는 툴들이 너무 환상적임. 아이디어만 있다면 대충 시작할 수 있을 정도\n5. 이것도 mempool이 없음\n\n# How to create and manage address or account?\n\n### Using Private Key\n\n```jsx\nconst account = Keypair.generate()\nconsole.log(JSON.stringify(account.publicKey.toBase58()))\n// output: \"gVazpxjimX3EP4mto53pEi4YSE36KP2nDwyEvLcKjmR\"\n```\n\n### Using Mnemonic\n\n```jsx\nconst mnemonic =\n\t'pill tomorrow foster begin walnut borrow virtual kick shift mutual shoe scatter'\nconst seed = bip39.mnemonicToSeedSync(mnemonic, '') // (mnemonic, password)\n\n// BIP39\nconst keypair = Keypair.fromSeed(seed.slice(0, 32))\n\n// BIP44\nfor (let i = 0; i < 10; i++) {\n\tconst path = `m/44'/501'/${i}'/0'`\n\tconst keypair = Keypair.fromSeed(derivePath(path, seed.toString('hex')).key)\n\tconsole.log(`${path} => ${keypair.publicKey.toBase58()}`)\n}\n\n// Check if a given public key has an associated private key\nconst key = new PublicKey('5oNDL3swdJJF1g9DzJiZ4ynHXgszjAEpUkxVYejchzrY')\nconsole.log(PublicKey.isOnCurve(key.toBytes()))\n```\n\n# How to sign and verify messages with wallets\n\n```jsx\nconst message = \"The quick brown fox jumps over the lazy dog\";\nconst messageBytes = decodeUTF8(message);\n\nconst signature = nacl.sign.detached(messageBytes, keypair.secretKey);\nconst result = nacl.sign.detached.verify(\n  messageBytes,\n  signature,\n  keypair.publicKey.toBytes()\n);\n\nconsole.log(result);\n\n-------------------------------------------------------------------------------\n\n{\n  let recoverTx = Transaction.populate(Message.from(realDataNeedToSign));\n  recoverTx.addSignature(feePayer.publicKey, Buffer.from(feePayerSignature));\n  recoverTx.addSignature(alice.publicKey, Buffer.from(aliceSignature));\n\n  console.log(\n    `txhash: ${await connection.sendRawTransaction(recoverTx.serialize())}`\n  );\n}\n\n-------------------------------------------------------------------------------\n\n{\n  let recoverTx = Transaction.populate(Message.from(realDataNeedToSign), [\n    bs58.encode(feePayerSignature),\n    bs58.encode(aliceSignature),\n  ]);\n  console.log(\n    `txhash: ${await connection.sendRawTransaction(recoverTx.serialize())}`\n  );\n}\n```\n\n# How to query balance of address on blockchain?\n\n# How to query transaction?\n\n# How to add a memo to a tx?\n\n```jsx\nconst transferTransaction = new Transaction().add(\n\tSystemProgram.transfer({\n\t\tfromPubkey: fromKeypair.publicKey,\n\t\ttoPubkey: toKeypair.publicKey,\n\t\tlamports: lamportsToSend,\n\t}),\n)\n\nawait transferTransaction.add(\n\tnew TransactionInstruction({\n\t\tkeys: [\n\t\t\t{ pubkey: fromKeypair.publicKey, isSigner: true, isWritable: true },\n\t\t],\n\t\tdata: Buffer.from('Data to send in transaction', 'utf-8'),\n\t\tprogramId: new PublicKey('MemoSq4gqABAXKb96qnH8TysNcWxMyWCqXgDLGmfcHr'),\n\t}),\n)\n\nawait sendAndConfirmTransaction(connection, transferTransaction, [fromKeypair])\n```\n\n# How to estimate fee?\n\n```jsx\ngetEstimatedFee\nconst recentBlockhash = await connection.getLatestBlockhash()\nconst transaction = new Transaction({\n\trecentBlockhash: recentBlockhash.blockhash,\n}).add(\n\tSystemProgram.transfer({\n\t\tfromPubkey: payer.publicKey,\n\t\ttoPubkey: payee.publicKey,\n\t\tlamports: 10,\n\t}),\n)\n\nconst fees = await transaction.getEstimatedFee(connection)\nconsole.log(`Estimated SOL transfer cost: ${fees} lamports`)\n// Estimated SOL transfer cost: 5000 lamports\n\n// getFeeForMessage\nconst message = new Message(messageParams)\n\nconst fees = await connection.getFeeForMessage(message)\nconsole.log(`Estimated SOL transfer cost: ${fees.value} lamports`)\n// Estimated SOL transfer cost: 5000 lamports\n```\n\n# How to Transfer tokens one to another?\n\n```jsx\n// Send SOL\nconst transferTransaction = new Transaction().add(\n\tSystemProgram.transfer({\n\t\tfromPubkey: fromKeypair.publicKey,\n\t\ttoPubkey: toKeypair.publicKey,\n\t\tlamports: lamportsToSend,\n\t}),\n)\n\nawait sendAndConfirmTransaction(connection, transferTransaction, [fromKeypair])\n\n// Send SPL Token\n// Add token transfer instructions to transaction\nconst transaction = new web3.Transaction().add(\n\tsplToken.Token.createTransferInstruction(\n\t\tsplToken.TOKEN_PROGRAM_ID,\n\t\tfromTokenAccount.address,\n\t\ttoTokenAccount.address,\n\t\tfromWallet.publicKey,\n\t\t[],\n\t\t1,\n\t),\n)\n\n// Sign transaction, broadcast, and confirm\nawait web3.sendAndConfirmTransaction(connection, transaction, [fromWallet])\n```\n\n---\n\n## Build Frontend and deploy contract to Solana devnet\n\nI using rust, anchor to develop smart contract. All I have to do is initializing some struct, and add methods in main program module. It’s quite easy to understand, and I think it’s simillar with creating and using gRPC.\n\n![](/images/solanademo.png)\nScreenshot of Demo project (thx to https://buildspace.so)",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 3,
    "wordCount": 501,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "tezos-xtz",
    "slug": "tezos-xtz",
    "path": "blockchain/others",
    "fullPath": "blockchain/others/tezos-xtz",
    "title": "Tezos (XTZ)",
    "excerpt": "Tezos (XTZ) 💡 Cheatsheet of XTZ for web3 developers. 요약 노드에 좀만 요청보내도 헐떡대다가 죽음 최소 잔고 맞춰줘야 함 라이브러리 편하게 잘되어 있긴 한데 노드 개...",
    "content": "# Tezos (XTZ)\n\n<aside>\n💡 Cheatsheet of XTZ for web3 developers.\n\n</aside>\n\n# 요약\n\n1. 노드에 좀만 요청보내도 헐떡대다가 죽음\n2. 최소 잔고 맞춰줘야 함\n3. 라이브러리 편하게 잘되어 있긴 한데 노드 개발을 Ocaml으로 해놔서 깊은 디버깅시에 제약이 있음\n4. 커뮤니티가 작아서 솔직히 자료 나오는게 뭐 없음",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 48,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "stellar-lumens-xlm",
    "slug": "stellar-lumens-xlm",
    "path": "blockchain/others",
    "fullPath": "blockchain/others/stellar-lumens-xlm",
    "title": "Stellar Lumens (XLM)",
    "excerpt": "Stellar Lumens (XLM) 💡 Cheatsheet of XLM for web3 developers. 요약 리플 v2 protobuf같은 프로토콜인 xdr을 사용하는 특이사항이 있음 트랜잭션...",
    "content": "# Stellar Lumens (XLM)\n\n<aside> 💡 Cheatsheet of XLM for web3 developers.\n\n</aside>\n\n# 요약\n\n1.  리플 v2\n2.  protobuf같은 프로토콜인 xdr을 사용하는 특이사항이 있음\n3.  트랜잭션 전파시에 가스 부족하면 타임아웃 남\n4.  트랜잭션 전파시에 mempool 없어서 재시도 하기 귀찮음\n5.  SDK 통해서 txHash 편하게 계산할 수 있기 때문에 찾아보면 좋음\n6.  주소 로컬에서 만든다고 네트웍에서 조회 안됨. 한번은 tx 생성을 통해서 노출해줘야함",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 65,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ripple-xrp",
    "slug": "ripple-xrp",
    "path": "blockchain/others",
    "fullPath": "blockchain/others/ripple-xrp",
    "title": "Ripple (XRP)",
    "excerpt": "Ripple (XRP) 💡 Cheatsheet of XRP for web3 developers. 요약 국제 송금망을 타겟으로 한 블록체인 속도를 위해서 탈중앙을 포기한 사례 개발자 도구나 커뮤니티가 훌...",
    "content": "# Ripple (XRP)\n\n<aside> 💡 Cheatsheet of XRP for web3 developers.\n\n</aside>\n\n# 요약\n\n1.  국제 송금망을 타겟으로 한 블록체인\n2.  속도를 위해서 탈중앙을 포기한 사례\n3.  개발자 도구나 커뮤니티가 훌륭함\n4.  리플과 Issuedtoken 갯수에 맞춰서 최소 잔고 들고 있어야함",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 41,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "istanbul-byzantine-fault",
    "slug": "istanbul-byzantine-fault",
    "path": "blockchain/others",
    "fullPath": "blockchain/others/istanbul-byzantine-fault",
    "title": "***Istanbul Byzantine Fault***",
    "excerpt": "Istanbul Byzantine Fault This article refers to link below! [Link](https://steemit.com/kr/@kanghamin/istanbul-byzantine-fault-toleranc...",
    "content": "# ***Istanbul Byzantine Fault***\n\n> This article refers to link below!\n> \n> [Link](https://steemit.com/kr/@kanghamin/istanbul-byzantine-fault-tolerance)\n\n## Consensus process\n\n1.  Pre-Prepare\n2.  Prepare\n3.  Commit\n\n-   Assuming that the bad node is F, System can run if the total number of nodes is 3F+1 or more.\n-   Validator is that derive consensus, and they choose a proposer before generating blocks(before every round).\n\n### Protocol: Propose\n\nFirst, the proposer choose and propose a new block, then announce it with message.\n\n![https://velog.velcdn.com/images%2Fasap0208%2Fpost%2F0a8048d2-f260-4e4c-be50-a88e40aca8a9%2Fimage.png](https://velog.velcdn.com/images%2Fasap0208%2Fpost%2F0a8048d2-f260-4e4c-be50-a88e40aca8a9%2Fimage.png)\n\n### Protocol: Pre-Prepare\n\nSecond, protocol is 'pre-prepare' state with sending 'prepare' message when validators receive a message from proposer. This process makes every validators notice that they're in same state.\n\n![https://velog.velcdn.com/images%2Fasap0208%2Fpost%2Fc3524567-17ce-4cc7-ad10-72bb78e13f9a%2Fimage.png](https://velog.velcdn.com/images%2Fasap0208%2Fpost%2Fc3524567-17ce-4cc7-ad10-72bb78e13f9a%2Fimage.png)\n\n### Protocol: Prepare\n\nThird, when validators get more than 2F+1 of messages, protocol is 'prepare' state and send 'commit' message.\n\n![https://velog.velcdn.com/images%2Fasap0208%2Fpost%2F7973d4cc-6dd8-43d7-9349-db904db584f3%2Fimage.png](https://velog.velcdn.com/images%2Fasap0208%2Fpost%2F7973d4cc-6dd8-43d7-9349-db904db584f3%2Fimage.png)\n\n### Protocol: Commit\n\nThis protocol makes peers notice that proposed blocks are permitted, and connects those to the chain. When they receive more than 2F+1 of messages, protocol should be 'commit' state, then blocks are connected to chain.\n\n![https://velog.velcdn.com/images%2Fasap0208%2Fpost%2Faf462703-0be6-439d-925f-2c14c0dc9412%2Fimage.png](https://velog.velcdn.com/images%2Fasap0208%2Fpost%2Faf462703-0be6-439d-925f-2c14c0dc9412%2Fimage.png)\n\n### Overall process\n\n![https://velog.velcdn.com/images%2Fasap0208%2Fpost%2F7939f561-1b3d-4801-8bda-e3df4b3482cc%2Fimage.png](https://velog.velcdn.com/images%2Fasap0208%2Fpost%2F7939f561-1b3d-4801-8bda-e3df4b3482cc%2Fimage.png)\n\nImage above shows the whole algorithm, when consensus is failed, go to 'Round change' and then restart over again.\n\n## Wrap up\n\nConsensus process is composed of three state, Success insertion of block means finality. Simply put, there's no way to change it at all.Therefore, it cannot be forked and valid block must be on the main blockchain.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 2,
    "wordCount": 232,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "hd-hierachy-deterministic-wallet",
    "slug": "hd-hierachy-deterministic-wallet",
    "path": "blockchain/others",
    "fullPath": "blockchain/others/hd-hierachy-deterministic-wallet",
    "title": "HD(Hierarchical Deterministic) Wallet",
    "excerpt": "HD 지갑의 개념과 BIP-32/39/44 표준, 키 파생 경로 구조를 알아봅니다.",
    "content": "# HD(Hierarchical Deterministic) Wallet\n\n## 개요\n\nHD 지갑은 하나의 **시드(Seed)**로부터 무한한 개수의 개인키/공개키 쌍을 계층적으로 파생할 수 있는 지갑 구조입니다. BIP-32 표준에 정의되어 있으며, 현대 대부분의 암호화폐 지갑이 이 방식을 사용합니다.\n\n## 왜 HD 지갑이 필요한가?\n\n### 기존 지갑의 문제점\n\n- 각 주소마다 별도의 개인키를 생성하고 백업해야 함\n- 새 주소를 만들 때마다 백업 갱신 필요\n- 개인키 관리가 복잡하고 분실 위험 증가\n\n### HD 지갑의 장점\n\n- **단일 시드 백업**: 12~24개 니모닉 단어만 백업하면 모든 키 복구 가능\n- **결정론적 파생**: 동일한 시드에서 항상 동일한 키 시퀀스 생성\n- **계층적 구조**: 용도별로 계정을 분리 관리 가능\n\n---\n\n## 핵심 BIP 표준\n\n### BIP-39: 니모닉 시드 구문\n\n사람이 읽을 수 있는 단어 목록으로 시드를 표현합니다.\n\n```\nabandon ability able about above absent absorb abstract absurd abuse access accident\n```\n\n- 12, 15, 18, 21, 24개 단어 지원\n- 2048개 단어 목록에서 선택\n- 마지막 단어에 체크섬 포함\n\n### BIP-32: 키 파생 구조\n\n시드로부터 마스터 키를 생성하고, 이를 기반으로 자식 키를 파생합니다.\n\n```\nSeed → Master Key → Child Key → Grandchild Key → ...\n```\n\n**키 파생 방식:**\n\n- **일반 파생 (Normal)**: 확장 공개키로 자식 공개키 파생 가능\n- **강화 파생 (Hardened)**: 개인키가 있어야만 자식 키 파생 가능 (보안 강화)\n\n### BIP-44: 경로 규약\n\n다중 코인, 다중 계정을 지원하기 위한 표준화된 경로 구조입니다.\n\n```\nm / purpose' / coin_type' / account' / change / address_index\n```\n\n| 레벨 | 설명 | 예시 |\n|-----|------|------|\n| `purpose'` | BIP 번호 (44 = BIP-44) | 44' |\n| `coin_type'` | 코인 종류 | 0' (BTC), 60' (ETH) |\n| `account'` | 계정 번호 | 0', 1', 2' |\n| `change` | 외부(0) / 내부(1) | 0 |\n| `address_index` | 주소 인덱스 | 0, 1, 2... |\n\n**경로 예시:**\n\n```\nm/44'/0'/0'/0/0   → 첫 번째 비트코인 주소\nm/44'/60'/0'/0/0  → 첫 번째 이더리움 주소\nm/44'/501'/0'/0'  → 첫 번째 솔라나 주소\n```\n\n---\n\n## 보안 고려사항\n\n### 강화 파생을 사용해야 하는 곳\n\n- `purpose`, `coin_type`, `account` 레벨은 반드시 강화 파생(`'`) 사용\n- 일반 파생 키가 노출되면 형제 개인키 추론 가능\n\n### 니모닉 보관\n\n- 오프라인 환경에서 생성\n- 물리적 매체(종이, 금속판)에 백업\n- 디지털 저장 절대 금지\n\n---\n\n## 주요 코인별 경로\n\n| 코인 | BIP-44 경로 | coin_type |\n|-----|------------|-----------|\n| Bitcoin | m/44'/0'/... | 0 |\n| Ethereum | m/44'/60'/... | 60 |\n| Solana | m/44'/501'/... | 501 |\n| Ripple | m/44'/144'/... | 144 |\n| Aptos | m/44'/637'/... | 637 |\n\n---\n\n## 참고 자료\n\n- [BIP-32: Hierarchical Deterministic Wallets](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki)\n- [BIP-39: Mnemonic code for generating deterministic keys](https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki)\n- [BIP-44: Multi-Account Hierarchy](https://github.com/bitcoin/bips/blob/master/bip-0044.mediawiki)\n- [SLIP-44: Registered coin types](https://github.com/satoshilabs/slips/blob/master/slip-0044.md)",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 3,
    "wordCount": 428,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "harmony-one",
    "slug": "harmony-one",
    "path": "blockchain/others",
    "fullPath": "blockchain/others/harmony-one",
    "title": "Harmony (ONE)",
    "excerpt": "Harmony (ONE) 💡 Cheatsheet of ONE for web3 developers. 요약 이더리움 복제품 다만 샤딩 적용되어 있는데 현재는 4개 체인 이외 특이사항 딱히 없다...",
    "content": "# Harmony (ONE)\n\n<aside>\n💡 Cheatsheet of ONE for web3 developers.\n\n</aside>\n\n# 요약\n\n1. 이더리움 복제품\n2. 다만 샤딩 적용되어 있는데 현재는 4개 체인\n3. 이외 특이사항 딱히 없다",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 31,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "filecoin-fil",
    "slug": "filecoin-fil",
    "path": "blockchain/others",
    "fullPath": "blockchain/others/filecoin-fil",
    "title": "Filecoin (FIL)",
    "excerpt": "Filecoin (FIL) 💡 애증의 파일코인 요약 트랜잭션 전파시에 타임아웃 나는거 고질병이라고 함 미리 txHash 만드는 방법있음. 아래 라이브러리 잘 찾으면 나옴 [https://docs.zondax.c...",
    "content": "# Filecoin (FIL)\n\n<aside>\n💡 애증의 파일코인\n\n</aside>\n\n# 요약\n\n1. 트랜잭션 전파시에 타임아웃 나는거 고질병이라고 함\n2. 미리 txHash 만드는 방법있음. 아래 라이브러리 잘 찾으면 나옴\n\n[https://docs.zondax.ch/filecoin-signing-tools](https://docs.zondax.ch/filecoin-signing-tools)\n\n1. filecoin vs arweave 검색하면 IPFS 관련해서 재밌는 주제 많음",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 39,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "biance-smart-chain-bnb",
    "slug": "biance-smart-chain-bnb",
    "path": "blockchain/others",
    "fullPath": "blockchain/others/biance-smart-chain-bnb",
    "title": "Biance Smart Chain (BNB)",
    "excerpt": "Biance Smart Chain (BNB) 💡 Cheatsheet of BSC/BNB for web3 developers. 설명 이더리움 복사본. BNB체인과 BSC체인이 따로 있어서 유의 필요...",
    "content": "# Biance Smart Chain (BNB)\n\n<aside>\n💡 Cheatsheet of BSC/BNB for web3 developers.\n\n</aside>\n\n# 설명\n\n1. 이더리움 복사본.\n2. BNB체인과 BSC체인이 따로 있어서 유의 필요",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 27,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "블록체인의-이해",
    "slug": "beulrogceinyi-ihae",
    "path": "blockchain/fundamentals",
    "fullPath": "blockchain/fundamentals/beulrogceinyi-ihae",
    "title": "블록체인의 이해",
    "excerpt": "블록체인의 이해 _보험연구원의 연구보고서 '[권호 : 18-24] 보험 산업의 블록체인 활용'을 읽고 작성한 내용입니다. 개인 공부 목적이라 정리가 미흡한 점 양해 부탁드립니다._ 블록체인의 의미 블록체인이란 P2P(Peer to Peer) 네트...",
    "content": "# 블록체인의 이해\n\n> _보험연구원의 연구보고서 '[권호 : 18-24] 보험 산업의 블록체인 활용'을 읽고 작성한 내용입니다. 개인 공부 목적이라 정리가 미흡한 점 양해 부탁드립니다._\n\n## 1. 블록체인의 의미\n\n블록체인이란 P2P(Peer to Peer) 네트워크를 통해 관리되는 분산 데이터베이스의 한 형태로, 거래 정보를 담은 장부를 중앙서버 한 곳에 저장하는 것이 아니라 블록체인 네트워크에 연결된 여러 컴퓨터에 저장 및 보관하는 기술로 다양한 분야에 활용이 가능한 기술이다.\n\n블록체인은 분산원장 기술이라고도 불리며, 이는 거래 정보를 기록한 원장 데이터를 중앙서버가 아닌 참가자들이 공동으로 기록 및 관리하는 것을 의미한다. 블록체인은 분산처리와 암호화 기술을 동시에 적용하며 높은 보안성을 확보하는 한편 거래과정의 신속성과 투명성을 특징으로 한다.\n\n보안성의 강화로 해커의 공격과 데이터의 왜곡 그리고 기존 중앙집중 서버 방식에서 가장 큰 문제인 디도스 공격을 원천적으로 방어할 수 있다. 그리고 블록체인 플랫폼을 이용하면 제 3자의 거래에 의존하던 여러 과정들을 생략할 수 있어, 그에 따른 비용을 획기적으로 절약할 수 있다. 제 3자가 거래 중심의 보장 및 증명 서비스의 항목들을 블록체인 시스템에 수렴할 수 있다.\n\n보안성이 높고 위변조가 어렵다는 특성 때문에 데이터 원본의 무결성이 요구되는 다양한 공공-민간 영역에 적용되고 있으며, 새로운 신뢰사회 구현의 기반 기술로 주목받고 있는 중이다. 또한, 블록체인 기술은 거래 장부인 데이터뿐 아니라 거래 계약도 중간 신뢰 담당자없이 거래를 할 수 있는데 이것이 바로 앞서 언급한 스마트계약이다.\n\n## 2. 블록체인의 원리\n\n블록체인 기술은 거래정보를 기록한 원장데이터를 중앙 서버가 아닌 네트워크에 참가하는 모든 공동체가 거래를 기록하고 관리하는 P2P거래를 지향하는 탈중앙화를 핵심 개념으로 하는 기술이다. 기존 금융 시스템에서는 금융회사들이 중앙 서버에 거래기록을 보관해온 반면, P2P 방식을 기반으로 하는 블록체인에서는 거래 정보를 블록에 담아 차례대로 연결하고 이를 모든 참여자가 공유한다.\n\n-   거래과정1) A가 B에게 송금희망 등의 거래 요청을 한다.2) 핻당 거래 정보가 담긴 블록이 생성된다.3) 블록이 네트워크 상의 모든 참여자에게 전송되면4) 참여자들은 거래 정보의 유효성을 상호 검증한다.5) 참여자 과반수의 데이터와 일치하는 거래내역은 정상 장부로 확인하는 방식으로 검증이 완료된 블록은 이전 블록에 연결되고, 그 사본이 만들어져 각 사용자의 컴퓨터에 분산 저장된다.6) A가 B에게 송금하여 거래가 완료된다.\n\n이렇게 거래할 때마다 거래 정보가 담긴 블록이 생성되어 계속 연결되면서 모든 참여자의 컴퓨터에 분산 저장되는데, 이를 해킹하여 임의로 수정하거나 위조 또는 변조하려면 전체 참여자의 과반수 이상의 거래 정보를 동시에 수정하여야 하기 때문에 사실상 불가능하다. 따라서 접근을 차단함으로써 거래 정보를 보호 및 관리하는 기존의 금융시스템과는 전혀 달리, 블록체인에서는 모든 거래 정보를 누구나 열람할 수 있도록 공개한 상태에서 은행 같은 공신력있는 제 3자의 보증 없이 당사자 간에 안전하게 거래가 이루어 진다.\n\n## 3. 블록체인의 기술적 개념\n\n### 가. 해시함수\n\n블록체인, 암호화폐 기술의 내용에 매번 등장하는 것 중 하나가 해시함수이다. 해시함수의 해시는 \"어떤 데이터를 고정된 길이의 데이터로 변환\"하는 것을 의미한다. 해시함수를 거치면 원본 데이터를 알아볼 수 없도록 특수한 문자열로 변환이 되는데, 해시함수는 압축이 아니라 단방향 변환이기 때문에 해시값을 이용해서 원본데이터를 복원할 수 없다는 특징을 가지고 있다.\n\n### 1) 해시함수의 유용성\n\n해시함수는 다음과 같은 성격이 있기 때문에 보안에서 유용하게 쓰인다. 원본데이터에 아주 작은 변화만 있어도 완전이 다른 해시값이 만들어지게 된다.즉 해시함수를 이용하게 되면, 원본데이터의 사소한 변화도 쉽게 확인할 수 있게 된다. 또한 해시함수는 눈사태 효과때문에 전자서명, 증명서 등에서 해시값을 많이 활용하고 있다. 본문에 약간의 수정만 가해져도 해시값이 완전히 달라져 위변조 판별이 용이하기 때문이다. 블록체인의 해시함수가 양방향 변환이 가능했더라면 암호화에 쓰일 수가 없었을 것이다.\n\n하지만 해시함수는 단방향 변환이며, 복원이 불가능하기 때문에 블록체인 기술 및 전자서명 등 암호화에 사용될 수 있다. 블록체인에서는 이 해시값을 이용해 해당 블록에 서명하고 이전 블록의 해시값을 다음 블록에 기록함으로써 체인 형태의 연결 리스트를 형성하게 된다. 따라서 특정 블록을 해킹하려면 그 블록에 연결된 다른 블록들도 수정을 해야하기 때문에 데이터의 위변조가 아주 어려운 구조를 가지고 있다.\n\n### 2) 해시함수의 특성\n\n(1) 어떤 길이의 데이터도 입력으로 사용될 수 있다.\n\n(2) 결과는 정해진 길이로 나온다.\n\n(3) 계산 시간이 합리적으로 추정 가능해야 한다.\n\n(4) 결과 값이 중복될 가능성이 거의 없다.\n\n(5) \\*\\*\\*\\*입력 값을 알 수 없다.\n\n(6) 결과 값을 알려주고 입력 값을 찾을 수 있는 특별한 공식이 없다.\n\n![https://velog.velcdn.com/images%2Fasap0208%2Fpost%2F0a7c3c00-b692-429b-be26-d1e25b481912%2Fimage.png](https://velog.velcdn.com/images%2Fasap0208%2Fpost%2F0a7c3c00-b692-429b-be26-d1e25b481912%2Fimage.png)\n\n### 3) 해시함수에 관한 추가 설명\n\n블록체인을 활용한 암호화폐에서 사용되는 암호기술은 해시함수, 전자서명, 공개키 암호화 알고리즘이라 말할 수 있다. 여기서 해시함수는 임의 데이터를 특정 길이의 문자, 숫자로 조합된 해시값으로 변환하는 암호 알고리즘의 일종이다. 해시함수에서 산출되는 해시값은 지문이라고도 하는데, 암호화폐에서 해시값 비교를 통하여 원본의 위변조 여부를 판단하는 무결성 검증에 사용될 수 있다.\n\n블록체인에서 해시함수를 사용하는 3가지 목적을 살펴보면 첫째, 공개키의 해시값을 지갑주소로 활용하여 익명화된 거래를 수행하고, 가상화폐의 전자지갑 주소는 공개키 기반 암호화 알고리즘에서 생성된 공개키의 해시값을 사용한다. 개인정보(정확히는 송신자의 계좌정보) 없이 익명화된 거래를 통해 송금자의 신원을 감추고, 송금할 수 있다.\n\n둘째, 해시함수를 사용하여 2가지의 무결성 검증에 사용하게 된다. 체인으로 연결된 블록헤더의 해시값을 활용하여 해시값 체인으로 연결된 블록의 무결성 검증에 사용된다. 또 다른 무결성 검증은 각 블록의 전체거래를 하나의 해시값(머클루트)으로 저장하고, 필요할 경우에는 언제든, 해당 블록의 머클루트 값으로 블록 내에 포함된 개별거래의 위변조 여부를 검증할 수 있다. 모든 거래 데이터의 해시값을 머클 트리를 이용하여 만들어지는 머클 루트에 저장하고 향후 거래내역의 위변조 여부를 검증할 때, 원본 해시값과 비교를 통하여, 각 거래의 무결성을 검증할 수 있다.또한 머클 루트는 1MB로 크기가 제한되어 있는 비트코인의 각 블록의 크기를 효율적으로 사용할 수 있게 한다. 전체 거래내역을 다 저장할 필요 없이, 머클루트라는 한 개의 해시값만 저장하면, 해당 블블록 내의 모든 거래내역의 진위를 필요할 때 비교할 수 있기 때문이다.\n\n마지막으로 합의 알고리즘에서 PoW방식을 사용할 경우, 해시값을 활용한 채굴문제에 활용한다. 해시값을 활용한 채굴문제에 먼저 맞추는 채굴자에게 채굴권한과 보상을 제공한다. 해시캐시 문제풀이를 통한 작업증명은 채굴이라고도 하는데, 채굴자에 대한 보상을 통해, 채굴을 경장해고, 채굴자가 자율적으로 새로운 블록을 생성하도록 유도할 수 있는 원리를 가지고 있다.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 5,
    "wordCount": 836,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "롤업이란",
    "slug": "roleobiran",
    "path": "blockchain/fundamentals",
    "fullPath": "blockchain/fundamentals/roleobiran",
    "title": "롤업이란?",
    "excerpt": "롤업이란? 롤업은 L2에서 트랜잭션을 실행한 뒤, 실행한 트랜잭션 데이터들과 변경된 상태의 요약본을 L1에 저장하는 솔루션 일반적으로 L1에 배패된 롤업 컨트랙트에 스테이트 루트(상태요약본)가 저장되어 있음 롤업의 경우 L1의 상태 변경시 TX실행 결과...",
    "content": "# 롤업이란?\n\n- 롤업은 L2에서 트랜잭션을 실행한 뒤, 실행한 트랜잭션 데이터들과 변경된 상태의 요약본을 L1에 저장하는 솔루션\n- 일반적으로 L1에 배패된 롤업 컨트랙트에 스테이트 루트(상태요약본)가 저장되어 있음\n- 롤업의 경우 L1의 상태 변경시 TX실행 결과 뿐만 아니라 데이터 또한 보관하는 방식을 통해서 누구든 데이터 유효성을 검증할 수 있게 했음\n<img src=\"https://cdn.coindeskkorea.com//news/photo/202207/80513_23505_1657.png\" />\n\n\n\nhttps://www.coindeskkorea.com/news/articleView.html?idxno=80513",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 54,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "단순-해시-검증-의-한계와-데이터-가용성-문제",
    "slug": "dansun-haesi-geomjeung-yi-hangyewa-deiteo-gayongseong-munje",
    "path": "blockchain/fundamentals",
    "fullPath": "blockchain/fundamentals/dansun-haesi-geomjeung-yi-hangyewa-deiteo-gayongseong-munje",
    "title": "\"단순 해시 검증\"의 한계와 데이터 가용성 문제",
    "excerpt": "\"단순 해시 검증\"의 한계와 데이터 가용성 문제 이것은 매우 핵심적인 질문입니다! 단순히 ODS의 해시만 검증하는 방식이 왜 부족한지, Celestia의 데이터 가용성 샘플링이 왜 필요한지 명확히 설명드리겠습니다. 단순 해시 검증의 근본적 문제 1...",
    "content": "# \"단순 해시 검증\"의 한계와 데이터 가용성 문제\n\n이것은 매우 핵심적인 질문입니다! 단순히 ODS의 해시만 검증하는 방식이 왜 부족한지, Celestia의 데이터 가용성 샘플링이 왜 필요한지 명확히 설명드리겠습니다.\n\n## 단순 해시 검증의 근본적 문제\n\n### 1. \"데이터 가용성 문제\"의 본질\n만약 우리가 해시만 검증한다면:\n\n- 블록 생성자는 데이터의 해시만 제출하고 **실제 데이터를 공개하지 않을 수 있습니다**.\n- 검증자들은 해시가 맞다는 것은 확인할 수 있지만, **데이터가 실제로 네트워크에 게시되었는지는 확인할 수 없습니다**.\n\n이것은 \"**데이터 가용성 문제**\"라고 부르며, 특히 롤업이나 확장성 솔루션에서 심각한 보안 위험을 초래합니다.\n\n### 2. \"무데이터 공격\" (Data Withholding Attack)\n이로 인해 가능한 공격 시나리오:\n\n1. 악의적인 블록 생성자가 유효한 데이터에 대한 해시만 포함한 블록 헤더를 제출\n2. 그러나 실제 데이터는 네트워크에 공개하지 않음\n3. 네트워크는 해시만 보고 블록이 유효하다고 판단\n4. 그러나 실제 데이터가 없어 트랜잭션 검증이나 롤업 상태 업데이트 불가능\n5. 블록체인이 멈추거나 심각한 손상 발생\n\n## 데이터 가용성 샘플링(DAS)의 필요성\n\n### 1. 샘플링을 통한 확률적 검증\nCelestia의 DAS는 이 문제를 해결하기 위해:\n\n- 노드들이 블록 데이터의 **무작위 샘플을 직접 요청**해서 확인\n- 만약 모든 샘플이 제공될 수 있다면, 높은 확률로 전체 데이터가 가용함을 의미\n- 단 하나의 샘플이라도 제공되지 않으면, 데이터 가용성 문제 감지\n\n### 2. Reed-Solomon 인코딩의 역할\nEDS 구조와 Reed-Solomon 인코딩은:\n\n- 데이터 일부가 손실되더라도 전체 복구 가능성 제공\n- 동시에 무작위 샘플링의 효과를 극대화 (데이터 누락 탐지 확률 향상)\n\n## 다른 블록체인과의 차이점\n\n### 1. 일반 블록체인의 접근법\nBitcoin이나 Ethereum 같은 전통적인 블록체인에서는:\n\n- **모든 풀노드가 모든 데이터를 다운로드하고 검증**\n- 따라서 데이터 가용성 문제가 덜 심각함 (풀노드는 모든 데이터를 직접 확인)\n\n### 2. Celestia의 혁신적 접근법\nCelestia는 확장성을 위해:\n\n- 모든 노드가 모든 데이터를 다운로드할 필요 없음\n- 대신 샘플링을 통해 데이터 가용성을 확률적으로 검증\n- 이는 라이트 노드에게도 강력한 보안 보장 제공\n\n## 결론: 왜 단순 해시 검증으로는 부족한가\n\n단순 해시 검증은:\n\n1. **데이터가 존재하는지** 검증하지 못함 (단지 해시가 맞다는 것만 검증)\n2. **데이터 가용성 공격**에 취약함\n3. **확장성 솔루션**에서 치명적인 보안 위험 초래\n\nCelestia의 DAS는 이 근본적인 문제를 해결하여, 확장성과 보안성을 동시에 달성할 수 있게 해줍니다. 샘플링을 통해 적은 양의 데이터만으로도 높은 확률로 데이터 가용성을 검증하고, 이를 통해 \"무데이터 공격\"을 방지합니다.\n\n이것이 Celestia가 ODS/ODSQ4 저장 전략과 데이터 가용성 샘플링을 사용하는 근본적인 이유입니다.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 2,
    "wordCount": 352,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "checksum",
    "slug": "checksum",
    "path": "blockchain/fundamentals",
    "fullPath": "blockchain/fundamentals/checksum",
    "title": "Checksum",
    "excerpt": "Checksum What is checksum ? A checksum is small-sized block of data derived from block of digital data for the purpose of deecting errors tah m...",
    "content": "# Checksum\n\n## What is checksum ?\n\nA checksum is small-sized block of data derived from block of digital data for the purpose of deecting errors tah may have been introduced during its transmission or storage. By themselves, checksums are often used to verify data integrity but are not relied upon to verify data authenticity.\n\n> The procedure which generates this checksum is called a checksum function or checksum algorithm. - Wikipedia\n\n![https://velog.velcdn.com/images%2Fasap0208%2Fpost%2F9c511980-dada-46e7-b0cc-f97143bcc6da%2Fimage.png](https://velog.velcdn.com/images%2Fasap0208%2Fpost%2F9c511980-dada-46e7-b0cc-f97143bcc6da%2Fimage.png)\n\n## Example\n\n0x25 = 370x62 = 980x3F = 630x52 = 82\n\n1.  0x25 + 0x62 + 0x3F + 0x52 = 0x118(280)-> 0001 0001 10002) drop carry nibble-> 0001 10003) 2's complements-> 1110 0111(1's complements) + 1-> 1110 10004) to Hex-> 0xE8 (=Checksum byte)\n\n### Test\n\n1.  0x118(origin) + 0xE8(checksum) = 0x200(5122) to binary-> 0010 0000 00003) drop carry nibble-> 0000 0000",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 135,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "how-base-gas-works",
    "slug": "how-base-gas-works",
    "path": "blockchain/ethereum",
    "fullPath": "blockchain/ethereum/how-base-gas-works",
    "title": "How Base Gas Works",
    "excerpt": "How Base Gas Works Aptos transactions by default charge a base gas fee, regardless of market conditions. For each transaction, this \"base gas\" amou...",
    "content": "# How Base Gas Works\n\nAptos transactions by default charge a base gas fee, regardless of market conditions.\nFor each transaction, this \"base gas\" amount is based on three conditions:\n\n1. Instructions.\n2. Storage.\n3. Payload.\n\nThe more function calls, branching conditional statements, etc. that a transaction requires, the more instruction gas it will cost.\nLikewise, the more reads from and writes into global storage that a transaction requires, the more storage gas it will cost.\nFinally, the more bytes in a transaction payload, the more it will cost.\n\nAs explained in the [optimization principles](#optimization-principles) section, storage gas has by far the largest effect on base gas. For background on the Aptos gas model, see [The Making of the Aptos Gas Schedule](https://aptoslabs.medium.com/the-making-of-the-aptos-gas-schedule-508d5686a350).\n\n## Instruction gas\n\nBasic instruction gas parameters are defined at [`instr.rs`] and include the following instruction types:\n\n### No-operation\n\n| Parameter | Meaning        |\n| --------- | -------------- |\n| `nop`     | A no-operation |\n\n### Control flow\n\n| Parameter  | Meaning                          |\n| ---------- | -------------------------------- |\n| `ret`      | Return                           |\n| `abort`    | Abort                            |\n| `br_true`  | Execute conditional true branch  |\n| `br_false` | Execute conditional false branch |\n| `branch`   | Branch                           |\n\n### Stack\n\n| Parameter           | Meaning                          |\n| ------------------- | -------------------------------- |\n| `pop`               | Pop from stack                   |\n| `ld_u8`             | Load a `u8`                      |\n| `ld_u64`            | Load a `u64`                     |\n| `ld_u128`           | Load a `u128`                    |\n| `ld_true`           | Load a `true`                    |\n| `ld_false`          | Load a `false`                   |\n| `ld_const_base`     | Base cost to load a constant     |\n| `ld_const_per_byte` | Per-byte cost to load a constant |\n\n### Local scope\n\n| Parameter                   | Meaning                  |\n| --------------------------- | ------------------------ |\n| `imm_borrow_loc`            | Immutably borrow         |\n| `mut_borrow_loc`            | Mutably borrow           |\n| `imm_borrow_field`          | Immutably borrow a field |\n| `mut_borrow_field`          | Mutably borrow a field   |\n| `imm_borrow_field_generic`  |                          |\n| `mut_borrow_field_generic`  |                          |\n| `copy_loc_base`             | Base cost to copy        |\n| `copy_loc_per_abs_val_unit` |                          |\n| `move_loc_base`             | Move                     |\n| `st_loc_base`               |                          |\n\n### Calling\n\n| Parameter                 | Meaning                       |\n| ------------------------- | ----------------------------- |\n| `call_base`               | Base cost for a function call |\n| `call_per_arg`            | Cost per function argument    |\n| `call_generic_base`       |                               |\n| `call_generic_per_ty_arg` | Cost per type argument        |\n| `call_generic_per_arg`    |                               |\n\n### Structs\n\n| Parameter                  | Meaning                              |\n| -------------------------- | ------------------------------------ |\n| `pack_base`                | Base cost to pack a `struct`         |\n| `pack_per_field`           | Cost to pack a `struct`, per field   |\n| `pack_generic_base`        |                                      |\n| `pack_generic_per_field`   |                                      |\n| `unpack_base`              | Base cost to unpack a `struct`       |\n| `unpack_per_field`         | Cost to unpack a `struct`, per field |\n| `unpack_generic_base`      |                                      |\n| `unpack_generic_per_field` |                                      |\n\n### References\n\n| Parameter                   | Meaning                            |\n| --------------------------- | ---------------------------------- |\n| `read_ref_base`             | Base cost to read from a reference |\n| `read_ref_per_abs_val_unit` |                                    |\n| `write_ref_base`            | Base cost to write to a reference  |\n| `freeze_ref`                | Freeze a reference                 |\n\n### Casting\n\n| Parameter   | Meaning          |\n| ----------- | ---------------- |\n| `cast_u8`   | Cast to a `u8`   |\n| `cast_u64`  | Cast to a `u64`  |\n| `cast_u128` | Cast to a `u128` |\n\n### Arithmetic\n\n| Parameter | Meaning  |\n| --------- | -------- |\n| `add`     | Add      |\n| `sub`     | Subtract |\n| `mul`     | Multiply |\n| `mod_`    | Modulo   |\n| `div`     | Divide   |\n\n### Bitwise\n\n| Parameter | Meaning                   |\n| --------- | ------------------------- |\n| `bit_or`  | `OR`: <code>&#124;</code> |\n| `bit_and` | `AND`: `&`                |\n| `xor`     | `XOR`: `^`                |\n| `shl`     | Shift left: `<<`          |\n| `shr`     | Shift right: `>>`         |\n\n### Boolean\n\n| Parameter | Meaning                         |\n| --------- | ------------------------------- |\n| `or`      | `OR`: <code>&#124;&#124;</code> |\n| `and`     | `AND`: `&&`                     |\n| `not`     | `NOT`: `!`                      |\n\n### Comparison\n\n| Parameter              | Meaning                        |\n| ---------------------- | ------------------------------ |\n| `lt`                   | Less than: `<`                 |\n| `gt`                   | Greater than: `>`              |\n| `le`                   | Less than or equal to: `<=`    |\n| `ge`                   | Greater than or equal to: `>=` |\n| `eq_base`              | Base equality cost: `==`       |\n| `eq_per_abs_val_unit`  |                                |\n| `neq_base`             | Base not equal cost: `!=`      |\n| `neq_per_abs_val_unit` |                                |\n\n### Global storage\n\n| Parameter                        | Meaning                                               |\n| -------------------------------- | ----------------------------------------------------- |\n| `imm_borrow_global_base`         | Base cost to immutably borrow: `borrow_global<T>()`   |\n| `imm_borrow_global_generic_base` |                                                       |\n| `mut_borrow_global_base`         | Base cost to mutably borrow: `borrow_global_mut<T>()` |\n| `mut_borrow_global_generic_base` |                                                       |\n| `exists_base`                    | Base cost to check existence: `exists<T>()`           |\n| `exists_generic_base`            |                                                       |\n| `move_from_base`                 | Base cost to move from: `move_from<T>()`              |\n| `move_from_generic_base`         |                                                       |\n| `move_to_base`                   | Base cost to move to: `move_to<T>()`                  |\n| `move_to_generic_base`           |                                                       |\n\n### Vectors\n\n| Parameter                      | Meaning                                  |\n| ------------------------------ | ---------------------------------------- |\n| `vec_len_base`                 | Length of a vector                       |\n| `vec_imm_borrow_base`          | Immutably borrow an element              |\n| `vec_mut_borrow_base`          | Mutably borrow an element                |\n| `vec_push_back_base`           | Push back                                |\n| `vec_pop_back_base`            | Pop from the back                        |\n| `vec_swap_base`                | Swap elements                            |\n| `vec_pack_base`                | Base cost to pack a vector               |\n| `vec_pack_per_elem`            | Cost to pack a vector per element        |\n| `vec_unpack_base`              | Base cost to unpack a vector             |\n| `vec_unpack_per_expected_elem` | Base cost to unpack a vector per element |\n\nAdditional storage gas parameters are defined in [`table.rs`], [`move_stdlib.rs`], and other assorted source files in [`aptos-gas/src/`].\n\n## Storage gas\n\nStorage gas is defined in [`storage_gas.move`], which is accompanied by a comprehensive and internally-linked DocGen file at [`storage_gas.md`].\n\nIn short:\n\n1. In [`initialize()`], [`base_8192_exponential_curve()`] is used to generate an exponential curve whereby per-item and per-byte costs increase rapidly as utilization approaches an upper bound.\n2. Parameters are reconfigured each epoch via [`on_reconfig()`], based on item-wise and byte-wise utilization ratios.\n3. Reconfigured parameters are stored in [`StorageGas`], which contains the following fields:\n\n| Field             | Meaning                                     |\n| ----------------- | ------------------------------------------- |\n| `per_item_read`   | Cost to read an item from global storage    |\n| `per_item_create` | Cost to create an item in global storage    |\n| `per_item_write`  | Cost to overwrite an item in global storage |\n| `per_byte_read`   | Cost to read a byte from global storage     |\n| `per_byte_create` | Cost to create a byte in global storage     |\n| `per_byte_write`  | Cost to overwrite a byte in global storage  |\n\nHere, an _item_ is either a resource having the `key` attribute, or an entry in a table, and notably, per-byte costs are assessed on the _entire_ size of an item.\nAs stated in [`storage_gas.md`], for example, if an operation mutates a `u8` field in a resource that has five other `u128` fields, the per-byte gas write cost will account for $(5 * 128) / 8 + 1 = 81$ bytes.\n\n### Vectors\n\nByte-wise fees are similarly assessed on vectors, which consume $\\sum_{i = 0}^{n - 1} e_i + b(n)$ bytes, where:\n\n- $n$ is the number of elements in the vector\n- $e_i$ is the size of element $i$\n- $b(n)$ is a \"base size\" which is a function of $n$\n\nSee the [BCS sequence specification] for more information on vector base size (technically a `ULEB128`), which typically occupies just one byte in practice, such that a vector of 100 `u8` elements accounts for $100 + 1 = 101$ bytes.\nHence per the item-wise read methodology described above, reading the last element of such a vector is treated as a 101-byte read.\n\n## Payload gas\n\nPayload gas is defined in [`transaction/mod.rs`](https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/aptos-gas/src/transaction/mod.rs), which incorporates storage gas with several payload- and pricing-associated parameters:\n\n| Parameter                       | Meaning                                                                                |\n| ------------------------------- | -------------------------------------------------------------------------------------- |\n| `min_transaction_gas_units`     | Minimum internal gas units for a transaction, charged at the start of execution        |\n| `large_transaction_cutoff`      | Size, in bytes, above which transactions will be charged an additional amount per byte |\n| `intrinsic_gas_per_byte`        | Internal gas units charged per byte for payloads above `large_transaction_cutoff`      |\n| `maximum_number_of_gas_units`   | Upper limit on external gas units for a transaction                                    |\n| `min_price_per_gas_unit`        | Minimum gas price allowed for a transaction                                            |\n| `max_price_per_gas_unit`        | Maximum gas price allowed for a transaction                                            |\n| `max_transaction_size_in_bytes` | Maximum transaction payload size in bytes                                              |\n| `gas_unit_scaling_factor`       | Conversion factor between internal gas units and external gas units                    |\n\nHere, \"internal gas units\" are defined as constants in source files like [`instr.rs`] and [`storage_gas.move`], which are more granular than \"external gas units\" by a factor of `gas_unit_scaling_factor`:\nto convert from internal gas units to external gas units, divide by `gas_unit_scaling_factor`.\nThen, to convert from external gas units to octas, multiply by the \"gas price\", which denotes the number of octas per unit of external gas.\n\n## Optimization principles\n\n### Unit and pricing constants\n\nAs of the time of this writing, `min_price_per_gas_unit` in [`transaction/mod.rs`](https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/aptos-gas/src/transaction/mod.rs) is defined as [`aptos_global_constants`]`::GAS_UNIT_PRICE` (which is itself defined as 100), with other noteworthy [`transaction/mod.rs`](https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/aptos-gas/src/transaction/mod.rs) constants as follows:\n\n| Constant                  | Value  |\n| ------------------------- | ------ |\n| `min_price_per_gas_unit`  | 100    |\n| `max_price_per_gas_unit`  | 10,000 |\n| `gas_unit_scaling_factor` | 10,000 |\n\nSee [Payload gas](#payload-gas) for the meaning of these constants.\n\n### Storage gas\n\nAs of the time of this writing, [`initialize()`] sets the following minimum storage gas amounts:\n\n| Data style | Operation | Symbol | Minimum internal gas |\n| ---------- | --------- | ------ | -------------------- |\n| Per item   | Read      | $r_i$  | 300,000              |\n| Per item   | Create    | $c_i$  | 5,000,000            |\n| Per item   | Write     | $w_i$  | 300,000              |\n| Per byte   | Read      | $r_b$  | 300                  |\n| Per byte   | Create    | $c_b$  | 5,000                |\n| Per byte   | Write     | $w_b$  | 5,000                |\n\nMaximum amounts are 100 times the minimum amounts, which means that for a utilization ratio of 40% or less, total gas costs will be on the order of 1 to 1.5 times the minimum amounts (see [`base_8192_exponential_curve()`] for supporting calculations).\nHence, in terms of octas, initial mainnet gas costs can be estimated as follows (divide internal gas by scaling factor, then multiply by minimum gas price):\n\n| Operation       | Operation | Minimum octas |\n| --------------- | --------- | ------------- |\n| Per-item read   | $r_i$     | 3000          |\n| Per-item create | $c_i$     | 50,000        |\n| Per-item write  | $w_i$     | 3000          |\n| Per-byte read   | $r_b$     | 3             |\n| Per-byte create | $c_b$     | 50            |\n| Per-byte write  | $w_b$     | 50            |\n\nHere, the most expensive per-item operation by far is creating a new item (via either `move_to<T>()` or adding to a table), which costs nearly 17 times as much as reading or overwriting an old item: $c_i = 16.\\overline{6} r_i = 16.\\overline{6} w_i$. Additionally:\n\n- Writes cost the same as reads on a per-item basis: $w_i = r_i$\n- On a per-byte basis, however, writes cost the same as creates: $w_b = c_b$\n- Per-byte writes and creates cost nearly 17 times as much as per-byte reads: $w_b = c_b = 16.\\overline{6} r_b$\n- Per-item reads cost 1000 times as much as per-byte reads: $r_i = 1000 r_b$\n- Per-item creates cost 1000 times as much as per-byte creates: $c_i = 1000 c_b$\n- Per-item writes cost 60 times as much as per-byte writes: $w_i = 60 w_b$\n\nHence per-item operations cost 1000 times more than per-byte operations for both reads and creates, but only 60 times more for writes.\n\nThus, in the absence of a legitimate economic incentive to deallocate from global storage (via either `move_from<T>()` or by removing from a table), the most effective storage gas optimization strategy is as follows:\n\n1. Minimize per-item creations\n2. Track unused items and overwrite them, rather than creating new items, when possible\n3. Contain per-item writes to as few items as possible\n4. Read, rather than write, whenever possible\n5. Minimize the number of bytes in all operations, especially writes\n\n### Instruction gas\n\nAs of the time of this writing, all instruction gas operations are multiplied by the `EXECUTION_GAS_MULTIPLIER` defined in [`gas_meter.rs`], which is set to 20.\nHence the following representative operations assume gas costs as follows (divide internal gas by scaling factor, then multiply by minimum gas price):\n\n| Operation                    | Minimum octas |\n| ---------------------------- | ------------- |\n| Table add/borrow/remove box  | 240           |\n| Function call                | 200           |\n| Load constant                | 130           |\n| Globally borrow              | 100           |\n| Read/write reference         | 40            |\n| Load `u128` on stack         | 16            |\n| Table box operation per byte | 2             |\n\n(Note that per-byte table box operation instruction gas does not account for storage gas, which is assessed separately).\n\nFor comparison, reading a 100-byte item costs $r_i + 100 * r_b = 3000 + 100 * 3 = 3300$ octas at minimum, some 16.5 times as much as a function call, and in general, instruction gas costs are largely dominated by storage gas costs.\n\nNotably, however, there is still technically an incentive to reduce the number of function calls in a program, but engineering efforts are more effectively dedicated to writing modular, decomposed code that is geared toward reducing storage gas costs, rather than attempting to write repetitive code blocks with fewer nested functions (in nearly all cases).\n\nIn extreme cases it is possible for instruction gas to far outweigh storage gas, for example if a loopwise mathematical function takes 10,000 iterations to converge; but again this is an extreme case and for most applications storage gas has a larger impact on base gas than does instruction gas.\n\n### Payload gas\n\nAs of the time of this writing, [`transaction/mod.rs`](https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/aptos-gas/src/transaction/mod.rs) defines the minimum amount of internal gas per transaction as 1,500,000 internal units (15,000 octas at minimum), an amount that increases by 2,000 internal gas units (20 octas minimum) per byte for payloads larger than 600 bytes, with the maximum number of bytes permitted in a transaction set at 65536.\nHence in practice, payload gas is unlikely to be a concern.\n\n<!--- Alphabetized reference links -->\n\n[#4540]: https://github.com/aptos-labs/aptos-core/pull/4540/files\n[`aptos-gas/src/`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/aptos-gas/src/\n[`aptos_global_constants`]: https://github.com/aptos-labs/aptos-core/blob/main/config/global-constants/src/lib.rs\n[`base_8192_exponential_curve()`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/framework/aptos-framework/doc/storage_gas.md#0x1_storage_gas_base_8192_exponential_curve\n[bcs sequence specification]: https://github.com/diem/bcs#fixed-and-variable-length-sequences\n[`gas_meter.rs`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/aptos-gas/src/gas_meter.rs\n[`initialize()`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/framework/aptos-framework/doc/storage_gas.md#0x1_storage_gas_initialize\n[`instr.rs`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/aptos-gas/src/instr.rs\n[`move_stdlib.rs`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/aptos-gas/src/move_stdlib.rs\n[`on_reconfig()`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/framework/aptos-framework/doc/storage_gas.md#@Specification_16_on_reconfig\n[`storage_gas.md`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/framework/aptos-framework/doc/storage_gas.md\n[`storage_gas.move`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/framework/aptos-framework/sources/storage_gas.move\n[`storagegas`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/framework/aptos-framework/doc/storage_gas.md#resource-storagegas\n[`table.rs`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/aptos-gas/src/table.rs\n[`transaction.rs`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/aptos-gas/src/transaction.rs",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 12,
    "wordCount": 2383,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ethereum-eth",
    "slug": "ethereum-eth",
    "path": "blockchain/ethereum",
    "fullPath": "blockchain/ethereum/ethereum-eth",
    "title": "Ethereum (ETH)",
    "excerpt": "Ethereum (ETH) 💡 이더리움은 월드 와이드 컴퓨터를 꿈꾼다 장점 개발자 커뮤니티 1등인듯, 어지간한 에러나 이슈는 이미 레퍼런스가 모두 있음. 디버깅 매우 편함 그러다보니 web3, ethers.j...",
    "content": "# Ethereum (ETH)\n\n<aside> 💡 이더리움은 월드 와이드 컴퓨터를 꿈꾼다\n\n</aside>\n\n# 장점\n\n1.  개발자 커뮤니티 1등인듯, 어지간한 에러나 이슈는 이미 레퍼런스가 모두 있음. 디버깅 매우 편함\n2.  그러다보니 web3, ethers.js 등의 라이브러리가 매우 잘 개발되어 있고 개발문서도 잘되어 있음\n3.  노드도 안정적임\n4.  이거 하나만 알아도 시장에 나온 블록체인 5할은 다룰 수 있음… (그만큼 복붙체인이 많음)\n\n# 단점\n\n1.  블록타임이 약 12 ~ 13초로 차세대 블록체인에 비해 느림\n2.  EVM은 자산을 일급객체로 다루지 않음, 일종의 지역 변수를 사칙연산한 값임. 그러다보니 EVM을 채택한 블록체인들에서 고질적인 해킹문제가 발생해옴\n3.  불장일때 수수료가 10만원을 넘음.\n4.  비탈릭 부테린의 압도적 마케팅 효과가 없어도 인기가 지속될까에 대한 의문",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 104,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "namespaced-merkle-tree-nmt-란",
    "slug": "namespaced-merkle-tree-nmt-ran",
    "path": "blockchain/celestia",
    "fullPath": "blockchain/celestia/namespaced-merkle-tree-nmt-ran",
    "title": "Namespaced Merkle Tree (NMT)란?",
    "excerpt": "Namespaced Merkle Tree (NMT)란? Namespaced Merkle Tree (NMT)는 Merkle Tree의 변형된 구조로, 네임스페이스(namespace)를 기반으로 하는 인증 가능한 데이터 구조다. Cel...",
    "content": "**Namespaced Merkle Tree (NMT)란?**\n\n  \n\n**Namespaced Merkle Tree (NMT)**는 **Merkle Tree의 변형된 구조**로, **네임스페이스(namespace)를 기반으로 하는 인증 가능한 데이터 구조**다. Celestia에서는 **특정 네임스페이스에 속하는 데이터만 검증 가능**하도록 설계된 이 구조를 사용한다.\n\n---\n\n**1. Merkle Tree와의 차이점**\n\n  \n\nMerkle Tree는 기본적으로 **전체 데이터 블록을 검증하기 위한 해시 트리 구조**다. 하지만 일반적인 Merkle Tree는 특정 데이터가 포함되어 있는지를 **빠르게 증명하는 기능은 제공하지만, 네임스페이스 단위로 검색하거나 검증하는 기능은 제공하지 않는다**.\n\n  \n\n반면 **NMT는 네임스페이스 단위로 증명(Proof)을 지원**하기 때문에 특정 네임스페이스에 속하는 데이터를 빠르게 검증할 수 있다.\n\n---\n\n**2. NMT의 특징**\n\n1. **네임스페이스 기반 Merkle 해싱**\n\n• 각 노드가 특정 네임스페이스 범위를 포함하고 있으며, 부모 노드는 자식 노드들의 네임스페이스 범위를 유지한다.\n\n• 즉, **Merkle 증명에서 네임스페이스별로 데이터 포함 여부를 쉽게 확인할 수 있다.**\n\n1. **Merkle Proof와 네임스페이스 필터링 지원**\n\n• 일반 Merkle Tree의 경우 특정 데이터가 트리에 포함되어 있는지만 증명할 수 있지만,\n\n**NMT는 특정 네임스페이스에 속하는 데이터가 있는지 없는지 증명할 수 있다.**\n\n1. **부분 데이터 접근성 향상**\n\n• 특정 네임스페이스의 데이터만 빠르게 검색하고, 다운로드하는 것이 가능하다.\n\n---\n\n**3. Celestia에서 NMT가 사용되는 이유**\n\n  \n\nCelestia에서는 **네임스페이스를 기반으로 데이터 가용성을 검증**해야 하기 때문에, 기존 Merkle Tree보다 **NMT가 더 적합**하다. Celestia에서 NMT가 사용되는 이유는 다음과 같다.\n\n1. **Blob 트랜잭션 검증**\n\n• Celestia에서는 트랜잭션과 블록 데이터를 Blob 형태로 저장하며, 각 Blob은 특정 네임스페이스에 속한다.\n\n• NMT를 활용하면 특정 네임스페이스의 Blob이 존재하는지 **효율적으로 증명할 수 있다**.\n\n1. **Data Availability Sampling (DAS) 최적화**\n\n• 라이트 노드는 **데이터 가용성을 검증하기 위해 일부 샘플을 요청**해야 한다.\n\n• NMT를 활용하면 특정 네임스페이스의 데이터를 포함하는 샘플을 보다 **효율적으로 검증 가능**하다.\n\n1. **네임스페이스 기반 데이터 검색 최적화**\n\n• Celestia의 모듈형 블록체인 구조에서는 **다양한 애플리케이션이 서로 다른 네임스페이스를 사용**한다.\n\n• 특정 애플리케이션이 필요한 데이터만 검색할 수 있도록, NMT가 효과적으로 동작한다.\n\n---\n\n**4. NMT의 동작 방식**\n\n  \n\n**📌 기본적인 Merkle Tree와 비교**\n\n  \n\n일반적인 Merkle Tree의 경우, 각 리프 노드(데이터 블록)는 해시로 변환되며, 부모 노드는 자식들의 해시를 조합하여 생성된다.\n\n  \n\n하지만 **Namespaced Merkle Tree (NMT)는 네임스페이스 정보를 추가하여 해싱**한다.\n\n  \n\n**📌 NMT의 해싱 규칙**\n\n1. **각 리프 노드**:\n\n• (namespace, data_hash) 형태로 저장됨.\n\n1. **각 내부 노드**:\n\n• namespace_min = min(left.namespace_min, right.namespace_min)\n\n• namespace_max = max(left.namespace_max, right.namespace_max)\n\n• 부모 노드는 자식의 네임스페이스 범위를 유지하면서 해싱됨.\n\n  \n\n**📌 NMT Merkle Proof 생성**\n\n• 특정 네임스페이스에 속하는 데이터를 포함하는지 검증할 때, **네임스페이스 범위 정보를 가진 Merkle Proof**를 사용하면 효율적으로 증명할 수 있다.\n\n• 특정 네임스페이스의 데이터를 포함하지 않는다면, **해당 네임스페이스 범위가 없음(Empty Proof)** 을 증명할 수 있다.\n\n---\n\n**5. 결론**\n\n• Namespaced Merkle Tree(NMT)는 **Merkle Tree의 확장 버전**으로, **네임스페이스별 데이터 검증이 가능**한 구조다.\n\n• Celestia에서는 **모듈형 블록체인에서 데이터 가용성을 효율적으로 검증하고, 특정 네임스페이스의 데이터만 빠르게 검색할 수 있도록 지원**하기 위해 사용된다.\n\n• **DAS (Data Availability Sampling)와 Blob 트랜잭션 검증에도 중요한 역할**을 한다.\n\n  \n\n👉 **즉, Celestia의 확장성과 데이터 가용성을 높이기 위해 NMT는 필수적인 요소다.** 🚀",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 3,
    "wordCount": 437,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "celestia의-데이터-저장-공간-사용량",
    "slug": "celestiayi-deiteo-jeojang-gonggan-sayongryang",
    "path": "blockchain/celestia",
    "fullPath": "blockchain/celestia/celestiayi-deiteo-jeojang-gonggan-sayongryang",
    "title": "Celestia의 데이터 저장 공간 사용량",
    "excerpt": "Celestia의 데이터 저장 공간 사용량 Celestia의 EDS(Extended Data Square) 구조에서 저장 공간 사용은 다음과 같습니다: 전체 EDS 용량 (이론적인 값) 전체 EDS는 원본 데이터의 4배 크기를 차지합니다: -...",
    "content": "# Celestia의 데이터 저장 공간 사용량\n\nCelestia의 EDS(Extended Data Square) 구조에서 저장 공간 사용은 다음과 같습니다:\n\n## 전체 EDS 용량 (이론적인 값)\n\n전체 EDS는 원본 데이터의 **4배** 크기를 차지합니다:\n\n- **Q1 (원본 데이터)**: 원본 데이터의 100%\n- **Q2 (행 패리티)**: 원본 데이터의 100%\n- **Q3 (열 패리티)**: 원본 데이터의 100%\n- **Q4 (행+열 패리티)**: 원본 데이터의 100%\n\n즉, 모든 사분면을 모두 저장한다면 원본 데이터 크기의 4배가 필요합니다.\n\n## 실제 Celestia의 저장 전략\n\n하지만 Celestia에서는 실제로 모든 사분면을 저장하지 않고, 필요에 따라 다음과 같은 저장 전략을 사용합니다:\n\n1. **ODS만 저장** (원본 데이터만):\n   - 원본 데이터의 **1배** 용량 사용\n   - 오래된 블록이나 아카이브 목적의 저장에 사용\n   - 필요시 나머지 사분면(Q2, Q3, Q4)은 계산으로 복구 가능\n\n2. **ODSQ4 저장** (원본 + Q4 사분면):\n   - 원본 데이터의 **2배** 용량 사용\n   - 가용성 윈도우 내의 최근 블록에 사용\n   - 특정 접근 패턴에서 계산 효율성 향상\n\n## 실제 코드에서의 저장 전략\n\n코드에서 이런 저장 전략은 다음과 같이 구현되어 있습니다:\n\n```go\n// 가용성 윈도우 내의 블록은 ODS와 Q4 모두 저장 (원본의 2배)\nif availability.IsWithinWindow(eh.Time(), availability.StorageWindow) {\n    err = store.PutODSQ4(ctx, eh.DAH, eh.Height(), eds)\n} else {\n    // 그 외 블록은 ODS만 저장 (원본의 1배)\n    err = store.PutODS(ctx, eh.DAH, eh.Height(), eds)\n}\n```\n\n## 저장 공간 최적화의 이유\n\n이렇게 저장 공간을 최적화하는 이유는:\n\n1. **디스크 공간 효율성**:\n   - 모든 블록의 모든 사분면을 저장하는 것은 비효율적\n   - 대부분의 경우 원본 데이터만으로도 충분\n\n2. **계산 비용과 저장 비용의 균형**:\n   - Q2와 Q3는 Q1과 Q4로부터 계산 가능 (필요할 때만 계산)\n   - 가장 자주 접근하는 최근 블록에만 Q4도 함께 저장하여 계산 비용 절약\n\n3. **가용성과 검증의 균형**:\n   - 데이터 가용성 검증은 최근 블록에 대해 더 중요\n   - 따라서 최근 블록은 더 많은 공간(ODS+Q4)을 사용하고, 오래된 블록은 공간 효율성(ODS만)을 우선시\n\n결론적으로, Celestia는 원본 데이터의 약 1~2배 정도의 저장 공간을 사용하면서도, 데이터 가용성 검증과 복구 기능을 효율적으로 제공합니다.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 2,
    "wordCount": 294,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "celestia의-데이터-가용성-증명-data-availability-proof-메커니즘",
    "slug": "celestiayi-deiteo-gayongseong-jeungmyeong-data-availability-proof-mekeonijeum",
    "path": "blockchain/celestia",
    "fullPath": "blockchain/celestia/celestiayi-deiteo-gayongseong-jeungmyeong-data-availability-proof-mekeonijeum",
    "title": "Celestia의 데이터 가용성 증명(Data Availability Proof) 메커니즘",
    "excerpt": "Celestia의 데이터 가용성 증명(Data Availability Proof) 메커니즘 Celestia는 블록체인 확장성 문제를 해결하기 위해 데이터 가용성 증명(Data Availability Proof) 메커니즘을 핵심으로 사용합니다. 이 메커니즘은 모든...",
    "content": "# Celestia의 데이터 가용성 증명(Data Availability Proof) 메커니즘\n\nCelestia는 블록체인 확장성 문제를 해결하기 위해 데이터 가용성 증명(Data Availability Proof) 메커니즘을 핵심으로 사용합니다. 이 메커니즘은 모든 노드가 전체 블록체인 데이터를 저장하지 않고도 데이터가 네트워크에 실제로 게시되었는지 확인할 수 있게 합니다.\n\n## 1. 데이터 가용성 문제\n\n기존 블록체인에서는 모든 노드가 모든 트랜잭션을 검증하고 저장해야 하므로 확장성에 한계가 있습니다. Celestia는 \"데이터 가용성 샘플링(Data Availability Sampling, DAS)\"이라는 기술을 통해 이 문제를 해결합니다.\n\n## 2. 핵심 기술 구성요소\n\n### 2.1 Extended Data Square (EDS)\n\n블록 데이터는 다음과 같은 과정으로 처리됩니다:\n\n1. **데이터 정렬**: 트랜잭션 데이터를 정사각형 형태(Original Data Square, ODS)로 배열합니다.\n2. **2D Reed-Solomon 인코딩**: ODS에 Reed-Solomon 오류 정정 코드를 적용하여 확장된 데이터 사각형(EDS)을 생성합니다.\n3. **사분면 구조**: EDS는 4개의 사분면으로 구성됩니다:\n   - Q1(좌상단): 원본 데이터(ODS)\n   - Q2(우상단): 행 패리티 데이터\n   - Q3(좌하단): 열 패리티 데이터\n   - Q4(우하단): 행+열 패리티 데이터\n\n```go\n// core/eds.go에서\nfunc extendBlock(data *types.Data, appVersion uint64, options ...nmt.Option) (*rsmt2d.ExtendedDataSquare, error) {\n    // 블록 데이터를 EDS로 확장하는 과정\n}\n```\n\n### 2.2 데이터 가용성 샘플링(DAS)\n\n경량 노드는 전체 블록을 다운로드하지 않고 EDS의 임의의 위치에서 소수의 샘플만 요청합니다:\n\n1. **무작위 샘플링**: 노드는 EDS의 무작위 위치에서 여러 개의 샘플(share)을 요청합니다.\n2. **통계적 검증**: 충분한 수의 샘플이 성공적으로 검색되면, 높은 확률로 전체 데이터가 가용하다고 결론지을 수 있습니다.\n3. **이론적 기반**: 데이터의 일부가 누락된 경우, 무작위 샘플링을 통해 높은 확률로 누락된 부분을 감지할 수 있습니다.\n\n### 2.3 Namespaced Merkle Tree (NMT)\n\nCelestia는 데이터를 네임스페이스(namespace)로 구분하여 구조화합니다:\n\n1. **데이터 분류**: 트랜잭션 데이터를 네임스페이스별로 분류합니다.\n2. **효율적인 검증**: 노드는 특정 네임스페이스의 데이터만 선택적으로 검증할 수 있습니다.\n3. **증명 최적화**: NMT는 특정 네임스페이스 데이터에 대한 증명 크기를 최적화합니다.\n\n## 3. 데이터 가용성 증명 프로세스\n\n### 3.1 블록 생성 및 제안\n\n1. 블록 제안자는 트랜잭션을 수집하여 블록을 구성합니다.\n2. 트랜잭션 데이터를 ODS 형태로 정렬합니다.\n3. 2D Reed-Solomon 인코딩을 적용하여 EDS를 생성합니다.\n4. 블록 헤더에 데이터 해시(Merkle root)를 포함시켜 네트워크에 제안합니다.\n\n### 3.2 경량 노드의 검증 과정\n\n1. 경량 노드는 블록 헤더를 받습니다.\n2. EDS의 무작위 위치에서 다수의 샘플을 요청합니다.\n3. 샘플이 Merkle root와 일치하는지 검증합니다.\n4. 충분한 샘플(일반적으로 수백 개)이 성공적으로 검증되면, 전체 데이터가 가용하다고 판단합니다.\n\n```go\n// 경량 노드의 샘플링 요청 처리 과정 (개념적 코드)\nfunc (odsq4 *ODSQ4) Sample(ctx context.Context, coords shwap.SampleCoords) (shwap.Sample, error) {\n    // 요청된 좌표에서 샘플 데이터 검색\n}\n```\n\n### 3.3 데이터 복구 능력\n\nEDS의 Reed-Solomon 속성 덕분에:\n\n1. 전체 EDS의 약 25%만 있으면 전체 데이터를 복구할 수 있습니다.\n2. 충분한 수의 노드가 샘플링을 수행하면, 네트워크 전체적으로 데이터 복구가 가능한 수준의 샘플이 존재합니다.\n\n```go\n// store/file/square.go에서\nfunc (s square) computeAxisHalf(axisType rsmt2d.Axis, axisIdx int) (eds.AxisHalf, error) {\n    // Reed-Solomon 인코딩을 사용하여 누락된 데이터 복구\n}\n```\n\n## 4. 저장 최적화\n\nCelestia는 모든 노드가 모든 데이터를 저장할 필요가 없도록 설계되었습니다:\n\n1. **선택적 저장**: 풀 노드는 블록 데이터의 ODS 부분만 저장합니다.\n2. **시간 기반 전략**: 가용성 윈도우 내의 최근 블록은 ODSQ4 형태(ODS+Q4)로 저장하고, 오래된 블록은 ODS 형태로만 저장합니다.\n\n```go\n// core/eds.go에서\nfunc storeEDS(ctx context.Context, eh *header.ExtendedHeader, eds *rsmt2d.ExtendedDataSquare, store *store.Store, window time.Duration, archival bool) error {\n    if availability.IsWithinWindow(eh.Time(), window) {\n        // 가용성 윈도우 내의 블록은 ODS와 Q4 모두 저장\n        err = store.PutODSQ4(ctx, eh.DAH, eh.Height(), eds)\n    } else {\n        // 오래된 블록은 ODS만 저장\n        err = store.PutODS(ctx, eh.DAH, eh.Height(), eds)\n    }\n}\n```\n\n## 5. 장점 및 의의\n\n1. **확장성 향상**: 경량 노드는 전체 블록을 다운로드하지 않고도 데이터 가용성을 검증할 수 있습니다.\n2. **보안 유지**: 충분한 샘플링을 통해 높은 확률로 데이터 가용성을 보장합니다.\n3. **모듈식 설계**: 데이터 가용성 레이어를 실행 레이어(execution layer)와 분리하여 모듈식 블록체인 설계를 가능하게 합니다.\n4. **롤업 확장**: Celestia의 데이터 가용성 레이어는 다양한 롤업(rollup)의 기반 레이어로 활용될 수 있습니다.\n\nCelestia의 데이터 가용성 증명 메커니즘은 블록체인의 \"데이터 가용성 문제\"를 효율적으로 해결하여, 확장성을 크게 향상시키면서도 보안을 유지하는 혁신적인 접근 방식입니다.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 3,
    "wordCount": 592,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "celestia의-eds-extended-data-square",
    "slug": "celestiayi-eds-extended-data-square",
    "path": "blockchain/celestia",
    "fullPath": "blockchain/celestia/celestiayi-eds-extended-data-square",
    "title": "Celestia의 EDS(Extended Data Square)",
    "excerpt": "EDS(Extended Data Square)란? EDS는 Celestia의 데이터 가용성(Data Availability) 보장 메커니즘의 핵심 구조입니다. EDS는 원본 데이터를 2D 사각형으로 구성한 후, Reed-Solomon 오류 정정 코딩을 적용하여...",
    "content": "## EDS(Extended Data Square)란?\n\nEDS는 Celestia의 데이터 가용성(Data Availability) 보장 메커니즘의 핵심 구조입니다. EDS는 원본 데이터를 2D 사각형으로 구성한 후, Reed-Solomon 오류 정정 코딩을 적용하여 확장된 2차원 데이터 구조를 말합니다.\n\n```mermaid\ngraph LR\n    Q1[\"🟦 Q1: ODS<br/>원본 데이터\"] --- Q2[\"🟩 Q2: Row Parity<br/>행 패리티\"] --- Q3[\"🟨 Q3: Col Parity<br/>열 패리티\"] --- Q4[\"🟪 Q4: Row+Col Parity<br/>행+열 패리티\"]\n```\n\n> **Extended Data Square (EDS)**: 원본 데이터(Q1)를 Reed-Solomon 인코딩하여 행/열/대각선 패리티(Q2, Q3, Q4)를 생성한 2D 구조\n\n## EDS의 구조와 특징\n\n1. **사분면 구조**:\n   - **Q1 (좌상단)**: 원본 데이터 사각형(ODS - Original Data Square)\n   - **Q2 (우상단)**: 행 패리티 데이터(Row Parity) - 행 단위 복구 가능\n   - **Q3 (좌하단)**: 열 패리티 데이터(Column Parity) - 열 단위 복구 가능\n   - **Q4 (우하단)**: 행과 열 패리티 데이터(Row+Column Parity) - 특정 복구 시나리오에서 필요\n\n2. **2D Reed-Solomon 인코딩**:\n   - 행과 열 모두에 대해 Reed-Solomon 인코딩 적용\n   - 25% 이상의 데이터가 사용 가능하면 전체 데이터 복구 가능\n\n3. **데이터 단위**:\n   - **Share**: 기본 데이터 단위. 고정 크기의 바이트 배열\n   - **Square**: n×n 크기의 Share 배열\n\n## EDS 관련 주요 기능\n\n첨부된 코드 스니펫에서 볼 수 있는 주요 기능들:\n\n### 1. EDS 생성\n\n```go\nfunc extendBlock(data *types.Data, appVersion uint64, options ...nmt.Option) (*rsmt2d.ExtendedDataSquare, error)\n```\n\n- 블록 데이터를 EDS로 확장합니다.\n- 트랜잭션 데이터를 2D 사각형으로 구성한 후 Reed-Solomon 인코딩을 적용합니다.\n\n```go\nfunc extendShares(s [][]byte, options ...nmt.Option) (*rsmt2d.ExtendedDataSquare, error)\n```\n\n- Share의 배열을 EDS로 확장합니다.\n\n### 2. EDS 저장\n\n```go\nfunc storeEDS(\n    ctx context.Context,\n    eh *header.ExtendedHeader,\n    eds *rsmt2d.ExtendedDataSquare,\n    store *store.Store,\n    window time.Duration,\n    archival bool,\n) error\n```\n\n- EDS를 저장합니다.\n- 가용성 윈도우 내에 있으면 ODS와 Q4 모두 저장(ODSQ4)\n- 가용성 윈도우 밖에 있으면 ODS만 저장\n\n## file 패키지와 EDS의 관계\n\n앞서 설명한 file 패키지는 EDS의 저장 및 접근 메커니즘을 제공합니다:\n\n1. **ODS**: EDS의 Q1 사분면(원본 데이터)만 저장하는 방식\n2. **ODSQ4**: ODS와 Q4 사분면을 함께 저장하는 방식\n   - 일부 쿼리 및 데이터 복구에 더 효율적\n   - 저장 공간을 더 필요로 함\n\n## EDS의 Celestia에서의 역할\n\n1. **데이터 가용성 검증**:\n   - 노드들은 EDS의 임의 샘플을 요청하여 데이터 가용성 검증\n   - 충분한 샘플이 확인되면 전체 블록이 가용하다고 판단\n\n2. **데이터 복구**:\n   - 일부 데이터만으로 전체 데이터 복구 가능\n   - 네트워크 효율성 향상: 전체 블록이 아닌 일부만 다운로드해도 됨\n\n3. **저장 최적화**:\n   - 모든 노드가 전체 EDS를 저장할 필요 없음\n   - 일반 노드는 ODS만 저장해도 충분\n   - 가용성 보장이 필요한 최근 블록만 ODSQ4 형태로 저장\n\n## 실제 구현에서의 고려사항\n\n1. **가용성 윈도우**:\n\n   ```go\n   if availability.IsWithinWindow(eh.Time(), availability.StorageWindow) {\n       err = store.PutODSQ4(ctx, eh.DAH, eh.Height(), eds)\n   } else {\n       err = store.PutODS(ctx, eh.DAH, eh.Height(), eds)\n   }\n   ```\n\n   - 최근 블록(가용성 윈도우 내)에 대해서만 ODSQ4 방식으로 저장\n   - 오래된 블록은 ODS 방식으로만 저장하여 디스크 공간 절약\n\n2. **성능 최적화**:\n   - 캐싱 및 버퍼링을 통한 I/O 최적화\n   - Q4 지연 로딩(필요할 때만 로드)\n   - Reed-Solomon 인코더 캐싱\n\nEDS는 Celestia의 데이터 가용성 레이어의 핵심 구성 요소로, 블록체인의 데이터 확장성과 가용성을 동시에 해결하기 위한 중요한 구조입니다.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 3,
    "wordCount": 465,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "celestia가-트랜잭션-배열을-ods로-변환하는-방법",
    "slug": "celestiaga-teuraenjaegsyeon-baeyeoleul-odsro-byeonhwanhaneun-bangbeob",
    "path": "blockchain/celestia",
    "fullPath": "blockchain/celestia/celestiaga-teuraenjaegsyeon-baeyeoleul-odsro-byeonhwanhaneun-bangbeob",
    "title": "Celestia가 트랜잭션 배열을 ODS로 변환하는 방법",
    "excerpt": "핵심 개념 Share (공간단위) Celestia의 기본 데이터 단위(512바이트) 네임스페이스 ID를 포함한 원시 데이터 저장 각 Share는 특정 네임스페이스에 속하며, 정보 바이트와 버전을 포함 Square (사각형) 데...",
    "content": "## 1. 핵심 개념\n\n### Share (공간단위)\n- Celestia의 기본 데이터 단위(512바이트)\n- 네임스페이스 ID를 포함한 원시 데이터 저장\n- 각 Share는 특정 네임스페이스에 속하며, 정보 바이트와 버전을 포함\n\n### Square (사각형)\n- 데이터의 2D 정사각형 배열로, 항상 변의 길이가 2의 제곱수\n- Share들의 집합으로 구성\n- 최종적인 데이터 가용성 계층의 구조\n\n### 트랜잭션 유형\n- 일반 트랜잭션: 기본 트랜잭션\n- PFB(Pay-for-Blob) 트랜잭션: 블롭 데이터를 포함하거나 참조하는 특수 트랜잭션\n\n## 2. 변환 프로세스\n\n### 초기화 (Builder 생성)\n```go\nbuilder, err := NewBuilder(maxSquareSize, subtreeRootThreshold)\n```\n- `maxSquareSize`: 최대 사각형 크기(2의 제곱수)\n- `subtreeRootThreshold`: 서브트리 루트 임계값 설정\n\n### 트랜잭션 처리 순서\n1. **트랜잭션 분류**: 입력된 트랜잭션을 일반 트랜잭션과 blob 트랜잭션으로 분류\n2. **트랜잭션 배치**: 일반 트랜잭션을 먼저 배치한 후 PFB 트랜잭션 배치\n\n### Share 분할 프로세스\n1. **Compact Share 사용**: 일반 트랜잭션과 PFB 트랜잭션을 Compact Share로 변환\n   ```go\n   txWriter := share.NewCompactShareSplitter(share.TxNamespace, share.ShareVersionZero)\n   pfbWriter := share.NewCompactShareSplitter(share.PayForBlobNamespace, share.ShareVersionZero)\n   ```\n\n2. **Sparse Share 사용**: Blob 데이터는 Sparse Share로 변환\n   ```go\n   blobWriter := share.NewSparseShareSplitter()\n   ```\n\n3. **Blob 정렬 및 배치**:\n   - 네임스페이스 기준으로 Blob 정렬\n   - Share commitment 규칙에 맞게 패딩 추가\n   - 첫 Blob의 시작 위치가 nonReservedStart 결정\n\n4. **패딩 추가**:\n   - 네임스페이스 패딩: 트랜잭션과 PFB 사이\n   - 테일 패딩: 사각형 크기를 맞추기 위해 추가\n\n### 사각형 생성\n```go\nsquare, err := WriteSquare(txWriter, pfbWriter, blobWriter, nonReservedStart, squareSize)\n```\n\n1. **사각형 크기 결정**:\n   - 필요한 최소 크기 계산 (`inclusion.BlobMinSquareSize`)\n   - 트랜잭션, PFB, 블롭 데이터를 모두 수용할 수 있는 2의 제곱수로 크기 설정\n\n2. **데이터 배치**:\n   - 일반 트랜잭션 → PFB 트랜잭션 → 패딩 → Blob 데이터 → 테일 패딩 순으로 배치\n   - 네임스페이스별로 데이터 구분 (TxNamespace, PayForBlobNamespace 등)\n\n## 3. 주요 최적화 및 특징\n\n### 메모리 효율성\n- Share Counter를 사용하여 정확한 크기 계산\n- 필요한 만큼만 패딩 추가로 공간 최적화\n\n### 네임스페이스 구분\n- 각 데이터 유형에 맞는 네임스페이스 할당\n- 이를 통해 데이터 유형별 검색 및 접근 용이\n\n### 머클 트리 최적화\n- 서브트리 임계값 설정으로 머클 트리 구조 최적화\n- 블롭 데이터 위치 조정으로 효율적인 증명 생성\n\n### Share commitment 규칙 \n- 각 Blob의 시작 위치는 Share commitment 규칙에 따라 결정\n- 이는 데이터 가용성 검증을 위한 효율적인 증명을 가능하게 함\n\n## 세부 스펙에 대한 Note\n\n## 1. maxSquareSize와 원본/패리티 share 관계\n\n- **ODS(Original Data Square)**: 모든 share가 원본 데이터입니다. 여기서는 트랜잭션, PFB, 블롭 데이터, 그리고 필요한 패딩만 포함됩니다. 이 단계에서는 패리티 데이터가 없습니다.\n\n- **EDS(Extended Data Square)**: ODS를 2D 리드-솔로몬 인코딩으로 확장한 것으로, 이 때 패리티 데이터가 추가됩니다. 이는 코드베이스의 다른 부분에서 처리됩니다.\n\nODS의 크기가 2^n x 2^n인 이유는:\n1. 머클 트리 구성의 효율성 \n2. 데이터 가용성 샘플링(DAS)의 효율성\n3. 2D 리드-솔로몬 인코딩의 요구사항\n\n## 2. share 크기와 square 크기 제한\n\n- **share 크기**: 각 share는 512바이트로 고정되어 있습니다. 이는 코드에서 `ShareSize` 상수로 정의됩니다.\n\n- **square 크기 제한**: 이론적으로는 2의 제곱수로 무한히 커질 수 있지만, 실제로는 제한이 있습니다. 코드를 보면:\n\n  ```go\n  // worstCaseShareIndexes 함수에서\n  squareSizeUpperBound := 128\n  worstCaseShareIndex := squareSizeUpperBound * squareSizeUpperBound\n  ```\n\n  이 부분에서 최대 square 크기를 128x128(16,384 shares)로 제한하고 있음을 알 수 있습니다. 이는 Celestia-app v1.x와의 호환성을 위한 것입니다.\n\n  또한 각 노드의 메모리 제한, 네트워크 처리량, 블록 시간 등 실용적인 제약 요소들도 최대 square 크기를 제한합니다. 128x128 square는 약 **8MB**(512바이트 * 16,384)의 데이터를 담을 수 있으며, 이는 현재 구현의 실용적인 상한선입니다.\n\n결론적으로, share 크기는 고정되어 있고 square 크기는 이론적으로는 2의 제곱수로 확장 가능하지만 실제 구현에서는 제한이 있습니다.\n\nRef: https://github.com/celestiaorg/go-square",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 3,
    "wordCount": 541,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "celestia-데이터-가용성-샘플링-das-분석",
    "slug": "celestia-deiteo-gayongseong-saempeulring-das-bunseog",
    "path": "blockchain/celestia",
    "fullPath": "blockchain/celestia/celestia-deiteo-gayongseong-saempeulring-das-bunseog",
    "title": "Celestia 데이터 가용성 샘플링(DAS) 분석",
    "excerpt": "데이터 가용성 샘플링(DAS)에서 샘플 수량 결정 방식 라이트 노드의 샘플링 설정 share/availability/light/options.go 파일에서, 라이트 노드의 기본 샘플 수량이 16개로 정의되어 있습니다. ``` // D...",
    "content": "**데이터 가용성 샘플링(DAS)에서 샘플 수량 결정 방식**\n\n1. **라이트 노드의 샘플링 설정**\n\n- share/availability/light/options.go 파일에서, 라이트 노드의 기본 샘플 수량이 **16개**로 정의되어 있습니다.\n\n```\n// DefaultSampleAmount는 라이트 노드가 블록이 사용 가능하다고 선언하기 전에 수행해야 하는 최소 샘플 수를 지정합니다.\nvar (\n    DefaultSampleAmount uint = 16\n)\n```\n\n1. **DAS 관련 주요 매개변수**\n\n- das/options.go 파일의 DefaultParameters() 함수에서 DAS의 기본 설정을 확인할 수 있습니다.\n\n```\n// DefaultParameters는 daser(샘플링 모듈)의 기본 설정값을 반환합니다.\nfunc DefaultParameters() Parameters {\n    concurrencyLimit := 16\n    return Parameters{\n        SamplingRange: 100,\n        ConcurrencyLimit: concurrencyLimit,\n        BackgroundStoreInterval: 10 * time.Minute,\n        SampleFrom: 1,\n        SampleTimeout: 15 * time.Second * time.Duration(concurrencyLimit),\n    }\n}\n```\n\n여기서 SamplingRange: 100은 한 번의 샘플링 작업에서 **최대 100개의 헤더를 처리할 수 있음을 의미**하지만, 블록당 샘플 수량을 직접적으로 정의하는 것은 아닙니다.\n\n1. **무작위 샘플링 구현 방식**\n\n- share/availability/light/sample.go 파일의 selectRandomSamples 함수는 데이터 정사각형(squares)에서 무작위로 샘플을 선택하는 방식을 보여줍니다.\n\n```\n// selectRandomSamples는 주어진 크기의 정사각형에서 무작위로 고유한 좌표를 선택합니다.\nfunc selectRandomSamples(squareSize, sampleCount int) []shwap.SampleCoords {\n    total := squareSize * squareSize\n    if sampleCount > total {\n        sampleCount = total\n    }\n    \n    samples := make(map[shwap.SampleCoords]struct{}, sampleCount)\n    for len(samples) < sampleCount {\n        s := shwap.SampleCoords{\n            Row: randInt(squareSize),\n            Col: randInt(squareSize),\n        }\n        samples[s] = struct{}{}\n    }\n    return slices.Collect(maps.Keys(samples))\n}\n```\n\n위 코드에서 **샘플링 개수는 블록 데이터의 정사각형 크기에 따라 다르며**, 주어진 개수만큼 무작위로 선택됩니다.\n\n1. **라이트 노드의 데이터 가용성 확인 과정**\n\n- share/availability/light/availability.go 파일의 SharesAvailable 메서드를 살펴보면, 특정 블록 헤더에 대한 기존 샘플링 결과가 없을 경우 새로운 샘플링을 수행하는 것을 확인할 수 있습니다.\n\n```\nsamples = NewSamplingResult(len(dah.RowRoots), int(la.params.SampleAmount))\n```\n\n여기서 dah.RowRoots의 길이(즉, 데이터 정사각형의 크기)와 **설정된 샘플 수량(기본값: 16개)** 을 기반으로 샘플링이 이루어집니다.\n\n---\n\n**16KB 블록의 샘플 수량 분석**\n\nCelestia에서 **16KB 블록의 샘플링 방식**은 다음과 같습니다.\n\n1. **라이트 노드의 기본 샘플 수량**\n- 기본적으로 라이트 노드는 **블록 크기에 상관없이 16개의 샘플을 사용**합니다.\n\n1. **16KB 블록의 경우 샘플링 과정**\n\n- 16KB 블록은 주어진 블록 크기에 따라 정사각형 형태로 변환됩니다.\n- 보통 **4×4(16개)의 공유 데이터(share)로 나뉘어 저장**됩니다.\n- 라이트 노드는 이 **16개의 공유 데이터 중 16개를 무작위로 선택하여 샘플링**합니다.\n- 이렇게 선택된 샘플을 통해 블록이 네트워크에서 제대로 전파되었는지를 확인합니다.\n\n1. **풀 노드(Full Node) vs. 라이트 노드(Light Node)**\n\n- share/doc.go 파일에 따르면, 라이트 노드는 **16개의 샘플만 사용하여 가용성을 확인**하지만, 풀 노드는 전체 데이터를 다운로드하여 가용성을 검증합니다.\n\n```\n// Light Availability 구현은 블록 데이터의 16개 샘플을 확인하여 가용성을 검증합니다.\n// Full Availability 구현은 데이터를 완전히 복구할 수 있을 만큼의 샘플을 다운로드합니다.\n```\n\n---\n\n**결론: 샘플 수량 분석 요약**\n\n1. **라이트 노드의 기본 샘플 수량은 16개**\n\n- share/availability/light/options.go에서 기본값으로 16개 설정\n- 블록 크기와 상관없이 동일한 수량으로 샘플링 진행\n\n1. **16KB 블록에서는 전체 데이터 공유 개수가 16개이므로, 전체를 샘플링**\n\n- 블록이 4×4 정사각형으로 변환되며, 16개 중 16개를 샘플링\n- 16개만 샘플링해도 충분한 확률로 가용성을 검증할 수 있음\n\n👉 **결론적으로, Celestia의 라이트 노드는 기본적으로 블록당 16개의 샘플을 추출하여 데이터 가용성을 검증하도록 설계되어 있습니다.** 🚀\n\n### 데이터 가용성 샘플링의 확률적 보장\n\nCelestia의 데이터 가용성 샘플링(DAS)에서 16개의 샘플은 다음과 같은 확률적 보장을 제공합니다:\n\n#### 이론적 배경\n\n데이터 가용성 샘플링은 정보 이론과 확률론에 기반합니다. 특히 \"샘플링 게임\" 이론에 따르면:\n\n- 블록 생성자가 데이터의 일부(예: 50%)를 숨기면, 무작위로 선택된 각 샘플이 그 숨겨진 부분을 발견할 확률은 50%입니다.\n    \n- 16개의 독립적인 샘플을 선택할 경우, 숨겨진 데이터를 하나도 발견하지 못할 확률은 $(0.5)^{16} = 0.0000152587890625$, 즉 약 0.0015%입니다.\n    \n- 이는 반대로 말하면, 데이터의 50%가 누락된 경우 16개의 샘플로 99.9985%의 확률로 최소 하나 이상의 문제를 발견할 수 있다는 뜻입니다.\n    \n\n#### 샘플 수와 보안성 사이의 관계\n\n실제 코드에서 DefaultSampleAmount가 16으로 설정된 이유는 다음과 같습니다:\n\n1. **공격자의 데이터 은닉 비율에 따른 탐지 확률**:\n    \n    - 25% 데이터 은닉: 16개 샘플로 약 99.9999% 탐지 확률\n        \n    - 10% 데이터 은닉: 16개 샘플로 약 82% 탐지 확률\n        \n2. **효율성과 보안성의 균형**:  \n    샘플 수가 증가할수록 보안성은 향상되지만, 네트워크 부하와 처리 시간도 증가합니다. 16개의 샘플은 다음을 고려한 균형점입니다:\n    \n    - 네트워크 부하 최소화\n        \n    - 충분한 보안 레벨 제공\n        \n    - 경량 노드에 적합한 계산 부담\n        \n3. **네트워크 전체의 집단적 검증**:  \n    개별 노드는 16개 샘플만 검사하지만, 수천 개의 노드가 각각 다른 무작위 샘플을 검사함으로써 네트워크 전체적으로는 모든 데이터가 검증됩니다.\n    \n\n#### 실제 보안성\n\n실제 Celestia 네트워크에서는 공격자가 데이터의 작은 부분만 숨기더라도, 충분한 수의 노드가 샘플링을 수행하면 높은 확률로 탐지됩니다. 여러 노드에서 각각 16개의 샘플을 취하면, 네트워크 전체적으로는 훨씬 더 높은 보안성을 달성합니다.\n\n따라서 16개의 샘플은 단일 라이트 노드에서 리소스 효율성과 적절한 보안성을 균형 있게 제공하는 수치이며, 네트워크 전체적으로는 매우 높은 수준의 데이터 가용성 보장을 제공합니다.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 4,
    "wordCount": 692,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "celestia-node-file-패키지-분석",
    "slug": "celestia-node-file-paekiji-bunseog",
    "path": "blockchain/celestia",
    "fullPath": "blockchain/celestia/celestia-node-file-paekiji-bunseog",
    "title": "celestia-node 'file' 패키지 분석",
    "excerpt": "celestia-node 'file' 패키지 분석 Celestia-node repository의 패키지는 Exten Data Square(EDS)라는 데이터 구조를 파일 시스템에 효율적으로 저장하고 접근하기 위한 구현을 제공합니다. 이 패키...",
    "content": "# celestia-node 'file' 패키지 분석\n\nCelestia-node repository의 `store/file` 패키지는 Exten Data Square(EDS)라는 데이터 구조를 파일 시스템에 효율적으로 저장하고 접근하기 위한 구현을 제공합니다. 이 패키지는 데이터의 무결성과 복구 가능성을 보장하면서도 디스크 공간을 최적화하는 방식으로 설계되었습니다.\n\n## 핵심 개념\n\n1. **Original Data Square (ODS)**: 블록체인의 트랜잭션 데이터를 정사각형 형태로 배열한 것으로, EDS의 첫 번째 사분면(Q1)을 구성합니다.\n2. **Extended Data Square (EDS)**: ODS에 Reed-Solomon 인코딩을 적용하여 확장된 2D 데이터 구조입니다. 총 4개의 사분면으로 구성됩니다.\n3. **Q4**: EDS의 네 번째 사분면으로, 행과 열 모두에 패리티가 적용된 데이터를 저장합니다.\n4. **Share**: 데이터의 기본 단위로, 고정 크기의 바이트 배열입니다.\n\n## 저장 전략과 공간 효율성\n\nCelestia는 저장 공간을 효율적으로 사용하기 위해 두 가지 저장 전략을 사용합니다:\n\n1. **ODS만 저장**: 트랜잭션 데이터(ODS)만 저장하는 방식으로, EDS 전체 크기의 약 1/4에 해당하는 저장 공간을 사용합니다. 오래된 블록이나 아카이브 목적의 저장에 주로 사용됩니다.\n2. **ODSQ4 저장**: ODS와 Q4 사분면을 함께 저장하는 방식으로, EDS 전체 크기의 약 1/2에 해당하는 저장 공간을 사용합니다. 가용성 윈도우(availability window) 내의 최근 블록에 사용하여 특정 데이터 접근 패턴에서 계산 효율성을 향상시킵니다.\n\n가용성 윈도우는 데이터 가용성 검증이 더 중요한 최근 블록들을 구분하는 시간적 경계로, 이 기간 내의 블록은 ODSQ4 방식으로, 그 외의 블록은 ODS 방식으로 저장됩니다.\n\n## 핵심 구성요소와 기능\n\n다음은 패키지의 주요 구조체와 그 관계를 mermaid 다이어그램으로 정리한 것입니다:\n\n```mermaid\nclassDiagram\n    class ODS {\n        -headerV0 hdr\n        -os.File fl\n        -sync.RWMutex lock\n        -square ods\n        -bool disableCache\n        +Size(context.Context) int\n        +DataHash(context.Context) (share.DataHash, error)\n        +AxisRoots(context.Context) (*share.AxisRoots, error)\n        +Close() error\n        +Sample(context.Context, shwap.SampleCoords) (shwap.Sample, error)\n        +AxisHalf(context.Context, rsmt2d.Axis, int) (eds.AxisHalf, error)\n        +RowNamespaceData(context.Context, libshare.Namespace, int) (shwap.RowNamespaceData, error)\n        +Shares(context.Context) ([]libshare.Share, error)\n        +Reader() (io.Reader, error)\n    }\n\n    class q4 {\n        -headerV0 hdr\n        -os.File file\n        +close() error\n        +axisHalf(rsmt2d.Axis, int) (eds.AxisHalf, error)\n    }\n\n    class ODSQ4 {\n        -ODS ods\n        -string pathQ4\n        -sync.Mutex q4Mu\n        -atomic.Bool q4OpenAttempted\n        -q4 q4\n        +Size(context.Context) int\n        +DataHash(context.Context) (share.DataHash, error)\n        +AxisRoots(context.Context) (*share.AxisRoots, error)\n        +Sample(context.Context, shwap.SampleCoords) (shwap.Sample, error)\n        +AxisHalf(context.Context, rsmt2d.Axis, int) (eds.AxisHalf, error)\n        +RowNamespaceData(context.Context, libshare.Namespace, int) (shwap.RowNamespaceData, error)\n        +Shares(context.Context) ([]libshare.Share, error)\n        +Reader() (io.Reader, error)\n        +Close() error\n    }\n\n    class square {\n        +reader() (io.Reader, error)\n        +size() int\n        +shares() ([]libshare.Share, error)\n        +axisHalf(rsmt2d.Axis, int) (eds.AxisHalf, error)\n        +computeAxisHalf(rsmt2d.Axis, int) (eds.AxisHalf, error)\n    }\n\n    class headerV0 {\n        -fileVersion fileVersion\n        -uint16 shareSize\n        -uint16 squareSize\n        -share.DataHash datahash\n        +SquareSize() int\n        +ShareSize() int\n        +Size() int\n        +RootsSize() int\n        +OffsetWithRoots() int\n        +WriteTo(io.Writer) (int64, error)\n        +ReadFrom(io.Reader) (int64, error)\n    }\n\n    class Codec {\n        <<interface>>\n        +Encoder(int) (reedsolomon.Encoder, error)\n    }\n\n    class codecCache {\n        -sync.Map cache\n        +Encoder(int) (reedsolomon.Encoder, error)\n    }\n\n    ODS --* headerV0 : contains\n    ODS --* square : caches\n    q4 --* headerV0 : contains\n    ODSQ4 --* ODS : contains\n    ODSQ4 --* q4 : contains\n    codecCache ..|> Codec : implements\n```\n\n## 주요 컴포넌트 상세 설명\n\n### 1. headerV0 (header.go)\n- 파일의 메타데이터를 저장하는 구조체입니다.\n- 파일 버전, share 크기, square 크기, 데이터 해시 등을 포함합니다.\n- 파일 읽기/쓰기 작업에 필요한 메타데이터를 제공하며, 파일 형식의 버전 관리를 지원합니다.\n\n### 2. ODS (ods.go)\n- Original Data Square(ODS)를 파일 시스템에 저장하고 접근하는 기능을 제공합니다.\n- EDS의 첫 번째 사분면(Q1)을 저장하며, 파일 헤더에 메타데이터를 함께 저장합니다.\n- 성능 최적화를 위한 메모리 캐싱 기능을 제공하여 반복적인 파일 I/O를 줄입니다.\n- `eds.AccessorStreamer` 인터페이스를 구현하여 표준화된 데이터 접근 방식을 제공합니다.\n\n### 3. q4 (q4.go)\n- EDS의 네 번째 사분면(Q4)을 파일 시스템에 저장하고 접근하는 기능을 제공합니다.\n- Q4는 데이터 복구나 특정 유형의 쿼리(특히 Q2, Q4 사분면 샘플링)에 효율적으로 활용됩니다.\n- 파일의 크기 검증 및 오류 처리 기능을 포함합니다.\n\n### 4. ODSQ4 (ods_q4.go)\n- ODS와 Q4를 결합하여 효율적인 데이터 접근을 제공합니다.\n- 첫 번째 요청 시 Q4 파일을 지연 로딩(lazy loading)하여 초기 로딩 시간과 메모리 사용량을 최적화합니다.\n- 샘플링 작업이나 특정 행/열 접근 시 계산 효율성을 높여줍니다.\n- `eds.AccessorStreamer` 인터페이스를 구현하여 ODS와 동일한 인터페이스로 접근 가능합니다.\n\n### 5. square (square.go)\n- 데이터 square를 표현하는 2차원 배열 구조체입니다.\n- share의 행과 열에 쉽게 접근할 수 있는 메소드를 제공합니다.\n- 필요한 데이터를 계산하거나 복구하는 기능을 구현합니다.\n- 병렬 처리를 통해 데이터 복구 성능을 최적화합니다.\n\n### 6. Codec/codecCache (codec.go)\n- Reed-Solomon 인코딩을 위한 인터페이스와 구현을 제공합니다.\n- `reedsolomon.New(ln/2, ln/2, reedsolomon.WithLeopardGF(true))` 형태로 인코더를 생성하여 데이터의 약 25%만으로도 전체 복구가 가능하게 합니다.\n- 인코더를 캐싱하여 반복적인 생성 비용을 줄이고 성능을 최적화합니다.\n\n## 핵심 기능\n\n1. **데이터 저장**: EDS의 일부(ODS와 선택적으로 Q4)를 파일 시스템에 효율적으로 저장합니다.\n2. **데이터 접근**: 저장된 데이터에 다양한 방식(행, 열, 샘플 등)으로 접근할 수 있는 API를 제공합니다.\n3. **데이터 복구**: Reed-Solomon 인코딩을 사용하여 필요한 경우 누락된 데이터를 복구할 수 있으며, EDS 전체의 약 25%만으로도 복구가 가능합니다.\n4. **최적화된 I/O**: 버퍼링, 캐싱, 지연 로딩 등을 통해 파일 I/O 작업을 최적화합니다.\n5. **무결성 검증**: 데이터 해시와 크기 검증을 통해 파일의 무결성을 보장합니다.\n\n## 실제 사용 예시\n\nCelestia 네트워크에서 이 패키지는 다음과 같은 상황에서 활용됩니다:\n\n1. **블록 데이터 저장**: 새로운 블록이 검증되면, 노드는 블록 데이터를 EDS로 확장한 후 저장 전략에 따라 ODS 또는 ODSQ4 형태로 저장합니다.\n\n   ```go\n   // 가용성 윈도우 내의 블록은 ODS와 Q4 모두 저장\n   if availability.IsWithinWindow(eh.Time(), availability.StorageWindow) {\n       err = store.PutODSQ4(ctx, eh.DAH, eh.Height(), eds)\n   } else {\n       // 그 외 블록은 ODS만 저장하여 공간 절약\n       err = store.PutODS(ctx, eh.DAH, eh.Height(), eds)\n   }\n   ```\n\n2. **데이터 가용성 샘플링(DAS)**: 다른 노드로부터 블록 데이터의 특정 샘플을 요청받으면, 저장된 ODS 또는 ODSQ4를 사용하여 요청된 샘플을 효율적으로 제공합니다.\n\n3. **데이터 복구**: 누락된 블록 데이터가 있을 경우, 네트워크의 다른 노드로부터 충분한 샘플을 수집하여 Reed-Solomon 인코딩을 통해 전체 EDS를 복구할 수 있습니다.\n\n이 패키지는 Celestia의 데이터 가용성 레이어의 핵심 구성 요소로, 블록체인의 데이터 확장성 문제를 해결하면서도 데이터의 가용성과 무결성을 보장하는 중요한 역할을 합니다.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 5,
    "wordCount": 847,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "비트코인-nft와-brc-20-ft",
    "slug": "biteukoin-nftwa-brc-20-ft",
    "path": "blockchain/bitcoin",
    "fullPath": "blockchain/bitcoin/biteukoin-nftwa-brc-20-ft",
    "title": "비트코인 NFT와 BRC-20(FT)",
    "excerpt": "비트코인 NFT와 BRC-20(FT) 연초에 비트코인에서 발행이 가능해졌다는 뉴스가 나왔다. 최근에는 트랜잭션 급증으로 비트코인 네트워크가 혼잡해지고 있다. 본 글에서는 단순했던 비트코인 네트워크에서 어떻게 이더리움같은 , `NF...",
    "content": "# 비트코인 NFT와 BRC-20(FT)\n\n연초에 비트코인에서 `NFT` 발행이 가능해졌다는 뉴스가 나왔다. 최근에는 `brc-20` 트랜잭션 급증으로 비트코인 네트워크가 혼잡해지고 있다.\n본 글에서는 단순했던 비트코인 네트워크에서 어떻게 이더리움같은 `FT`, `NFT`를 발행할 수 있었는지 정리한다.\n\n## 비트코인에서의 Ordinals Protocal (서수체계) 도입과 비트코인 NFT의 출현\n\n[Ordinal Numbers](https://github.com/casey/ord/blob/master/bip.mediawiki)의 제안 동기는 비트코인 애플리케이션에서 사용될 수도 있는 안정적인 식별자를 제공하기 위함이라고 하는데, 이는 비트코인 주소 체계 상 안정적으로 사용할 만한 공개 신원 정보가 없기 때문입니다.\n\n서수체계 도입을 위해 모든 사토시를 채굴된 순서로 일련번호를 지정합니다. 이 숫자를 서수, `ordinal number`로 부릅니다. 구체적인 알고리즘은 다음과 같습니다. 이 코드는 블록체인에서 새 블록의 채굴 보상을 계산하고, 해당 블록에 속한 트랜잭션의 출력값에 일련번호(ordinal)를 부여하는 기능을 수행합니다.\n\n```python\n# subsidy 함수는 블록의 높이(height)를 입력으로 받아 그 높이에 해당하는 블록의 채굴 보상을 계산합니다. \n# 비트코인의 경우, 채굴 보상은 50 BTC로 시작하여 210,000 블록마다 절반으로 줄어들기 때문에, 이 함수는 높이에 따른 보상을 계산하기 위해 이러한 수식을 사용합니다.\ndef subsidy(height):\n  return 50 * 100_000_000 >> height // 210_000\n\n# first_ordinal 함수는 블록의 높이를 입력으로 받아 해당 높이의 첫 번째 트랜잭션 출력값에 할당될 일련번호를 계산합니다. \n# 이 함수는 이전 블록의 모든 출력값에 대한 일련번호를 더하고 블록의 채굴 보상을 더하여 구합니다.\ndef first_ordinal(height):\n  start = 0\n  for height in range(height):\n    start += subsidy(height)\n  return start\n\n# assign_ordinals 함수는 블록을 입력으로 받아, 각 트랜잭션 출력값에 일련번호를 부여합니다. \n# 이 함수는 먼저 블록 채굴 보상에 대한 일련번호를 계산하고, 이어서 블록 내의 모든 트랜잭션에 대해 다음을 수행합니다. \n# 각 트랜잭션의 입력값에서 일련번호를 가져와 출력값에 할당하고, 출력값에 일련번호를 부여합니다. \n# 마지막으로, 채굴 보상 출력값에 일련번호를 할당합니다. \n# 이러한 과정을 통해 블록 내의 모든 출력값에 일련번호가 부여됩니다.\ndef assign_ordinals(block):\n  first = first_ordinal(block.height)\n  last = first + subsidy(block.height)\n  coinbase_ordinals = list(range(first, last))\n\n  for transaction in block.transactions[1:]:\n    ordinals = []\n    for input in transaction.inputs:\n      ordinals.extend(input.ordinals)\n\n    for output in transaction.outputs:\n      output.ordinals = ordinals[:output.value]\n      del ordinals[:output.value]\n\n    coinbase_ordinals.extend(ordinals)\n\n  for output in block.transaction[0].outputs:\n    output.ordinals = coinbase_ordinals[:output.value]\n    del coinbase_ordinals[:output.value]\n```\n\n<br/>\n\n여기에 더해 inscription이 적용되면서 ordinal inscription은 NFT와 유사한 디지털 자산으로 볼 수 있게 되었습니다. 기존의 NFT는 스마트체인 또는 사이드체인의 컨트랙트를 통해 호스팅 되었지만, 비트코인 NFT는 사토시에 바인딩 되어 있기 때문에 사이드 체인이나 별도의 토큰이 필요하지 않습니다.\n\nOrdinals는 단순히 가치 이전만 제공하던 비트코인 네트워크의 새로운 사용 사례를 만들었습니다. 최근 트랜잭션이 늘어나면서 네트워크 비용이 증가하고 기존 비트코인의 단순성을 훼손하는 것이 아니냐는 지적이 있습니다. 그러나 Ordinals의 지지자들은 미래에 블록 보상이 감소하더라도 네트워크 수수료가 비트코인에 대한 해시 파워를 약속하는 주요 인센티브가 될 수 있다고 주장합니다.\n\n## BRC-20의 등장\n\n앞서 설명한 서수의 등장과 탭루트 업그레이드를 통해서 비트코인 블록체인에서 NFT 발행이 가능해졌고, 최근 이를 이용한 BRC-20이 등장하게 되었습니다. BRC-20을 제안한 `domo`가 밝혔듯이, BRC-20은 inscriptions를 활용한 실험이며 아직 진지하게 받아들일 만한 공식 표준이 아닙니다. BRC-20의 기본적인 목적은 서수 이론을 통해 비트코인에 fungibility를 도입할 수 있는지 확인하는 것입니다. 자세한 내용은 [다음 문서](https://domo-2.gitbook.io/brc-20-experiment/)에서 확인할 수 있습니다.\n\n## 참고\n\n1. [비트코인 서수와 NFT - 바이낸스 아카데미](https://academy.binance.com/en/articles/what-are-ordinals-an-overview-of-bitcoin-nfts)\n2. [BRC-20 - domo](https://domo-2.gitbook.io/brc-20-experiment/)\n3. [BRC-20 토큰 - 바이낸스 아카데미](https://academy.binance.com/cs/glossary/brc-20-tokens)\n4. [Ordinal Numbers bip.memiawiki](https://github.com/casey/ord/blob/master/bip.mediawiki)\n5. [Ordinal Theory document](https://docs.ordinals.com/overview.html)\n6. [Bitcoin Taproot - 바이낸스 아카데미](https://academy.binance.com/en/articles/what-is-taproot-and-how-it-will-benefit-bitcoin)",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 3,
    "wordCount": 472,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "spv-simplified-payment-verification",
    "slug": "spv-simplified-payment-verification",
    "path": "blockchain/bitcoin",
    "fullPath": "blockchain/bitcoin/spv-simplified-payment-verification",
    "title": "SPV (Simplified Payment Verification)",
    "excerpt": "SPV (Simplified Payment Verification) SPV란? 거래에 대한 모든 블록체인을 저장하지 않고도 트랜잭션을 검증하는 방법입니다. 라이트 웨이트 노드 또는 경량노드라고도 불립니다. 특징 블록체인의 사본을 보관하지 않...",
    "content": "# SPV (Simplified Payment Verification)\n\n## SPV란?\n\n거래에 대한 모든 블록체인을 저장하지 않고도 트랜잭션을 검증하는 방법입니다. 라이트 웨이트 노드 또는 경량노드라고도 불립니다.\n\n## 특징\n\n-   블록체인의 사본을 보관하지 않고 트랜잭션 검증과정에도 참여하지 않으므로 네트워크 보안에 기여하지 않는다\n-   그러므로 다른 풀노드 정보에 의존하여 거래를 진행한다\n-   블록 헤더 구성\n    -   버전: 4바이트\n    -   이전 블록해시 : 32바이트\n    -   머클루트 해시 : 32바이트\n    -   블록 시간 : 4바이트\n    -   비츠 : 4바이트\n    -   논스값 : 4바이트로 구성되어있으며, 총 80바이트로, 1년 동안 발생하는 52,560개의 블록 헤더 용량이 4MB 정도이니 현재 150GB를 넘긴 풀 노드에 비해 매우 가볍다고 할 수 있다.\n\n## 원리\n\n![merkle-root](https://miro.medium.com/v2/resize:fit:720/format:webp/0*ZLrIO_B67108JStC.jpg)\n\n거래3가 블록에 실렸는지 확인하고 싶다면, 블록 생성 시마다 해당 블록의 머클루트 획득에 필요한 해시들만 가져오면 된다.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 118,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "bitcoin-segwit",
    "slug": "bitcoin-segwit",
    "path": "blockchain/bitcoin",
    "fullPath": "blockchain/bitcoin/bitcoin-segwit",
    "title": "Bitcoin Segwit",
    "excerpt": "Bitcoin Segwit 비트코인에서, 거래는 입력(inputs)과 출력(outputs)을 포함합니다. 트랜잭션이 네트워크로 전송될 때 노드에서 트랜잭션의 유효성을 검사하여 트랜잭션이 합법적이고 입력이 이중으로 사용되지 않는지 확인해야 합니다. 원래 비트코인 프...",
    "content": "# Bitcoin Segwit\n\n비트코인에서, 거래는 입력(inputs)과 출력(outputs)을 포함합니다. 트랜잭션이 네트워크로 전송될 때 노드에서 트랜잭션의 유효성을 검사하여 트랜잭션이 합법적이고 입력이 이중으로 사용되지 않는지 확인해야 합니다. 원래 비트코인 프로토콜에서는 디지털 서명(증인 데이터)이 트랜잭션 입력에 포함되어 트랜잭션이 커지고 네트워크 처리량이 감소했습니다. 이를 해결하기 위해 2017년에 분리 증인(SegWit)이라는 새로운 거래 형식이 도입되었습니다. SegWit는 거래의 입력에서 증인 데이터를 분리하여 증인이라는 거래의 별도 섹션에 배치합니다. SegWit 주소는 SegWit 트랜잭션 형식과 호환되는 비트코인 주소입니다. SegWit 주소로 자금을 보낼 때 자금의 소유권을 증명하는 증인을 거래에 포함시킬 수 있습니다. 감시자와 트랜잭션을 만들려면 SegWit 주소를 트랜잭션의 출력 주소로 사용해야 하며, 트랜잭션에 서명할 때 감시자 데이터를 포함해야 합니다. SegWit 트랜잭션의 script_pubkey에는 자금을 사용할 수 있는 조건을 지정하는 감시 프로그램이 포함되어 있으며, SegWit 주소를 만드는 데 사용됩니다.\n\n```rust\nuse bitcoin::{\n    blockdata::transaction::{Transaction, TxIn, TxOut},\n    network::constants::Network,\n    util::bip143::{SigHashCache, SigHashType},\n    PublicKey,\n    Script,\n    TxBuilder,\n    TxInWitness,\n};\n\nfn create_transaction() -> Transaction {\n    // Create inputs\n    let txin = TxIn {\n        previous_output: bitcoin::OutPoint::new(\n            bitcoin::hash::Sha256dHash::default(),\n            0,\n        ),\n        script_sig: Script::new(),\n        sequence: 0xFFFFFFFF,\n        witness: TxInWitness::default(),\n    };\n\n    // Create outputs\n    let txout = TxOut {\n        value: 1000000, // Satoshis\n        script_pubkey: Script::new(),\n    };\n\n    // Create transaction\n    let mut tx_builder = TxBuilder::new();\n    let tx = tx_builder\n        .add_input(txin)\n        .add_output(txout)\n        .build()\n        .unwrap();\n\n    // Sign transaction\n    let key = bitcoin::PrivateKey::from_wif(\"cTJtmP6oZmnBp9jWsnJzBvTuwT54qfN3rq8WxjZgCCzU6DDHMC6N\").unwrap();\n    let public_key = PublicKey::from_private_key(&key, true).unwrap();\n    let sig_hash = SigHashCache::new(&tx).signature_hash(0, &Script::new(), 1000000, SigHashType::All);\n    let signature = key.sign(&sig_hash);\n    let mut witness = TxInWitness::default();\n    witness.push(signature.serialize_der().to_vec());\n    witness.push(public_key.to_bytes());\n    tx_builder.set_witness(0, witness);\n\n    tx\n}\n\nfn main() {\n    let tx = create_transaction();\n    println!(\"{}\", tx);\n}\n```\n\n```toml\n[dependencies]\nbitcoin = \"0.28.1\"\nsecp256k1 = \"0.17.0\"\nhex = \"0.4.2\"\n```\n\n이 예에서는 먼저 하나의 입력과 하나의 출력으로 트랜잭션을 만듭니다. 그런 다음 개인 키를 사용하여 트랜잭션에 서명하고 디지털 서명과 공개 키를 포함하는 증인을 생성합니다. 마지막으로 TxBuilder의 set_witness 메서드를 사용하여 트랜잭션에 대한 감시를 설정하고 트랜잭션을 콘솔에 인쇄합니다.\n\n이 예에서는 트랜잭션에 더미 입력 및 출력을 사용하고 서명에 임의로 생성된 개인 키를 사용했습니다. 실제 응용 프로그램에서는 이 값을 사용 사례의 실제 값으로 대체해야 합니다.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 2,
    "wordCount": 303,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "bitcoin-script",
    "slug": "bitcoin-script",
    "path": "blockchain/bitcoin",
    "fullPath": "blockchain/bitcoin/bitcoin-script",
    "title": "Bitcoin Script",
    "excerpt": "Bitcoin Script 비트코인은 트랜잭션에 스크립팅 시스템을 사용합니다. 스크립트는 단순하고, 스택 기반이며, 좌에서 우로 진행합니다. 이것은 의도적으로 튜링완전하지 않으며, 루프가 허용되지 않습니다. 스크립트는 본질적으로 각 트랜잭션에 기록된 지시어들의...",
    "content": "# Bitcoin Script\n\n비트코인은 트랜잭션에 스크립팅 시스템을 사용합니다. 스크립트는 단순하고, 스택 기반이며, 좌에서 우로 진행합니다. 이것은 의도적으로 튜링완전하지 않으며, 루프가 허용되지 않습니다.\n\n스크립트는 본질적으로 각 트랜잭션에 기록된 지시어들의 배열이고, 지시어들은 전송되는 비트코인을 사용하려는 다음 사람이 접근 할 수 있는 지를 설명합니다.\n\n일반적으로 비트코인을 수신자 주소로 전송하는 스크립트는 두가지이고 사용자가 제공해야 합니다. 첫번째로 트랜잭션이 해시될 때 스크립트에 포함된 수신자 주소를 생성하는 Public Key, 두번째로 첫번째로 제공한 Public Key에 대응하는 Private Key 소유권을 증명하는 서명입니다.\n\n스크립팅은 전송된 비트코인을 사용하는데 필요한 매개변수를 변경할 수 있는 유연성을 제공합니다. 예를 들어, 스크립팅 시스템을 사용하여 두 개의 개인 키를 요구하거나 여러개의 키를 조합하거나 아예 키를 사용하지 않을 수 있습니다.\n\n트랜잭션은 스크립트 조합에서 에러를 발생하지 않고, 최상위 스택 항목이 True(non-zero)인 경우에 유효합니다.\n\n## 예시\n\n### p2pkh(pay-to-pubkey-hash)\n\n```\nscriptPubKey: OP_DUP OP_HASH160 <pubKeyHash> OP_EQUALVERIFY OP_CHECKSIG\nscriptSig: <sig> <pubKey>\n```\n\n| Stack                                             | Script                                                                         | Description                              |\n| ------------------------------------------------- | ------------------------------------------------------------------------------ | ---------------------------------------- |\n| Empty                                             | `<sig>` `<pubKey>` OP_DUP OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG | scriptSig와 scriptPubKey가 결합된다      |\n| `<sig>` `<pubKey>`                                | OP_DUP OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                    | 상수를 스택에 추가한다                   |\n| `<sig>` `<pubKey>` `<pubKey>`                     | OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                           | 최상위 스택 아이템이 복사되었다          |\n| `<sig>` `<pubKey>` `<pubKeyHashA>`                | `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                                      | 최상위 스택 아이템이 해싱되었다          |\n| `<sig>` `<pubKey>` `<pubKeyHashA>` `<pubkeyHash>` | OP_EQUALVERIFY OP_CHECKSIG                                                     | 상수를 스택에 추가한다                   |\n| `<sig>` `<pubKey>`                                | OP_CHECKSIG                                                                    | 최상위 두개 아이템이 동일함을 확인하였다 |\n| true                                              | Empty                                                                          | 최상위 두개 아이템의 서명이 확인되었다   |\n\n### p2wpkh(pay-to-witness-pubkey-hash)\n\n```\nscriptPubKey: OP_0 <pubkey.hash:20>\nscriptSig: Empty.\nwitness: <sig> <pubkey>\n```\n\n| Stack                                             | Script                                                                         | Description                                                                        |\n| ------------------------------------------------- | ------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------- |\n| Empty                                             | OP_0 <pubkey.hash:20>                                                          | scriptSig와 scriptPubKey가 결합된다                                                |\n| Empty                                             | `<sig>` `<pubKey>` OP_DUP OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG | p2wpkh패턴, witness정보의 `<sig>` `<pubkey>`을 통해 `<pubkey.hash:20>`을 치환한다. |\n| `<sig>` `<pubKey>`                                | OP_DUP OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                    | 상수를 스택에 추가한다                                                             |\n| `<sig>` `<pubKey>` `<pubKey>`                     | OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                           | 최상위 스택 아이템이 복사되었다                                                    |\n| `<sig>` `<pubKey>` `<pubKeyHashA>`                | `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                                      | 최상위 스택 아이템이 해싱되었다                                                    |\n| `<sig>` `<pubKey>` `<pubKeyHashA>` `<pubkeyHash>` | OP_EQUALVERIFY OP_CHECKSIG                                                     | 상수를 스택에 추가한다                                                             |\n| `<sig>` `<pubKey>`                                | OP_CHECKSIG                                                                    | 최상위 두개 아이템이 동일함을 확인하였다                                           |\n| true                                              | Empty                                                                          | 최상위 두개 아이템의 서명이 확인되었다                                             |\n\n### p2sh-p2wpkh\n\n```\nscriptPubKey: <OP_HASH160> <script-hash> <OP_EQUAL>\nscriptSig(redeemScript): hash160(<OP_2> <pubkey-a> <pubkey-b> <pubkey-c> <OP_3> <OP_CHECKMULTISIG>)\nwitness: <sig> <pubkey>\n```\n\n| Stack                                             | Script                                                                         | Description                                                                        |\n| ------------------------------------------------- | ------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------- |\n| Empty                                             | `<redeem_script>` OP_HASH160 `<script-hash>` `OP_EQUAL`                        | scriptSig와 scriptPubKey가 결합된다                                                |\n| Empty                                             | `<sig>` `<pubKey>` OP_DUP OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG | p2wpkh패턴, witness정보의 `<sig>` `<pubkey>`을 통해 `<pubkey.hash:20>`을 치환한다. |\n| `<sig>` `<pubKey>`                                | OP_DUP OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                    | 상수를 스택에 추가한다                                                             |\n| `<sig>` `<pubKey>` `<pubKey>`                     | OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                           | 최상위 스택 아이템이 복사되었다                                                    |\n| `<sig>` `<pubKey>` `<pubKeyHashA>`                | `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                                      | 최상위 스택 아이템이 해싱되었다                                                    |\n| `<sig>` `<pubKey>` `<pubKeyHashA>` `<pubkeyHash>` | OP_EQUALVERIFY OP_CHECKSIG                                                     | 상수를 스택에 추가한다                                                             |\n| `<sig>` `<pubKey>`                                | OP_CHECKSIG                                                                    | 최상위 두개 아이템이 동일함을 확인하였다                                           |\n| true                                              | Empty                                                                          | 최상위 두개 아이템의 서명이 확인되었다                                             |\n\n### 미래 특정 시점까지 자금을 동결하고 싶은 경우\n\n```\nscriptPubKey: <expiry-time> OP_CHECKLOCKTIMEVERIFY OP_DROP OP_DUP OP_HASH160 <pubKeyHash> OP_EQUALVERIFY OP_CHECKSIG\nscriptSig: <sig> <pubKey>\n```\n\n| Stack                                             | Script                                                                                                                | Description                                                   |\n| ------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------- |\n| Empty                                             | `<sig>` `<pubKey>` `<expiry-time>` OP_CHECKLOCKTIMEVERIFY OP_DUP OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG | scriptSig와 scriptPubKey가 결합된다                           |\n| `<sig>` `<pubKey>` `<expiry-time>`                | OP_CHECKLOCKTIMEVERIFY OP_DROP OP_DUP OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                            | 상수를 스택에 추가한다                                        |\n| `<sig>` `<pubKey>` `<expiry-time>`                | OP_DROP OP_DUP OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                                                   | 최상위 스택 아이템를 현재 시간 또는 블록 높이와 비교 확인한다 |\n| `<sig>` `<pubKey>`                                | OP_DUP OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                                                           | 최상위 스택 아이템을 제거한다                                 |\n| `<sig>` `<pubKey>` `<pubKey>`                     | OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                                                                  | 최상위 스택 아이템이 복사되었다                               |\n| `<sig>` `<pubKey>` `<pubKeyHashA>`                | `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                                                                             | 최상위 스택 아이템이 해싱되었다                               |\n| `<sig>` `<pubKey>` `<pubKeyHashA>` `<pubkeyHash>` | OP_EQUALVERIFY OP_CHECKSIG                                                                                            | 상수를 스택에 추가한다                                        |\n| `<sig>` `<pubKey>`                                | OP_CHECKSIG                                                                                                           | 최상위 두개 아이템이 동일함을 확인하였다                      |\n| true                                              | Empty                                                                                                                 | 최상위 두개 아이템의 서명이 확인되었다                        |\n\n### pay-to-multi-signature (2-out-of-3)\n\n```\nscriptPubKey: <OP_2> <pubkey-a> <pubkey-b> <pubkey-c> <OP_3> <OP_CHECKMULTISIG>\nscriptSig: <OP_0> <sig> <sig> <OP_2>\n```\n\n### pay-to-script\n\n```\nscriptPubKey: <OP_HASH160> <script-hash> <OP_EQUAL>\nscriptSig(redeemScript): hash160(<OP_2> <pubkey-a> <pubkey-b> <pubkey-c> <OP_3> <OP_CHECKMULTISIG>)\n```\n\n![](https://learnmeabitcoin.com/technical/images/address/encode-p2sh.png)\n\n위 그림을 보면 중간에 hash160(script)가 있는데 이게 `redeem-script`의 해시값이라고 볼 수 있다. 이것을 base58로 변환했을때 a로 시작하는 P2SH주소 형태로 상대방에게 전달할 수 있습니다.\n\n## Ref\n\n- https://learnmeabitcoin.com/technical/p2sh\n- https://medium.com/programming-bitcoin/chapter-13-세그윗-865a0c3f6414\n- https://dev-notes.eu/2020/11/Bitcoin-Pay-To-Script-Hash/\n- https://bitcoin.design/guide/glossary/address/\n- https://developer.bitcoin.org/reference/rpc/\n- https://en.bitcoin.it/wiki/Category:Technical",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 4,
    "wordCount": 758,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "address",
    "slug": "address",
    "path": "blockchain/bitcoin",
    "fullPath": "blockchain/bitcoin/address",
    "title": "Address",
    "excerpt": "Address 비트코인 주소는 비트코인을 받는 데 사용되는 26-35개의 영숫자 식별자입니다. 서로 다른 사양을 기반으로 하는 여러 주소 형식이 있습니다. 사용자가 주소를 입력할 때 이러한 형식에는 특정 접두사가 있으므로 사용 중인 형식을 확인할 수 있습니다....",
    "content": "# Address\n\n비트코인 주소는 비트코인을 받는 데 사용되는 26-35개의 영숫자 식별자입니다. 서로 다른 사양을 기반으로 하는 여러 주소 형식이 있습니다.\n\n사용자가 주소를 입력할 때 이러한 형식에는 특정 접두사가 있으므로 사용 중인 형식을 확인할 수 있습니다.\n\n다음은 오늘날 사용되는 일반적인 주소 형식입니다:\n\n## Taproot address - P2TR\n\nTaproot 또는 Bech32m 주소라고도 하는 P2TR(Pay-to-Taproot)은 가장 최신의 고급 비트코인 주소 형식입니다. Taproot는 비트코인에 더욱 향상된 보안, 개인 정보 보호, 유연성 및 확장 기능을 제공합니다. SegWit와 마찬가지로 Taproot 주소는 옵트인이며 현재 널리 지원되지 않습니다.\n\nTaproot의 장점은 슈노르 서명(Schnorr Signatures)를 사용할 수 있는 기능, 더 나은 보안, 더 낮은 수수료, 더 유연한 다중 키 트랜잭션을 제공하는 것입니다. P2TR을 사용하는 다중 키 주소는 단일 키 주소와 동일하게 보여 다중 키 사용자에게 향상된 개인 정보를 제공합니다. Taproot는 또한 더 고급 스크립팅을 가능하게 하여 비트코인을 기반으로 더 복잡한 스마트 계약을 구축할 수 있게 합니다.\n\nTaproot 주소는 bc1p로 시작하며 대소문자를 구분하지 않습니다.\n\nExample: bc1pmzfrwwndsqmk5yh69yjr5lfgfg4ev8c0tsc06e\n\n## SegWit address - P2WPKH\n\n네이티브 SegWit 또는 Bech32 주소로도 알려진 P2WPKH(pay-to-witness-public-key-hash)는 현대적이고 보다 효율적인 주소 형식입니다. SegWit 주소는 옵트인이므로 모든 애플리케이션이 이를 지원하는 것은 아니지만 오늘날 대부분이 지원하고 지원해야 합니다. SegWit 채택 여부는 여기에서 확인할 수 있습니다.\n\nSegWit 주소의 이점은 대소문자를 구분하지 않고 오류 수정 코드를 사용하기 때문에 입력 오류에 대한 저항성이 향상되고 트랜잭션 비용이 절감된다는 것입니다. 수수료 절감은 거래 유형에 따라 다르겠지만 일반적인 자금 이체의 경우 30-40%가 될 수 있습니다.\n\nSegWit 주소는 bc1q로 시작하며 대소문자를 구분하지 않습니다.\n\nExample: bc1qar0srrr7xfkvy5l643lydnw9re59gtzzwf5mdq\n\n## Script address - P2SH\n\nP2SH(Pay-to-Script-Hash) 또는 스크립트 주소는 주소에 추가 규칙 및 기능을 첨부할 수 있습니다. 스크립트 주소는 일반적으로 트랜잭션을 인증하는 데 여러 키의 서명이 필요함을 지정할 수 있는 다중 서명 주소에 사용됩니다.\n\n스크립트 주소는 숫자 3으로 시작하고 대문자와 소문자를 포함할 수 있으며 대소문자를 구분합니다.\n\nExample: 3J98t1WpEZ73CNmQviecrnyiWrnqRhWNLy\n\n## Legacy address - P2PKH\n\nP2PKH(pay-to-pubkey-hash) 또는 레거시 주소는 가장 오래되고 원래의 비트코인 주소 형식입니다. 이 주소 형식은 이 형식을 사용하면 트랜잭션 비용이 더 높기 때문에 오늘날 널리 사용되지 않습니다. 그러나 여전히 일부 새로운 주소체계를 적용하지 않은 레거시 애플리케이션이 존재하므로, 이들과의 호환성을 지키는 것이 권장됩니다.\n\nLegacy address는 숫자 1로 시작하고 대문자와 소문자를 포함할 수 있으며 대소문자를 구분합니다.\n\nExample: 1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN2\n\n## Address 호환성\n\n레거시 주소가 여전히 사용되고 있기 때문에, 일부 오래된 애플리케이션은 여전히 업그레이드가 필요합니다. 스크립트 주소를 사용하면 송신자의 애플리케이션이 수신자에 의해 제공받은 세그윗 주소를 유효하게 인식하지 못하는 호환성 문제를 해결할 수 있습니다.\n\n그러므로 수신자는 기본적으로 세그윗 주소를 사용한다고 하더라도 송신자의 호환성을 맞춰주기 위해서 스크립트 또는 탭루트 주소로 전환할 수 있어야 합니다.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 2,
    "wordCount": 386,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "upgrade-move-code",
    "slug": "upgrade-move-code",
    "path": "blockchain/aptos",
    "fullPath": "blockchain/aptos/upgrade-move-code",
    "title": "Upgrade Move code",
    "excerpt": "Upgrade Move code Move code (e.g., Move modules) on the Aptos blockchain can be upgraded. This allows code owners and module developers to update a...",
    "content": "# Upgrade Move code\n\nMove code (e.g., Move modules) on the Aptos blockchain can be upgraded. This\nallows code owners and module developers to update and evolve their contracts\nunder a single, stable, well-known account address that doesn't change. If a\nmodule upgrade happens, all consumers of that module will automatically receive\nthe latest version of the code (e.g., the next time they interact with it).\n\nThe Aptos blockchain natively supports different _upgrade policies_, which allow\nmove developers to explicitly define the constraints around how their move code\ncan be upgraded. The default policy is _(backwards) compatible_. This means that\ncode upgrades are accepted only if they guarantee that no existing resource storage\nor public APIs are broken by the upgrade (including public functions).\nThis compatibility checking is possible because of Move's strongly typed bytecode\nsemantics.\n\nWe note, however, that even compatible upgrades can have hazardous effects on\napplications and dependent Move code (for example, if the semantics of the underlying\nmodule are modified). As a result, developers should be careful when depending on\nthird-party Move code that can be upgraded on-chain. See\n[Security considerations for dependencies](#security-considerations-for-dependencies)\nfor more details.\n\n## How it works\n\nMove code upgrades on the Aptos blockchain happen at the [Move package](https://move-language.github.io/move/packages.html)\ngranularity. A package specifies an upgrade policy in the `Move.toml` manifest:\n\n```toml\n[package]\nname = \"MyApp\"\nversion = \"0.0.1\"\nupgrade_policy = \"compatible\"\n...\n```\n\n:::tip Compatibility check\nAptos checks compatibility at the time a [Move package](https://move-language.github.io/move/packages.html) is published via an Aptos transaction. This transaction will abort if deemed incompatible.\n:::\n\n## How to upgrade\n\nTo upgrade already published Move code, simply attempt to republish the code at\nthe same address that it was previously published. This can be done by following the\ninstructions for code compilation and publishing using the\n[Aptos CLI](../../cli-tools/aptos-cli-tool/use-aptos-cli.md). For an example,\nsee the [Your First Move Module](../../tutorials/first-move-module.md) tutorial.\n\n## Upgrade policies\n\nThere are two different upgrade policies currently supported by Aptos:\n\n- `compatible`: these upgrades must be backwards compatible, specifically:\n  - For storage, all old struct declarations must be the same in\n    the new code. This ensures that the existing state of storage is\n    correctly interpreted by the new code. However, new struct declarations\n    can be added.\n  - For APIs, all existing public functions must have the same signature as\n    before. New functions, including public and entry functions, can be added.\n- `immutable`: the code is not upgradeable and is guaranteed to stay the same\n  forever.\n\nThose policies are ordered regarding strength such that `compatible < immutable`,\ni.e., compatible is weaker than immutable. The policy of a package on-chain can\nonly get stronger, not weaker. Moreover, the policy of all dependencies of a\npackage must be stronger or equal to the policy of the given package. For example,\nan `immutable` package cannot refer directly or indirectly to a `compatible` package.\nThis gives users the guarantee that no unexpected updates can happen under the hood.\n\nNote that there is one exception to the above rule: framework packages\ninstalled at addresses `0x1` to `0xa` are exempted from the dependency check.\nThis is necessary so one can define an `immutable` package based on the standard\nlibraries, which have the `compatible` policy to allow critical upgrades and fixes.\n\n## Compatibility rules\n\nWhen using `compatible` upgrade policy, a module package can be upgraded. However, updates to existing modules already\npublished previously need to be compatible and follow the rules below:\n\n- All existing structs' fields cannot be updated. This means no new fields can be added and existing fields cannot be\n  modified. Struct abilities also cannot be changed (no new ones added or existing removed).\n- All public and entry functions cannot change their signature (argument types, type argument, return types). However,\n  argument names can change.\n- Public(friend) functions are treated as private and thus their signature can arbitrarily change. This is safe as\n  only modules in the same package can call friend functions anyway and they need to be updated if the signature changes.\n\nWhen updating your modules, if you see an incompatible error, make sure to check the above rules and fix any violations.\n\n## Security considerations for dependencies\n\nAs mentioned above, even compatible upgrades can have disastrous effects for\napplications that depend on the upgraded code. These effects can come from bugs,\nbut they can also be the result of malicious upgrades. For example,\nan upgraded dependency can suddenly make all functions abort, breaking the\noperation of your Move code. Alternatively, an upgraded dependency can make\nall functions suddenly cost much more gas to execute then before the upgrade.\nAs result, dependencies to upgradeable packages need to be handled with care:\n\n- The safest dependency is, of course, an `immutable` package. This guarantees\n  that the dependency will never change, including its transitive dependencies.\n  In order to update an immutable package, the owner would have to introduce a\n  new major version, which is practically like deploying a new, separate\n  and independent package. This is because major versioning can be expressed\n  only by name (e.g. `module feature_v1` and `module feature_v2`). However,\n  not all package owners like to publish their code as `immutable`, because this\n  takes away the ability to fix bugs and update the code in place.\n- If you have a dependency to a `compatible` package, it is highly\n  recommended you know and understand the entity publishing the package.\n  The highest level of assurance is when the package is governed by a\n  Decentralized Autonomous Organization (DAO) where no single user can initiate\n  an upgrade; a vote or similar has to be taken. This is the case for the Aptos\n  framework.\n\n## Programmatic upgrade\n\nIn general, Aptos offers, via the Move module `aptos_framework::code`,\nways to publish code from anywhere in your smart contracts. However,\nnotice that code published in the current transaction can be executed\nonly after that transaction ends.\n\nThe Aptos framework itself, including all the on-chain administration logic, is\nan example for programmatic upgrade. The framework is marked as `compatible`.\nUpgrades happen via specific generated governance scripts. For more details,\nsee [Aptos Governance](../../concepts/governance.md).",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 6,
    "wordCount": 1002,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "move-on-aptos",
    "slug": "move-on-aptos",
    "path": "blockchain/aptos",
    "fullPath": "blockchain/aptos/move-on-aptos",
    "title": "Move on Aptos",
    "excerpt": "Move on Aptos 앱토스 블록체인은 합의 프로토콜을 실행하는 밸리데이터 노드들로 구성됩니다. 합의 프로토콜은 Move Virtual Machine()에서 실행될 때 트랜잭션들의 순서와 출력()에 동의합니다. 각 밸리데이터 노드는...",
    "content": "# Move on Aptos\n\n앱토스 블록체인은 합의 프로토콜을 실행하는 밸리데이터 노드들로 구성됩니다. 합의 프로토콜은 Move Virtual Machine(`MoveVM`)에서 실행될 때 트랜잭션들의 순서와 출력(`output`)에 동의합니다. 각 밸리데이터 노드는 현재 블록체인 원장 상태와 함께 트랜잭션을 VM에 대한 입력으로 변환합니다. `MoveVM은` 이 입력을 처리하여 변변경사항(`changeset`) 또는 스토리지 델타를 출력으로 생성합니다. 합의 프로토콜이 동의하고 출력이 커밋되면 이는 공개적으로 보이는 상태가 됩니다.\n\n## What is Move?\n\n`Move`는 희소성과 접근 제어를 강조하는, Web3를 위한 안전하고 보안이 뛰어난 프로그래밍 언어입니다. `Move`의 모든 자산은 리소스로 또는 리소스에 저장되어 표현될 수 있습니다. 희소성은 구조체(`struct`)를 복제할 수 없기 때문에 기본적으로 강제됩니다. 오직 바이트코드 계층에서 명시적으로 복사본(`copy`)으로 정의된 경우에만 복제할 수 있습니다.\n\n접근제어(`Access Control`)는 모듈(`module`) 접근 권한과 계정(`accounts`)의 개념에서 비롯됩니다. `Move`의 모듈은 자산을 생성(`create`), 저장(`store`), 전송(`transfer`)하는 라이브러리 또는 프로그램일 수 있습니다. `Move`는 오직 공개(`public`)모듈 기능(`functions`)들만 다른 모듈에서 접근할 수 있음을 보증합니다. 구조체가 `public`  생성자(`constructor`)를 가진 것이 아니라면, 오직 해당 구조체를 정의하고 있는 모듈에서만 생성될 수 있습니다. 마찬가지로, 구조체 내의 필드는 해당 모듈 내에서 또는 `public` `getter`또는 `setter`를 통해서만 액세스하고 변경할 수 있습니다.\n\n`Move`에서 트랜잭션의 전송자는 특정 `account`의 확인된 소유자인 `signer`로 표현합니다.  `signer`는 `Move`에서 가장 높은 권한 레벨을 가지고 있고 `account`에 `resources`를 추가할 수 있는 유일한 엔티티입니다. 게다가, 모듈 개발자는 `resource`에 접근하거나 계정에 저장된 자산을 수정하기 위해  `signer`가 있어야한다고 요구할 수 있습니다.\n\n## Comparison to other VMs\n\n| | Aptos/Move | Solana/SeaLevel | EVM |\n|--|------|------|------|\n| Data Storage | 소유자의 계정에 저장됨 Stored within the owner's account | 프로그램과 관련된 소유자 계정 내에 저장 Stored within the owner's account associated with a program | 컨트랙트와 관련된 계정내에 저장 Stored within the account associated with a smart contract |\n| Parallelization | 앱토스 내에서 런타임에 병렬화를 추론할 수 있음 Capable of inferring parallelization at runtime within Aptos | 트랜잭션 내에서 액세스하는 모든 계정 및 프로그램을 지정해야 함 Requires specifying within the transaction all accounts and programs accessed | 현재로선 직렬화 하지 않음 Currently serial nothing in production |\n| Transaction safety | Sequence number | 트랜잭션 고유성 + 트랜잭션 기억 Transaction uniqueness + remembering transactions | nonces, similar to sequence numbers |\n| Type safety | Module structs and generics| Program structs | Contract types |\n| Function calling | 제네릭이 아닌 정적 디스패치 Static dispatch not on generics | 정적 디스패치 Static dispatch | 동적 디스패치 Dynamic dispatch |\n\n## Aptos Move features\n\n`MoveVM`의 각 배포는 어댑터 레이어를 통해 코어 `MoveVM`을 추가 기능으로 확장할 수 있는 기능이 있습니다. 또한 `MoveVM`에는 컴퓨터가 `OS`를 가진 것 처럼 표준작업을 지원하는 프레임워크가 있습니다.\n\n`Aptos Move Adapter`에는 아래 기능들이 있습니다.\n- 계정에 저장된 데이터 양을 분리하여 계정과 관련된 트랜잭션의 가스 요금에 영향을 미치는 세분화된 스토리지 (`Fine grained storage`) \n- 크기에 맞게 계정 내에 키, 값 데이터를 저장할 수 있는 `Tables`\n- 사용자의 입력 없이 트랜잭션을 동시에 실행할 수 있는 `Block-STM`을 통한 병렬성\n\n`Aptos framework`는 여러 유용한 라이브러리를 포함합니다.\n- 스마트 계약을 게시하지 않고도 NFT 및 기타 풍부한 토큰을 생성할 수 있는 `Token standard`\n- 가벼운 모듈을 게시하여 안전한 유형의 코인 생성을 가능하게 하는 `Coin standard`\n- 스테이킹 및 위임 프레임워크\n- 주어진 유형의 주소, 모듈 및 구조체 이름을 런타임에 식별하는 `type_of` 서비스\n- 여러 `signer` 엔터티를 허용하는 `Multi-Signer` 프레임워크\n- 실제 현재 유닉스타임에 매핑되는 단조롭게 증가하는 시계를 제공하는 `timestamp` 서비스\n\n## Key Concepts in Aptos Move\n\n- 데이터는 모듈을 게시한 계정이 아닌 데이터를 소유한 계정 내에 저장되어야 합니다.\n- 데이터 흐름에는 생태계 유용성에 중점을 둔 최소한의 제약이 있어야 합니다.\n- 제네릭을 통한 런타임 안전보다 정적 유형 안전을 선호합니다.\n- 명시적으로 명확하지 않은 경우 `signer`는 자산을 계정에 추가하거나 제거할 수 있는 액세스를 제한해야 합니다.\n\n### Data ownership\n\n데이터는 모듈을 게시한 계정이 아니라 데이터를 소유한 계정에 저장해야 합니다.  \n\n`Solidity`에서 데이터는 계약을 생성한 계정의 네임스페이스 내에 저장됩니다. 일반적으로 이것은 값에 대한 주소의 맵 또는 소유자의 주소에 대한 인스턴스 ID의 맵으로 표시됩니다.  \n\n`Solana`에서 데이터는 계약과 관련된 별개의 계정에 저장됩니다.  \n\n`Move`에서 데이터는 모듈 소유자의 계정에 저장될 수 있지만, 이는 소유권 모호성 문제를 야기하며 다음 두 가지 문제를 암시한다:  \n1. 자산에 소유자와 연결된 리소스가 없기 때문에 소유권이 모호해집니다  \n2. 모듈 작성자는 임대, 회수 등과 같은 리소스의 수명에 대한 책임을 집니다  \n\n첫 번째로, 자산을 계정 내의 신뢰할 수 있는 리소스에 배치함으로써 소유자는 악의적으로 프로그래밍된 모듈조차도 해당 자산을 수정할 수 없도록 할 수 있습니다. `Move`에서는 표준 주문서 구조와 인터페이스를 프로그래밍하여 상단에 구축된 애플리케이션이 계정이나 주문서 항목에 백도어 액세스를 할 수 없도록 할 수 있습니다.  \n  \n다음 두 가지 `Coin` 보관 전략을 비교해 보십시오:  \n  \n다음은 인덱스로 표시된 소유권을 가진 단일 `Account`로 코인을 배치합니다.\n\n```move\nstruct CoinStore has key {\n\tcoins: table<address, Coin>,\n}\n```\n\n대신에  `Account` 에 `Coin`을 저장하는 접근법에서는 소유권을 명시적으로 할 수 있습니다.\n\n```move\nstruct CoinStore has key {\n    coin: Coin,\n}\n```\n\n### Data flow\n\n데이터 흐름은 생태계 유용성에 중점을 두고 최소한의 제약 조건을 가져야 합니다.\n\nMove에서 어떤 인터페이스도 해당 구조체(`struct`)를 값 형태로 나타내지 않도록 하고, 대신 모듈내에 정의된 데이터를 조작하기 위한 기능만을 제공하도록 할 수 있습니다(캡슐화, `encapsulation`). 이를 통해 자산이 오직 모듈 내에서만 접근 할 수 있도록 프로그래밍 할 수 있습니다. 이것은 직접적인 읽기와 쓰기 접근을 제한하여 다른 모듈과의 상호운용성이 차단됩니다. \n\n`Coin<T>`를 `input`으로 하고 `Ticket`을 `return`하는 컨트랙트가 있다고 가정해보겠습니다. 만약 `Coin<T>`를 모듈 내에서만 정의했고 외부로 내보낼 수 없는 경우, 해당 `Coin<T>`에 대한 요청들은 모듈이 정의한 항목으로만 제한됩니다.\n  \n입금 및 인출을 사용하여 코인 전송을 구현하는 다음의 두 가지 기능을 비교해 보십시오:\n\n```move\npublic fun transfer<T>(sender: &signer, recipient: address, amount: u64) {\n    let coin = withdraw(&sender, amount);\n    deposit(recipient, coin);\n}\n```\n\n모듈 외부에서 코인을 사용할 수 있는 범위는 다음과 같습니다.\n\n```move\nfun withdraw<T>(account: &signer, amount: u64): Coin<T>\nfun deposit<T>(account: address, coin: Coin<T>)\n```\n\n인출 및 입금에 `public accessors`를 추가하면 코인을 모듈 외부로 가져와 다른 모듈에서 사용하고 모듈로 반환할 수 있습니다.\n\n```move\npublic fun withdraw<T>(account: &signer, amount: u64): Coin<T>\npublic fun deposit<T>(account: address, coin: Coin<T>)\n```\n\n### Type-safety\n\n`Move`에서 `A`라고 하는 특정 구조체가 주어지면 서로 다른 인스턴스를 두 가지 방식으로 구분할 수 있습니다.\n\n- GUID와 같은 내부 식별자\n- `T`가 또 다른 구조체인 `A<T>`와 같은 제네릭\n\n내부 식별자는 단순하고 프로그래밍이 용이하기 때문에 편리할 수 있습니다. 그러나 제네릭은 명시적 컴파일 또는 유효성 검사 시간을 포함한 훨씬 더 높은 보장을 제공하지만 일부 비용이 발생합니다.\n\n제네릭은 이러한 유형을 예상하는 완전히 다른 유형과 리소스 및 인터페이스를 허용합니다. 예를 들어, 주문서에는 모든 주문에 대해 두 개의 통화가 예상되지만 그 중 하나는 수정되어야 한다고 명시할 수 있습니다. 예를 들어, `buy<T>(Coin: Coin<APT>): Coin<T>`. 이것은 사용자가 어떠한 `Coin<T>`도 구매할 수 있지만 `Coin<APT>`로 결제해야 한다는 것을 명시합니다.\n\n제네릭의 복잡성은 데이터를 `T`에 저장하는 것이 바람직할 때 발생합니다. `Move`는 제네릭에서 정적 디스패치를 지원하지 않으므로, `create<T>(...) : Coin<T>`와 같은 함수에서 T는 팬텀 유형이어야 합니다. 즉, Coin에서 유형 매개 변수로만 사용되거나 생성할 입력으로 지정되어야 합니다. 모든 `T`가 해당 함수를 구현하더라도 `T:: function`과 같이 사용될 수 없습니다.\n\n대량으로 생성될 수 있는 구조체의 경우 제네릭은 데이터 추적 및 이벤트 방출과 관련된 많은 새 저장소 및 리소스를 생성하는 결과를 낳습니다.\n\n이 때문에, 우리는 두 가지 \"토큰\" 표준을 만드는 어려운 선택을 했습니다. 하나는 `Coin`이라는 통화와 관련된 토큰에 대한 것이고, 다른 하나는 자산과 또는 NFT로 불리는 `Token`에 대한 것입니다. `Coin`은 제네릭을 통해 정적 유형의 안전성을 활용하지만 훨씬 단순한 계약입니다. `Token`은 자체 범용 식별자를 통해 동적 유형 안전을 활용하고 사용의 인체공학에 영향을 미치는 복잡성 때문에 일반론을 회피합니다\n\n### Data access\n\n- `signer`는 명시적으로 명확하지 않은 한 계정에 자산을 추가하거나 제거하는 액세스를 제한해야 합니다.\n\n`Move`에서 모듈은 계정 소유자 서명자의 존재에 관계없이 리소스에 액세스하는 방법과 리소스 내용을 수정하는 방법을 정의할 수 있습니다. 즉, 프로그래머가 실수로 다른 사용자의 계정에서 임의로 자산을 생성하거나 제거할 수 있는 리소스를 만들 수 있습니다.\n\nAptos Core Framework를 개발하면서 접근 권한을 허용 또는 방지한 위치에 대한 몇 가지 예가 있습니다:\n\n- A유저가 이미  `U Token`을 보유한 것이 아니라면, `U Token`은 A유저의 `account`에 직접 생성할 수 없습니다.\n- 접근 제어 목록을 효과적으로 사용해서  `TokenTransfers`가 유저가 명시적으로 다른 유저의 리소스의 토큰을 요구할 수 있도록 한다.\n- A유저가 `Coin<T>`을 저장할 `CoinStore<Coin<T>>`리소스를 가지고 있는 한, 어떤 유저든 A유저에게 `Coin<T>`을 직접 전송할 수 있습니다.\n\n`Token`에 대한 엄격한 노력이 부족하면 사용자가 다른 사용자 계정으로 직접 `Token`을 에어드롭할 수 있으며, 이는 사용자 계정에 추가 스토리지를 추가하고 처음 승인하지 않은 콘텐츠의 소유자가 됩니다.\n\n구체적인 예로, 인출 기능이 있는 이전 `Coin` 케이스로 돌아가십시오. 대신 `withdraw` 함수가 다음과 같이 정의된 경우(`signer` 인수가 없음에 유의하십시오):\n\n```move\npublic fun withdraw<T>(account: address, amount: u64): Coin<T>\n```\n\n누구나 계정에서 코인을 제거할 수 있습니다.\n\n### Resource accounts\n\n`Move` 모델은 종종 트랜잭션의 `signer`를 알아야 하기 때문에, Aptos는 `signer` 기능을 할당하기 위한 리소스 계정을 제공합니다. 리소스 계정을 생성하면 `signer` 기능에 액세스하여 자동으로 사용할 수 있습니다. `signer` 기능은 리소스 계정의 `signer`가 리소스 계정을 생성하거나 모듈의 로컬 저장소에 배치된 원본 계정의 주소와 함께 검색할 수 있습니다. `create_nft_with_resource_account.move`에서 `resource_signer_cap` 참조를 참조하십시오.\n\n리소스 계정을 생성할 때 해당 계정에 `signer` 기능도 부여합니다. `signer` 기능 내의 유일한 필드는 `signer`의 주소입니다. `signer` 기능에서 `signer`를 생성하는 방법을 보려면 `create_nft_with_resource_account.move`의 `let resource_signer` 함수를 검토하십시오.  \n  \n보안 침해를 방지하기 위해 모듈과 리소스 계정만 서명자 기능을 호출할 수 있습니다. 서명자 기능에서 서명자를 역으로 생성할 수 없으며, 대신 새 리소스 계정을 생성해야 합니다. 예를 들어 개인 키에서 서명자 기능을 생성할 수 없습니다.\n\n`signer` 취약성을 추가로 방지하려면 지갑 이벤트를 모니터링하고 다음 사항을 확인하십시오:  \n  \n- 공제되는 금액이 맞습니다.  \n- NFT 생성 이벤트가 있습니다.  \n- NFT 인출 이벤트가 없습니다.\n\nSee [resource accounts](https://aptos.dev/guides/resource-accounts) to learn more.\n\n### Coins\n\nAPT(Aptos Token)는 임의의 주소로 전송할 수 있으므로 존재하지 않는 주소에 대한 계정을 만들 수 있습니다. 예를 들어, USDC를 구입했으며 이를 APT로 변환하려고 합니다. 사용자를 보호하려면 해당 토큰을 수락해야 합니다.\n\n### Wrapping\n\n`Coin` 대신 밸런스를 직접 보관하는 이유는 무엇입니까? 우리는 당신이 `wrapper` 기능을 추가할 수 있도록 간접적으로 추가합니다.\n\n예를 들어, `Coin`에서 출금 및 입금 이벤트를 전송할 수 있습니다.\n\n하지만 에스크로의 경우 `Coin`을 보유하기 위한 이벤트도 방출할 수 있습니다.\n\n모듈 내에서 다른 구조체들을(`structure`)를 `destructure` 하여 간접적으로 잔액 대신 직접 `Coin`에서 동작시킬 수 있습니다.\n\n그것은 개별 구현에 달려 있습니다. 동일한 모듈에서 `Coin`과 밸런스를 모두 정의하는 경우 `destructure`를 통해 내부의 `Coin`에 대한 참조를 얻을 수 있으며 구조 자체에 대한 변경 가능한 참조를 얻을 수 있습니다. 대신 `Coin` 모듈에 의존하는 경우, 사용자에게 입금하기 위해 `Balance` 메서드를 사용하거나 `BalanceWithdraw` 메서드를 사용하여 실제 `Coin`을 얻어야 합니다. 함께 추가하려면 `CoinMerge를` 사용하십시오.\n\n### Generics\n\n사용자 지정 토큰과 Aptos 토큰 모두에 제네릭을 사용할 수 있습니다. Aptos가 제공하는 유일한 마법은 Aptos가 애그리게이터를 사용한다는 것입니다. 다른 코인 유형에는 아직 사용할 수 없습니다.\n\n### Visibility\n\n`Functions`는 `private`이 기본 값이므로 같은 파일에서만 호출될 수 있습니다. [`public`, `public(entry)`, etc.] 를 사용해서 파일 바깥에서 호출 하도록 할 수 있습니다.\n\n-   `entry` - 함수 호출을 실제 entry 함수로 만들어 분리합니다. 재중복성을 방지합니다(컴파일러 오류 발생)\n-   `public` - 누구나 어디서나 함수를 호출할 수 있습니다\n-   `public(entry)` - 관련 트랜잭션에 정의된 메서드만 함수를 호출할 수 있습니다\n-   `public(friend)` - 현재 모듈이 신뢰하는 모듈을 선언하는 데 사용됩니다.\n-   `public(script)` - Aptos 네트워크에서 임의 이동 코드를 제출, 컴파일 및 실행할 수 있습니다\n\n가능한 `public(entry)`이 아닌 `entry`를 사용하는 것을 권장하는데, 코드가 추가 개체(`object`)로 감싸이지 않도록 해주기 때문입니다.\n\n`Move`는 두 가지 방법으로 재진입을 방지합니다\n\n1.  동적 디스패치가 없는 경우 모듈 내의 다른 모듈을 호출하려면 해당 모듈에 명시적으로 의존해야 합니다. 따라서 다른 모듈은 사용자에게 의존해야 합니다.\n2.  순환 종속성은 허용되지 않습니다. 따라서 A가 B를 호출하고 B가 A에 상호 의존하면 모듈 B를 배포할 수 없습니다.\n\nFind out more about the Move programming language among the [Move Guides](https://aptos.dev/).",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 9,
    "wordCount": 1763,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "interact-with-the-move-vm",
    "slug": "interact-with-the-move-vm",
    "path": "blockchain/aptos",
    "fullPath": "blockchain/aptos/interact-with-the-move-vm",
    "title": "Interact with the Move VM",
    "excerpt": "Interact with the Move VM The Aptos blockchain uses the Move virtual machine (VM) for executing operations...",
    "content": "# Interact with the Move VM\n\nThe Aptos blockchain uses the [Move](https://github.com/move-language/move) virtual machine (VM) for executing operations. While many blockchains implement a set of\nnative operations, Aptos delegates all operations to Move, including: account creation, fund transfer and publishing Move modules.\nTo support these operations, blockchains built on top of Move must provide a framework (akin to\nan operating system for a computer or a minimal viable set of functions) for interacting with the blockchain. In this section, we discuss\nthese functions, exposed via the Aptos Framework's `script` functions.\n\nThis guide (in concert with the [Move module tutorial](../tutorials/first-move-module.md) ) will unlock the minimal amount of information required to start building rich applications on top of the Aptos blockchain. Note: the Aptos Framework is under heavy development and this document may not\nbe up to date. The most recent framework can be found in the [source code](https://github.com/aptos-labs/aptos-core/tree/main/aptos-move/framework).\n\nThe core functions provided to users within the Aptos Framework include:\n\n- Sending and receiving the network coin `Coin<AptosCoin>`\n- Creating a new account\n- Publishing a new Move module\n\nNote: this document assumes readers are already familiar with submitting transactions, as described in the [Your first transaction tutorial](../tutorials/first-transaction.md).\n\n## Sending and Receiving the network coin `Coin<AptosCoin>`\n\n`Coin<AptosCoin>` is required for paying gas fees when submitting and executing transactions. `Coin<AptosCoin>` can be obtained by calling the Devnet Faucet. See the [Your first transaction](../tutorials/first-transaction.md) tutorial for an example.\n\nThe payload for instructing the blockchain to perform a transfer is:\n\n```\n{\n  \"type\": \"entry_function_payload\",\n  \"function\": \"0x1::coin::transfer\",\n  \"type_arguments\": [\"0x1::aptos_coin::AptosCoin\"],\n  \"arguments\": [\n    \"0x737b36c96926043794ed3a0b3eaaceaf\",\n    \"1000\",\n  ]\n}\n```\n\nThis instructs the VM to execute the `script` `0x1::coin::transfer` with a type argument of 0x1::aptos_coin::AptosCoin. Type is required here as Coin is our standard module that can be used to create many types of Coins. See the [Your first coin tutorial](../tutorials/first-coin.md) for an example of creating a custom Coin. The first argument is the recipient address, `0x737b36c96926043794ed3a0b3eaaceaf`, and the second is the amount to transfer, `1000`. The sender address is the account\naddress that sent the transaction querying this `script`.\n\n## Creating a new account\n\nThe payload for instructing the blockchain to create a new account is:\n\n```\n{\n  \"type\": \"entry_function_payload\",\n  \"function\": \"0x1::aptos_account::create_account\",\n  \"type_arguments\": [],\n  \"arguments\": [\n    \"0x0c7e09cd9185a27104fa218a0b26ea88\",\n    \"0xaacf87ae9d8a5e523c7f1107c668cb28dec005933c4a3bf0465ffd8a9800a2d900\",\n  ]\n}\n```\n\nThis instructs the Move virtual machine to execute the `script` `0x1::aptos_account::create_account`. The first argument is the address of the account to create and the second is the authentication key pre-image (which is mentioned in [Accounts](../concepts/accounts.md). For single signer authentication, this is the public key concatenated with the `0` byte (or `pubkey_A | 0x00`). This is required to prevent account address land grabbing. The execution of this instruction verifies that the 32-bytes of the authentication key are the same as the 32-byte account address. We are actively working on improving this API to support taking in a 32-byte account address that would eliminate concerns around land grabbing or account manipulation.\n\n## Publishing a new Move module\n\nThe payload for publishing a new module is:\n\n```\n\"type\": \"module_bundle_payload\",\n\"modules\": [\n    {\"bytecode\": \"0x...\"},\n],\n```\n\nThis instructs the VM to publish the module bytecode under the sender's account. For a full-length tutorial, see [Your first move module](../tutorials/first-move-module.md).\n\nIt is important to note that the Move bytecode must specify the same address as the sender's account, otherwise the transaction will be rejected. For example, assuming account address `0xe110`, the Move module would need to be updated as such `module 0xe110::Message`, `module 0xbar::Message` would be rejected. Alternatively an aliased address could be used, such as `module HelloBlockchain::Message` but the `HelloBlockchain` alias would need to updated to `0xe110` in the `Move.toml` file. We are working with the Move team and planning on incorporating a compiler into our REST interface to mitigate this issue.\n\n[accounts]: /concepts/accounts\n[your-first-coin]: /tutorials/your-first-coin\n[your-first-move-module]: /tutorials/first-move-module\n[your-first-transaction]: /tutorials/your-first-transaction\n[move_url]: https://diem.github.io/move/\n[aptos_framework]: https://github.com/aptos-labs/aptos-core/tree/main/aptos-move/framework/aptos-framework/sources",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 4,
    "wordCount": 632,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "aptos-whitepaper",
    "slug": "aptos-whitepaper",
    "path": "blockchain/aptos",
    "fullPath": "blockchain/aptos/aptos-whitepaper",
    "title": "Aptos Whitepaper",
    "excerpt": "Aptos Whitepaper https://aptos.dev/aptos-white-paper/aptos-white-paper-in-korean...",
    "content": "# Aptos Whitepaper\n\nhttps://aptos.dev/aptos-white-paper/aptos-white-paper-in-korean",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 5,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "aptos-move-structure",
    "slug": "aptos-move-structure",
    "path": "blockchain/aptos",
    "fullPath": "blockchain/aptos/aptos-move-structure",
    "title": "Aptos Move Structure",
    "excerpt": "Aptos Move Structure 여러분의 Move 코드를 어떻게 구성하는게 좋은지 이해하는 시간을 가져봅시다 Move의 구조체는 함수 클래스 역할을 하는 Rust와 같은 다른 프로그래밍 언어의 구조체와 비슷합니다. 원하는 만큼 구조체에 많은 필드를 가질 수...",
    "content": "# Aptos Move Structure\n\n여러분의 Move 코드를 어떻게 구성하는게 좋은지 이해하는 시간을 가져봅시다\n\nMove의 구조체는 함수 클래스 역할을 하는 Rust와 같은 다른 프로그래밍 언어의 구조체와 비슷합니다. 원하는 만큼 구조체에 많은 필드를 가질 수 있지만 개체 지향 프로그래밍에서와 같이 구조체에 메서드를 가질 수는 없습니다. 마찬가지로 Move에는 상속이 없습니다. 대신 구조체를 다시 생성하려면 구조체를 복제해야 합니다.\n\n일단 게시되면 Move의 구조체 정의는 변경할 수 없습니다. 구조체 자체는 업그레이드할 수 없지만 해당 필드의 값은 변경될 수 있습니다. Move의 보안을 위해 구조체가 정의된 모듈만 구조체를 분해하거나 해당 속성에 액세스할 수 있습니다.\n\n## Abilities\n\nMove의 구조체(Structures)에는 해당 유형으로 수행할 수 있는 작업을 설명하는 다양한 기능이 부여될 수 있습니다. 다음과 같은 네 가지 기능이 있습니다.\n\n- copy: 복사할 수 있는 기능. 지리적 ID가 좋은 활용 사례가 될 것입니다. NFT는 이 기능을 가지고 있어서는 안 됩니다.  values of types with this ability to be copied. A geographic ID would be a good use case. NFTs should not have this ability.\n- drop: 팝/드롭할 수 있는 기능.\n- store: 글로벌 스토리지의 구조체 내부에 저장 또는 저장할 수 있는 기능.\n- key: 글로벌 스토리지 작업의 키 역할을 하는 유형입니다. 이 기능을 사용하면 값을 계정 내 최상위 항목으로 저장할 수 있습니다.\n\n## Global storage\n\nMove에서 각 계정은 주어진 유형의 리소스를 하나만 가질 수 있습니다. `Coin`예를 들어 Move의 계정은 하나의 유형 만 존재하는 해시맵과 유사하기 때문입니다 . 해시맵은 리소스 유형 또는 모듈 이름을 리소스 값에 매핑한 것입니다. 이것이 Aptos가 여러 코인과 토큰을 보유하기 위한 추상화를 제공하기 위해 `CoinStore`및 의 홀더 패턴을 제공하는 이유입니다. `TokenStore`이러한 홀더는 테이블을 포함하거나 저장을 위해 제네릭을 사용합니다.\n\nAptos 는 효율적인 상태 동기화 및 인증된 스토리지 읽기를 위해 [Merkle 트리 를 사용합니다.](https://aptos.dev/reference/glossary/#merkle-trees)\n\n## Signers\n\nAptos에서 서명자는 엄청나게 강력합니다. 구조체는 서명자 주소로 게시됩니다. 서명자는 트랜잭션에 서명하고 제출할 때 생성됩니다. 트랜잭션을 제출할 때 서명자는 기본적으로 첫 번째 매개변수입니다. 서명자는 자신의 구조체를 체인에 포함하는 데 동의했습니다. 서명자에게는 저장 또는 키 기능이 없고 복사 기능만 있습니다.\n\n## key\n\n다른 사용자가 서명자를 사용할 수 있도록 하기 위해 서명자는 리소스에 저장됩니다. `key` 기능을 통해 `type`은 `store` 기능이 있는 `Coin`과 같은 글로벌 스토리지 작업의 `key` 역할을 할 수 있습니다. `Balance`에는 `key`기능이 있으므로 계정 내 최상위 항목으로 `store`할 수 있습니다.\n\nAptos는 서명자를 저장하지 않고 서명자 기능을 저장합니다. 제한된 `native` 기능만 서명자 기능을 만들 수 있습니다. NFT를 `minting`하려면 컬렉션을 생성한 서명자에 대한 액세스 권한이 필요합니다. 이것이 동적 발행을 수행할 때 많은 사람들이 NFT를 미리 발행(`pre-mint`)하는 이유입니다. Aptos는 트랜잭션에 자율적으로 서명할 수 있는 리소스 계정을 제공합니다.\n\n## acquires\n\n유저가 구조체와 같은 글로벌 리소스를 사용할때마다 항상 먼저 이것을 획득(`acquire`)해야합니다. 예를 들어,  NFT 입출금시 `TokenStore`를 획득해야 합니다. 리소스를 획득하는 모듈 내부의 함수를 호출하는 다른 모듈의 함수가 있는 경우, 첫 번째 함수에 `acquires()`로 레이블을 지정할 필요가 없습니다.\n\n리소스가 계정 내부에 저장되므로 소유권이 명확해집니다. 계정은 리소스를 생성할 수 있는지 여부를 결정할 수 있습니다. 해당 리소스를 정의하는 모듈은 해당 구조체를 읽고 수정하는 권한을 가집니다. 따라서 해당 모듈 내부의 코드는 해당 구조체를 명시적으로 획득해야 합니다.\n\n그래도 Move에서 빌리거나 이동하는 모든 위치에서 자동으로 리소스를 획득하게 됩니다. 명확성을 위해 명시적 포함을 위해 취득을 사용하십시오. 마찬가지로 `exists()` 함수에는 `acquires()` 함수가 필요하지 않습니다.\n\n참고: 자신의 모듈에 정의된 구조체에서 모든 계정의 모듈 내 전역(`global`)을 빌릴 수 있습니다. 모듈 외부에서 전역을 빌릴 수 없습니다.\n\n## move_to\n\n그런 다음 `move_to` 함수를 서명자 및 계정에 대한 참조와 함께 사용하여 구조체를 계정으로 이동할 수 있습니다. 그 과정에서 우리는 가치가 있는 코인의 새로운 인스턴스를 생성합니다.\nYou may then use the `move_to` function along with a reference to signer and account to move the struct into an account. In the process, we create a new instance of coin with value.\n\n## Initialization\n\n`init_module`은 모듈이 배포될 때 자동으로 호출되고 실행됩니다.\nThe `init_module` automatically gets called and run when the module is published:\n\n```shell\n    fun init_module(resource_account: &signer) {\n        let resource_signer_cap = resource_account::retrieve_resource_account_cap(resource_account, @source_addr);\n        let resource_signer = account::create_signer_with_capability(&resource_signer_cap);\n```\n\n`mint_nft_ticket()` 함수는 콜렉션을 가져오고 토큰을 생성합니다.\nThe `mint_nft_ticket()` function gets a collection and creates a token.\n\nTokenData ID의 결과로, 함수는 모듈의 리소스 서명자를 사용하여 토큰을 NFT 수신자에 `mint`합니다.\n\nFor example:\n\n```shell\n    public entry fun mint_nft(receiver: &signer) acquires ModuleData {\n        let receiver_addr = signer::address_of(receiver);\n```\n\n## Signing\n\n모든 `entry fun`은 `&signer` 유형을 첫 번째 매개변수로 사용합니다. Move와 Aptos 모두 트랜잭션을 제출할 때마다 트랜잭션에 서명하는 개인 키가 자동으로 연결된 계정을 서명자의 첫 번째 매개변수로 만듭니다.\n\n서명자에서 주소로 이동할 수 있지만 일반적으로 그 반대는 아닙니다. 따라서 NFT를 청구할 때 아래 지침과 같이 생성자와 수신자의 개인 키가 모두 필요합니다.\n\n`init_module`에서 서명자는 항상 계약을 업로드하는 계정입니다. 이것은 다음과 결합됩니다.\n\n```move\n        token::create_collection(&resource_signer, collection, description, collection_uri, maximum_supply, mutate_setting);\n\n```\n\nThen:\n\n```move\n        signer_cap: account::SignerCapability,\n```\n\n서명자 기능을 사용하면 모듈이 자율적으로 서명할 수 있습니다. 자원 계정은 누구도 개인 키를 가져오지 못하도록 하며 전적으로 계약에 의해 제어됩니다.\n\n## Module data\n\n그런 다음 'ModuleData'가 초기화되고 서명자 기능이 있는 리소스 계정으로 `_moved_`됩니다.\n\n```shell\n        move_to(resource_account, ModuleData {\n```\n\n`mint_nft_ticket()` 함수에서 첫 번째 단계는 `ModuleData` 구조체를 차용하는 것입니다.\n\n```shell\n        let module_data = borrow_global_mut<ModuleData>(@mint_nft);\n```\n\n그런 다음 `ModuleData` 구조체의 서명자 기능에 대한 참조를 사용하여 `resource_signer`를 만듭니다.\n\n```shell\n        let resource_signer = account::create_signer_with_capability(&module_data.signer_cap);\n```\n\n이러한 방식으로 모듈에 이미 저장된 서명자 기능을 나중에 사용할 수 있습니다. 모듈과 해당 구조를 계정으로 이동하면 해당 계정과 연결된 Aptos Explorer에 표시됩니다.\n\n## Accounts\n\n예를 들어 NFT를 발행할 때 NFT는 계정 주소에 저장됩니다. 트랜잭션을 제출하면 트랜잭션에 서명합니다. `aptos init`를 실행하는 위치(아래)와 관련된 `.aptos/config.yaml`에서 계정 구성 정보를 찾으십시오.\n\n리소스 계정을 사용하면 트랜잭션 서명을 위임할 수 있습니다. 리소스 계정을 생성하여 동일한 계정의 새 리소스에 저장할 수 있고 자율적으로 트랜잭션에 서명할 수 있는 서명자 기능을 부여합니다. 아무도 자원 계정의 개인 키에 액세스할 수 없으므로 서명자 기능이 보호됩니다.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 5,
    "wordCount": 872,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "aptos-integration-guide",
    "slug": "aptos-integration-guide",
    "path": "blockchain/aptos",
    "fullPath": "blockchain/aptos/aptos-integration-guide",
    "title": "Aptos Integration Guide",
    "excerpt": "Aptos Integration Guide https://aptos.dev/guides/system-integrators-guide...",
    "content": "# Aptos Integration Guide\n\nhttps://aptos.dev/guides/system-integrators-guide",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 6,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "확장 가능한 실시간 WebSocket 서버 아키텍처 설계",
    "slug": "hwagjang-ganeunghan-silsigan-websocket-seobeo-akitegceo-seolgye",
    "path": "backend/websocket",
    "fullPath": "backend/websocket/hwagjang-ganeunghan-silsigan-websocket-seobeo-akitegceo-seolgye",
    "title": "확장 가능한 실시간 WebSocket 서버 아키텍처 설계",
    "excerpt": "확장 가능한 실시간 WebSocket 서버 아키텍처 설계",
    "content": "# 확장 가능한 실시간 WebSocket 서버 아키텍처 설계\n\n실시간 통신 환경에서 서버를 수평 확장(Horizontal Scaling)할 때 가장 큰 기술적 과제는 **'상태(State)의 중앙 집중화'**입니다. 특정 클라이언트가 어느 서버 인스턴스에 점유되어 있는지를 관리하고, 서버 간 메시지를 정확히 라우팅하는 설계가 핵심입니다.\n\n---\n\n### 1. 전체 시스템 아키텍처\n\n여러 대의 WebSocket 서버가 로드밸런서 하단에 배치되며, Redis를 공유 저장소 및 메시지 브로커로 활용하여 클러스터를 구성합니다.\n\n\n```mermaid\ngraph TD\n    C1[Client A] --> LB(\"Load Balancer\")\n    C2[Client B] --> LB\n    LB --> WS1[\"WS Server 1<br/>(ID: SVR_1)\"]\n    LB --> WS2[\"WS Server 2<br/>(ID: SVR_2)\"]\n    \n    subgraph Redis_Layer [Redis Instance / Cluster]\n        R_HASH[(\"Redis Hash<br/>(Session Store)\")]\n        R_PUBSUB((\"(Redis Pub/Sub<br/>(Message Broker)\"))\n        R_STREAM[\"Redis Streams<br/>(Event Log)\"]\n    end\n\n    WS1 <--> R_HASH\n    WS2 <--> R_HASH\n    WS1 <--> R_PUBSUB\n    WS2 <--> R_PUBSUB\n    WS1 -.-> R_STREAM\n    WS2 -.-> R_STREAM\n\n    style LB fill:#f5f5f5,stroke:#333\n    style Redis_Layer fill:#fff5f5,stroke:#ff0000,stroke-dasharray: 5 5\n```\n\n---\n\n### 2. Redis 컴포넌트별 역할 정의\n\n|**컴포넌트**|**핵심 기능**|**비고**|\n|---|---|---|\n|**Hash**|**전역 세션 관리:** `{UserId: ServerId}` 매핑 정보 저장|세션 위치 확인용|\n|**Pub/Sub**|**실시간 라우팅:** 서버 간 이벤트 전파 (Fire-and-forget)|낮은 지연 시간 우선|\n|**Streams**|**신뢰성 보장:** 메시지 영속화 및 재연결 시 누락 데이터 복구|At-least-once 전달 보장|\n\n---\n\n### 3. 핵심 운영 워크플로우\n\n#### ① 세션 등록 프로세스\n\n클라이언트가 핸드셰이크를 완료하면 Redis Hash에 세션 정보를 기록하여 전역 상태를 업데이트합니다.\n\n\n```mermaid\nsequenceDiagram\n    participant C as Client A\n    participant S as WS Server 1 (SVR_1)\n    participant R as Redis Hash\n\n    C->>S: WebSocket Upgrade\n    S->>R: HSET user:session \"User_A\" \"SVR_1\"\n    R-->>S: OK\n    S-->>C: 101 Switching Protocols\n```\n\n#### ② 서버 간 메시지 라우팅\n\n상대방이 다른 서버 인스턴스에 접속 중일 경우, 세션 정보를 조회한 후 해당 서버 전용 채널로 메시지를 발행합니다.\n\n\n```mermaid\nsequenceDiagram\n    participant S1 as Server 1 (User A)\n    participant RH as Redis Hash\n    participant RP as Redis Pub/Sub\n    participant S2 as Server 2 (User B)\n\n    S1->>RH: HGET user:session \"User_B\"\n    RH-->>S1: return \"SVR_2\"\n    S1->>RP: PUBLISH \"server:SVR_2\" \"{ payload }\"\n    Note over RP, S2: S2는 'server:SVR_2' 채널 구독 중\n    RP-->>S2: Message Received\n    S2->>S2: Local Session 매핑 후 클라이언트 전송\n```\n\n#### ③ Redis Streams를 이용한 데이터 복구\n\n네트워크 순단 시 클라이언트가 `Last-Event-ID`를 기반으로 누락된 메시지를 요청하는 흐름입니다.\n\n\n```mermaid\nflowchart TD\n    Start([클라이언트 재연결]) --> Request[Last-Event-ID 포함 요청 전송]\n    Request --> Check{ID 유효성 검사}\n    \n    Check -- \"유효\" --> Read[XREAD: Last-ID 이후 데이터 인출]\n    Check -- \"만료/없음\" --> Default[XREAD: 최신 데이터 $ 부터 구독]\n    \n    Read --> Push[WebSocket으로 누락분 Push]\n    Push --> RealTime[실시간 모드 전환]\n    Default --> RealTime\n\n    style Start fill:#f9f,stroke:#333\n    style RealTime fill:#ccf,stroke:#333\n```\n\n---\n\n### 4. 설계 및 운영 가이드라인\n\n- **좀비 세션(Zombie Session) 방지:** 서버 비정상 종료 시 세션 정보가 남지 않도록 세션 키에 **TTL**을 설정하고, **Heartbeat(Ping/Pong)** 시점에 수시로 갱신해야 합니다.\n    \n- **Graceful Shutdown:** 배포 시 서버 인스턴스가 종료 신호(`SIGTERM`)를 받으면, 관리 중인 모든 세션을 Redis Hash에서 삭제(`HDEL`)한 후 프로세스를 종료해야 합니다.\n    \n- **Backpressure 제어:** 클라이언트의 수신 속도가 느릴 경우 서버 메모리 부하가 발생할 수 있습니다. Redis Streams의 **Consumer Group** 기능을 활용해 처리량을 조절하는 설계를 검토하십시오.",
    "docType": "original",
    "category": "Backend_Websocket",
    "tags": [],
    "readingTime": 3,
    "wordCount": 453,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "데코레이터-decorator",
    "slug": "dekoreiteo-decorator",
    "path": "backend/patterns",
    "fullPath": "backend/patterns/dekoreiteo-decorator",
    "title": "데코레이터 (Decorator)",
    "excerpt": "데코레이터 (Decorator) 정의 데코레이터는 클래스 선언, 메서드, 접근자, 프로퍼티, 또는 매개변수에 첨부할 수 있는 특수한 종류의 선언입니다. 사용 형식은 과 같으며, 여기서 은 데코레이팅 된 선언...",
    "content": "# 데코레이터 (Decorator)\n\n## 정의\n\n데코레이터는 클래스 선언, 메서드, 접근자, 프로퍼티, 또는 매개변수에 첨부할 수 있는 특수한 종류의 선언입니다.  사용 형식은 `@expression`과 같으며, 여기서 `@expression`은 데코레이팅 된 선언에 대한 정보와 함께 런타임에 호출되는 함수여야 합니다. 데코레이터는 클래스와 클래스 멤버에 어노테이션과 메타-프로그래밍 구문을 추가할 수 있는 방법을 제공합니다.\n\n## 데코레이터 팩토리 (Decorator Factories)\n\n```typescript\nfunction color(value: string) { // 데코레이터 팩토리\n\treturn function (target) { // 데코레이터\n\t\t// 'target'과 'value' 변수를 가지고 무언가를 수행합니다\n\t\t...\n\t}\n}\n\n# Reference\n- https://www.typescriptlang.org/docs/handbook/decorators.html#handbook-content\n```\n\n## 데코레이터 합성 (Decorator Composition)\n\nTypeScript에서 단일 선언에서 여러 데코레이터를 사용할 때 다음 단계가 수행됩니다.\n\n1.  각 데코레이터의 표현은 위에서 아래로 평가됩니다.\n2.  그런 다음 결과는 아래에서 위로 함수로 호출됩니다.\n\n여러 데코레이터가 단일 선언에 적용되는 경우 예시를 보여드리겠습니다.\n\n```typescript\nfunction first() {  \n  console.log('first(): factory evaluated');  \n  return (target: any, propertyKey: string, descriptor: PropertyDescriptor) => {  \n    console.log('first(): called');  \n  };  \n}  \n  \nfunction second() {  \n  console.log('second(): factory evaluated');  \n  return (target: any, propertyKey: string, descriptor: PropertyDescriptor) => {  \n    console.log('second(): called');  \n  };  \n}  \n  \nclass ExampleClass {  \n  @first()  \n  @second()  \n  method() {}  \n}\n\n# Reference\n- https://www.typescriptlang.org/docs/handbook/decorators.html#handbook-content\n```\n위 `ExampleClass`의  `method` 를 호출하면 아래 결과를 출력하게 됩니다\n```\nfirst(): factory evaluated\nsecond(): factory evaluated\nsecond(): called\nfirst(): called\n```\n\n## 데코레이터 평가 (Decorator Evaluation)\n\n클래스에서 다양한 선언에 데코레이터를 적용하는 방법은 다음과 같이 잘 정의되어 있습니다.\n1. _Parameter Decorators_, followed by _Method_, _Accessor_, or _Property Decorators_ are applied for each instance member.\n2.  _Parameter Decorators_, followed by _Method_, _Accessor_, or _Property Decorators_ are applied for each static member.\n3.  _Parameter Decorators_ are applied for the constructor.\n4.  _Class Decorators_ are applied for the class.\n\n## 클래스 데코레이터 (Class Decorators)\n\n클래스 데코레이터는 클래스 선언 직전에 선언되며, 해당 클래스의 생성자에 적용됩니다.\n```typescript\n@sealed\nclass BugReport {\n  type = \"report\";\n  title: string;\n  \n  constructor(t: string) {\n    this.title = t;\n  }\n}\n\nfunction sealed(constructor: Function) {\n  Object.seal(constructor);\n  Object.seal(constructor.prototype);\n}\n```\n\n```typescript\nfunction reportableClassDecorator<T extends { new (...args: any[]): {} }>(  \n  constructor: T,  \n) {  \n  return class extends constructor {  \n    reportingURL = 'http://www...';  \n  };  \n}  \n  \n@reportableClassDecorator  \nclass BugReport {  \n  type = 'report';  \n  title: string;  \n  \n  constructor(t: string) {  \n    this.title = t;  \n  }  \n}  \n  \nconst bug = new BugReport('Needs dark mode');  \nconsole.log(bug.title); // Prints \"Needs dark mode\"  \nconsole.log(bug.type); // Prints \"report\"  \n  \n// Note that the decorator *does not* change the TypeScript type  \n// and so the new property `reportingURL` is not known  \n// to the type system:  \nbug.reportingURL;\n\nProperty 'reportingURL' does not exist on type 'BugReport'.\n```\n\n## 메소드 데코레이터 (Method Decorators)\n\n메서드 데코레이터는 메서드 선언 직전에 선언됩니다. 데코레이터는 메서드의 프로퍼티 설명자(Property Descriptor) 에 적용되며 메서드 정의를 관찰, 수정 또는 대체하는 데 사용할 수 있습니다. 메서드 데코레이터는 선언 파일, 오버로드 또는 기타 주변 컨텍스트(예: `선언` 클래스)에서 사용할 수 없습니다.\n\n메서드 데코레이터의 표현식은 런타임에 다음 세 개의 인수와 함께 함수로 호출됩니다:\n\n1.  정적 멤버에 대한 클래스의 생성자 함수 또는 인스턴스 멤버에 대한 클래스의 프로토타입입니다.\n2.  멤버의 이름\n3.  멤버의 _프로퍼티 설명자_\n\n> 참고  스크립트 대상이 ‘ES5’보다 낮은 경우 _프로퍼티 설명자_ 는 ‘undefined’이 됩니다.\n\n메서드 데코레이터가 값을 반환하면, 메서드의 _프로퍼티 설명자_ 로 사용됩니다.\n\n> 참고  스크립트 대상이 ‘ES5’보다 낮은 경우 반환 값은 무시됩니다.\n\n```typescript\nfunction printClassName(enabled: boolean) {  \n  return function (  \n    target: any,  \n    propertyKey: string,  \n    descriptor: PropertyDescriptor,  \n  ) {  \n    if (enabled) {  \n      console.log(target.constructor.name);  \n    }  \n  };  \n}  \n  \nclass Person {  \n  msg: string;  \n  constructor(msg: string) {  \n    this.msg = msg;  \n  }  \n  @printClassName(true)  \n  sayMessage() {  \n    return this.msg;  \n  }  \n}\n```\n\n## 접근자 데코레이터 (Accessor Decorators)\n\n접근자 데코레이터는 접근자 선언 바로 전에 선언됩니다. 접근자 데코레이터는 접근자의 _프로퍼티 설명자_에 적용되며 접근자의 정의를 관찰, 수정 또는 교체하는 데 사용할 수 있습니다. 접근자 데코레이터는 선언 파일이나 다른 주변 컨텍스트(예: `선언` 클래스)에서 사용할 수 없습니다.\n\n> 참고  TypeScript는 단일 멤버에 대해 `get` 및 `set` 접근자를 데코레이팅 할 수 없습니다. 대신 멤버의 모든 데코레이터를 문서 순서대로 지정된 첫 번째 접근자에 적용해야 합니다. 왜냐하면, 데코레이터는 각각의 선언이 아닌 `get`과 `set` 접근자를 결합한 프로퍼티 설명자에 적용되기 때문입니다.\n\n접근자 데코레이터의 표현 식은 런타임에 다음 세 가지 인수와 함께 함수로 호출됩니다:\n\n1.  정적 멤버에 대한 클래스의 생성자 함수 또는 인스턴스 멤버에 대한 클래스의 프로토타입\n2.  멤버의 이름\n3.  멤버의 _프로퍼티 설명자_\n\n> 참고  스크립트 대상이 ‘ES5’보다 낮은 경우 프로퍼티 설명자는 `undefined`가 됩니다.\n\n접근자 데코레이터가 값을 반환하면 멤버의 프로퍼티 설명자로 사용됩니다.\n\n> 참고  스크립트 대상이 ‘ES5’보다 낮은 경우 반환 값은 무시됩니다.\n\n## 프로퍼티 데코레이터 (Property Decorators)\n\n프로퍼티 데코레이터는 프로퍼티 선언 바로 전에 선언됩니다. 프로퍼티 데코레이터는 선언 파일이나 다른 주변 컨텍스트(예: `선언` 클래스)에서 사용할 수 없습니다.\n\n프로퍼티 데코레이터의 표현 식은 런타임에 다음 두 개의 인수와 함께 함수로 호출됩니다:\n\n1.  정적 멤버에 대한 클래스의 생성자 함수 또는 인스턴스 멤버에 대한 클래스의 프로토타입\n2.  멤버의 이름\n\n> 참고  TypeScript에서 `프로퍼티 데코레이터`가 초기화되는 방식으로 인해 프로퍼티 설명자가 프로퍼티 데코레이터에 대한 인수로 제공되지 않습니다. 현재 프로토타입의 멤버를 정의할 때 인스턴스 프로퍼티를 설명하는 메커니즘이 없고 프로퍼티의 이니셜라이저를 관찰하거나 수정할 수 있는 방법이 없기 때문입니다. 반환 값도 무시됩니다. 따라서 프로퍼티 데코레이터는 특정 이름의 프로퍼티가 클래스에 선언되었음을 관찰하는 데만 사용할 수 있습니다.\n\n이 정보를 사용하여 다음 예와 같이 프로퍼티에 대한 메타데이터를 기록할 수 있습니다:\n\n```typescript\nclass Greeter {  \n  @format(\"Hello, %s\")  \n  greeting: string;  \n  \n  constructor(message: string) {  \n    this.greeting = message;  \n  }  \n  \n  greet() {  \n    let formatString = getFormat(this, \"greeting\");  \n    return formatString.replace(\"%s\", this.greeting);  \n  }  \n}\n```\n\n다음 함수 선언을 사용하여 `@format` 데코레이터와 `getFormat` 함수를 정의 할 수 있습니다:\n\n```typescript\nimport \"reflect-metadata\";  \n\nconst formatMetadataKey = Symbol(\"format\");  \n\nfunction format(formatString: string) {  \n  return Reflect.metadata(formatMetadataKey, formatString);  \n}  \n\nfunction getFormat(target: any, propertyKey: string) {  \n  return Reflect.getMetadata(formatMetadataKey, target, propertyKey);  \n}\n```\n\n`@format(\"Hello, %s\")` 데코레이터는 [데코레이터 팩토리](https://www.typescriptlang.org/ko/docs/handbook/decorators.html#%EB%8D%B0%EC%BD%94%EB%A0%88%EC%9D%B4%ED%84%B0-%ED%8C%A9%ED%86%A0%EB%A6%AC-Decorator-Factories)입니다. `@format(\"Hello, %s\")`가 호출되면 `reflect-metadata` 라이브러리의 `Reflect.metadata` 함수를 사용하여 프로퍼티에 대한 메타데이터 항목을 추가합니다. `getFormat`이 호출되면 형식의 메타데이터 값을 읽습니다.\n\n> 참고  이 예제에는 `reflect-metadata` 라이브러리가 필요합니다. `reflect-metadata` 라이브러리에 대한 자세한 내용은 [메타데이터](https://www.typescriptlang.org/ko/docs/handbook/decorators.html#%EB%A9%94%ED%83%80%EB%8D%B0%EC%9D%B4%ED%84%B0-metadata)를 참조하십시오.\n\n\n## 메타데이터 (Metadata)\n\n일부 예제는 [실험적 메타데이터 API](https://github.com/rbuckton/ReflectDecorators)에 대한 폴리필(polyfill)을 추가하는 `reflect-metadata` 라이브러리를 사용합니다. 이 라이브러리는 아직 ECMAScript (JavaScript) 표준의 일부가 아닙니다. 그러나 데코레이터가 공식적으로 ECMAScript 표준의 일부로 채택되면 이러한 확장을 채택하게 될 것입니다.\n\nnpm을 통해 설치할 수 있습니다.\n\n`   npm i reflect-metadata --save   `\n\nTypeScript에는 데코레이터가 있는 선언에 대해 특정 타입의 메타 데이터를 내보내는 실험적인 지원을 포함합니다. 이 실험적인 지원을 가능하게 하려면, 명령행 또는`tsconfig.json`에서 `emitDecoratorMetadata` 컴파일러 옵션을 설정해야 합니다.\n\n**명령줄**:\n\n`tsc --target ES5 --experimentalDecorators --emitDecoratorMetadata`\n\n**tsconfig.json**:\n\n```typescript\n{  \n  \"compilerOptions\": {  \n    \"target\": \"ES5\",  \n    \"experimentalDecorators\": true,  \n    \"emitDecoratorMetadata\": true  \n  }  \n}\n```\n\n활성화되면 `reflect-metadata`라이브러리를 가져오기만 하면 추가 디자인-타임 타입 정보가 런타임에 사용 가능합니다.\n\n다음 예제에서 이를 확인할 수 있습니다.\n\n```typescript\nimport 'reflect-metadata';  \n  \nclass Point {  \n  constructor(public x: number, public y: number) {}  \n}  \n  \nclass Line {  \n  private _start: Point;  \n  private _end: Point;  \n  \n  @validate  \n  set start(value: Point) {  \n    this._start = value;  \n  }  \n  \n  get start() {  \n    return this._start;  \n  }  \n  \n  @validate  \n  set end(value: Point) {  \n    this._end = value;  \n  }  \n  \n  get end() {  \n    return this._end;  \n  }  \n}  \n  \nfunction validate<T>(  \n  target: any,  \n  propertyKey: string,  \n  descriptor: TypedPropertyDescriptor<T>,  \n) {  \n  let set = descriptor.set!;  \n  \n  descriptor.set = function (value: T) {  \n    let type = Reflect.getMetadata('design:type', target, propertyKey);  \n  \n    if (!(value instanceof type)) {  \n      throw new TypeError(  \n        `Invalid type, got ${typeof value} not ${type.name}.`,  \n      );  \n    }  \n  \n    set.call(this, value);  \n  };  \n}  \n  \nconst line = new Line();  \nline.start = new Point(0, 0);  \n  \n// @ts-ignore  \n// line.end = {}  \n  \n// Fails at runtime with:  \n// > Invalid type, got object not Point\n```\n\nTypeScript 컴파일러는 `@Reflect.metadata` 데코레이터를 사용하여 디자인-타임 타입 정보를 주입합니다. 다음 TypeScript와 동일하다고 생각할 수 있습니다.\n\n```typescript\nclass Line {  \n  private _start: Point;  \n  private _end: Point;  \n  @validate  \n  @Reflect.metadata('design:type', Point)  \n  set start(value: Point) {  \n    this._start = value;  \n  }  \n  get start() {  \n    return this._start;  \n  }  \n  @validate  \n  @Reflect.metadata('design:type', Point)  \n  set end(value: Point) {  \n    this._end = value;  \n  }  \n  get end() {  \n    return this._end;  \n  }  \n}\n```",
    "docType": "original",
    "category": "Research",
    "tags": [],
    "readingTime": 6,
    "wordCount": 1199,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "개체-지향-프로그래밍-object-oriented-programming",
    "slug": "gaece-jihyang-peurogeuraeming-object-oriented-programming",
    "path": "backend/patterns",
    "fullPath": "backend/patterns/gaece-jihyang-peurogeuraeming-object-oriented-programming",
    "title": "개체 지향 프로그래밍 (Object Oriented Programming)",
    "excerpt": "개체 지향 프로그래밍 (Object Oriented Programming) 개체 지향 프로그래밍 언어의 네가지 기본 원칙 추상화 - 관련 특성 및 엔터티의 상호 작용을 클래스로 모델링하여 시스템의 추상적 표현을 정의합니다. 캡슐화 - 개체의 내부...",
    "content": "# 개체 지향 프로그래밍 (Object Oriented Programming)\n\n## 개체 지향 프로그래밍 언어의 네가지 기본 원칙\n\n1. 추상화 - 관련 특성 및 엔터티의 상호 작용을 클래스로 모델링하여 시스템의 추상적 표현을 정의합니다.\n2. 캡슐화 - 개체의 내부 상태와 기능을 숨기고 public 함수 세트를 통해서만 개체에 엑세스할 수 있습니다.\n3. 상속 - 기존 추상화를 기반으로 새 추상화를 만들 수 있습니다.\n4. 다형성 - 여러 추상화 에서 다양한 방법으로 상속된 속성 또는 메서드를 구현할 수 있습니다.\n\n## C#의 클래스, 구조체 및 레코드 개요\n\n### 캡슐화\n\n캡슐화는 경우에 따라 개체 지향 프로그래밍의 첫 번째 원래로 인식됩니다. 클래스 또는 구조체는 그것의 외부의 코드에 각 멤버가 엑세스하는 방법을 지정할 수 있습니다. 클래스 또는 어셈블리 외부에서 사용하지 않으려는 메서드 및 변수를 숨겨 코딩 오류 또는 악의적인 악용 가능성을 제한할 수 있습니다.\n\n### 멤버\n\n멤버는 모든 메서드, 필드, 상수, 속성, 이벤트를 포함합니다. 다른 언어에는 있지만 C#에는 전역 변수 또는 메서드가 없습니다. 프로그램의 진입점인 `Main`메서드까지도 클래스나 구조체 내에 선언되어야 합니다.\n\n## 은행 계좌 예제를 통한 개체지향 프로그래밍 살펴보기\n\n### 은행 계좌의 기능\n\n1. 은행계좌를 고유하게 식별하는 10자리 숫자가 있습니다.\n2. 소유자의 이름을 저장하는 문자열이 있습니다.\n3. 잔액을 검사할 수 있습니다.\n4. 예금을 허용합니다.\n5. 인출을 허용합니다.\n6. 초기 잔액은 양수여야 합니다.\n7. 인출은 마이너스 잔액을 초래할 수 없습니다.\n\n### 은행 계좌 형식 정리\n\n```cs\nnamespace Classes;\n\npublic class BankAccount {\n  public string Number { get; }\n  public string Owner { get; set; }\n  public decimal Balance { get; }\n\n  public void MakeDeposit(decimal amount, DateTime date, string note)\n  {\n  }\n\n  public vode MakeWithdraw(decimal amount, DateTime date, string note)\n  {\n  }\n}\n```\n\n`BankAccount` 클래스는 다섯 개의 멤버를 가집니다. 앞의 세가지는 속성인데, 속성은 데이터 요소이며 유효성 검사 또는 기타 규칙을 적용하는 코드가 있을 수 있습니다. 나머지 두 가지는 메서드입니다. 메서드는 단일 함수를 수행하는 코드 블록입니다. 클래스의 각 멤버는 그것의 이름을 읽으면 타인이 해당 클래스가 수행하는 작업을 이해하기에 충분한 정보를 제공해야 합니다.\n\n### 새 계좌 개설 (생성자)\n\n`BankAccount` 형식의 새 개체를 만드는 것은 해당 값을 할당하는 생성자를 정의함을 의미합니다. C#에서 생성자는 클래스와 이름이 같은 멤버입니다.\n\n```cs\nnamespace Classes;\n\npublic class BankAccount {\n  private static int accountNumberSeed = 1234567890;\n  public string Number { get; }\n  public string Owner { get; set; }\n  public decimal Balance { get; }\n\n  public BankAccount(string, name, decimal initialBalance)\n  {\n    // C#는 this 한정자가 선택사항이며 일반적으로 생략할 수 있습니다.\n    // this.Owner = name;\n    // this.Balance = initialBalance;\n    Owner = name;\n    Balance = initialBalance;\n    // 계좌 번호\n    Number = accountNumberSeed.ToString();\n    accountNumberSeed++;\n  }\n\n  public void MakeDeposit(decimal amount, DateTime date, string note)\n  {\n  }\n\n  public vode MakeWithdraw(decimal amount, DateTime date, string note)\n  {\n  }\n}\n```\n\n생성자는 new를 사용하여 개체를 만들 때 호출됩니다.\n\n```cs\nusing Classes;\n\nvar account = new BankAccount(\"<name>\", 1000);\nConsole.WriteLine($\"Account {account.Number} was created for {account.Owner} with {account.Balance} initial balance.\");\n```\n\n### 예금 및 인출 만들기\n\n은행 계좌 클래스의 예금과 인출을 구현해보도록 하겠습니다. 먼저 트랜잭션을 나타내는 새 클래스를 생성합니다.\n그리고 Transaction 개체의 `List<T>`를 `BankAccount`클래스에 추가하고 Balance 속성을 업데이트 합니다.\n다음으로 `MakeDeposit`, `MakeWithdrawal`메서드를 구현합니다.\n그 뒤에는 생성자에서 최초 입금 건을 추가해줄 수 있도록 `MakeDeposit`을 호출하게 해보겠습니다.\n\n```cs\nnamespace Classses\n\npublic class Transaction\n{\n  public decimal Amount { get; }\n  public DateTime Date { get; }\n  public string Notes { get; }\n\n  public Transaction(decimal amount, DateTime date, string note)\n  {\n    Amount = amount;\n    Date = date;\n    Notes = note;\n  }\n}\n\npublic class BankAccount {\n  private static int accountNumberSeed = 1234567890;\n  private List<Transaction> allTransactions = new List<Transaction>();\n\n  public string Number { get; }\n  public string Owner { get; set; }\n  public decimal Balance\n  {\n     get\n     {\n        decimal balance = 0;\n        foreach (var item in allTransactions)\n        {\n          balanace += item.Amount;\n        }\n        return balance;\n     }\n  }\n\n  public BankAccount(string, name, decimal initialBalance)\n  {\n    // C#는 this 한정자가 선택사항이며 일반적으로 생략할 수 있습니다.\n    // this.Owner = name;\n    // this.Balance = initialBalance;\n    Owner = name;\n    MakeDeposit(initialBalance, DateTime.Now, \"Initial balance\");\n    // 계좌 번호\n    Number = accountNumberSeed.ToString();\n    accountNumberSeed++;\n  }\n\n  public void MakeDeposit(decimal amount, DateTime date, string note)\n  {\n    if (amount <= 0)\n    {\n      throw new ArgumentOutOfRangeException(nameof(amount), \"Amount of deposit must be positive\");\n    }\n    var deposit = new Transaction(amount, date, note);\n    allTransactions.add(deposit);\n  }\n\n  public vode MakeWithdraw(decimal amount, DateTime date, string note)\n  {\n    if (amount <= 0)\n    {\n      throw new ArgumentOutOfRangeException(nameof(amount), \"Amount of withdrawal must be positive\");\n    }\n    if (Balance - amount < 0)\n    {\n      throw new InvalidOperationException(\"Not sufficient funds for this withdrawal\");\n    }\n    var deposit = new Transaction(-amount, date, note);\n    allTransactions.add(deposit);\n  }\n}\n```\n\n## Ref\n\n- [클래스 및 개체를 사용한 개체 지향 프로그래밍 살펴보기](https://learn.microsoft.com/ko-kr/dotnet/csharp/fundamentals/tutorials/classes)\n- [개체 지향 프로그래밍(C#)](https://learn.microsoft.com/ko-kr/dotnet/csharp/fundamentals/tutorials/oop)\n- [C#의 클래스, 구조체 및 레코드 개요](https://learn.microsoft.com/ko-kr/dotnet/csharp/fundamentals/object-oriented/)",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [],
    "readingTime": 4,
    "wordCount": 726,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "개체-vs-객체",
    "slug": "gaece-vs-gaegce",
    "path": "backend/patterns",
    "fullPath": "backend/patterns/gaece-vs-gaegce",
    "title": "개체 vs 객체",
    "excerpt": "개체 vs 객체 또는 라는 용어는 개발자, 특히 백엔드 개발자 사이에서 빼놓을 수 없다. 매체에 따라서 개체 또는 객체라고 표현하는 는 무엇일까? 객체 철학 의사나 행위가 미치는 대상. 언어 문장 내에서 동사의 행...",
    "content": "# 개체 vs 객체\n\n`개체` 또는 `객체`라는 용어는 개발자, 특히 백엔드 개발자 사이에서 빼놓을 수 없다. 매체에 따라서 개체 또는 객체라고 표현하는 `Object`는 무엇일까?\n\n## 객체\n\n1. 철학 의사나 행위가 미치는 대상.\n2. 언어 문장 내에서 동사의 행위가 미치는 대상.\n3. 철학 작용의 대상이 되는 쪽.\n\n## 개체\n\n1. 전체나 집단에 상대하여 하나하나의 낱개를 이르는 말.\n2. 생명 하나의 독립된 생물체. 살아가는 데에 필요한 독립적인 기능을 갖고 있다.\n3. 철학 단일하고 독립적인 통일적 존재. 철학 사상의 발전 과정에서 이 통일성은 물질적ㆍ양적 측면, 또는 정신적ㆍ질적 측면 따위의 여러 관점에서 고찰되었다.\n\n## 나만의 정리\n\n개체가 맞는 것 같다. 객체는 개체보다 수동적인 개념으로 이해할 수 있는데, 실제 클래스들은 서로 다른 클래스들과 상호작용을 하기 때문에 영향을 받는 것 뿐만 아니라 영향을 주기도 하기 때문이다.\n\n## Ref\n\n- [개체지향프로그래밍(C#)](https://learn.microsoft.com/ko-kr/dotnet/csharp/fundamentals/tutorials/oop)\n- [객체 정의](https://ko.dict.naver.com/#/entry/koko/9ef345f0a0884eca89d5c3155653fe94)\n- [개체 정의](https://ko.dict.naver.com/#/entry/koko/1f8d5f43afa2434f8d56f9ced1231c60)",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [],
    "readingTime": 1,
    "wordCount": 131,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "solid-원칙",
    "slug": "solid-weoncig",
    "path": "backend/patterns",
    "fullPath": "backend/patterns/solid-weoncig",
    "title": "SOLID 원칙",
    "excerpt": "SOLID 원칙 Ref https://ko.wikipedia.org/wiki/SOLID_(%EA%B0%9D%EC%B2%B4_%EC%A7%80%ED%96%A5_%EC%84%A4%EA%B3%84)...",
    "content": "# SOLID 원칙\n\n## Ref\n\nhttps://ko.wikipedia.org/wiki/SOLID_(%EA%B0%9D%EC%B2%B4_%EC%A7%80%ED%96%A5_%EC%84%A4%EA%B3%84)",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [],
    "readingTime": 1,
    "wordCount": 7,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "데코레이터를-이용한-nest-js에서의-aop-적용",
    "slug": "dekoreiteoreul-iyonghan-nest-jseseoyi-aop-jeogyong",
    "path": "backend/nestjs",
    "fullPath": "backend/nestjs/dekoreiteoreul-iyonghan-nest-jseseoyi-aop-jeogyong",
    "title": "데코레이터를 이용한 Nest.js에서의 AOP 적용",
    "excerpt": "데코레이터를 이용한 Nest.js에서의 AOP 적용 데코레이터? [데코레이터](<obsidian://open?vault=seogyugim.coinone&file=Typescript%2F%EB%8D%B0%EC%BD%94%EB%A0%88%EC%9D%B4%ED%...",
    "content": "# 데코레이터를 이용한 Nest.js에서의 AOP 적용\n\n## 데코레이터?\n\n[데코레이터](<obsidian://open?vault=seogyugim.coinone&file=Typescript%2F%EB%8D%B0%EC%BD%94%EB%A0%88%EC%9D%B4%ED%84%B0%20(Decorator)>) 문서를 참조해주세요!",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [
      "Backend",
      "NestJS"
    ],
    "readingTime": 1,
    "wordCount": 12,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "nestjs-dynamic-module-주의점",
    "slug": "nestjs-dynamic-module-juyijeom",
    "path": "backend/nestjs",
    "fullPath": "backend/nestjs/nestjs-dynamic-module-juyijeom",
    "title": "NestJS Dynamic Module 주의점",
    "excerpt": "NestJS Dynamic Module 주의점 ```typescript export const databaseProviders = [ { provide: MysqlDatasourceKey, inject: [ConfigService, MysqlConfigS...",
    "content": "# NestJS Dynamic Module 주의점\n\n```typescript\nexport const databaseProviders = [\n\t{\n\t\tprovide: MysqlDatasourceKey,\n\t\tinject: [ConfigService, MysqlConfigService],\n\t\tuseFactory: async (\n\t\t\tconfigService: ConfigService,\n\t\t\tdatabaseConfigService: MysqlConfigService,\n\t\t) => {\n\t\t\tconst dataSource = new DataSource(\n\t\t\t\tdatabaseConfigService.getTypeormConfig(),\n\t\t\t)\n\t\t\treturn dataSource.initialize()\n\t\t},\n\t},\n]\n```\n\n> 위와 같은 상황에서 useFactory에 인자에는 무조건 **`inject`배열의 순서**대로 인스턴스가 들어온다\n\n- 참고\n  - `MySqlConfigService` 는 `ConfigService`에 의존성이 있다.",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [
      "Backend",
      "NestJS"
    ],
    "readingTime": 1,
    "wordCount": 60,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "nestjs-cache-manager-v5-사용시-문제-해결법",
    "slug": "nestjs-cache-manager-v5-sayongsi-munje-haegyeolbeob",
    "path": "backend/nestjs",
    "fullPath": "backend/nestjs/nestjs-cache-manager-v5-sayongsi-munje-haegyeolbeob",
    "title": "NestJS, Cache-Manager v5 사용시 문제 해결법",
    "excerpt": "NestJS, Cache-Manager v5 사용시 문제 해결법 문제 공식문서에 나온대로 하면 발생 원인 가 로 올라가면서 사용법이 아예...",
    "content": "# NestJS, Cache-Manager v5 사용시 문제 해결법\n\n## 문제\n\n공식문서에 나온대로 하면 `this.cacheManager.get is not a function` 발생\n\n## 원인\n\n`cache-manager` 가 `v5`로 올라가면서 사용법이 아예 바뀜..\n\n## 해결\n\n```typescript\nimport { registerAs } from '@nestjs/config'\nimport { CACHE_CONFIG_KEY } from '../constants/index.js'\nimport { redisStore } from 'cache-manager-redis-store'\nimport { CacheConfigSchema } from './config.zod.js'\n\nexport const cacheConfig = registerAs(CACHE_CONFIG_KEY, async () =>\n\tCacheConfigSchema.parseAsync({\n\t\thost: process.env.CACHE_HOST,\n\t\tport: Number(process.env.CACHE_PORT),\n\t\tttl: Number(process.env.CACHE_TTL),\n\t\tstore: redisStore,\n\t}),\n)\n\n@Module({\n  providers: [\n    {\n      provide: CACHE_MANAGER,\n      inject: [cacheConfig.KEY],\n      useFactory: ({ store, ...config }: ConfigType<typeof cacheConfig>) =>\n        caching(store, config),\n    },\n  ],\n})\nexport class AppCacheModule\n```",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [
      "Backend",
      "NestJS"
    ],
    "readingTime": 1,
    "wordCount": 99,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "http-쿠키-보안-설정",
    "slug": "http-kuki-boan-seoljeong",
    "path": "backend/http",
    "fullPath": "backend/http/http-kuki-boan-seoljeong",
    "title": "HTTP 쿠키 보안 설정",
    "excerpt": "HTTP 쿠키 보안 설정 [세션 하이재킹과 XSS](https://developer.mozilla.org/ko/docs/Web/HTTP/Cookies%EC%84%B8%EC%85%98_%ED%95%98%EC%9D%B4%EC%9E%AC%ED%82%B9%EA%B...",
    "content": "# HTTP 쿠키 보안 설정\n\n1. [세션 하이재킹과 XSS](https://developer.mozilla.org/ko/docs/Web/HTTP/Cookies#%EC%84%B8%EC%85%98_%ED%95%98%EC%9D%B4%EC%9E%AC%ED%82%B9%EA%B3%BC_xss)\n2. [CSRF](https://developer.mozilla.org/ko/docs/Web/HTTP/Cookies#cross-site_%EC%9A%94%EC%B2%AD_%EC%9C%84%EC%A1%B0_csrf)\n3. [SameSite](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-Cookie/SameSite)\n\n\n## 참고 문서\n\n- [HTTP cookies 보안](https://developer.mozilla.org/ko/docs/Web/HTTP/Cookies#%EB%B3%B4%EC%95%88)",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [
      "Backend",
      "HTTP"
    ],
    "readingTime": 1,
    "wordCount": 21,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "http-요청-최소화-기법",
    "slug": "http-yoceong-coesohwa-gibeob",
    "path": "backend/http",
    "fullPath": "backend/http/http-yoceong-coesohwa-gibeob",
    "title": "Http 요청 최소화 기법",
    "excerpt": "Http 요청 최소화 기법 뭐가 있을까? 다른 공급자에 대해 HTTP 요구를 수행하는 것은 많은 크롤 프로그램이나 배치 프로그램에서 필요할 수 있지만, 요청 수를 최소화하여 성능을 향상시키고 오류나 타임아웃 위험을 줄이는 것이 중요합니다. HTTP 요청을...",
    "content": "# Http 요청 최소화 기법\n\n## 뭐가 있을까?\n\n다른 공급자에 대해 HTTP 요구를 수행하는 것은 많은 크롤 프로그램이나 배치 프로그램에서 필요할 수 있지만, 요청 수를 최소화하여 성능을 향상시키고 오류나 타임아웃 위험을 줄이는 것이 중요합니다. HTTP 요청을 최소화하기 위한 일반적인 모범 사례는 다음과 같습니다.\n\n- 데이터 로컬 캐시하기: 동일한 데이터에 대해 반복적으로 요구하는 경우 불필요한 HTTP 요청을 수행하지 않도록 데이터 로컬 캐시를 검토하십시오. Redis나 Memcached 등의 도구를 사용하여 자주 액세스되는 데이터를 메모리에 저장하거나 로컬 파일 캐시를 사용하여 데이터를 디스크에 저장할 수 있습니다.\n- 배치 요청: 데이터마다 개별 요청을 작성하는 대신 요청을 배치 처리하여 HTTP 요청 수를 줄이는 것을 검토하십시오. 예를 들어, 하나의 API 호출을 사용하여 한 번에 여러 레코드를 페치하거나 여러 요청을 하나의 배치 요청에 결합할 수 있습니다.\n- 페이지네이션 사용: 대량의 데이터를 요구하는 경우 페이지네이션을 사용하여 한 번에 작은 배치의 데이터를 페치하는 것을 검토하십시오. 이를 통해 요청의 전체 수를 줄이고 성능을 향상시킬 수 있습니다.\n- HTTP 캐시 사용: 요구하는 데이터가 자주 변경될 가능성이 낮은 경우 HTTP 캐시를 사용하여 클라이언트 캐시에 응답을 저장하는 것을 검토하십시오. 이를 통해 서버에 대한 요청 수를 줄이고 성능을 향상시킬 수 있습니다.\n- 요청에 우선 순위를 매김: 여러 리소스에 대해 요청을 할 경우 가장 중요한 데이터가 먼저 페치되도록 요청에 우선 순위를 매기는 것을 검토하십시오. 이를 통해 요청의 전체 수를 줄이고 성능을 향상시킬 수 있습니다.\n\n이러한 모범 사례를 따름으로써 크롤 프로그램 또는 배치 프로그램에 의해 수행되는 HTTP 요청의 수를 최소화하여 성능을 향상시키고 오류 및 타임아웃 위험을 줄일 수 있습니다.",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [
      "Backend",
      "HTTP"
    ],
    "readingTime": 2,
    "wordCount": 232,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "http-transfer-크기를-줄이는-법",
    "slug": "http-transfer-keugireul-julineun-beob",
    "path": "backend/http",
    "fullPath": "backend/http/http-transfer-keugireul-julineun-beob",
    "title": "HTTP Transfer 크기를 줄이는 법",
    "excerpt": "HTTP Transfer 크기를 줄이는 법 종단 간 압축 !종단 간 압축 Accept-...",
    "content": "# HTTP Transfer 크기를 줄이는 법\n\n1. 종단 간 압축\n   ![종단 간 압축](https://developer.mozilla.org/en-US/docs/Web/HTTP/Compression/httpcompression1.png)\n    1. Accept-Encoding, Content-Encoding 클라이언트와 서버에서 헤더를 통해 정보를 주고 받는다\n    2. 이미지, 오디오나 비디오 등의 미디어 파일은 이미 압축되어 있을 확률이 높다.\n    3. CPU가 상대적으로 많이 든다.\n    4. gzip이 가장 보편적이며, 구글에서 개발한 br이 후발주자로 떠오른다.\n2. Hop-By-Hop 압축\n   ![Hop-By-Hop 압축](https://developer.mozilla.org/en-US/docs/Web/HTTP/Compression/httpcomp2.png)\n    1. 클라이언트와 서버 사이의 경로에 있는 노드들에서 압축이 발생한다.\n    2. Transfer-Encoding 헤더를 통해서 압축 방법을 주고 받는다.\n    3. 보통 프록시 계층에서 적용된다\n    4. `Transfer-Encoding: chunked`\n       헤더가 사용될 때는 요청이 완전히 처리되기 전까지 응답의 전체 크기를 알 수 없다. 데이터베이스 쿼리의 결과가 될 큰 HTML 테이블을 생성하는 경우나, 큰 이미지를 전송하는 경우가 예시가 된다.\n\n## 참고 문서\n\n- [Web HTTP Compression](https://developer.mozilla.org/ko/docs/Web/HTTP/Compression)\n- [HTTP Headers Transfer-Encoding](https://developer.mozilla.org/ko/docs/Web/HTTP/Headers/Transfer-Encoding)\n- [ExpressJS Compression Middleware Library](https://github.com/expressjs/compression)",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [
      "Backend",
      "HTTP"
    ],
    "readingTime": 1,
    "wordCount": 124,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "eaddrnotavail",
    "slug": "eaddrnotavail",
    "path": "backend/http",
    "fullPath": "backend/http/eaddrnotavail",
    "title": "EADDRNOTAVAIL",
    "excerpt": "EADDRNOTAVAIL 문제 상황 node.js 서버 애플리케이션을 리눅스 환경에서 사용중이다. 아래와 같은 에러가 발생했다 ```json { \"message\": \"request to aa failed, reason: connect EAD...",
    "content": "# EADDRNOTAVAIL\n\n## 문제 상황\n\n1. node.js 서버 애플리케이션을 리눅스 환경에서 사용중이다.\n2. 아래와 같은 에러가 발생했다\n\n```json\n{\n  \"message\": \"request to aa failed, reason: connect EADDRNOTAVAIL aa:30000 - Local (aa:0)\",\n  \"type\": \"system\",\n  \"errno\": \"EADDRNOTAVAIL\",\n  \"code\": \"EADDRNOTAVAIL\",\n  \"isFetchError\": true,\n  \"url\": \"aa\"\n}\n```\n\n## 해결 방법 조사\n\n`etc/sysctl.conf`의 `net.ipv4.tcp_tw_recycle`를 활성화 하는 것은 좋은 생각이 아니다.\n\n위 옵션 변경으로 튜닝 하라는 가이드 문서가 많이 보이지만, tcp(7) 매뉴얼 페이지에 의하면 NAT를 가지고 동작할 때 문제가 발생할 수 있어서 권장하지 않는다고 한다.\n\nTIME_WAIT 상태의 접속은 접속 테이블에서 1분동안 유지된다. 즉 같은 네 쌍둥이(소스주소, 소스포트, 목적지주소, 목적지포트)에 다른 연결은 존재할 수 없다는 것을 뜻한다.\n웹 서버로 예를 들면 목적지 주소와 목적지 포트는 상수일 것이다. 웹 서버가 L7 로드밸런서 뒤에 있다면 소스 주소 도한 상수일 수 있다. 리눅스에서 클라이언트 포트는 기본적으로 30,000개의 포트 범위에서 할당\n된다(`net.ipv4.ip_local_port_range`를 튜닝하여 변경가능). 즉, 웹서버와 로드밸런서 사이에 연결이 매 분당 총 30,000개이며 매초 평균 500개의 연결이 맺어질 수 있다는 것을\n의미하겠다.\n이것을 초과하면 EADDRNOTAVAIL을 리턴한다. 해결방법은 네 쌍둥이를 더 많이 가질 수 있게 하는 것 밖에 없다.\n난이도가 어려운 순서대로 나열하면 다음과 같다.\n\n- `net.ipv4.ip_local_port_range`를 좀 더 넓게 세팅해서 더 많은 클라이언트 포트를 사용한다\n- 리스닝하는 웹서버에 추가적인 포트(81,82,83,...)를 할당함으로써 좀 더 많은 서버 포트를 사용한다\n- 로드 밸런서에 아이피를 추가하거나 라운드 로빈 기능을 적용해서 좀 더 많은 클라이언트 아이피를 사용한다.\n- 웹 서버에 아이피를 추가하여 좀 더 많은 아이피를 사용한다.\n\nNodeJS에서의 우회적인 방법으로는 다음과 같은 방법이 있다.\n\n```js\nvar http = require( \"http\"),\n    agent = new http.Agent( {maxSockets: 1} );\n\n\nfunction httpRequest( callback ) {\n    var options = {\n            host: 'localhost',\n            port: 80,\n            path: '',\n\n            agent: agent\n        },\n...\n```\n\n```js\nvar http = require(\"http\");\n\nfunction httpRequest(callback) {\n    var options = {\n        ...,\n        agent: false\n    };\n...\n```\n\n또한 아래를 `/etc/sysctl.conf`에 추가해주었다.\n\n`net.ipv4.ip_local_port_range = 1024 65535`\n\n## 참고 문서\n\n- [리눅스 man-pages](https://man7.org/linux/man-pages/index.html)\n- [[번역] 바쁜 리눅스 서버에서 TCP TIME-WAIT 상태 대처하기](https://linux.systemv.pe.kr/%EB%B2%88%EC%97%AD-%EB%B0%94%EC%81%9C-%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%84%9C%EB%B2%84%EC%97%90%EC%84%9C-tcp-time-wait-%EC%83%81%ED%83%9C-%EB%8C%80%EC%B2%98%ED%95%98%EA%B8%B0/)\n- [2014년도 구글 그룹에서의 한 문답..](https://groups.google.com/g/nodejs/c/68SYd6_ksns?pli=1)\n- [[Linux] 커널 파라미터 수정 - TCP 성능향상](https://bangu4.tistory.com/135)",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [
      "Backend",
      "HTTP"
    ],
    "readingTime": 2,
    "wordCount": 317,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "trunk-based-development",
    "slug": "trunk-based-development",
    "path": "backend/devops",
    "fullPath": "backend/devops/trunk-based-development",
    "title": "Trunk-Based Development",
    "excerpt": "Trunk-Based Development 트렁크 기반 개발은 개발자들이 라고 부르는 단일 브랜치에서 공동 작업을 수행하는 소스 제어 브랜칭 모델이며, 수명이 긴 다른 개발 브랜치를 생성하지 않기 위한 방법입니다. 그러므로 이 방법을 사용하면 병합지옥...",
    "content": "# Trunk-Based Development\n\n트렁크 기반 개발은 개발자들이 `trunk`라고 부르는 단일 브랜치에서 공동 작업을 수행하는 소스 제어 브랜칭 모델이며, 수명이 긴 다른 개발 브랜치를 생성하지 않기 위한 방법입니다. 그러므로 이 방법을 사용하면 병합지옥을 피하고 빌드를 깨지 않을 수 있습니다.\n\n## 트렁크 기반 개발의 Good or Bad Practice\n\n### Shared branches off mainline/main/trunk are bad at any release cadence\n\n![trunk-1a](https://trunkbaseddevelopment.com/trunk1a.png)\n\n### Trunk-Based Development For Smaller Teams\n\n![trunk-1b](https://trunkbaseddevelopment.com/trunk1b.png)\n\n### Scaled Trunk-Based Development\n\n![trunk-1c](https://trunkbaseddevelopment.com/trunk1c.png)\n\n## 정리\n\n개발자들은 피쳐브랜치에서 작업한 뒤 Trunk (or main) 브랜치로 병합하고, 릴리즈가 진행될떄 릴리즈 버전을 명시한 브랜치를 분기한다. 이때 분기된 브랜치에는 추가적인 커밋을 하지 않으며, 변경사항은 다음 릴리즈에 반영한다. 이런 작업 방식에서 서로 다른 버전에서 반영되어야할 사항이 main 브랜치에 공존하는 상황이 있을 수 있는데, 이를 통제하기 위해 기능 별 플래그처리를 사용하는 것이 좋다. 이 내용을 공식 사이트에서는 다음과 같이 소개하고 있다. [링크](https://trunkbaseddevelopment.com/feature-flags/)\n\n## NodeJS에서 플래그 처리를 어떻게 하는게 좋을까?\n\npackage.json의 버전을 적극적으로 사용하는게 어떨까 싶다. semver를 사용하고 있다면, release 버전을 부버전까지는 고정시켜 놓고 수버전을 올리면서 패치를 하게 될텐데, 한 릴리즈에서의 모든 기능을 보장한다면 플래그 처리를 부버전까지를 검사하는 것으로 할 수 있다.\n\n아래는 v2.5.x 릴리즈를 하는 경우의 예시이다\n\n```ts\nconst version = process.env.npm_package_version || '100'\n\nif (version.startsWith('2.5')) {\n   someCode();\n}\n\nif (Number(version) > 2.5) {\n   runAnotherCode();\n}\n...\n\n```\n\n## NodeJS에서 버전을 가져오는 방법\n\n1. package.json의 version을 가져오는 것\n   다만 참조한 스택오버플로우 답변에도 적혀있듯이, 브라우저에서 사용할 시엔 package.json의 내용을 클라이언트에서 참조할 수 있으므로 NodeJS를 활용한 서버에서만 활용하는게 좋을 듯 하다.\n\n   ```ts\n   // You need to set resolveJsonModule true in tsconfig.json\n   import { version } from './package.json'\n   console.log(`version = ${version}`)\n   console.log(`Is flag enabled? ${version.startsWith('2.5')}`)\n   // version = 1.0.0\n   // Is flag enabled? false\n   ```\n\n2. process 환경변수에서 가져오는 것\n   만약 NodeJS 애플리케이션이 npm 등의 패키지 매니저로 시작되었다면 아래와 같이 가져올 수 있다.\n\n   ```ts\n   // run npm start\n   const version = process.env.npm_package_version\n   console.log(`version = ${version}`)\n   console.log(\n   \t`Is flag enabled? ${version?.startsWith('2.5') ?? 'version is undefined'}`,\n   )\n   // version = 1.0.0\n   // Is flag enabled? false\n   ```\n\n물론 버전을 통한 플래그 처리를 하기 위해서는 철저한 package.json 관리가 필요하다.\n\n## Ref\n\n- [트렁크기반개발](https://trunkbaseddevelopment.com/)\n- [semver](https://semver.org/lang/ko/)\n- [is-there-a-way-to-get-version-from-package-json-in-nodejs-code](https://stackoverflow.com/questions/9153571/is-there-a-way-to-get-version-from-package-json-in-nodejs-code)",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [
      "Backend",
      "DevOps"
    ],
    "readingTime": 2,
    "wordCount": 332,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  }
]