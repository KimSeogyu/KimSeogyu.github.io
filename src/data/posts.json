[
  {
    "id": "gitlab-ci-02-variables-secrets",
    "slug": "gitlab-ci-02-variables-secrets",
    "path": "backend/devops",
    "fullPath": "backend/devops/gitlab-ci-02-variables-secrets",
    "title": "GitLab CI/CD ì‹œë¦¬ì¦ˆ #2: Variablesì™€ Secrets ê´€ë¦¬",
    "excerpt": "GitLab CI/CDì˜ ë³€ìˆ˜ ìœ í˜•ê³¼ ìš°ì„ ìˆœìœ„, Protected/Masked ë³€ìˆ˜, ê·¸ë¦¬ê³  Vault ì—°ë™ê¹Œì§€ ì‹œí¬ë¦¿ ê´€ë¦¬ ì „ëµì„ ë‹¤ë£¹ë‹ˆë‹¤.",
    "content": "# GitLab CI/CD ì‹œë¦¬ì¦ˆ #2: Variablesì™€ Secrets ê´€ë¦¬\n\n## ì‹œë¦¬ì¦ˆ ê°œìš”\n\n| # | ì£¼ì œ | í•µì‹¬ ë‚´ìš© |\n|---|------|----------|\n| 1 | ê¸°ì´ˆ | .gitlab-ci.yml êµ¬ì¡°, Stages, Jobs, Pipeline íë¦„ |\n| **2** | **Variables & Secrets** | ë³€ìˆ˜ ìœ í˜•, ìš°ì„ ìˆœìœ„, ì™¸ë¶€ Vault ì—°ë™ |\n| 3 | Runners & Executors | Docker, Kubernetes, Docker-in-Docker |\n| 4 | Pipeline ì•„í‚¤í…ì²˜ | Parent-Child, Multi-Project Pipeline |\n| 5 | ê³ ê¸‰ Job ì œì–´ | rules, needs, DAG, extends |\n| 6 | ì™¸ë¶€ í†µí•© | Triggers, Webhooks, API |\n\n---\n\n## Variables ê°œìš”\n\nGitLab CI/CDì—ì„œ **Variables**ëŠ” íŒŒì´í”„ë¼ì¸ ë™ì‘ì„ ì œì–´í•˜ê³ , í™˜ê²½ë³„ ì„¤ì •ì„ ê´€ë¦¬í•˜ë©°, ì‹œí¬ë¦¿ì„ ì „ë‹¬í•˜ëŠ” í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.\n\n### ë³€ìˆ˜ ìœ í˜•\n\n| ìœ í˜• | ì •ì˜ ìœ„ì¹˜ | ì˜ˆì‹œ |\n|-----|----------|------|\n| **Predefined** | GitLab ìë™ ì œê³µ | `$CI_COMMIT_SHA`, `$CI_JOB_ID` |\n| **File-defined** | `.gitlab-ci.yml` | `variables:` ë¸”ë¡ |\n| **Project** | Settings > CI/CD | API í‚¤, ë°°í¬ í† í° |\n| **Group** | ê·¸ë£¹ Settings | ê³µí†µ ì‹œí¬ë¦¿ |\n| **Instance** | Admin > CI/CD | ì „ì—­ ì„¤ì • |\n\n---\n\n## Predefined Variables\n\nGitLabì€ **200ê°œ ì´ìƒì˜ ì‚¬ì „ ì •ì˜ ë³€ìˆ˜**ë¥¼ ì œê³µí•©ë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ ì½˜í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.\n\n### ìì£¼ ì‚¬ìš©í•˜ëŠ” ë³€ìˆ˜\n\n```yaml\njob:\n  script:\n    # Pipeline ì •ë³´\n    - echo \"Pipeline ID: $CI_PIPELINE_ID\"\n    - echo \"Pipeline Source: $CI_PIPELINE_SOURCE\"\n    \n    # Commit ì •ë³´\n    - echo \"Branch: $CI_COMMIT_BRANCH\"\n    - echo \"Tag: $CI_COMMIT_TAG\"\n    - echo \"SHA: $CI_COMMIT_SHA\"\n    - echo \"Short SHA: $CI_COMMIT_SHORT_SHA\"\n    - echo \"Message: $CI_COMMIT_MESSAGE\"\n    \n    # Merge Request\n    - echo \"MR IID: $CI_MERGE_REQUEST_IID\"\n    - echo \"Target Branch: $CI_MERGE_REQUEST_TARGET_BRANCH_NAME\"\n    \n    # Project\n    - echo \"Project Path: $CI_PROJECT_PATH\"\n    - echo \"Project URL: $CI_PROJECT_URL\"\n    \n    # Job\n    - echo \"Job Name: $CI_JOB_NAME\"\n    - echo \"Job Stage: $CI_JOB_STAGE\"\n```\n\n### Pipeline Source ê°’\n\n| ê°’ | íŠ¸ë¦¬ê±° |\n|---|--------|\n| `push` | ë¸Œëœì¹˜ í‘¸ì‹œ |\n| `merge_request_event` | MR ìƒì„±/ì—…ë°ì´íŠ¸ |\n| `schedule` | ìŠ¤ì¼€ì¤„ íŒŒì´í”„ë¼ì¸ |\n| `web` | UIì—ì„œ ìˆ˜ë™ ì‹¤í–‰ |\n| `api` | API í˜¸ì¶œ |\n| `trigger` | ë‹¤ë¥¸ íŒŒì´í”„ë¼ì¸ì—ì„œ íŠ¸ë¦¬ê±° |\n| `pipeline` | ë©€í‹° í”„ë¡œì íŠ¸ íŒŒì´í”„ë¼ì¸ |\n| `parent_pipeline` | Parent íŒŒì´í”„ë¼ì¸ì—ì„œ |\n\n---\n\n## File-defined Variables\n\n`.gitlab-ci.yml`ì—ì„œ ì§ì ‘ ë³€ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n\n### ì „ì—­ ë³€ìˆ˜\n\n```yaml\nvariables:\n  NODE_ENV: \"production\"\n  DOCKER_REGISTRY: \"registry.gitlab.com\"\n  \nstages:\n  - build\n  - test\n\nbuild:\n  script:\n    - echo \"Environment: $NODE_ENV\"\n    - docker build -t $DOCKER_REGISTRY/myapp .\n```\n\n### Job ë ˆë²¨ ë³€ìˆ˜\n\n```yaml\nvariables:\n  GLOBAL_VAR: \"global\"\n\njob1:\n  variables:\n    JOB_VAR: \"job1-specific\"\n  script:\n    - echo \"$GLOBAL_VAR\"  # global\n    - echo \"$JOB_VAR\"     # job1-specific\n\njob2:\n  script:\n    - echo \"$GLOBAL_VAR\"  # global\n    - echo \"$JOB_VAR\"     # (ë¹„ì–´ìˆìŒ)\n```\n\n### ë³€ìˆ˜ í™•ì¥ (Expansion)\n\në³€ìˆ˜ ì•ˆì—ì„œ ë‹¤ë¥¸ ë³€ìˆ˜ë¥¼ ì°¸ì¡°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n```yaml\nvariables:\n  PROJECT: \"myapp\"\n  VERSION: \"1.0.0\"\n  IMAGE_TAG: \"$PROJECT:$VERSION\"  # myapp:1.0.0\n\njob:\n  script:\n    - docker build -t $IMAGE_TAG .\n```\n\n### í™•ì¥ ë¹„í™œì„±í™”\n\n```yaml\nvariables:\n  # ë°©ë²• 1: ë”°ì˜´í‘œì™€ ì´ìŠ¤ì¼€ì´í”„\n  LITERAL: \"$$NOT_EXPANDED\"\n  \n  # ë°©ë²• 2: expand: false (GitLab 15.6+)\n  PASSWORD:\n    value: \"pa$$word\"\n    expand: false\n```\n\n---\n\n## Project/Group Variables (UI ì„¤ì •)\n\në¯¼ê°í•œ ì •ë³´ëŠ” `.gitlab-ci.yml`ì— ì§ì ‘ ì‘ì„±í•˜ì§€ ì•Šê³  **GitLab UI**ì—ì„œ ì„¤ì •í•©ë‹ˆë‹¤.\n\n### ì„¤ì • ê²½ë¡œ\n\n- **Project**: `Settings > CI/CD > Variables`\n- **Group**: `Group Settings > CI/CD > Variables`\n- **Instance**: `Admin > Settings > CI/CD > Variables`\n\n### ë³€ìˆ˜ ì˜µì…˜\n\n| ì˜µì…˜ | ì„¤ëª… |\n|------|------|\n| **Protected** | Protected ë¸Œëœì¹˜/íƒœê·¸ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥ |\n| **Masked** | Job ë¡œê·¸ì—ì„œ `[MASKED]`ë¡œ í‘œì‹œ |\n| **Expanded** | ë‹¤ë¥¸ ë³€ìˆ˜ ì°¸ì¡° í—ˆìš© (ê¸°ë³¸ true) |\n\n### Protected Variables\n\n```mermaid\nflowchart TB\n    subgraph Variables [Protected Variable: PROD_API_KEY]\n        V[PROD_API_KEY = \"secret\"]\n    end\n    \n    subgraph Branches\n        Main[main - Protected âœ“]\n        Feature[feature/* - ì¼ë°˜]\n    end\n    \n    V -->|ì ‘ê·¼ ê°€ëŠ¥| Main\n    V -->|ì ‘ê·¼ ë¶ˆê°€| Feature\n```\n\n> [!IMPORTANT]\n> **Protected ë³€ìˆ˜**ëŠ” Protected ë¸Œëœì¹˜/íƒœê·¸ì—ì„œ ì‹¤í–‰ë˜ëŠ” íŒŒì´í”„ë¼ì¸ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. `feature/*` ë¸Œëœì¹˜ì—ì„œ í”„ë¡œë•ì…˜ ì‹œí¬ë¦¿ ì ‘ê·¼ì„ ë°©ì§€í•©ë‹ˆë‹¤.\n\n### Masked Variables\n\n```yaml\njob:\n  script:\n    - echo \"Token: $SECRET_TOKEN\"\n    # ë¡œê·¸ ì¶œë ¥: Token: [MASKED]\n```\n\n**Masking ì¡°ê±´**:\n\n- ìµœì†Œ 8ì\n- Base64 ìœ íš¨ ë¬¸ìë§Œ (`A-Za-z0-9+/=@:.~`)\n- ì—¬ëŸ¬ ì¤„ ë¶ˆê°€\n\n---\n\n## Variable ìš°ì„ ìˆœìœ„\n\në™ì¼í•œ ì´ë¦„ì˜ ë³€ìˆ˜ê°€ ì—¬ëŸ¬ ìœ„ì¹˜ì— ì •ì˜ëœ ê²½ìš°, **ìš°ì„ ìˆœìœ„**ì— ë”°ë¼ ê°’ì´ ê²°ì •ë©ë‹ˆë‹¤.\n\n### ìš°ì„ ìˆœìœ„ (ë†’ì€ ìˆœ)\n\n```mermaid\nflowchart TB\n    T[Trigger variables] --> M\n    M[Manual pipeline variables] --> S\n    S[Scheduled pipeline variables] --> J\n    J[Job-level variables] --> G\n    G[Global .gitlab-ci.yml] --> P\n    P[Project variables] --> GR\n    GR[Group variables] --> I\n    I[Instance variables] --> PR\n    PR[Predefined variables]\n```\n\n1. **Trigger variables** (íŠ¸ë¦¬ê±° ì‹œ ì „ë‹¬)\n2. **Manual pipeline variables** (ìˆ˜ë™ ì‹¤í–‰ ì‹œ ì…ë ¥)\n3. **Scheduled pipeline variables** (ìŠ¤ì¼€ì¤„ ì„¤ì •)\n4. **Job-level** (`.gitlab-ci.yml` Job ë‚´)\n5. **Global** (`.gitlab-ci.yml` ì „ì—­)\n6. **Project variables** (UI ì„¤ì •)\n7. **Group variables** (ìƒìœ„ ê·¸ë£¹ í¬í•¨)\n8. **Instance variables** (Admin ì„¤ì •)\n9. **Predefined variables** (GitLab ì œê³µ)\n\n### ì˜ˆì‹œ: ìš°ì„ ìˆœìœ„ í™•ì¸\n\n```yaml\nvariables:\n  MY_VAR: \"from-gitlab-ci\"  # 5ë²ˆ ìš°ì„ ìˆœìœ„\n\njob:\n  variables:\n    MY_VAR: \"from-job\"      # 4ë²ˆ ìš°ì„ ìˆœìœ„ (ì´ ê°’ ì‚¬ìš©)\n  script:\n    - echo \"$MY_VAR\"        # from-job\n```\n\n---\n\n## dotenv Artifacts\n\nJob ê°„ì— **ë™ì ìœ¼ë¡œ ìƒì„±ëœ ë³€ìˆ˜**ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n### ë³€ìˆ˜ ìƒì„± ë° ì „íŒŒ\n\n```yaml\ngenerate-version:\n  stage: build\n  script:\n    - VERSION=$(date +%Y%m%d%H%M%S)\n    - echo \"VERSION=$VERSION\" >> build.env\n    - echo \"COMMIT_HASH=$CI_COMMIT_SHORT_SHA\" >> build.env\n  artifacts:\n    reports:\n      dotenv: build.env\n\nuse-version:\n  stage: deploy\n  needs:\n    - job: generate-version\n      artifacts: true\n  script:\n    - echo \"Deploying version: $VERSION\"\n    - echo \"Commit: $COMMIT_HASH\"\n```\n\n### ë™ì‘ ì›ë¦¬\n\n```mermaid\nsequenceDiagram\n    participant Gen as generate-version\n    participant GitLab as GitLab\n    participant Use as use-version\n    \n    Gen->>Gen: echo \"VERSION=123\" >> build.env\n    Gen->>GitLab: artifacts upload (dotenv)\n    GitLab->>Use: artifacts download\n    Use->>Use: $VERSION ë³€ìˆ˜ ì‚¬ìš© ê°€ëŠ¥\n```\n\n> [!TIP]\n> **dotenv**ëŠ” Job ê°„ ë™ì  ê°’ ì „ë‹¬ì— ìœ ìš©í•©ë‹ˆë‹¤. ë¹Œë“œ ë²„ì „, Git íƒœê·¸, ê³„ì‚°ëœ ê°’ ë“±ì„ ì „ë‹¬í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\n---\n\n## File Type Variables\n\ní° ë°ì´í„°ë‚˜ ì¸ì¦ì„œëŠ” **File íƒ€ì… ë³€ìˆ˜**ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n\n### ì„¤ì • (UI)\n\n1. `Settings > CI/CD > Variables`\n2. `Type: File` ì„ íƒ\n3. íŒŒì¼ ë‚´ìš© ì…ë ¥\n\n### ì‚¬ìš©\n\n```yaml\njob:\n  script:\n    # ë³€ìˆ˜ ê°’ì€ íŒŒì¼ ê²½ë¡œ\n    - cat $KUBE_CONFIG  # íŒŒì¼ ë‚´ìš© ì¶œë ¥\n    - kubectl --kubeconfig=$KUBE_CONFIG get pods\n```\n\n### ì¼ë°˜ì ì¸ ìš©ë„\n\n- Kubernetes kubeconfig\n- GCP ì„œë¹„ìŠ¤ ê³„ì • JSON\n- SSH í”„ë¼ì´ë¹— í‚¤\n- TLS ì¸ì¦ì„œ\n\n---\n\n## External Secrets ì—°ë™\n\n### HashiCorp Vault\n\nGitLab Premium ì´ìƒì—ì„œ Vaultì™€ ë„¤ì´í‹°ë¸Œ í†µí•©ì„ ì§€ì›í•©ë‹ˆë‹¤.\n\n```yaml\njob:\n  id_tokens:\n    VAULT_ID_TOKEN:\n      aud: https://vault.example.com\n  secrets:\n    DATABASE_PASSWORD:\n      vault: production/db/password@secrets\n      file: false\n  script:\n    - echo \"Password: $DATABASE_PASSWORD\"\n```\n\n### Vault JWT ì¸ì¦ (ë¬´ë£Œ)\n\n```yaml\nvariables:\n  VAULT_ADDR: \"https://vault.example.com\"\n\nget-secrets:\n  image: hashicorp/vault:latest\n  script:\n    - export VAULT_TOKEN=$(vault write -field=token auth/jwt/login role=myapp jwt=$CI_JOB_JWT)\n    - vault kv get -field=password secret/myapp/db > db_password.txt\n  artifacts:\n    paths:\n      - db_password.txt\n```\n\n### AWS Secrets Manager\n\n```yaml\nvariables:\n  AWS_DEFAULT_REGION: ap-northeast-2\n\nget-aws-secret:\n  image: amazon/aws-cli:latest\n  script:\n    - SECRET=$(aws secretsmanager get-secret-value --secret-id prod/db/password --query SecretString --output text)\n    - echo \"SECRET=$SECRET\" >> aws.env\n  artifacts:\n    reports:\n      dotenv: aws.env\n```\n\n---\n\n## ë³€ìˆ˜ ë³´ì•ˆ Best Practices\n\n### 1. Protected ë³€ìˆ˜ í™œìš©\n\n```yaml\n# í”„ë¡œë•ì…˜ ë°°í¬ëŠ” Protected ë¸Œëœì¹˜ì—ì„œë§Œ\ndeploy-prod:\n  script:\n    - deploy --token $PROD_DEPLOY_TOKEN  # Protected ë³€ìˆ˜\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n```\n\n### 2. Masked ë³€ìˆ˜ í•„ìˆ˜í™”\n\në¯¼ê°í•œ ëª¨ë“  ë³€ìˆ˜ì— Masked ì˜µì…˜ì„ í™œì„±í™”í•©ë‹ˆë‹¤.\n\n### 3. ìµœì†Œ ê¶Œí•œ ì›ì¹™\n\n```yaml\n# ì˜ëª»ëœ ì˜ˆ: ëª¨ë“  Jobì—ì„œ í”„ë¡œë•ì…˜ ì‹œí¬ë¦¿ ì ‘ê·¼\nvariables:\n  PROD_DB_PASSWORD: $PROD_DB_PASSWORD  # ìœ„í—˜!\n\n# ì˜¬ë°”ë¥¸ ì˜ˆ: í•„ìš”í•œ Jobì—ì„œë§Œ ì‚¬ìš©\ndeploy-prod:\n  variables:\n    DB_PASSWORD: $PROD_DB_PASSWORD\n  script:\n    - deploy.sh\n```\n\n### 4. ì‹œí¬ë¦¿ ë¡œí…Œì´ì…˜\n\nì •ê¸°ì ìœ¼ë¡œ ì‹œí¬ë¦¿ì„ êµì²´í•˜ê³ , ë³€ìˆ˜ ì—…ë°ì´íŠ¸ ì‹œ ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ì— ì˜í–¥ ì—†ì´ ìƒˆ ê°’ì´ ì ìš©ë©ë‹ˆë‹¤.\n\n---\n\n## ì •ë¦¬\n\n| ê°œë… | ì„¤ëª… |\n|-----|------|\n| **Predefined** | GitLabì´ ìë™ ì œê³µí•˜ëŠ” 200+ ë³€ìˆ˜ |\n| **File-defined** | `.gitlab-ci.yml`ì— ì •ì˜ |\n| **Project/Group** | UIì—ì„œ ì„¤ì •, ì‹œí¬ë¦¿ ì €ì¥ |\n| **Protected** | Protected ë¸Œëœì¹˜/íƒœê·¸ì—ì„œë§Œ ì ‘ê·¼ |\n| **Masked** | ë¡œê·¸ì—ì„œ `[MASKED]` ì²˜ë¦¬ |\n| **dotenv** | Job ê°„ ë™ì  ë³€ìˆ˜ ì „ë‹¬ |\n| **ìš°ì„ ìˆœìœ„** | Trigger > Job > Global > Project > Group > Instance |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**3í¸: Runnersì™€ Executors**ì—ì„œëŠ” ë‹¤ìŒì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- Runner ìœ í˜• (Shared, Group, Project)\n- Executor ì¢…ë¥˜ (Shell, Docker, Kubernetes)\n- **Docker-in-Docker (DinD)** ì‹¬í™”\n- TLS ì„¤ì •ê³¼ ë³´ì•ˆ\n- Runner ì„±ëŠ¥ ìµœì í™”\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [GitLab CI/CD Variables](https://docs.gitlab.com/ee/ci/variables/)\n- [Predefined Variables Reference](https://docs.gitlab.com/ee/ci/variables/predefined_variables.html)\n- [Using External Secrets](https://docs.gitlab.com/ee/ci/secrets/)\n- [Variable Precedence](https://docs.gitlab.com/ee/ci/variables/#cicd-variable-precedence)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitLab",
      "CI/CD",
      "Pipeline",
      "Secrets"
    ],
    "readingTime": 7,
    "wordCount": 1249,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitlab-ci",
    "date": "Mon Jan 05 2026 09:00:00 GMT+0900 (Korean Standard Time)"
  },
  {
    "id": "gitops-06-ci-integration",
    "slug": "gitops-06-ci-integration",
    "path": "backend/gitops",
    "fullPath": "backend/gitops/gitops-06-ci-integration",
    "title": "GitOps ì‹¬í™” ì‹œë¦¬ì¦ˆ #6: CI/CD íŒŒì´í”„ë¼ì¸ í†µí•©ê³¼ Progressive Delivery",
    "excerpt": "GitOpsì—ì„œ CIì™€ CDì˜ ë¶„ë¦¬, Image Updater, Progressive Delivery(Argo Rollouts, Flagger)ë¥¼ ë‹¤ë£¨ëŠ” ì‹œë¦¬ì¦ˆ ì™„ê²°í¸ì…ë‹ˆë‹¤.",
    "content": "# GitOps ì‹¬í™” ì‹œë¦¬ì¦ˆ #6: CI/CD íŒŒì´í”„ë¼ì¸ í†µí•©ê³¼ Progressive Delivery\n\n## ì‹œë¦¬ì¦ˆ ê°œìš”\n\n| # | ì£¼ì œ | í•µì‹¬ ë‚´ìš© |\n|---|------|----------|\n| 1 | GitOps ê°œìš” | ì² í•™ê³¼ ì›ì¹™, Push vs Pull ë°°í¬, Reconciliation |\n| 2 | ArgoCD Deep Dive | ì•„í‚¤í…ì²˜, Application CRD, Sync ì „ëµ |\n| 3 | Flux CD & GitOps Toolkit | ì»¨íŠ¸ë¡¤ëŸ¬ ì•„í‚¤í…ì²˜, GitRepository, Kustomization |\n| 4 | í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬ | Kustomize vs Helm, ì „ëµ ì„ íƒ ê¸°ì¤€ |\n| 5 | Secrets Management | Sealed Secrets, External Secrets, SOPS |\n| **6** | **CI/CD íŒŒì´í”„ë¼ì¸ í†µí•©** | Image Updater, Progressive Delivery |\n\n---\n\n## GitOpsì—ì„œ CIì™€ CDì˜ ë¶„ë¦¬\n\nì „í†µì ì¸ CI/CD íŒŒì´í”„ë¼ì¸ì—ì„œëŠ” CIì™€ CDê°€ í•˜ë‚˜ì˜ íŒŒì´í”„ë¼ì¸ì—ì„œ ì—°ì†ì ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤. GitOpsì—ì„œëŠ” **ëª…í™•í•˜ê²Œ ë¶„ë¦¬**ë©ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph CI [CI Pipeline - ì†ŒìŠ¤ ë ˆí¬]\n        Code[ì½”ë“œ ë³€ê²½] --> Build[ë¹Œë“œ]\n        Build --> Test[í…ŒìŠ¤íŠ¸]\n        Test --> Push[ì´ë¯¸ì§€ Push]\n        Push --> UpdateManifest[ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸]\n    end\n    \n    subgraph GitOps [GitOps ë ˆí¬]\n        Manifest[K8s Manifests]\n    end\n    \n    subgraph CD [CD - GitOps Agent]\n        Agent[ArgoCD / Flux]\n        Cluster[Kubernetes Cluster]\n    end\n    \n    UpdateManifest -->|PR ìƒì„± ë˜ëŠ” ì§ì ‘ ì»¤ë°‹| Manifest\n    Manifest -->|Watch| Agent\n    Agent -->|Sync| Cluster\n```\n\n### ì™œ ë¶„ë¦¬í•˜ëŠ”ê°€?\n\n| ê´€ì  | í†µí•© CI/CD | ë¶„ë¦¬ëœ CI + GitOps |\n|-----|-----------|------------------|\n| **ë°°í¬ ê¶Œí•œ** | CIê°€ í´ëŸ¬ìŠ¤í„° ì ‘ê·¼ | í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ Agentë§Œ ì ‘ê·¼ |\n| **ê°ì‚¬ ì¶”ì ** | CI ë¡œê·¸ì—ë§Œ ê¸°ë¡ | Git ì»¤ë°‹ íˆìŠ¤í† ë¦¬ |\n| **ë¡¤ë°±** | ì¬ë¹Œë“œ í•„ìš” | `git revert` |\n| **í™˜ê²½ ì¼ê´€ì„±** | CI íŒŒì´í”„ë¼ì¸ì— ì˜ì¡´ | Gitì´ SSOT |\n\n> [!IMPORTANT]\n> **í•µì‹¬ ì›ì¹™**: CIëŠ” **ì•„í‹°íŒ©íŠ¸(ì´ë¯¸ì§€)ë¥¼ ìƒì„±**í•˜ê³ , CDëŠ” **GitOps Agentê°€ ë‹´ë‹¹**í•©ë‹ˆë‹¤. CIê°€ í´ëŸ¬ìŠ¤í„°ì— ì§ì ‘ ì ‘ê·¼í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n---\n\n## CI íŒŒì´í”„ë¼ì¸ì—ì„œ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸\n\nì´ë¯¸ì§€ ë¹Œë“œ í›„ GitOps ë ˆí¬ì˜ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ì—¬ëŸ¬ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.\n\n### ë°©ë²• 1: CIì—ì„œ ì§ì ‘ ì»¤ë°‹\n\n```yaml\n# .github/workflows/build.yaml\nname: Build and Update Manifest\n\non:\n  push:\n    branches: [main]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout source\n        uses: actions/checkout@v4\n      \n      - name: Build and push image\n        run: |\n          docker build -t ghcr.io/myorg/myapp:${{ github.sha }} .\n          docker push ghcr.io/myorg/myapp:${{ github.sha }}\n      \n      - name: Update GitOps repo\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITOPS_PAT }}\n        run: |\n          git clone https://$GITHUB_TOKEN@github.com/myorg/gitops-repo.git\n          cd gitops-repo\n          \n          # Kustomize ì‚¬ìš© ì‹œ\n          cd apps/myapp/overlays/prod\n          kustomize edit set image ghcr.io/myorg/myapp:${{ github.sha }}\n          \n          git config user.name \"GitHub Actions\"\n          git config user.email \"actions@github.com\"\n          git add .\n          git commit -m \"Update myapp to ${{ github.sha }}\"\n          git push\n```\n\n### ë°©ë²• 2: PR ìƒì„± (ê¶Œì¥)\n\n```yaml\n      - name: Create PR to GitOps repo\n        uses: peter-evans/create-pull-request@v6\n        with:\n          token: ${{ secrets.GITOPS_PAT }}\n          repository: myorg/gitops-repo\n          branch: update-myapp-${{ github.sha }}\n          title: \"Update myapp to ${{ github.sha }}\"\n          body: |\n            Automated image update\n            - Commit: ${{ github.sha }}\n            - Build: ${{ github.run_id }}\n          commit-message: \"chore: update myapp image to ${{ github.sha }}\"\n```\n\n> [!TIP]\n> **PR ë°©ì‹ì˜ ì¥ì **: ì½”ë“œ ë¦¬ë·° í”„ë¡œì„¸ìŠ¤ í™œìš©, ìë™í™”ëœ í…ŒìŠ¤íŠ¸ ì‹¤í–‰, ìŠ¹ì¸ í›„ ë°°í¬\n\n### ë°©ë²• 3: ì´ë¯¸ì§€ íƒœê·¸ ìë™ ì—…ë°ì´íŠ¸ (ê¶Œì¥)\n\nCIëŠ” ì´ë¯¸ì§€ë§Œ í‘¸ì‹œí•˜ê³ , **GitOps Agentê°€ ìë™ìœ¼ë¡œ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ë¥¼ ì—…ë°ì´íŠ¸**í•©ë‹ˆë‹¤. ë‹¤ìŒ ì„¹ì…˜ì—ì„œ ìì„¸íˆ ë‹¤ë£¹ë‹ˆë‹¤.\n\n---\n\n## ArgoCD Image Updater\n\n**ArgoCD Image Updater**ëŠ” ì»¨í…Œì´ë„ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³ , ìƒˆ ì´ë¯¸ì§€ íƒœê·¸ê°€ ë°œê²¬ë˜ë©´ ìë™ìœ¼ë¡œ Applicationì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n\n### ë™ì‘ ì›ë¦¬\n\n```mermaid\nsequenceDiagram\n    participant Registry as Container Registry\n    participant Updater as Image Updater\n    participant Git as GitOps Repo\n    participant ArgoCD as ArgoCD\n    participant K8s as Kubernetes\n    \n    loop ì£¼ê¸°ì  ìŠ¤ìº”\n        Updater->>Registry: ìƒˆ ì´ë¯¸ì§€ íƒœê·¸ í™•ì¸\n        Registry-->>Updater: v1.2.4 ë°œê²¬ (í˜„ì¬: v1.2.3)\n    end\n    \n    Updater->>Git: ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ ì»¤ë°‹\n    Git-->>ArgoCD: ë³€ê²½ ê°ì§€\n    ArgoCD->>K8s: Sync\n```\n\n### ì„¤ì¹˜\n\n```bash\nkubectl apply -n argocd \\\n  -f https://raw.githubusercontent.com/argoproj-labs/argocd-image-updater/stable/manifests/install.yaml\n```\n\n### Application ì„¤ì •\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: myapp\n  namespace: argocd\n  annotations:\n    # Image Updater í™œì„±í™”\n    argocd-image-updater.argoproj.io/image-list: myapp=ghcr.io/myorg/myapp\n    \n    # ì—…ë°ì´íŠ¸ ì „ëµ: semver\n    argocd-image-updater.argoproj.io/myapp.update-strategy: semver\n    \n    # SemVer ì œì•½\n    argocd-image-updater.argoproj.io/myapp.allow-tags: regexp:^v[0-9]+\\.[0-9]+\\.[0-9]+$\n    \n    # Gitì— ì»¤ë°‹ (write-back)\n    argocd-image-updater.argoproj.io/write-back-method: git\n    argocd-image-updater.argoproj.io/git-branch: main\nspec:\n  source:\n    repoURL: https://github.com/myorg/gitops.git\n    path: apps/myapp\n```\n\n### Update Strategies\n\n| ì „ëµ | ì„¤ëª… | ì˜ˆì‹œ |\n|-----|------|-----|\n| `semver` | SemVer ìµœì‹  | v1.2.3 < v1.2.4 < v1.3.0 |\n| `latest` | ìµœì‹  í‘¸ì‹œëœ íƒœê·¸ | ë‚ ì§œ/ì‹œê°„ ê¸°ì¤€ |\n| `name` | ì•ŒíŒŒë²³ìˆœ ìµœì‹  | a < b < c |\n| `digest` | íŠ¹ì • íƒœê·¸ì˜ digest ë³€ê²½ | :latestì˜ ì‹¤ì œ ì´ë¯¸ì§€ ë³€ê²½ |\n\n### Write-back Methods\n\n```yaml\n# 1. Git ì§ì ‘ ì»¤ë°‹ (ê¶Œì¥)\nargocd-image-updater.argoproj.io/write-back-method: git\n\n# 2. ArgoCDì— ì˜¤ë²„ë¼ì´ë“œ (Gitì— ê¸°ë¡ ì•ˆ ë¨)\nargocd-image-updater.argoproj.io/write-back-method: argocd\n```\n\n> [!WARNING]\n> `argocd` ë°©ì‹ì€ Gitì— ê¸°ë¡ë˜ì§€ ì•Šì•„ **GitOps ì›ì¹™ì— ìœ„ë°°**ë©ë‹ˆë‹¤. í”„ë¡œë•ì…˜ì—ì„œëŠ” `git` ë°©ì‹ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n\n---\n\n## Flux Image Automation\n\nFluxëŠ” Image Automationì„ **í•µì‹¬ ê¸°ëŠ¥ìœ¼ë¡œ ë‚´ì¥**í•˜ê³  ìˆìŠµë‹ˆë‹¤ (3í¸ì—ì„œ ê°„ëµíˆ ì†Œê°œí–ˆìŠµë‹ˆë‹¤).\n\n### ì „ì²´ êµ¬ì„±\n\n```yaml\n# 1. ImageRepository: ë ˆì§€ìŠ¤íŠ¸ë¦¬ ìŠ¤ìº”\napiVersion: image.toolkit.fluxcd.io/v1beta2\nkind: ImageRepository\nmetadata:\n  name: myapp\n  namespace: flux-system\nspec:\n  image: ghcr.io/myorg/myapp\n  interval: 5m\n  secretRef:\n    name: ghcr-auth\n\n---\n# 2. ImagePolicy: íƒœê·¸ ì„ íƒ ì •ì±…\napiVersion: image.toolkit.fluxcd.io/v1beta2\nkind: ImagePolicy\nmetadata:\n  name: myapp\n  namespace: flux-system\nspec:\n  imageRepositoryRef:\n    name: myapp\n  policy:\n    semver:\n      range: \">=1.0.0\"\n\n---\n# 3. ImageUpdateAutomation: Git ì—…ë°ì´íŠ¸\napiVersion: image.toolkit.fluxcd.io/v1beta2\nkind: ImageUpdateAutomation\nmetadata:\n  name: myapp\n  namespace: flux-system\nspec:\n  interval: 5m\n  sourceRef:\n    kind: GitRepository\n    name: myapp\n  git:\n    checkout:\n      ref:\n        branch: main\n    commit:\n      author:\n        name: Flux\n        email: flux@myorg.com\n      messageTemplate: |\n        Auto-update images\n        \n        {{range .Changed.Changes}}\n        - {{.OldValue}} -> {{.NewValue}}\n        {{end}}\n    push:\n      branch: main\n  update:\n    path: ./deploy\n    strategy: Setters\n```\n\n### ë§ˆì»¤ ê¸°ë°˜ ì—…ë°ì´íŠ¸\n\n```yaml\n# deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  template:\n    spec:\n      containers:\n      - name: myapp\n        image: ghcr.io/myorg/myapp:v1.2.3  # {\"$imagepolicy\": \"flux-system:myapp\"}\n```\n\nFluxê°€ ìƒˆ ë²„ì „ì„ ë°œê²¬í•˜ë©´:\n\n```yaml\n        image: ghcr.io/myorg/myapp:v1.2.4  # {\"$imagepolicy\": \"flux-system:myapp\"}\n```\n\n---\n\n## Progressive Delivery\n\nìƒˆ ë²„ì „ì„ **ì ì§„ì ìœ¼ë¡œ ë°°í¬**í•˜ì—¬ ìœ„í—˜ì„ ìµœì†Œí™”í•˜ëŠ” ì „ëµì…ë‹ˆë‹¤.\n\n### ë°°í¬ ì „ëµ ë¹„êµ\n\n```mermaid\nflowchart TB\n    subgraph Recreate [Recreate]\n        R1[v1 ì‚­ì œ] --> R2[v2 ìƒì„±]\n        R_Note[ë‹¤ìš´íƒ€ì„ ë°œìƒ]\n    end\n    \n    subgraph Rolling [Rolling Update]\n        RU1[v1: 3ê°œ] --> RU2[v1: 2ê°œ, v2: 1ê°œ]\n        RU2 --> RU3[v1: 1ê°œ, v2: 2ê°œ]\n        RU3 --> RU4[v2: 3ê°œ]\n    end\n    \n    subgraph BlueGreen [Blue-Green]\n        BG1[Blue: v1 ìš´ì˜] --> BG2[Green: v2 ëŒ€ê¸°]\n        BG2 --> BG3[íŠ¸ë˜í”½ ì „í™˜]\n        BG3 --> BG4[Green: v2 ìš´ì˜]\n    end\n    \n    subgraph Canary [Canary]\n        C1[v1: 100%] --> C2[v1: 90%, v2: 10%]\n        C2 --> C3[v1: 50%, v2: 50%]\n        C3 --> C4[v2: 100%]\n    end\n```\n\n| ì „ëµ | ë‹¤ìš´íƒ€ì„ | ë¦¬ì†ŒìŠ¤ | ë¡¤ë°± ì†ë„ | ìœ„í—˜ ë…¸ì¶œ |\n|-----|---------|-------|---------|---------|\n| Recreate | âœ… ìˆìŒ | ë‚®ìŒ | ëŠë¦¼ | ë†’ìŒ |\n| Rolling | âŒ ì—†ìŒ | ì¼ì‹œ ì¦ê°€ | ì¤‘ê°„ | ì¤‘ê°„ |\n| Blue-Green | âŒ ì—†ìŒ | 2ë°° | ë¹ ë¦„ | ë‚®ìŒ |\n| Canary | âŒ ì—†ìŒ | ì•½ê°„ ì¦ê°€ | ë¹ ë¦„ | ë‚®ìŒ |\n\n---\n\n## Argo Rollouts\n\n**Argo Rollouts**ëŠ” Kubernetes Deploymentë¥¼ ëŒ€ì²´í•˜ì—¬ Blue-Green, Canary ë°°í¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n\n### ì„¤ì¹˜\n\n```bash\nkubectl create namespace argo-rollouts\nkubectl apply -n argo-rollouts \\\n  -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml\n```\n\n### Canary Rollout\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: myapp\nspec:\n  replicas: 5\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: myapp\n        image: ghcr.io/myorg/myapp:v1.2.3\n        ports:\n        - containerPort: 8080\n  strategy:\n    canary:\n      # ë‹¨ê³„ë³„ ë°°í¬\n      steps:\n      - setWeight: 10      # 10% íŠ¸ë˜í”½\n      - pause: {duration: 5m}  # 5ë¶„ ëŒ€ê¸°\n      - setWeight: 30\n      - pause: {duration: 5m}\n      - setWeight: 50\n      - pause: {}          # ìˆ˜ë™ ìŠ¹ì¸ ëŒ€ê¸°\n      - setWeight: 100\n      \n      # ë¶„ì„ ì‹¤í–‰ (ìë™ ë¡¤ë°±)\n      analysis:\n        templates:\n        - templateName: success-rate\n        startingStep: 1\n        args:\n        - name: service-name\n          value: myapp\n```\n\n### AnalysisTemplate\n\në©”íŠ¸ë¦­ ê¸°ë°˜ ìë™ ë¡¤ë°±:\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: AnalysisTemplate\nmetadata:\n  name: success-rate\nspec:\n  args:\n  - name: service-name\n  metrics:\n  - name: success-rate\n    interval: 1m\n    failureLimit: 3\n    successCondition: result[0] >= 0.95\n    provider:\n      prometheus:\n        address: http://prometheus.monitoring:9090\n        query: |\n          sum(rate(http_requests_total{\n            service=\"{{args.service-name}}\",\n            status=~\"2..\"\n          }[5m])) /\n          sum(rate(http_requests_total{\n            service=\"{{args.service-name}}\"\n          }[5m]))\n```\n\n### Blue-Green Rollout\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: myapp\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    # ...\n  strategy:\n    blueGreen:\n      activeService: myapp-active\n      previewService: myapp-preview\n      autoPromotionEnabled: false  # ìˆ˜ë™ ìŠ¹ì¸\n      prePromotionAnalysis:\n        templates:\n        - templateName: smoke-test\n```\n\n### ArgoCDì™€ í†µí•©\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: myapp\nspec:\n  source:\n    repoURL: https://github.com/myorg/gitops.git\n    path: apps/myapp\n  # Rollout ë¦¬ì†ŒìŠ¤ë¥¼ ì¸ì‹í•˜ë„ë¡ ì„¤ì •\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n```\n\n---\n\n## Flagger\n\n**Flagger**ëŠ” Fluxì™€ í•¨ê»˜ ì‚¬ìš©ë˜ëŠ” Progressive Delivery ë„êµ¬ì…ë‹ˆë‹¤. Istio, Linkerd, NGINX Ingress ë“±ê³¼ í†µí•©ë©ë‹ˆë‹¤.\n\n### ì„¤ì¹˜ (with Flux)\n\n```yaml\napiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: flagger\n  namespace: flux-system\nspec:\n  interval: 1h\n  url: https://flagger.app\n\n---\napiVersion: helm.toolkit.fluxcd.io/v2\nkind: HelmRelease\nmetadata:\n  name: flagger\n  namespace: flagger-system\nspec:\n  interval: 1h\n  chart:\n    spec:\n      chart: flagger\n      sourceRef:\n        kind: HelmRepository\n        name: flagger\n        namespace: flux-system\n  values:\n    meshProvider: istio\n    metricsServer: http://prometheus.monitoring:9090\n```\n\n### Canary CRD\n\n```yaml\napiVersion: flagger.app/v1beta1\nkind: Canary\nmetadata:\n  name: myapp\n  namespace: production\nspec:\n  targetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: myapp\n  \n  # Istio VirtualService ìë™ ìƒì„±\n  service:\n    port: 8080\n    targetPort: 8080\n  \n  # ë¶„ì„ ì„¤ì •\n  analysis:\n    interval: 1m           # ë¶„ì„ ì£¼ê¸°\n    threshold: 5           # ìµœëŒ€ ì‹¤íŒ¨ íšŸìˆ˜\n    maxWeight: 50          # ìµœëŒ€ Canary íŠ¸ë˜í”½\n    stepWeight: 10         # ë‹¨ê³„ë³„ ì¦ê°€ëŸ‰\n    \n    metrics:\n    - name: request-success-rate\n      thresholdRange:\n        min: 99\n      interval: 1m\n    \n    - name: request-duration\n      thresholdRange:\n        max: 500\n      interval: 1m\n    \n    # Webhook í…ŒìŠ¤íŠ¸\n    webhooks:\n    - name: smoke-test\n      type: pre-rollout\n      url: http://flagger-loadtester/\n      timeout: 30s\n      metadata:\n        type: bash\n        cmd: \"curl -s http://myapp-canary:8080/health | grep ok\"\n```\n\n### ë™ì‘ íë¦„\n\n```mermaid\nsequenceDiagram\n    participant Git as GitOps Repo\n    participant Flux as Flux\n    participant Flagger as Flagger\n    participant Istio as Istio\n    participant Prom as Prometheus\n    \n    Git->>Flux: ìƒˆ ì´ë¯¸ì§€ ê°ì§€\n    Flux->>Flux: Deployment ì—…ë°ì´íŠ¸\n    \n    Flagger->>Flagger: Canary Deployment ìƒì„±\n    Flagger->>Istio: VirtualService ì—…ë°ì´íŠ¸ (10% canary)\n    \n    loop ë¶„ì„\n        Flagger->>Prom: ì„±ê³µë¥  í™•ì¸\n        alt ì„±ê³µë¥  >= 99%\n            Flagger->>Istio: íŠ¸ë˜í”½ ì¦ê°€ (20%, 30%, ...)\n        else ì‹¤íŒ¨\n            Flagger->>Istio: ë¡¤ë°± (100% primary)\n            Flagger->>Flagger: Canary ì‚­ì œ\n        end\n    end\n    \n    Flagger->>Flux: Primary ì—…ë°ì´íŠ¸\n    Flagger->>Istio: 100% primary\n```\n\n---\n\n## í”„ë¡œë•ì…˜ GitOps ì›Œí¬í”Œë¡œìš°\n\nëª¨ë“  ê²ƒì„ ì¢…í•©í•œ í”„ë¡œë•ì…˜ ì›Œí¬í”Œë¡œìš°ì…ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph Dev [ê°œë°œ]\n        Code[ì½”ë“œ ì‘ì„±]\n        PR[Pull Request]\n        Review[ì½”ë“œ ë¦¬ë·°]\n    end\n    \n    subgraph CI [CI Pipeline]\n        Build[ë¹Œë“œ]\n        Test[í…ŒìŠ¤íŠ¸]\n        Push[ì´ë¯¸ì§€ Push]\n    end\n    \n    subgraph GitOps [GitOps Repo]\n        ImageUpdate[Image Updater\\në˜ëŠ” Flux Image Automation]\n        Manifest[K8s Manifests]\n    end\n    \n    subgraph CD [CD - Progressive Delivery]\n        Agent[ArgoCD / Flux]\n        Rollout[Argo Rollouts / Flagger]\n        Canary[Canary ë¶„ì„]\n    end\n    \n    subgraph Prod [Production]\n        Cluster[Kubernetes Cluster]\n        Monitor[Prometheus/Grafana]\n    end\n    \n    Code --> PR --> Review\n    Review -->|Merge| Build --> Test --> Push\n    Push -->|ìƒˆ ì´ë¯¸ì§€ ê°ì§€| ImageUpdate\n    ImageUpdate --> Manifest\n    Manifest --> Agent\n    Agent --> Rollout --> Canary\n    Canary -->|ì„±ê³µ| Cluster\n    Canary <-->|ë©”íŠ¸ë¦­ ìˆ˜ì§‘| Monitor\n    \n    Canary -->|ì‹¤íŒ¨| Rollback[ìë™ ë¡¤ë°±]\n```\n\n### ë ˆí¬ì§€í† ë¦¬ êµ¬ì¡° (ìµœì¢…)\n\n```\ngitops-repo/\nâ”œâ”€â”€ clusters/\nâ”‚   â”œâ”€â”€ dev/\nâ”‚   â”‚   â””â”€â”€ kustomization.yaml\nâ”‚   â”œâ”€â”€ staging/\nâ”‚   â”‚   â””â”€â”€ kustomization.yaml\nâ”‚   â””â”€â”€ prod/\nâ”‚       â””â”€â”€ kustomization.yaml\nâ”‚\nâ”œâ”€â”€ infrastructure/\nâ”‚   â”œâ”€â”€ base/\nâ”‚   â”‚   â”œâ”€â”€ ingress-nginx/\nâ”‚   â”‚   â”œâ”€â”€ cert-manager/\nâ”‚   â”‚   â”œâ”€â”€ monitoring/\nâ”‚   â”‚   â””â”€â”€ flagger/\nâ”‚   â””â”€â”€ overlays/\nâ”‚       â”œâ”€â”€ dev/\nâ”‚       â””â”€â”€ prod/\nâ”‚\nâ”œâ”€â”€ apps/\nâ”‚   â”œâ”€â”€ frontend/\nâ”‚   â”‚   â”œâ”€â”€ base/\nâ”‚   â”‚   â”‚   â”œâ”€â”€ deployment.yaml\nâ”‚   â”‚   â”‚   â”œâ”€â”€ service.yaml\nâ”‚   â”‚   â”‚   â”œâ”€â”€ canary.yaml        # Flagger Canary\nâ”‚   â”‚   â”‚   â””â”€â”€ kustomization.yaml\nâ”‚   â”‚   â””â”€â”€ overlays/\nâ”‚   â”‚       â”œâ”€â”€ dev/\nâ”‚   â”‚       â”œâ”€â”€ staging/\nâ”‚   â”‚       â””â”€â”€ prod/\nâ”‚   â””â”€â”€ backend/\nâ”‚       â””â”€â”€ ...\nâ”‚\nâ””â”€â”€ image-automation/              # Flux Image Automation\n    â”œâ”€â”€ image-repositories.yaml\n    â”œâ”€â”€ image-policies.yaml\n    â””â”€â”€ image-update-automation.yaml\n```\n\n---\n\n## ì •ë¦¬: GitOps ì‹œë¦¬ì¦ˆ ì´ì •ë¦¬\n\n6í¸ì˜ ì‹œë¦¬ì¦ˆë¥¼ í†µí•´ ë‹¤ë£¬ ë‚´ìš©ì„ ì •ë¦¬í•©ë‹ˆë‹¤.\n\n| í¸ | ì£¼ì œ | í•µì‹¬ ë©”ì‹œì§€ |\n|---|------|-----------|\n| 1 | GitOps ê°œìš” | Git = SSOT, Pull ëª¨ë¸, Reconciliation |\n| 2 | ArgoCD | ëª¨ë†€ë¦¬ì‹ ì•„í‚¤í…ì²˜, ê°•ë ¥í•œ UI, ApplicationSet |\n| 3 | Flux CD | ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì»¨íŠ¸ë¡¤ëŸ¬, Image Automation ë‚´ì¥ |\n| 4 | ì„¤ì • ê´€ë¦¬ | Kustomize(ìˆœìˆ˜ YAML) vs Helm(í…œí”Œë¦¿) |\n| 5 | Secrets | Sealed Secrets, ESO, SOPS |\n| 6 | CI/CD í†µí•© | CIì™€ CD ë¶„ë¦¬, Progressive Delivery |\n\n### GitOps ì„±ìˆ™ë„ ëª¨ë¸\n\n```mermaid\nflowchart LR\n    L1[Level 1\\nGitì— ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì €ì¥]\n    L2[Level 2\\nGitOps Agent ë„ì…]\n    L3[Level 3\\ní™˜ê²½ë³„ ì„¤ì • ë¶„ë¦¬]\n    L4[Level 4\\nSecrets ìë™í™”]\n    L5[Level 5\\nProgressive Delivery]\n    \n    L1 --> L2 --> L3 --> L4 --> L5\n```\n\n### ì‹œì‘í•˜ê¸° ê¶Œì¥ ìˆœì„œ\n\n1. **ArgoCD ë˜ëŠ” Flux ì„¤ì¹˜**\n2. **ê°„ë‹¨í•œ ì•±ìœ¼ë¡œ GitOps ì›Œí¬í”Œë¡œìš° ê²½í—˜**\n3. **Kustomizeë¡œ í™˜ê²½ë³„ ì„¤ì • ë¶„ë¦¬**\n4. **Sealed Secrets ë˜ëŠ” ESO ë„ì…**\n5. **Image Updaterë¡œ ìë™í™”**\n6. **Argo Rollouts ë˜ëŠ” Flaggerë¡œ Progressive Delivery**\n\n---\n\n## ë§ˆì¹˜ë©°\n\nGitOpsëŠ” ë‹¨ìˆœí•œ ë„êµ¬ê°€ ì•„ë‹Œ **ìš´ì˜ ì² í•™**ì…ë‹ˆë‹¤. Gitì„ ì¤‘ì‹¬ìœ¼ë¡œ ì„ ì–¸ì  ì¸í”„ë¼ë¥¼ ê´€ë¦¬í•˜ê³ , ìë™í™”ëœ Reconciliationìœ¼ë¡œ ì¼ê´€ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.\n\nì´ ì‹œë¦¬ì¦ˆê°€ ì—¬ëŸ¬ë¶„ì˜ GitOps ì—¬ì •ì— ë„ì›€ì´ ë˜ê¸¸ ë°”ëë‹ˆë‹¤. ğŸš€\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [ArgoCD Image Updater](https://argocd-image-updater.readthedocs.io/)\n- [Flux Image Automation](https://fluxcd.io/flux/guides/image-update/)\n- [Argo Rollouts](https://argoproj.github.io/argo-rollouts/)\n- [Flagger](https://flagger.app/)\n- [Progressive Delivery with Flux](https://fluxcd.io/flagger/)\n- [GitOps Working Group - CNCF](https://github.com/cncf/tag-app-delivery/tree/main/gitops-wg)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitOps",
      "Kubernetes",
      "CI/CD",
      "ArgoCD"
    ],
    "readingTime": 10,
    "wordCount": 1933,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitops",
    "date": "2026-01-05"
  },
  {
    "id": "gitops-05-secrets-management",
    "slug": "gitops-05-secrets-management",
    "path": "backend/gitops",
    "fullPath": "backend/gitops/gitops-05-secrets-management",
    "title": "GitOps ì‹¬í™” ì‹œë¦¬ì¦ˆ #5: Secrets Management - Gitì— ë¹„ë°€ì„ ì•ˆì „í•˜ê²Œ ì €ì¥í•˜ê¸°",
    "excerpt": "GitOpsì—ì„œ Secretsë¥¼ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ëŠ” ë°©ë²•. Sealed Secrets, External Secrets Operator, SOPSì˜ ë™ì‘ ì›ë¦¬ì™€ ì„ íƒ ê¸°ì¤€ì„ ë‹¤ë£¹ë‹ˆë‹¤.",
    "content": "# GitOps ì‹¬í™” ì‹œë¦¬ì¦ˆ #5: Secrets Management - Gitì— ë¹„ë°€ì„ ì•ˆì „í•˜ê²Œ ì €ì¥í•˜ê¸°\n\n## ì‹œë¦¬ì¦ˆ ê°œìš”\n\n| # | ì£¼ì œ | í•µì‹¬ ë‚´ìš© |\n|---|------|----------|\n| 1 | GitOps ê°œìš” | ì² í•™ê³¼ ì›ì¹™, Push vs Pull ë°°í¬, Reconciliation |\n| 2 | ArgoCD Deep Dive | ì•„í‚¤í…ì²˜, Application CRD, Sync ì „ëµ |\n| 3 | Flux CD & GitOps Toolkit | ì»¨íŠ¸ë¡¤ëŸ¬ ì•„í‚¤í…ì²˜, GitRepository, Kustomization |\n| 4 | í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬ | Kustomize vs Helm, ì „ëµ ì„ íƒ ê¸°ì¤€ |\n| **5** | **Secrets Management** | Sealed Secrets, External Secrets, SOPS |\n| 6 | CI/CD íŒŒì´í”„ë¼ì¸ í†µí•© | Image Updater, Progressive Delivery |\n\n---\n\n## GitOpsì—ì„œ Secretsì˜ ë”œë ˆë§ˆ\n\nGitOpsì˜ í•µì‹¬ ì›ì¹™ì€ **Gitì„ Single Source of Truth**ë¡œ ì‚¼ëŠ” ê²ƒì…ë‹ˆë‹¤. í•˜ì§€ë§Œ Kubernetes Secretì„ Gitì— ì €ì¥í•˜ë©´?\n\n```yaml\n# âš ï¸ ì ˆëŒ€ ì´ë ‡ê²Œ í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤!\napiVersion: v1\nkind: Secret\nmetadata:\n  name: db-credentials\ntype: Opaque\ndata:\n  username: YWRtaW4=        # base64ëŠ” ì•”í˜¸í™”ê°€ ì•„ë‹˜!\n  password: c3VwZXJzZWNyZXQ=  # ëˆ„êµ¬ë‚˜ ë””ì½”ë”© ê°€ëŠ¥\n```\n\n```bash\n# ì¦‰ì‹œ í‰ë¬¸ ë…¸ì¶œ\necho \"c3VwZXJzZWNyZXQ=\" | base64 -d\n# ì¶œë ¥: supersecret\n```\n\n> [!CAUTION]\n> **Base64ëŠ” ì¸ì½”ë”©ì´ì§€ ì•”í˜¸í™”ê°€ ì•„ë‹™ë‹ˆë‹¤**. Git íˆìŠ¤í† ë¦¬ì— í•œ ë²ˆ ë“¤ì–´ê°€ë©´ ì˜êµ¬íˆ ë…¸ì¶œë©ë‹ˆë‹¤.\n\n### ë”œë ˆë§ˆì˜ ë³¸ì§ˆ\n\n```mermaid\nflowchart TB\n    subgraph Problem [GitOps Secrets ë”œë ˆë§ˆ]\n        SSOT[Git = Single Source of Truth]\n        Secret[Secretsë„ Gitì— ìˆì–´ì•¼?]\n        Risk[í‰ë¬¸ ë…¸ì¶œ ìœ„í—˜]\n    end\n    \n    SSOT --> Secret\n    Secret --> Risk\n    \n    subgraph Solutions [í•´ê²° ë°©ë²•]\n        Encrypted[ì•”í˜¸í™”í•´ì„œ Gitì— ì €ì¥]\n        External[ì™¸ë¶€ ì €ì¥ì†Œ ì°¸ì¡°]\n    end\n    \n    Risk --> Encrypted\n    Risk --> External\n    \n    Encrypted --> SealedSecrets[Sealed Secrets]\n    Encrypted --> SOPS[SOPS]\n    External --> ESO[External Secrets Operator]\n    External --> CSI[Secrets Store CSI Driver]\n```\n\n---\n\n## í•´ê²°ì±… 1: Sealed Secrets\n\n**Sealed Secrets**ëŠ” Bitnamiì—ì„œ ê°œë°œí•œ Kubernetes ì»¨íŠ¸ë¡¤ëŸ¬ë¡œ, í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œë§Œ ë³µí˜¸í™” ê°€ëŠ¥í•œ ì•”í˜¸í™”ëœ Secretì„ Gitì— ì €ì¥í•©ë‹ˆë‹¤.\n\n### ë™ì‘ ì›ë¦¬\n\n```mermaid\nsequenceDiagram\n    participant Dev as ê°œë°œì\n    participant CLI as kubeseal CLI\n    participant Controller as Sealed Secrets Controller\n    participant K8s as Kubernetes API\n    \n    Note over Controller: RSA í‚¤ ìŒ ë³´ìœ  (private/public)\n    \n    Dev->>CLI: ì›ë³¸ Secret YAML ì œê³µ\n    CLI->>Controller: ê³µê°œí‚¤ ìš”ì²­\n    Controller-->>CLI: ê³µê°œí‚¤ ë°˜í™˜\n    CLI->>CLI: ê³µê°œí‚¤ë¡œ ì•”í˜¸í™”\n    CLI->>Dev: SealedSecret YAML ìƒì„±\n    \n    Dev->>K8s: git push â†’ GitOps â†’ SealedSecret ì ìš©\n    K8s->>Controller: SealedSecret ê°ì§€\n    Controller->>Controller: ë¹„ë°€í‚¤ë¡œ ë³µí˜¸í™”\n    Controller->>K8s: ì¼ë°˜ Secret ìƒì„±\n```\n\n### ì„¤ì¹˜\n\n```bash\n# ì»¨íŠ¸ë¡¤ëŸ¬ ì„¤ì¹˜\nhelm repo add sealed-secrets https://bitnami-labs.github.io/sealed-secrets\nhelm install sealed-secrets sealed-secrets/sealed-secrets \\\n  -n kube-system\n\n# CLI ì„¤ì¹˜ (macOS)\nbrew install kubeseal\n```\n\n### ì‚¬ìš©ë²•\n\n```bash\n# 1. ì›ë³¸ Secret ìƒì„± (ì ìš©í•˜ì§€ ì•ŠìŒ!)\nkubectl create secret generic db-credentials \\\n  --from-literal=username=admin \\\n  --from-literal=password=supersecret \\\n  --dry-run=client -o yaml > secret.yaml\n\n# 2. Sealed Secretìœ¼ë¡œ ì•”í˜¸í™”\nkubeseal --format yaml < secret.yaml > sealed-secret.yaml\n\n# 3. ì›ë³¸ ì‚­ì œ, Sealed Secretë§Œ Gitì— ì»¤ë°‹\nrm secret.yaml\ngit add sealed-secret.yaml\ngit commit -m \"Add encrypted database credentials\"\n```\n\n### ìƒì„±ëœ SealedSecret\n\n```yaml\napiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  name: db-credentials\n  namespace: default\nspec:\n  encryptedData:\n    username: AgBy8BQ...ì•”í˜¸í™”ëœ_ë°ì´í„°...==\n    password: AgCtr2I...ì•”í˜¸í™”ëœ_ë°ì´í„°...==\n  template:\n    metadata:\n      name: db-credentials\n      namespace: default\n    type: Opaque\n```\n\n### Scope ì„¤ì •\n\nSealedSecretì€ **ì–´ë–¤ ì¡°ê±´ì—ì„œ ë³µí˜¸í™”ë¥¼ í—ˆìš©í• ì§€** ë²”ìœ„(scope)ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n```bash\n# strict (ê¸°ë³¸): ë™ì¼í•œ namespace + nameë§Œ í—ˆìš©\nkubeseal --scope strict\n\n# namespace-wide: ë™ì¼ namespace ë‚´ ë‹¤ë¥¸ ì´ë¦„ í—ˆìš©\nkubeseal --scope namespace-wide\n\n# cluster-wide: ëª¨ë“  namespaceì—ì„œ ì‚¬ìš© ê°€ëŠ¥\nkubeseal --scope cluster-wide\n```\n\n| Scope | ì´ë¦„ ë³€ê²½ | ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë³€ê²½ | ë³´ì•ˆ ìˆ˜ì¤€ |\n|-------|---------|----------------|----------|\n| `strict` | âŒ | âŒ | ë†’ìŒ |\n| `namespace-wide` | âœ… | âŒ | ì¤‘ê°„ |\n| `cluster-wide` | âœ… | âœ… | ë‚®ìŒ |\n\n### í‚¤ ê´€ë¦¬\n\n> [!WARNING]\n> Sealed Secrets ì»¨íŠ¸ë¡¤ëŸ¬ì˜ **ë¹„ë°€í‚¤ê°€ ìœ ì¶œë˜ë©´ ëª¨ë“  SealedSecretì´ ë³µí˜¸í™”**ë©ë‹ˆë‹¤. í‚¤ ë°±ì—…ì´ í•„ìˆ˜ì…ë‹ˆë‹¤.\n\n```bash\n# í‚¤ ë°±ì—…\nkubectl get secret -n kube-system \\\n  -l sealedsecrets.bitnami.com/sealed-secrets-key \\\n  -o yaml > sealed-secrets-key-backup.yaml\n\n# í‚¤ ë³µì› (ì¬í•´ ë³µêµ¬ ì‹œ)\nkubectl apply -f sealed-secrets-key-backup.yaml\nkubectl delete pod -n kube-system -l app.kubernetes.io/name=sealed-secrets\n```\n\n### í•œê³„\n\n1. **í´ëŸ¬ìŠ¤í„° ì¢…ì†**: ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì—ì„œëŠ” ë³µí˜¸í™” ë¶ˆê°€\n2. **í‚¤ ë¡œí…Œì´ì…˜ ë³µì¡**: í‚¤ ë³€ê²½ ì‹œ ëª¨ë“  SealedSecret ì¬ì•”í˜¸í™” í•„ìš”\n3. **ë‹¨ì¼ ì‹¤íŒ¨ì **: ì»¨íŠ¸ë¡¤ëŸ¬ ì¥ì•  ì‹œ Secret ìƒì„± ë¶ˆê°€\n\n---\n\n## í•´ê²°ì±… 2: External Secrets Operator (ESO)\n\n**External Secrets Operator**ëŠ” ì™¸ë¶€ ë¹„ë°€ ê´€ë¦¬ ì‹œìŠ¤í…œ(AWS Secrets Manager, HashiCorp Vault ë“±)ì—ì„œ Secretì„ ê°€ì ¸ì™€ Kubernetes Secretìœ¼ë¡œ ë™ê¸°í™”í•©ë‹ˆë‹¤.\n\n### ë™ì‘ ì›ë¦¬\n\n```mermaid\nflowchart LR\n    subgraph External [ì™¸ë¶€ Secret ì €ì¥ì†Œ]\n        AWS[AWS Secrets Manager]\n        Vault[HashiCorp Vault]\n        GCP[GCP Secret Manager]\n        Azure[Azure Key Vault]\n    end\n    \n    subgraph Cluster [Kubernetes Cluster]\n        ESO[External Secrets\\nOperator]\n        ExtSecret[ExternalSecret CR]\n        K8sSecret[Kubernetes Secret]\n    end\n    \n    ExtSecret -->|ì°¸ì¡°| ESO\n    ESO -->|API í˜¸ì¶œ| AWS & Vault & GCP & Azure\n    ESO -->|ìƒì„±/ë™ê¸°í™”| K8sSecret\n```\n\n**í•µì‹¬ í¬ì¸íŠ¸**: Gitì—ëŠ” **ExternalSecret (ì°¸ì¡° ì •ë³´)**ë§Œ ì €ì¥í•˜ê³ , ì‹¤ì œ ë¹„ë°€ ê°’ì€ ì™¸ë¶€ ì €ì¥ì†Œì— ìˆìŠµë‹ˆë‹¤.\n\n### ì„¤ì¹˜\n\n```bash\nhelm repo add external-secrets https://charts.external-secrets.io\nhelm install external-secrets external-secrets/external-secrets \\\n  -n external-secrets --create-namespace\n```\n\n### SecretStore & ClusterSecretStore\n\në¨¼ì € ì™¸ë¶€ ì €ì¥ì†Œì™€ì˜ ì—°ê²°ì„ ì„¤ì •í•©ë‹ˆë‹¤:\n\n```yaml\n# ClusterSecretStore (í´ëŸ¬ìŠ¤í„° ì „ì—­)\napiVersion: external-secrets.io/v1beta1\nkind: ClusterSecretStore\nmetadata:\n  name: aws-secrets-manager\nspec:\n  provider:\n    aws:\n      service: SecretsManager\n      region: ap-northeast-2\n      auth:\n        jwt:\n          serviceAccountRef:\n            name: external-secrets-sa\n            namespace: external-secrets\n```\n\n```yaml\n# AWS IAM Role for Service Account (IRSA) ì„¤ì • í•„ìš”\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-secrets-sa\n  namespace: external-secrets\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::123456789:role/external-secrets-role\n```\n\n### ExternalSecret\n\n```yaml\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: db-credentials\n  namespace: production\nspec:\n  # ë™ê¸°í™” ì£¼ê¸°\n  refreshInterval: 1h\n  \n  # SecretStore ì°¸ì¡°\n  secretStoreRef:\n    name: aws-secrets-manager\n    kind: ClusterSecretStore\n  \n  # ìƒì„±í•  Secret ì„¤ì •\n  target:\n    name: db-credentials\n    creationPolicy: Owner\n  \n  # ë°ì´í„° ë§¤í•‘\n  data:\n    - secretKey: username      # K8s Secretì˜ í‚¤\n      remoteRef:\n        key: prod/database     # AWS Secrets Managerì˜ ê²½ë¡œ\n        property: username     # JSON ë‚´ ì†ì„±\n    \n    - secretKey: password\n      remoteRef:\n        key: prod/database\n        property: password\n```\n\n### ì „ì²´ Secret ê°€ì ¸ì˜¤ê¸°\n\n```yaml\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: all-db-credentials\nspec:\n  # ...\n  dataFrom:\n    - extract:\n        key: prod/database  # JSON ì „ì²´ë¥¼ ê°€ì ¸ì™€ì„œ ê° í‚¤ë¥¼ Secret ë°ì´í„°ë¡œ\n```\n\n### ì§€ì›í•˜ëŠ” Provider\n\n| Provider | ì„¤ëª… |\n|----------|------|\n| AWS Secrets Manager | AWS ë„¤ì´í‹°ë¸Œ |\n| AWS Parameter Store | SSM íŒŒë¼ë¯¸í„° |\n| HashiCorp Vault | ì˜¨í”„ë ˆë¯¸ìŠ¤/í´ë¼ìš°ë“œ |\n| GCP Secret Manager | GCP ë„¤ì´í‹°ë¸Œ |\n| Azure Key Vault | Azure ë„¤ì´í‹°ë¸Œ |\n| 1Password | íŒ€ ë¹„ë°€ ê³µìœ  |\n| Doppler | SaaS ë¹„ë°€ ê´€ë¦¬ |\n\n### ì¥ì ê³¼ ë‹¨ì \n\n**ì¥ì **:\n\n- Gitì— ë¹„ë°€ ê°’ì´ ì „í˜€ ì—†ìŒ\n- ìë™ ë¡œí…Œì´ì…˜ ì§€ì›\n- ì¤‘ì•™ ì§‘ì¤‘ì‹ ë¹„ë°€ ê´€ë¦¬\n\n**ë‹¨ì **:\n\n- ì™¸ë¶€ ì„œë¹„ìŠ¤ ì˜ì¡´ì„±\n- ë„¤íŠ¸ì›Œí¬ ì§€ì—°\n- ì¶”ê°€ ë¹„ìš© (AWS Secrets Manager ë“±)\n\n---\n\n## í•´ê²°ì±… 3: SOPS (Secrets OPerationS)\n\n**SOPS**ëŠ” Mozillaì—ì„œ ê°œë°œí•œ íŒŒì¼ ë ˆë²¨ ì•”í˜¸í™” ë„êµ¬ì…ë‹ˆë‹¤. YAML, JSON, ENV íŒŒì¼ì˜ **ê°’ë§Œ ì„ íƒì ìœ¼ë¡œ ì•”í˜¸í™”**í•©ë‹ˆë‹¤.\n\n### ë™ì‘ ì›ë¦¬\n\n```mermaid\nflowchart LR\n    subgraph Encryption [ì•”í˜¸í™” í‚¤]\n        AGE[age key]\n        AWS_KMS[AWS KMS]\n        GCP_KMS[GCP KMS]\n        Azure_KMS[Azure Key Vault]\n        PGP[PGP]\n    end\n    \n    subgraph Files [íŒŒì¼]\n        Plain[í‰ë¬¸ YAML]\n        Encrypted[ì•”í˜¸í™”ëœ YAML\\ní‚¤ëŠ” í‰ë¬¸, ê°’ë§Œ ì•”í˜¸í™”]\n    end\n    \n    Plain -->|sops -e| Encrypted\n    Encrypted -->|sops -d| Plain\n    \n    Encryption --> Plain\n```\n\n### íŠ¹ì§•: í‚¤ëŠ” í‰ë¬¸, ê°’ë§Œ ì•”í˜¸í™”\n\n```yaml\n# ì•”í˜¸í™” í›„ (ì½ê¸° ì‰¬ì›€!)\napiVersion: v1\nkind: Secret\nmetadata:\n  name: db-credentials\nstringData:\n  username: ENC[AES256_GCM,data:6FLiRc8=,iv:...,tag:...,type:str]\n  password: ENC[AES256_GCM,data:HdNqmY3WUr8=,iv:...,tag:...,type:str]\nsops:\n  age:\n    - recipient: age1...\n      enc: |\n        -----BEGIN AGE ENCRYPTED FILE-----\n        ...\n        -----END AGE ENCRYPTED FILE-----\n  lastmodified: \"2024-01-15T10:00:00Z\"\n  mac: ENC[AES256_GCM,...]\n  version: 3.8.1\n```\n\n> [!TIP]\n> **Git diffê°€ ì˜ë¯¸ìˆìŠµë‹ˆë‹¤**. ì–´ë–¤ í‚¤ê°€ ë³€ê²½ë˜ì—ˆëŠ”ì§€ ë°”ë¡œ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n### Age í‚¤ë¡œ ì‚¬ìš©í•˜ê¸° (ê¶Œì¥)\n\n```bash\n# age ì„¤ì¹˜\nbrew install age\n\n# í‚¤ ìŒ ìƒì„±\nage-keygen -o key.txt\n# Public key: age1abc...\n# Private key ì €ì¥ë¨\n\n# SOPS ì„¤ì¹˜\nbrew install sops\n\n# .sops.yaml ì„¤ì • (ë ˆí¬ì§€í† ë¦¬ ë£¨íŠ¸)\ncat > .sops.yaml << EOF\ncreation_rules:\n  - path_regex: .*secrets.*\\.yaml$\n    age: age1abc...  # ê³µê°œí‚¤\nEOF\n```\n\n### ì•”í˜¸í™”/ë³µí˜¸í™”\n\n```bash\n# ì•”í˜¸í™”\nsops -e secrets.yaml > secrets.enc.yaml\n\n# ë³µí˜¸í™”\nsops -d secrets.enc.yaml > secrets.yaml\n\n# ì œìë¦¬ í¸ì§‘ (ë³µí˜¸í™” â†’ í¸ì§‘ â†’ ì €ì¥ ì‹œ ì¬ì•”í˜¸í™”)\nsops secrets.enc.yaml\n```\n\n### Fluxì™€ SOPS í†µí•©\n\nFluxëŠ” SOPSë¥¼ **ë„¤ì´í‹°ë¸Œë¡œ ì§€ì›**í•©ë‹ˆë‹¤:\n\n```yaml\n# ë³µí˜¸í™” í‚¤ë¥¼ Secretìœ¼ë¡œ ì €ì¥\napiVersion: v1\nkind: Secret\nmetadata:\n  name: sops-age\n  namespace: flux-system\nstringData:\n  age.agekey: |\n    # created: 2024-01-15\n    # public key: age1abc...\n    AGE-SECRET-KEY-1...\n\n---\n# Kustomizationì—ì„œ ë³µí˜¸í™” í™œì„±í™”\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: my-app\n  namespace: flux-system\nspec:\n  # ...\n  decryption:\n    provider: sops\n    secretRef:\n      name: sops-age\n```\n\n### ArgoCDì™€ SOPS\n\nArgoCDëŠ” í”ŒëŸ¬ê·¸ì¸ì„ í†µí•´ SOPSë¥¼ ì§€ì›í•©ë‹ˆë‹¤:\n\n```yaml\n# argocd-cm ConfigMap\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cm\n  namespace: argocd\ndata:\n  configManagementPlugins: |\n    - name: kustomize-sops\n      generate:\n        command: [\"bash\", \"-c\"]\n        args: [\"kustomize build . | sops -d /dev/stdin\"]\n```\n\n### AWS KMSì™€ í•¨ê»˜ ì‚¬ìš©\n\n```yaml\n# .sops.yaml\ncreation_rules:\n  - path_regex: .*prod.*secrets.*\\.yaml$\n    kms: arn:aws:kms:ap-northeast-2:123456789:key/abc-def\n    \n  - path_regex: .*dev.*secrets.*\\.yaml$\n    kms: arn:aws:kms:ap-northeast-2:123456789:key/xyz-123\n```\n\n```bash\n# í™˜ê²½ë³„ ë‹¤ë¥¸ KMS í‚¤ ì‚¬ìš©\nsops -e --kms arn:aws:kms:... secrets.yaml\n```\n\n---\n\n## ì „ëµ ì„ íƒ ê°€ì´ë“œ\n\n```mermaid\nflowchart TB\n    Start[Secrets ê´€ë¦¬ ì „ëµ ì„ íƒ] --> Q1{ì™¸ë¶€ ë¹„ë°€ ê´€ë¦¬ ì‹œìŠ¤í…œ\\nì´ë¯¸ ì‚¬ìš© ì¤‘?}\n    \n    Q1 -->|Yes| ESO[External Secrets Operator]\n    Q1 -->|No| Q2{ë©€í‹° í´ëŸ¬ìŠ¤í„°?}\n    \n    Q2 -->|Yes| Q3{ì¤‘ì•™ ë¹„ë°€ ì €ì¥ì†Œ\\në„ì… ê°€ëŠ¥?}\n    Q3 -->|Yes| ESO\n    Q3 -->|No| SOPS[SOPS + age/KMS]\n    \n    Q2 -->|No| Q4{ë‹¨ìˆœí•¨ ìš°ì„ ?}\n    Q4 -->|Yes| SealedSecrets[Sealed Secrets]\n    Q4 -->|No| SOPS\n```\n\n### ë¹„êµ í‘œ\n\n| ê´€ì  | Sealed Secrets | ESO | SOPS |\n|-----|----------------|-----|------|\n| **Gitì— ì €ì¥ë˜ëŠ” ê²ƒ** | ì•”í˜¸í™”ëœ SealedSecret | ì°¸ì¡°(ExternalSecret) | ì•”í˜¸í™”ëœ íŒŒì¼ |\n| **ë³µí˜¸í™” ìœ„ì¹˜** | í´ëŸ¬ìŠ¤í„° ë‚´ | ì™¸ë¶€ ì €ì¥ì†Œ | í´ëŸ¬ìŠ¤í„°/ë¡œì»¬ |\n| **ë©€í‹° í´ëŸ¬ìŠ¤í„°** | âŒ (ê° í´ëŸ¬ìŠ¤í„° í‚¤ ë‹¤ë¦„) | âœ… (ì¤‘ì•™ ì €ì¥ì†Œ) | âœ… (ê°™ì€ í‚¤ ê³µìœ ) |\n| **ë¹„ë°€ ë¡œí…Œì´ì…˜** | ìˆ˜ë™ | ìë™ (refreshInterval) | ìˆ˜ë™ |\n| **ì™¸ë¶€ ì˜ì¡´ì„±** | ì—†ìŒ | ìˆìŒ (Vault, AWS ë“±) | ì„ íƒì  (age vs KMS) |\n| **Flux ì§€ì›** | âœ… | âœ… | âœ… (ë„¤ì´í‹°ë¸Œ) |\n| **ArgoCD ì§€ì›** | âœ… | âœ… | âœ… (í”ŒëŸ¬ê·¸ì¸) |\n| **í•™ìŠµ ê³¡ì„ ** | ë‚®ìŒ | ì¤‘ê°„ | ì¤‘ê°„ |\n\n### ê¶Œì¥ ì‹œë‚˜ë¦¬ì˜¤\n\n| ìƒí™© | ê¶Œì¥ ì†”ë£¨ì…˜ |\n|-----|-----------|\n| ë‹¨ì¼ í´ëŸ¬ìŠ¤í„°, ë¹ ë¥¸ ì‹œì‘ | Sealed Secrets |\n| AWS/GCP/Azure ì‚¬ìš©, ì¤‘ì•™ ê´€ë¦¬ | External Secrets Operator |\n| ë©€í‹° í´ëŸ¬ìŠ¤í„°, Flux ì‚¬ìš© | SOPS + age |\n| ê¸°ì¡´ Vault ì¸í”„ë¼ | ESO + Vault |\n| ê·œì œ ìš”êµ¬ì‚¬í•­ (ê°ì‚¬ ë¡œê·¸ í•„ìš”) | ESO + AWS Secrets Manager |\n\n---\n\n## ë³´ì•ˆ ëª¨ë²” ì‚¬ë¡€\n\n### 1. ìµœì†Œ ê¶Œí•œ ì›ì¹™\n\n```yaml\n# ESO ServiceAccountì— í•„ìš”í•œ ê¶Œí•œë§Œ ë¶€ì—¬\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"secretsmanager:GetSecretValue\"\n      ],\n      \"Resource\": [\n        \"arn:aws:secretsmanager:*:*:secret:prod/*\"\n      ]\n    }\n  ]\n}\n```\n\n### 2. ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê²©ë¦¬\n\n```yaml\n# SecretStoreë¥¼ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ë¡œ ë¶„ë¦¬\napiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: team-a-secrets\n  namespace: team-a\nspec:\n  provider:\n    aws:\n      service: SecretsManager\n      region: ap-northeast-2\n      # team-a ì „ìš© IAM Role\n      auth:\n        jwt:\n          serviceAccountRef:\n            name: team-a-eso-sa\n```\n\n### 3. ìë™ ë¡œí…Œì´ì…˜\n\n```yaml\n# ESO: 1ì‹œê°„ë§ˆë‹¤ ë™ê¸°í™”\nspec:\n  refreshInterval: 1h\n\n# AWS Secrets Manager: ìë™ ë¡œí…Œì´ì…˜ í™œì„±í™”\n# Lambda í•¨ìˆ˜ê°€ ì£¼ê¸°ì ìœ¼ë¡œ ë¹„ë°€ ê°’ ë³€ê²½\n```\n\n### 4. ê°ì‚¬ ë¡œê¹…\n\n```bash\n# AWS CloudTrailì—ì„œ Secret ì ‘ê·¼ ë¡œê·¸ í™•ì¸\naws cloudtrail lookup-events \\\n  --lookup-attributes AttributeKey=EventName,AttributeValue=GetSecretValue\n```\n\n---\n\n## ì •ë¦¬\n\n| ì†”ë£¨ì…˜ | í•µì‹¬ ê°œë… | ì¥ì  | ë‹¨ì  |\n|-------|----------|------|------|\n| **Sealed Secrets** | í´ëŸ¬ìŠ¤í„° í‚¤ë¡œ ì•”í˜¸í™” | ë‹¨ìˆœ, ì™¸ë¶€ ì˜ì¡´ì„± ì—†ìŒ | í´ëŸ¬ìŠ¤í„° ì¢…ì†, í‚¤ ê´€ë¦¬ |\n| **ESO** | ì™¸ë¶€ ì €ì¥ì†Œ ì°¸ì¡° | ì¤‘ì•™ ê´€ë¦¬, ìë™ ë¡œí…Œì´ì…˜ | ì™¸ë¶€ ì˜ì¡´ì„±, ë¹„ìš© |\n| **SOPS** | íŒŒì¼ ë ˆë²¨ ì•”í˜¸í™” | Git diff ê°€ëŠ¥, ìœ ì—°í•¨ | ìˆ˜ë™ ë¡œí…Œì´ì…˜ |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**6í¸: CI/CD íŒŒì´í”„ë¼ì¸ í†µí•©**ì—ì„œëŠ” ë‹¤ìŒì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- GitOpsì—ì„œ CIì™€ CDì˜ ë¶„ë¦¬\n- ArgoCD Image Updater\n- Flux Image Automation\n- Progressive Delivery (Argo Rollouts, Flagger)\n- í”„ë¡œë•ì…˜ GitOps ì›Œí¬í”Œë¡œìš°\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Sealed Secrets](https://github.com/bitnami-labs/sealed-secrets)\n- [External Secrets Operator](https://external-secrets.io/)\n- [SOPS](https://github.com/getsops/sops)\n- [Flux SOPS Integration](https://fluxcd.io/flux/guides/mozilla-sops/)\n- [ArgoCD Secrets Management](https://argo-cd.readthedocs.io/en/stable/operator-manual/secret-management/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitOps",
      "Kubernetes",
      "Security",
      "Secrets"
    ],
    "readingTime": 9,
    "wordCount": 1785,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitops",
    "date": "2026-01-05"
  },
  {
    "id": "gitops-04-config-management",
    "slug": "gitops-04-config-management",
    "path": "backend/gitops",
    "fullPath": "backend/gitops/gitops-04-config-management",
    "title": "GitOps ì‹¬í™” ì‹œë¦¬ì¦ˆ #4: í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬ - Kustomize vs Helm",
    "excerpt": "Kustomizeì˜ Base/Overlays íŒ¨í„´, Helmì˜ values ê´€ë¦¬ ì „ëµ, ê·¸ë¦¬ê³  ì–¸ì œ ë¬´ì—‡ì„ ì„ íƒí•´ì•¼ í•˜ëŠ”ì§€ ì‹¤ë¬´ ê´€ì ì—ì„œ ë‹¤ë£¹ë‹ˆë‹¤.",
    "content": "# GitOps ì‹¬í™” ì‹œë¦¬ì¦ˆ #4: í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬ - Kustomize vs Helm\n\n## ì‹œë¦¬ì¦ˆ ê°œìš”\n\n| # | ì£¼ì œ | í•µì‹¬ ë‚´ìš© |\n|---|------|----------|\n| 1 | GitOps ê°œìš” | ì² í•™ê³¼ ì›ì¹™, Push vs Pull ë°°í¬, Reconciliation |\n| 2 | ArgoCD Deep Dive | ì•„í‚¤í…ì²˜, Application CRD, Sync ì „ëµ |\n| 3 | Flux CD & GitOps Toolkit | ì»¨íŠ¸ë¡¤ëŸ¬ ì•„í‚¤í…ì²˜, GitRepository, Kustomization |\n| **4** | **í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬** | Kustomize vs Helm, ì „ëµ ì„ íƒ ê¸°ì¤€ |\n| 5 | Secrets Management | Sealed Secrets, External Secrets, SOPS |\n| 6 | CI/CD íŒŒì´í”„ë¼ì¸ í†µí•© | Image Updater, Progressive Delivery |\n\n---\n\n## ì™œ í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬ê°€ ì¤‘ìš”í•œê°€?\n\nì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ë™ì¼í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ **ì—¬ëŸ¬ í™˜ê²½(dev, staging, prod)**ì— ë°°í¬í•©ë‹ˆë‹¤. í™˜ê²½ë§ˆë‹¤ ë‹¤ë¥¸ ê²ƒë“¤:\n\n| í•­ëª© | dev | staging | prod |\n|-----|-----|---------|------|\n| replicas | 1 | 2 | 5 |\n| CPU request | 100m | 200m | 500m |\n| DB host | dev-db.local | staging-db.local | prod-db.aws |\n| Log level | debug | info | warn |\n| Ingress domain | dev.example.com | staging.example.com | example.com |\n\n**ë¬¸ì œ**: í™˜ê²½ë§ˆë‹¤ ì™„ì „íˆ ë³„ê°œì˜ YAML íŒŒì¼ì„ ê´€ë¦¬í•˜ë©´?\n\n- ì¤‘ë³µ ì½”ë“œê°€ ëŒ€ëŸ‰ ë°œìƒ\n- í•œ ê³³ ìˆ˜ì • ì‹œ ëª¨ë“  í™˜ê²½ì— ë™ê¸°í™” í•„ìš”\n- ì‹¤ìˆ˜ë¡œ ëˆ„ë½ë˜ëŠ” ë³€ê²½ ë°œìƒ\n\n**í•´ê²°ì±…**: Kustomize ë˜ëŠ” Helmìœ¼ë¡œ **ê³µí†µ ë¶€ë¶„(Base)ê³¼ í™˜ê²½ë³„ ì°¨ì´(Overlays/Values)ë¥¼ ë¶„ë¦¬**\n\n---\n\n## Kustomize: Template-free Configuration\n\nKustomizeëŠ” **í…œí”Œë¦¿ ì—†ì´** YAMLì„ íŒ¨ì¹˜í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. Kubernetes 1.14ë¶€í„° `kubectl` ì— ë‚´ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n### í•µì‹¬ ì² í•™\n\n- **Base**: ê³µí†µ ë¦¬ì†ŒìŠ¤ ì •ì˜\n- **Overlays**: í™˜ê²½ë³„ íŒ¨ì¹˜\n- **No templating**: Go template ê°™ì€ ë¬¸ë²• ì—†ìŒ\n- **Pure YAML**: ê²°ê³¼ë¬¼ë„ ìˆœìˆ˜ YAML\n\n### ë””ë ‰í† ë¦¬ êµ¬ì¡°\n\n```\nmyapp/\nâ”œâ”€â”€ base/\nâ”‚   â”œâ”€â”€ kustomization.yaml\nâ”‚   â”œâ”€â”€ deployment.yaml\nâ”‚   â”œâ”€â”€ service.yaml\nâ”‚   â””â”€â”€ configmap.yaml\nâ””â”€â”€ overlays/\n    â”œâ”€â”€ dev/\n    â”‚   â”œâ”€â”€ kustomization.yaml\n    â”‚   â””â”€â”€ patch-replicas.yaml\n    â”œâ”€â”€ staging/\n    â”‚   â”œâ”€â”€ kustomization.yaml\n    â”‚   â””â”€â”€ patch-resources.yaml\n    â””â”€â”€ prod/\n        â”œâ”€â”€ kustomization.yaml\n        â”œâ”€â”€ patch-replicas.yaml\n        â”œâ”€â”€ patch-resources.yaml\n        â””â”€â”€ patch-ingress.yaml\n```\n\n### Base ì •ì˜\n\n```yaml\n# base/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n  - deployment.yaml\n  - service.yaml\n  - configmap.yaml\n\ncommonLabels:\n  app: myapp\n```\n\n```yaml\n# base/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: myapp\n        image: myapp:latest\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 200m\n            memory: 256Mi\n```\n\n### Overlay ì •ì˜\n\n```yaml\n# overlays/prod/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n  - ../../base\n\n# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì„¤ì •\nnamespace: production\n\n# ê³µí†µ ë ˆì´ë¸” ì¶”ê°€\ncommonLabels:\n  environment: production\n\n# ì´ë¯¸ì§€ íƒœê·¸ ë³€ê²½\nimages:\n  - name: myapp\n    newTag: v1.2.3\n\n# íŒ¨ì¹˜ ì ìš©\npatches:\n  - path: patch-replicas.yaml\n  - path: patch-resources.yaml\n```\n\n```yaml\n# overlays/prod/patch-replicas.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  replicas: 5\n```\n\n```yaml\n# overlays/prod/patch-resources.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  template:\n    spec:\n      containers:\n      - name: myapp\n        resources:\n          requests:\n            cpu: 500m\n            memory: 512Mi\n          limits:\n            cpu: 1000m\n            memory: 1024Mi\n```\n\n### ë¹Œë“œ ë° í™•ì¸\n\n```bash\n# ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°\nkubectl kustomize overlays/prod\n\n# ì§ì ‘ ì ìš©\nkubectl apply -k overlays/prod\n\n# ë˜ëŠ” ArgoCDì—ì„œ\nargocd app create myapp-prod \\\n  --repo https://github.com/myorg/myapp.git \\\n  --path overlays/prod \\\n  --dest-server https://kubernetes.default.svc\n```\n\n### íŒ¨ì¹˜ ì „ëµ\n\n#### 1. Strategic Merge Patch (ê¸°ë³¸)\n\n```yaml\n# ë°°ì—´ì˜ íŠ¹ì • ìš”ì†Œë§Œ ìˆ˜ì • (nameìœ¼ë¡œ ë§¤ì¹­)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  template:\n    spec:\n      containers:\n      - name: myapp  # ì´ë¦„ìœ¼ë¡œ ë§¤ì¹­\n        env:\n        - name: LOG_LEVEL\n          value: warn\n```\n\n#### 2. JSON Patch\n\në” ì •ë°€í•œ ì œì–´ê°€ í•„ìš”í•  ë•Œ:\n\n```yaml\n# overlays/prod/kustomization.yaml\npatches:\n  - target:\n      kind: Deployment\n      name: myapp\n    patch: |-\n      - op: replace\n        path: /spec/replicas\n        value: 5\n      - op: add\n        path: /spec/template/spec/containers/0/env/-\n        value:\n          name: NEW_VAR\n          value: \"some-value\"\n```\n\n#### 3. Replace\n\nê¸°ì¡´ ê°’ì„ ì™„ì „íˆ ëŒ€ì²´:\n\n```yaml\npatches:\n  - target:\n      kind: ConfigMap\n      name: myconfig\n    patch: |-\n      - op: replace\n        path: /data\n        value:\n          key1: newvalue1\n          key2: newvalue2\n```\n\n### Components (ì¬ì‚¬ìš© ê°€ëŠ¥í•œ íŒ¨ì¹˜ ëª¨ìŒ)\n\nKustomize 3.7+ì—ì„œ ì§€ì›í•˜ëŠ” **Components**ëŠ” ì—¬ëŸ¬ Overlayì—ì„œ ê³µìœ í•  ìˆ˜ ìˆëŠ” íŒ¨ì¹˜ ëª¨ìŒì…ë‹ˆë‹¤:\n\n```\nmyapp/\nâ”œâ”€â”€ base/\nâ”œâ”€â”€ components/\nâ”‚   â”œâ”€â”€ high-availability/\nâ”‚   â”‚   â””â”€â”€ kustomization.yaml\nâ”‚   â”œâ”€â”€ monitoring/\nâ”‚   â”‚   â””â”€â”€ kustomization.yaml\nâ”‚   â””â”€â”€ security/\nâ”‚       â””â”€â”€ kustomization.yaml\nâ””â”€â”€ overlays/\n    â”œâ”€â”€ dev/\n    â”‚   â””â”€â”€ kustomization.yaml\n    â””â”€â”€ prod/\n        â””â”€â”€ kustomization.yaml\n```\n\n```yaml\n# components/high-availability/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1alpha1\nkind: Component\n\npatches:\n  - patch: |-\n      apiVersion: apps/v1\n      kind: Deployment\n      metadata:\n        name: myapp\n      spec:\n        replicas: 3\n        strategy:\n          type: RollingUpdate\n          rollingUpdate:\n            maxSurge: 1\n            maxUnavailable: 0\n```\n\n```yaml\n# overlays/prod/kustomization.yaml\nresources:\n  - ../../base\n\ncomponents:\n  - ../../components/high-availability\n  - ../../components/monitoring\n```\n\n---\n\n## Helm: The Package Manager for Kubernetes\n\nHelmì€ **í…œí”Œë¦¿ ê¸°ë°˜** íŒ¨í‚¤ì§€ ë§¤ë‹ˆì €ì…ë‹ˆë‹¤. Chartë¼ëŠ” íŒ¨í‚¤ì§€ í˜•íƒœë¡œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë°°í¬í•©ë‹ˆë‹¤.\n\n### í•µì‹¬ ê°œë…\n\n- **Chart**: íŒ¨í‚¤ì§€ (í…œí”Œë¦¿ + ê¸°ë³¸ê°’)\n- **Values**: ì„¤ì • ê°’\n- **Release**: Chartì˜ ì„¤ì¹˜ëœ ì¸ìŠ¤í„´ìŠ¤\n- **Repository**: Chart ì €ì¥ì†Œ\n\n### Chart êµ¬ì¡°\n\n```\nmychart/\nâ”œâ”€â”€ Chart.yaml           # ì°¨íŠ¸ ë©”íƒ€ë°ì´í„°\nâ”œâ”€â”€ values.yaml          # ê¸°ë³¸ values\nâ”œâ”€â”€ values-dev.yaml      # í™˜ê²½ë³„ values\nâ”œâ”€â”€ values-prod.yaml\nâ”œâ”€â”€ templates/\nâ”‚   â”œâ”€â”€ _helpers.tpl     # í…œí”Œë¦¿ í—¬í¼\nâ”‚   â”œâ”€â”€ deployment.yaml\nâ”‚   â”œâ”€â”€ service.yaml\nâ”‚   â”œâ”€â”€ configmap.yaml\nâ”‚   â”œâ”€â”€ ingress.yaml\nâ”‚   â””â”€â”€ NOTES.txt        # ì„¤ì¹˜ í›„ ì¶œë ¥ ë©”ì‹œì§€\nâ””â”€â”€ charts/              # ì˜ì¡´ ì°¨íŠ¸\n```\n\n### Chart.yaml\n\n```yaml\napiVersion: v2\nname: mychart\nversion: 1.0.0\nappVersion: \"1.2.3\"\ndescription: My application chart\ntype: application\n\ndependencies:\n  - name: postgresql\n    version: \"12.x.x\"\n    repository: https://charts.bitnami.com/bitnami\n    condition: postgresql.enabled\n```\n\n### í…œí”Œë¦¿ ì‘ì„±\n\n```yaml\n# templates/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}\n  labels:\n    {{- include \"mychart.labels\" . | nindent 4 }}\nspec:\n  replicas: {{ .Values.replicaCount }}\n  selector:\n    matchLabels:\n      {{- include \"mychart.selectorLabels\" . | nindent 6 }}\n  template:\n    metadata:\n      labels:\n        {{- include \"mychart.selectorLabels\" . | nindent 8 }}\n    spec:\n      containers:\n      - name: {{ .Chart.Name }}\n        image: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\n        ports:\n        - containerPort: {{ .Values.service.port }}\n        resources:\n          {{- toYaml .Values.resources | nindent 12 }}\n        {{- if .Values.env }}\n        env:\n          {{- range $key, $value := .Values.env }}\n          - name: {{ $key }}\n            value: {{ $value | quote }}\n          {{- end }}\n        {{- end }}\n```\n\n### values.yaml\n\n```yaml\n# values.yaml (ê¸°ë³¸ê°’)\nreplicaCount: 1\n\nimage:\n  repository: myapp\n  tag: \"\"\n  pullPolicy: IfNotPresent\n\nservice:\n  type: ClusterIP\n  port: 8080\n\nresources:\n  requests:\n    cpu: 100m\n    memory: 128Mi\n  limits:\n    cpu: 200m\n    memory: 256Mi\n\ningress:\n  enabled: false\n  host: \"\"\n\nenv: {}\n```\n\n```yaml\n# values-prod.yaml\nreplicaCount: 5\n\nimage:\n  tag: v1.2.3\n\nresources:\n  requests:\n    cpu: 500m\n    memory: 512Mi\n  limits:\n    cpu: 1000m\n    memory: 1024Mi\n\ningress:\n  enabled: true\n  host: myapp.example.com\n  tls:\n    - secretName: myapp-tls\n      hosts:\n        - myapp.example.com\n\nenv:\n  LOG_LEVEL: warn\n  DB_HOST: prod-db.example.com\n```\n\n### ë°°í¬\n\n```bash\n# ë¡œì»¬ ì°¨íŠ¸ ì„¤ì¹˜\nhelm install myapp-prod ./mychart -f values-prod.yaml -n production\n\n# ë¦¬í¬ì§€í† ë¦¬ì—ì„œ ì„¤ì¹˜\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install nginx bitnami/nginx -f my-values.yaml\n\n# ì—…ê·¸ë ˆì´ë“œ\nhelm upgrade myapp-prod ./mychart -f values-prod.yaml\n\n# ë¡¤ë°±\nhelm rollback myapp-prod 1\n```\n\n### Helm Hooks\n\níŠ¹ì • ì‹œì ì— ì‘ì—… ì‹¤í–‰:\n\n```yaml\n# templates/pre-upgrade-job.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}-db-migrate\n  annotations:\n    \"helm.sh/hook\": pre-upgrade\n    \"helm.sh/hook-weight\": \"0\"\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    spec:\n      containers:\n      - name: migrate\n        image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\"\n        command: [\"./migrate.sh\"]\n      restartPolicy: Never\n```\n\n| Hook | ì‹œì  |\n|------|-----|\n| `pre-install` | ì„¤ì¹˜ ì „ |\n| `post-install` | ì„¤ì¹˜ í›„ |\n| `pre-upgrade` | ì—…ê·¸ë ˆì´ë“œ ì „ |\n| `post-upgrade` | ì—…ê·¸ë ˆì´ë“œ í›„ |\n| `pre-delete` | ì‚­ì œ ì „ |\n| `post-delete` | ì‚­ì œ í›„ |\n| `pre-rollback` | ë¡¤ë°± ì „ |\n| `post-rollback` | ë¡¤ë°± í›„ |\n\n---\n\n## Kustomize vs Helm: ì„ íƒ ê¸°ì¤€\n\n```mermaid\nflowchart TB\n    Start[í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬ í•„ìš”] --> Q1{ì¨ë“œíŒŒí‹° ì†Œí”„íŠ¸ì›¨ì–´?}\n    \n    Q1 -->|Yes| Helm1[Helm ì‚¬ìš©\\nì´ë¯¸ ì°¨íŠ¸ê°€ ìˆìŒ]\n    Q1 -->|No| Q2{ë³µì¡í•œ ì¡°ê±´ë¶€ ë¡œì§?}\n    \n    Q2 -->|Yes| Helm2[Helm ì‚¬ìš©\\nif/range í…œí”Œë¦¿]\n    Q2 -->|No| Q3{íŒ€ì´ YAMLì— ìµìˆ™?}\n    \n    Q3 -->|Yes| Kustomize1[Kustomize ì‚¬ìš©\\nìˆœìˆ˜ YAML ìœ ì§€]\n    Q3 -->|No| Q4{ë°°í¬ ëŒ€ìƒì´ ë§ìŒ?}\n    \n    Q4 -->|Yes| Helm3[Helm ì‚¬ìš©\\nì¬ì‚¬ìš©ì„± ì¤‘ìš”]\n    Q4 -->|No| Kustomize2[Kustomize ì‚¬ìš©\\në‹¨ìˆœí•¨ ìš°ì„ ]\n```\n\n### ë¹„êµ í‘œ\n\n| ê´€ì  | Kustomize | Helm |\n|-----|-----------|------|\n| **í•™ìŠµ ê³¡ì„ ** | ë‚®ìŒ (ìˆœìˆ˜ YAML) | ì¤‘ê°„ (Go template) |\n| **ë””ë²„ê¹…** | ì‰¬ì›€ (YAML ê·¸ëŒ€ë¡œ) | ì–´ë ¤ì›€ (í…œí”Œë¦¿ ë Œë”ë§ í•„ìš”) |\n| **ì¡°ê±´ë¶€ ë¡œì§** | ì œí•œì  | ê°•ë ¥í•¨ (if/range) |\n| **ì¬ì‚¬ìš©ì„±** | Componentë¡œ ê°€ëŠ¥ | Chartë¡œ íŒ¨í‚¤ì§• |\n| **ì˜ì¡´ì„± ê´€ë¦¬** | ì—†ìŒ | Chart dependencies |\n| **ë²„ì „ ê´€ë¦¬** | Gitìœ¼ë¡œ ì§ì ‘ | Chart ë²„ì „ + values |\n| **ì—ì½”ì‹œìŠ¤í…œ** | kubectl ë‚´ì¥ | ArtifactHub, ìˆ˜ë§ì€ ì°¨íŠ¸ |\n| **GitOps ì¹œí™”ì„±** | ë†’ìŒ | ì¤‘ê°„ (ë Œë”ë§ í•„ìš”) |\n\n### Kustomizeë¥¼ ì„ íƒí•´ì•¼ í•  ë•Œ\n\n1. **ë‚´ë¶€ ì• í”Œë¦¬ì¼€ì´ì…˜**ì„ ë°°í¬í•  ë•Œ\n2. **ë‹¨ìˆœí•œ í™˜ê²½ë³„ ì°¨ì´**ë§Œ ìˆì„ ë•Œ (replicas, resources, env)\n3. **í…œí”Œë¦¿ ì—†ì´ ìˆœìˆ˜ YAML**ì„ ìœ ì§€í•˜ê³  ì‹¶ì„ ë•Œ\n4. **Git diffê°€ ëª…í™•**í•´ì•¼ í•  ë•Œ\n\n### Helmì„ ì„ íƒí•´ì•¼ í•  ë•Œ\n\n1. **ì¨ë“œíŒŒí‹° ì†Œí”„íŠ¸ì›¨ì–´** ì„¤ì¹˜ (nginx, postgresql, prometheus)\n2. **ë³µì¡í•œ ì¡°ê±´ë¶€ ë¡œì§**ì´ í•„ìš”í•  ë•Œ\n3. **ì¬ì‚¬ìš© ê°€ëŠ¥í•œ íŒ¨í‚¤ì§€**ë¡œ ë°°í¬í•  ë•Œ\n4. **ì˜ì¡´ì„± ê´€ë¦¬**ê°€ í•„ìš”í•  ë•Œ\n\n> [!TIP]\n> **ì‹¤ë¬´ ì¡°ì–¸**: ë‚´ë¶€ ì•±ì€ Kustomize, ì¸í”„ë¼ ì»´í¬ë„ŒíŠ¸ëŠ” Helmìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.\n\n---\n\n## í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼: Helm + Kustomize\n\nArgoCDì™€ Flux ëª¨ë‘ Helm ì°¨íŠ¸ì— Kustomizeë¥¼ ì ìš©í•˜ëŠ” **post-rendering**ì„ ì§€ì›í•©ë‹ˆë‹¤.\n\n### ArgoCDì—ì„œ Helm + Kustomize\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: nginx-prod\nspec:\n  source:\n    repoURL: https://charts.bitnami.com/bitnami\n    chart: nginx\n    targetRevision: 15.0.0\n    helm:\n      values: |\n        replicaCount: 3\n    # Kustomize post-rendering\n    kustomize:\n      patches:\n        - target:\n            kind: Deployment\n            name: nginx\n          patch: |-\n            - op: add\n              path: /spec/template/metadata/annotations\n              value:\n                custom-annotation: \"added-by-kustomize\"\n```\n\n### Fluxì—ì„œ Helm + Kustomize\n\n```yaml\napiVersion: helm.toolkit.fluxcd.io/v2\nkind: HelmRelease\nmetadata:\n  name: nginx\nspec:\n  chart:\n    spec:\n      chart: nginx\n      sourceRef:\n        kind: HelmRepository\n        name: bitnami\n  # Kustomize post-rendering\n  postRenderers:\n    - kustomize:\n        patches:\n          - target:\n              kind: Deployment\n              name: nginx\n            patch: |\n              - op: add\n                path: /metadata/annotations/custom\n                value: \"post-rendered\"\n```\n\n### ì‚¬ìš© ì‚¬ë¡€\n\n1. **Helm ì°¨íŠ¸ì— ì»¤ìŠ¤í…€ ë ˆì´ë¸”/ì–´ë…¸í…Œì´ì…˜ ì¶”ê°€**\n2. **Helm ì°¨íŠ¸ê°€ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„¤ì • íŒ¨ì¹˜**\n3. **ì¡°ì§ í‘œì¤€ ì •ì±… ê°•ì œ ì ìš©** (ì˜ˆ: ëª¨ë“  Podì— sidecar ì¶”ê°€)\n\n---\n\n## ì‹¤ì „ ë ˆí¬ì§€í† ë¦¬ êµ¬ì¡°\n\n### Kustomize ê¸°ë°˜ êµ¬ì¡°\n\n```\ngitops-repo/\nâ”œâ”€â”€ apps/\nâ”‚   â”œâ”€â”€ frontend/\nâ”‚   â”‚   â”œâ”€â”€ base/\nâ”‚   â”‚   â””â”€â”€ overlays/\nâ”‚   â”‚       â”œâ”€â”€ dev/\nâ”‚   â”‚       â”œâ”€â”€ staging/\nâ”‚   â”‚       â””â”€â”€ prod/\nâ”‚   â””â”€â”€ backend/\nâ”‚       â”œâ”€â”€ base/\nâ”‚       â””â”€â”€ overlays/\nâ”‚           â”œâ”€â”€ dev/\nâ”‚           â”œâ”€â”€ staging/\nâ”‚           â””â”€â”€ prod/\nâ”œâ”€â”€ infrastructure/\nâ”‚   â”œâ”€â”€ base/\nâ”‚   â”‚   â”œâ”€â”€ cert-manager/\nâ”‚   â”‚   â”œâ”€â”€ ingress-nginx/\nâ”‚   â”‚   â””â”€â”€ monitoring/\nâ”‚   â””â”€â”€ overlays/\nâ”‚       â”œâ”€â”€ dev/\nâ”‚       â””â”€â”€ prod/\nâ””â”€â”€ clusters/\n    â”œâ”€â”€ dev/\n    â”‚   â”œâ”€â”€ apps.yaml      # ArgoCD Application\n    â”‚   â””â”€â”€ infra.yaml\n    â””â”€â”€ prod/\n        â”œâ”€â”€ apps.yaml\n        â””â”€â”€ infra.yaml\n```\n\n### Helm ê¸°ë°˜ êµ¬ì¡°\n\n```\ngitops-repo/\nâ”œâ”€â”€ charts/\nâ”‚   â”œâ”€â”€ frontend/\nâ”‚   â”‚   â”œâ”€â”€ Chart.yaml\nâ”‚   â”‚   â”œâ”€â”€ values.yaml\nâ”‚   â”‚   â””â”€â”€ templates/\nâ”‚   â””â”€â”€ backend/\nâ”‚       â”œâ”€â”€ Chart.yaml\nâ”‚       â”œâ”€â”€ values.yaml\nâ”‚       â””â”€â”€ templates/\nâ”œâ”€â”€ releases/\nâ”‚   â”œâ”€â”€ dev/\nâ”‚   â”‚   â”œâ”€â”€ frontend.yaml  # HelmRelease\nâ”‚   â”‚   â””â”€â”€ backend.yaml\nâ”‚   â””â”€â”€ prod/\nâ”‚       â”œâ”€â”€ frontend.yaml\nâ”‚       â””â”€â”€ backend.yaml\nâ””â”€â”€ infrastructure/\n    â”œâ”€â”€ cert-manager/\n    â”‚   â”œâ”€â”€ helmrepository.yaml\n    â”‚   â””â”€â”€ helmrelease.yaml\n    â””â”€â”€ ingress-nginx/\n        â”œâ”€â”€ helmrepository.yaml\n        â””â”€â”€ helmrelease.yaml\n```\n\n---\n\n## ì •ë¦¬\n\n| ë„êµ¬ | ì¥ì  | ë‹¨ì  | ì‚¬ìš© ì‹œê¸° |\n|-----|------|------|----------|\n| **Kustomize** | ìˆœìˆ˜ YAML, í•™ìŠµ ì‰¬ì›€, kubectl ë‚´ì¥ | ì¡°ê±´ë¶€ ë¡œì§ ì œí•œ | ë‚´ë¶€ ì•±, ë‹¨ìˆœ í™˜ê²½ ì°¨ì´ |\n| **Helm** | ê°•ë ¥í•œ í…œí”Œë¦¿, íŒ¨í‚¤ì§€í™”, ì—ì½”ì‹œìŠ¤í…œ | ë””ë²„ê¹… ì–´ë ¤ì›€ | ì¨ë“œíŒŒí‹°, ë³µì¡í•œ ë¡œì§ |\n| **í•˜ì´ë¸Œë¦¬ë“œ** | ì–‘ìª½ ì¥ì  í™œìš© | ë³µì¡ë„ ì¦ê°€ | Helm ì°¨íŠ¸ ì»¤ìŠ¤í„°ë§ˆì´ì§• |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**5í¸: Secrets Management**ì—ì„œëŠ” ë‹¤ìŒì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- GitOpsì—ì„œ Secretsì˜ ë”œë ˆë§ˆ\n- Sealed Secrets ë™ì‘ ì›ë¦¬ì™€ í•œê³„\n- External Secrets Operator\n- SOPS (Secrets OPerationS)\n- ì „ëµ ì„ íƒ ê°€ì´ë“œ\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Kustomize Official](https://kustomize.io/)\n- [Kustomize Components](https://kubectl.docs.kubernetes.io/guides/config_management/components/)\n- [Helm Documentation](https://helm.sh/docs/)\n- [Helm Best Practices](https://helm.sh/docs/chart_best_practices/)\n- [ArgoCD + Kustomize](https://argo-cd.readthedocs.io/en/stable/user-guide/kustomize/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitOps",
      "Kubernetes",
      "Kustomize",
      "Helm"
    ],
    "readingTime": 10,
    "wordCount": 1820,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitops",
    "date": "2026-01-05"
  },
  {
    "id": "gitops-03-flux-cd",
    "slug": "gitops-03-flux-cd",
    "path": "backend/gitops",
    "fullPath": "backend/gitops/gitops-03-flux-cd",
    "title": "GitOps ì‹¬í™” ì‹œë¦¬ì¦ˆ #3: Flux CD - GitOps Toolkitê³¼ ì»¨íŠ¸ë¡¤ëŸ¬ ì•„í‚¤í…ì²˜",
    "excerpt": "Fluxì˜ GitOps Toolkit ì•„í‚¤í…ì²˜, Source/Kustomize/Helm Controllerì˜ ë™ì‘ ì›ë¦¬, ê·¸ë¦¬ê³  ArgoCDì™€ì˜ ìƒì„¸ ë¹„êµë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.",
    "content": "# GitOps ì‹¬í™” ì‹œë¦¬ì¦ˆ #3: Flux CD - GitOps Toolkitê³¼ ì»¨íŠ¸ë¡¤ëŸ¬ ì•„í‚¤í…ì²˜\n\n## ì‹œë¦¬ì¦ˆ ê°œìš”\n\n| # | ì£¼ì œ | í•µì‹¬ ë‚´ìš© |\n|---|------|----------|\n| 1 | GitOps ê°œìš” | ì² í•™ê³¼ ì›ì¹™, Push vs Pull ë°°í¬, Reconciliation |\n| 2 | ArgoCD Deep Dive | ì•„í‚¤í…ì²˜, Application CRD, Sync ì „ëµ |\n| **3** | **Flux CD & GitOps Toolkit** | ì»¨íŠ¸ë¡¤ëŸ¬ ì•„í‚¤í…ì²˜, GitRepository, Kustomization |\n| 4 | í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬ | Kustomize vs Helm, ì „ëµ ì„ íƒ ê¸°ì¤€ |\n| 5 | Secrets Management | Sealed Secrets, External Secrets, SOPS |\n| 6 | CI/CD íŒŒì´í”„ë¼ì¸ í†µí•© | Image Updater, Progressive Delivery |\n\n---\n\n## Fluxë€?\n\n**Flux**ëŠ” Kubernetesë¥¼ ìœ„í•œ GitOps ë„êµ¬ ì§‘í•©ì…ë‹ˆë‹¤. 2019ë…„ Weaveworksì—ì„œ ì‹œì‘í•˜ì—¬ 2022ë…„ CNCF Graduated í”„ë¡œì íŠ¸ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.\n\nArgoCDê°€ ë‹¨ì¼ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ë¼ë©´, FluxëŠ” **GitOps Toolkit**ì´ë¼ëŠ” ë…ë¦½ì ì¸ ì»¨íŠ¸ë¡¤ëŸ¬ë“¤ì˜ ì§‘í•©ì…ë‹ˆë‹¤.\n\n### í•µì‹¬ íŠ¹ì§•\n\n- **CNCF Graduated í”„ë¡œì íŠ¸**: í”„ë¡œë•ì…˜ ê²€ì¦ ì™„ë£Œ\n- **ëª¨ë“ˆëŸ¬ ì•„í‚¤í…ì²˜**: í•„ìš”í•œ ì»¨íŠ¸ë¡¤ëŸ¬ë§Œ ì„ íƒì ìœ¼ë¡œ ì„¤ì¹˜ ê°€ëŠ¥\n- **Kubernetes Native**: ëª¨ë“  ì„¤ì •ì´ CRDë¡œ ê´€ë¦¬ë¨\n- **Image Automation ë‚´ì¥**: ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ìë™ ì—…ë°ì´íŠ¸ ì§€ì›\n- **Multi-tenancy ì§€ì›**: ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê¸°ë°˜ì˜ íŒ€ ê²©ë¦¬\n\n> [!NOTE]\n> Weaveworksê°€ 2024ë…„ íì—…í–ˆì§€ë§Œ, FluxëŠ” CNCF Graduated í”„ë¡œì íŠ¸ë¡œì„œ ì»¤ë®¤ë‹ˆí‹°ì™€ CNCFì˜ ì§€ì› í•˜ì— í™œë°œíˆ ê°œë°œë˜ê³  ìˆìŠµë‹ˆë‹¤.\n\n---\n\n## Flux vs ArgoCD: ì² í•™ì˜ ì°¨ì´\n\në³¸ê²©ì ìœ¼ë¡œ ë“¤ì–´ê°€ê¸° ì „ì—, ë‘ ë„êµ¬ì˜ ê·¼ë³¸ì ì¸ ì„¤ê³„ ì² í•™ ì°¨ì´ë¥¼ ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph ArgoCD [ArgoCD - ëª¨ë†€ë¦¬ì‹]\n        A_ALL[ë‹¨ì¼ ì• í”Œë¦¬ì¼€ì´ì…˜\\nAPI Server + Repo Server + Controller]\n        A_UI[ê°•ë ¥í•œ Web UI]\n        A_APP[Application CRD]\n    end\n    \n    subgraph Flux [Flux - ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤]\n        F_SOURCE[Source Controller]\n        F_KUSTOMIZE[Kustomize Controller]\n        F_HELM[Helm Controller]\n        F_NOTIFY[Notification Controller]\n        F_IMAGE[Image Automation]\n    end\n    \n    A_ALL --> A_UI\n    A_ALL --> A_APP\n    \n    F_SOURCE --> F_KUSTOMIZE\n    F_SOURCE --> F_HELM\n    F_KUSTOMIZE --> F_NOTIFY\n    F_HELM --> F_NOTIFY\n```\n\n| ê´€ì  | ArgoCD | Flux |\n|-----|--------|------|\n| **ì•„í‚¤í…ì²˜** | ëª¨ë†€ë¦¬ì‹ (ë‹¨ì¼ ë°”ì´ë„ˆë¦¬) | ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ (ë…ë¦½ ì»¨íŠ¸ë¡¤ëŸ¬) |\n| **UI** | ë‚´ì¥ Web UI | CLI ì¤‘ì‹¬, UIëŠ” ë³„ë„ (Weave GitOps) |\n| **ì„¤ì¹˜** | í•œ ë²ˆì— ì „ì²´ ì„¤ì¹˜ | í•„ìš”í•œ ì»¨íŠ¸ë¡¤ëŸ¬ë§Œ ì„ íƒ ì„¤ì¹˜ |\n| **ë©€í‹° í´ëŸ¬ìŠ¤í„°** | ì¤‘ì•™ ì§‘ì¤‘ (Hub-Spoke) | ë¶„ì‚° (ê° í´ëŸ¬ìŠ¤í„°ì— Flux ì„¤ì¹˜) |\n| **í•™ìŠµ ê³¡ì„ ** | ë‚®ìŒ (UI ì¹œí™”ì ) | ì¤‘ê°„ (CRD ì´í•´ í•„ìš”) |\n| **ë¦¬ì†ŒìŠ¤ ì‚¬ìš©** | ë†’ìŒ | ë‚®ìŒ |\n\n> [!NOTE]\n> **ì„ íƒ ê¸°ì¤€**: í™”ë ¤í•œ UIì™€ ì¤‘ì•™ ì§‘ì¤‘ ê´€ë¦¬ê°€ í•„ìš”í•˜ë©´ ArgoCD, ê²½ëŸ‰í™”ì™€ ìœ ì—°ì„±ì´ ì¤‘ìš”í•˜ë©´ Fluxê°€ ì í•©í•©ë‹ˆë‹¤.\n\n---\n\n## GitOps Toolkit ì•„í‚¤í…ì²˜\n\nFlux v2ëŠ” **GitOps Toolkit**ì´ë¼ëŠ” ì»¨íŠ¸ë¡¤ëŸ¬ ì§‘í•©ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph Sources [Source Controllers]\n        Git[GitRepository]\n        Helm[HelmRepository]\n        OCI[OCIRepository]\n        Bucket[Bucket]\n    end\n    \n    subgraph Reconcilers [Reconciler Controllers]\n        Kustomize[Kustomization]\n        HelmRelease[HelmRelease]\n    end\n    \n    subgraph Automation [Automation Controllers]\n        ImageRepo[ImageRepository]\n        ImagePolicy[ImagePolicy]\n        ImageUpdate[ImageUpdateAutomation]\n    end\n    \n    subgraph Notifications [Notification Controller]\n        Alert[Alert]\n        Provider[Provider]\n        Receiver[Receiver]\n    end\n    \n    Git --> Kustomize\n    Git --> HelmRelease\n    Helm --> HelmRelease\n    OCI --> Kustomize\n    OCI --> HelmRelease\n    \n    ImageRepo --> ImagePolicy\n    ImagePolicy --> ImageUpdate\n    ImageUpdate --> Git\n    \n    Kustomize --> Alert\n    HelmRelease --> Alert\n    Alert --> Provider\n```\n\n### ì»¨íŠ¸ë¡¤ëŸ¬ ì—­í• \n\n| ì»¨íŠ¸ë¡¤ëŸ¬ | ì—­í•  | CRD |\n|---------|-----|-----|\n| **Source Controller** | Git, Helm, OCIì—ì„œ ì•„í‹°íŒ©íŠ¸ ê°€ì ¸ì˜¤ê¸° | GitRepository, HelmRepository, OCIRepository, Bucket |\n| **Kustomize Controller** | Kustomize ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì ìš© | Kustomization |\n| **Helm Controller** | Helm ë¦´ë¦¬ìŠ¤ ê´€ë¦¬ | HelmRelease |\n| **Notification Controller** | ì´ë²¤íŠ¸ ì•Œë¦¼ | Alert, Provider, Receiver |\n| **Image Automation** | ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ì—…ë°ì´íŠ¸ ìë™í™” | ImageRepository, ImagePolicy, ImageUpdateAutomation |\n\n---\n\n## Source Controller\n\n**Source Controller**ëŠ” ì™¸ë¶€ ì†ŒìŠ¤ì—ì„œ ì•„í‹°íŒ©íŠ¸ë¥¼ ê°€ì ¸ì™€ í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.\n\n### GitRepository\n\nê°€ì¥ ê¸°ë³¸ì ì¸ ì†ŒìŠ¤. Git ì €ì¥ì†Œì—ì„œ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤:\n\n```yaml\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: my-app\n  namespace: flux-system\nspec:\n  # ë™ê¸°í™” ì£¼ê¸°\n  interval: 1m\n  \n  # Git ì €ì¥ì†Œ URL\n  url: https://github.com/myorg/my-app.git\n  \n  # ë¸Œëœì¹˜/íƒœê·¸/ì»¤ë°‹\n  ref:\n    branch: main\n    # ë˜ëŠ”\n    # tag: v1.0.0\n    # commit: abc123\n  \n  # ì¸ì¦ (í•„ìš”ì‹œ)\n  secretRef:\n    name: git-credentials\n  \n  # íŠ¹ì • ê²½ë¡œë§Œ ê°€ì ¸ì˜¤ê¸°\n  ignore: |\n    # ì „ì²´ ì œì™¸\n    /*\n    # íŠ¹ì • ë””ë ‰í† ë¦¬ë§Œ í¬í•¨\n    !/deploy/\n```\n\n**ì¸ì¦ ì„¤ì •**:\n\n```yaml\n# HTTPS ì¸ì¦\napiVersion: v1\nkind: Secret\nmetadata:\n  name: git-credentials\n  namespace: flux-system\ntype: Opaque\nstringData:\n  username: git\n  password: ghp_xxxxxxxxxxxx  # GitHub PAT\n\n---\n# SSH ì¸ì¦\napiVersion: v1\nkind: Secret\nmetadata:\n  name: git-ssh-key\n  namespace: flux-system\ntype: Opaque\nstringData:\n  identity: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n    ...\n    -----END OPENSSH PRIVATE KEY-----\n  known_hosts: |\n    github.com ssh-rsa AAAA...\n```\n\n### HelmRepository\n\nHelm Chart Repositoryì—ì„œ ì°¨íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤:\n\n```yaml\napiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: bitnami\n  namespace: flux-system\nspec:\n  interval: 1h\n  url: https://charts.bitnami.com/bitnami\n  \n  # OCI Registryë„ ì§€ì›\n  # type: oci\n  # url: oci://ghcr.io/myorg/charts\n```\n\n### OCIRepository\n\nOCI Registryì—ì„œ ì•„í‹°íŒ©íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ (Helm ì°¨íŠ¸ ë¿ ì•„ë‹ˆë¼ Kustomize ë§¤ë‹ˆí˜ìŠ¤íŠ¸ë„ ê°€ëŠ¥):\n\n```yaml\napiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: OCIRepository\nmetadata:\n  name: podinfo\n  namespace: flux-system\nspec:\n  interval: 5m\n  url: oci://ghcr.io/stefanprodan/manifests/podinfo\n  ref:\n    tag: latest\n```\n\n> [!TIP]\n> **OCI ì•„í‹°íŒ©íŠ¸**ëŠ” ì»¨í…Œì´ë„ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— Kubernetes ë§¤ë‹ˆí˜ìŠ¤íŠ¸ë¥¼ ì €ì¥í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. Git ì—†ì´ë„ GitOpsê°€ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤.\n\n---\n\n## Kustomize Controller\n\n**Kustomize Controller**ëŠ” GitRepositoryì—ì„œ ê°€ì ¸ì˜¨ Kustomize ë§¤ë‹ˆí˜ìŠ¤íŠ¸ë¥¼ í´ëŸ¬ìŠ¤í„°ì— ì ìš©í•©ë‹ˆë‹¤.\n\n### Kustomization CRD\n\n```yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: my-app\n  namespace: flux-system\nspec:\n  # ë™ê¸°í™” ì£¼ê¸°\n  interval: 10m\n  \n  # ì¬ì‹œë„ ì£¼ê¸° (ì‹¤íŒ¨ ì‹œ)\n  retryInterval: 2m\n  \n  # ì†ŒìŠ¤ ì°¸ì¡°\n  sourceRef:\n    kind: GitRepository\n    name: my-app\n  \n  # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ê²½ë¡œ\n  path: ./deploy/production\n  \n  # íƒ€ê²Ÿ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ (ëª¨ë“  ë¦¬ì†ŒìŠ¤ì— ì ìš©)\n  targetNamespace: production\n  \n  # Gitì—ì„œ ì‚­ì œëœ ë¦¬ì†ŒìŠ¤ ì •ë¦¬\n  prune: true\n  \n  # íƒ€ì„ì•„ì›ƒ\n  timeout: 5m\n  \n  # Health Check ëŒ€ê¸°\n  wait: true\n  healthChecks:\n    - apiVersion: apps/v1\n      kind: Deployment\n      name: my-app\n      namespace: production\n```\n\n### Kustomization ê°„ ì˜ì¡´ì„±\n\nFluxì˜ ê°•ë ¥í•œ ê¸°ëŠ¥ ì¤‘ í•˜ë‚˜ëŠ” **ì˜ì¡´ì„± ê´€ë¦¬**ì…ë‹ˆë‹¤:\n\n```yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: infrastructure\n  namespace: flux-system\nspec:\n  interval: 10m\n  sourceRef:\n    kind: GitRepository\n    name: infra\n  path: ./infrastructure\n  prune: true\n\n---\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: apps\n  namespace: flux-system\nspec:\n  interval: 10m\n  # infrastructureê°€ Ready ìƒíƒœê°€ ëœ í›„ì—ë§Œ ì ìš©\n  dependsOn:\n    - name: infrastructure\n  sourceRef:\n    kind: GitRepository\n    name: apps\n  path: ./apps\n  prune: true\n```\n\n```mermaid\nflowchart LR\n    subgraph Dependencies [ì˜ì¡´ì„± ì²´ì¸]\n        CRDs[CRDs] --> Controllers[Controllers]\n        Controllers --> Infrastructure[Infrastructure]\n        Infrastructure --> Apps[Applications]\n    end\n    \n    CRDs --> |\"dependsOn\"| Controllers\n    Controllers --> |\"dependsOn\"| Infrastructure\n    Infrastructure --> |\"dependsOn\"| Apps\n```\n\n### ë³€ìˆ˜ ì¹˜í™˜ (Post-build Substitution)\n\nKustomize ì ìš© í›„ ë³€ìˆ˜ë¥¼ ì¹˜í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n```yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: my-app\n  namespace: flux-system\nspec:\n  # ...\n  postBuild:\n    substitute:\n      ENVIRONMENT: production\n      REPLICAS: \"3\"\n    substituteFrom:\n      - kind: ConfigMap\n        name: cluster-config\n      - kind: Secret\n        name: cluster-secrets\n```\n\n```yaml\n# deployment.yaml (ì¹˜í™˜ ëŒ€ìƒ)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  replicas: ${REPLICAS}\n  template:\n    spec:\n      containers:\n      - name: app\n        env:\n        - name: ENVIRONMENT\n          value: ${ENVIRONMENT}\n```\n\n---\n\n## Helm Controller\n\n**Helm Controller**ëŠ” Helm ë¦´ë¦¬ìŠ¤ë¥¼ ì„ ì–¸ì ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.\n\n### HelmRelease CRD\n\n```yaml\napiVersion: helm.toolkit.fluxcd.io/v2\nkind: HelmRelease\nmetadata:\n  name: nginx\n  namespace: default\nspec:\n  # ë™ê¸°í™” ì£¼ê¸°\n  interval: 10m\n  \n  # Helm Chart ì†ŒìŠ¤\n  chart:\n    spec:\n      chart: nginx\n      version: \"15.x\"  # SemVer ë²”ìœ„ ì§€ì›\n      sourceRef:\n        kind: HelmRepository\n        name: bitnami\n        namespace: flux-system\n  \n  # Values ì„¤ì •\n  values:\n    replicaCount: 3\n    service:\n      type: ClusterIP\n  \n  # Values íŒŒì¼ ì°¸ì¡°\n  valuesFrom:\n    - kind: ConfigMap\n      name: nginx-values\n      valuesKey: values.yaml\n    - kind: Secret\n      name: nginx-secrets\n      valuesKey: credentials\n  \n  # Upgrade ì„¤ì •\n  upgrade:\n    remediation:\n      retries: 3\n      remediateLastFailure: true\n  \n  # Rollback ì„¤ì •\n  rollback:\n    recreate: true\n    cleanupOnFail: true\n```\n\n### Gitì—ì„œ Helm Chart ì‚¬ìš©\n\nHelmRepository ëŒ€ì‹  GitRepositoryì—ì„œ ì§ì ‘ ì°¨íŠ¸ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:\n\n```yaml\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: my-charts\n  namespace: flux-system\nspec:\n  interval: 5m\n  url: https://github.com/myorg/helm-charts.git\n  ref:\n    branch: main\n\n---\napiVersion: helm.toolkit.fluxcd.io/v2\nkind: HelmRelease\nmetadata:\n  name: my-app\n  namespace: default\nspec:\n  interval: 10m\n  chart:\n    spec:\n      chart: ./charts/my-app  # Git ë‚´ ê²½ë¡œ\n      sourceRef:\n        kind: GitRepository\n        name: my-charts\n        namespace: flux-system\n```\n\n---\n\n## Image Automation\n\nFluxì˜ ì°¨ë³„í™”ëœ ê¸°ëŠ¥: **ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ë²„ì „ ìë™ ì—…ë°ì´íŠ¸**\n\n```mermaid\nsequenceDiagram\n    participant Registry as Container Registry\n    participant Image as ImageRepository\n    participant Policy as ImagePolicy\n    participant Update as ImageUpdateAutomation\n    participant Git as Git Repository\n    participant Flux as Flux Controllers\n    participant K8s as Kubernetes\n    \n    Image->>Registry: ìƒˆ ì´ë¯¸ì§€ íƒœê·¸ ìŠ¤ìº”\n    Registry-->>Image: v1.2.3 ë°œê²¬\n    Image->>Policy: ì •ì±…ê³¼ ë§¤ì¹­\n    Policy->>Policy: SemVer í•„í„°ë§\n    Policy->>Update: ì—…ë°ì´íŠ¸ íŠ¸ë¦¬ê±°\n    Update->>Git: manifest ìˆ˜ì • & commit\n    Git->>Flux: ë³€ê²½ ê°ì§€\n    Flux->>K8s: ë°°í¬\n```\n\n### ImageRepository\n\nì»¨í…Œì´ë„ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ì´ë¯¸ì§€ íƒœê·¸ë¥¼ ìŠ¤ìº”í•©ë‹ˆë‹¤:\n\n```yaml\napiVersion: image.toolkit.fluxcd.io/v1beta2\nkind: ImageRepository\nmetadata:\n  name: my-app\n  namespace: flux-system\nspec:\n  # ìŠ¤ìº”í•  ì´ë¯¸ì§€\n  image: ghcr.io/myorg/my-app\n  \n  # ìŠ¤ìº” ì£¼ê¸°\n  interval: 5m\n  \n  # ì¸ì¦ (í•„ìš”ì‹œ)\n  secretRef:\n    name: ghcr-auth\n```\n\n### ImagePolicy\n\nì–´ë–¤ íƒœê·¸ë¥¼ ì„ íƒí• ì§€ ì •ì±…ì„ ì •ì˜í•©ë‹ˆë‹¤:\n\n```yaml\napiVersion: image.toolkit.fluxcd.io/v1beta2\nkind: ImagePolicy\nmetadata:\n  name: my-app\n  namespace: flux-system\nspec:\n  imageRepositoryRef:\n    name: my-app\n  \n  # ì •ì±…: SemVer ë²”ìœ„\n  policy:\n    semver:\n      range: \">=1.0.0\"\n  \n  # ë˜ëŠ”: ì•ŒíŒŒë²³ìˆœ ìµœì‹ \n  # policy:\n  #   alphabetical:\n  #     order: asc\n  \n  # ë˜ëŠ”: ìˆ«ììˆœ ìµœì‹ \n  # policy:\n  #   numerical:\n  #     order: asc\n```\n\n### ImageUpdateAutomation\n\nGit ì €ì¥ì†Œì˜ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ë¥¼ ìë™ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤:\n\n```yaml\napiVersion: image.toolkit.fluxcd.io/v1beta2\nkind: ImageUpdateAutomation\nmetadata:\n  name: my-app\n  namespace: flux-system\nspec:\n  interval: 5m\n  \n  # ì—…ë°ì´íŠ¸í•  Git ì €ì¥ì†Œ\n  sourceRef:\n    kind: GitRepository\n    name: my-app\n  \n  # Git ì„¤ì •\n  git:\n    checkout:\n      ref:\n        branch: main\n    commit:\n      author:\n        name: Flux\n        email: flux@myorg.com\n      messageTemplate: |\n        Update images\n        \n        {{range .Changed.Changes -}}\n        - {{.OldValue}} -> {{.NewValue}}\n        {{end}}\n    push:\n      branch: main\n  \n  # ì—…ë°ì´íŠ¸ ëŒ€ìƒ íŒŒì¼\n  update:\n    path: ./deploy\n    strategy: Setters  # ë§ˆì»¤ ê¸°ë°˜ ì—…ë°ì´íŠ¸\n```\n\n**ë§ˆì»¤ ê¸°ë°˜ ì—…ë°ì´íŠ¸**:\n\n```yaml\n# deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  template:\n    spec:\n      containers:\n      - name: app\n        image: ghcr.io/myorg/my-app:v1.0.0  # {\"$imagepolicy\": \"flux-system:my-app\"}\n```\n\n> [!IMPORTANT]\n> ArgoCDëŠ” Image Updaterê°€ ë³„ë„ í”„ë¡œì íŠ¸ì¸ ë°˜ë©´, FluxëŠ” **Image Automationì´ í•µì‹¬ ê¸°ëŠ¥**ìœ¼ë¡œ ë‚´ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n\n---\n\n## Notification Controller\n\në°°í¬ ì´ë²¤íŠ¸ë¥¼ ì™¸ë¶€ ì‹œìŠ¤í…œìœ¼ë¡œ ì•Œë¦½ë‹ˆë‹¤.\n\n### Alert\n\nì–´ë–¤ ì´ë²¤íŠ¸ë¥¼ ì•Œë¦´ì§€ ì •ì˜:\n\n```yaml\napiVersion: notification.toolkit.fluxcd.io/v1beta3\nkind: Alert\nmetadata:\n  name: on-call-alerts\n  namespace: flux-system\nspec:\n  # ì‹¬ê°ë„ í•„í„°\n  eventSeverity: error\n  \n  # ëª¨ë‹ˆí„°ë§ ëŒ€ìƒ\n  eventSources:\n    - kind: Kustomization\n      name: '*'\n    - kind: HelmRelease\n      name: '*'\n  \n  # ì•Œë¦¼ ëŒ€ìƒ\n  providerRef:\n    name: slack\n```\n\n### Provider\n\nì•Œë¦¼ì„ ë³´ë‚¼ ì±„ë„:\n\n```yaml\napiVersion: notification.toolkit.fluxcd.io/v1beta3\nkind: Provider\nmetadata:\n  name: slack\n  namespace: flux-system\nspec:\n  type: slack\n  channel: devops-alerts\n  secretRef:\n    name: slack-webhook\n\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: slack-webhook\n  namespace: flux-system\nstringData:\n  address: https://hooks.slack.com/services/T00/B00/XXX\n```\n\n**ì§€ì› Provider**: Slack, Discord, Microsoft Teams, GitHub, GitLab, PagerDuty, Datadog, Sentry ë“±\n\n### Receiver\n\nì™¸ë¶€ì—ì„œ Fluxë¡œ Webhook ìˆ˜ì‹ :\n\n```yaml\napiVersion: notification.toolkit.fluxcd.io/v1\nkind: Receiver\nmetadata:\n  name: github-webhook\n  namespace: flux-system\nspec:\n  type: github\n  events:\n    - push\n  secretRef:\n    name: github-webhook-secret\n  resources:\n    - kind: GitRepository\n      name: my-app\n```\n\n---\n\n## Flux Bootstrap\n\nFlux ì„¤ì¹˜ì˜ ê¶Œì¥ ë°©ë²•ì€ **Bootstrap**ì…ë‹ˆë‹¤. Flux ìì²´ë¥¼ GitOpsë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤:\n\n```bash\n# GitHub ì €ì¥ì†Œë¡œ Bootstrap\nflux bootstrap github \\\n  --owner=myorg \\\n  --repository=fleet-infra \\\n  --branch=main \\\n  --path=clusters/production \\\n  --personal\n\n# ìƒì„±ë˜ëŠ” êµ¬ì¡°\nfleet-infra/\nâ””â”€â”€ clusters/\n    â””â”€â”€ production/\n        â””â”€â”€ flux-system/\n            â”œâ”€â”€ gotk-components.yaml  # Flux ì»´í¬ë„ŒíŠ¸\n            â”œâ”€â”€ gotk-sync.yaml        # ìê¸° ìì‹ ì„ ê´€ë¦¬í•˜ëŠ” Kustomization\n            â””â”€â”€ kustomization.yaml\n```\n\n```mermaid\nflowchart TB\n    subgraph GitRepo [fleet-infra Repository]\n        FluxSys[flux-system/]\n        Infra[infrastructure/]\n        Apps[apps/]\n    end\n    \n    subgraph Cluster [Kubernetes Cluster]\n        Flux[Flux Controllers]\n        InfraRes[Infrastructure Resources]\n        AppRes[Applications]\n    end\n    \n    Flux -->|Sync| FluxSys\n    FluxSys -->|Update| Flux\n    Flux -->|Sync| Infra\n    Infra --> InfraRes\n    Flux -->|Sync| Apps\n    Apps --> AppRes\n    \n    Note[Fluxê°€ ìê¸° ìì‹ ì„ ê´€ë¦¬!]\n```\n\n---\n\n## Multi-tenancy íŒ¨í„´\n\nFluxëŠ” ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê¸°ë°˜ ê²©ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤:\n\n```yaml\n# íŒ€ A ì „ìš© ì†ŒìŠ¤ì™€ Kustomization\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: team-a-apps\n  namespace: team-a  # íŒ€ ë„¤ì„ìŠ¤í˜ì´ìŠ¤\nspec:\n  url: https://github.com/myorg/team-a-apps.git\n  # ...\n\n---\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: team-a-apps\n  namespace: team-a\nspec:\n  # í•´ë‹¹ íŒ€ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¡œ ì œí•œ\n  targetNamespace: team-a\n  serviceAccountName: team-a-reconciler  # ì œí•œëœ ê¶Œí•œì˜ SA\n  # ...\n```\n\n```mermaid\nflowchart TB\n    subgraph FluxSystem [flux-system namespace]\n        FluxCtrl[Flux Controllers]\n    end\n    \n    subgraph TeamA [team-a namespace]\n        GitA[GitRepository]\n        KustomA[Kustomization]\n        SAa[ServiceAccount]\n        Apps_A[Team A Apps]\n    end\n    \n    subgraph TeamB [team-b namespace]\n        GitB[GitRepository]\n        KustomB[Kustomization]\n        SAb[ServiceAccount]\n        Apps_B[Team B Apps]\n    end\n    \n    FluxCtrl --> GitA & GitB\n    GitA --> KustomA --> Apps_A\n    GitB --> KustomB --> Apps_B\n    \n    KustomA -.->|ì‚¬ìš©| SAa\n    KustomB -.->|ì‚¬ìš©| SAb\n```\n\n---\n\n## ArgoCD vs Flux ìƒì„¸ ë¹„êµ\n\n| ê¸°ëŠ¥ | ArgoCD | Flux |\n|-----|--------|------|\n| **ì„¤ì¹˜ ë³µì¡ë„** | helm install í•œ ì¤„ | flux bootstrap |\n| **ë¦¬ì†ŒìŠ¤ ì‚¬ìš©** | ~400MB ë©”ëª¨ë¦¬ | ~100MB ë©”ëª¨ë¦¬ |\n| **Web UI** | ê°•ë ¥í•œ ë‚´ì¥ UI | Weave GitOps (ë³„ë„) |\n| **ë©€í‹° í´ëŸ¬ìŠ¤í„°** | ì¤‘ì•™ ArgoCDê°€ ê´€ë¦¬ | ê° í´ëŸ¬ìŠ¤í„°ì— Flux |\n| **ì´ë¯¸ì§€ ìë™í™”** | Image Updater (ë³„ë„) | ë‚´ì¥ |\n| **Helm ì§€ì›** | Applicationì—ì„œ ì²˜ë¦¬ | HelmRelease CRD |\n| **OCI ì§€ì›** | ì§€ì› | ë„¤ì´í‹°ë¸Œ ì§€ì› |\n| **Health Check** | ë‚´ì¥ (ê°•ë ¥) | wait/healthChecks |\n| **Webhook** | GitHub, GitLab ë“± | Receiverë¡œ ë‹¤ì–‘í•œ ì†ŒìŠ¤ |\n| **SOPS ì§€ì›** | ì œí•œì  | ë„¤ì´í‹°ë¸Œ |\n\n### ì–¸ì œ ArgoCDë¥¼ ì„ íƒí• ê¹Œ?\n\n- **UIê°€ ì¤‘ìš”**í•  ë•Œ (ìš´ì˜íŒ€ì—ì„œ ì‹œê°ì  ëª¨ë‹ˆí„°ë§ ì„ í˜¸)\n- **ì¤‘ì•™ ì§‘ì¤‘ ê´€ë¦¬**ê°€ í•„ìš”í•  ë•Œ (í•œ ê³³ì—ì„œ ëª¨ë“  í´ëŸ¬ìŠ¤í„° ê´€ë¦¬)\n- **ë³µì¡í•œ Sync ì „ëµ**ì´ í•„ìš”í•  ë•Œ (Sync Waves, Hooks)\n- **í•™ìŠµ ê³¡ì„ ì„ ì¤„ì´ê³  ì‹¶ì„ ë•Œ**\n\n### ì–¸ì œ Fluxë¥¼ ì„ íƒí• ê¹Œ?\n\n- **ê²½ëŸ‰í™”**ê°€ ì¤‘ìš”í•  ë•Œ (Edge, ë¦¬ì†ŒìŠ¤ ì œí•œ í™˜ê²½)\n- **ì´ë¯¸ì§€ ìë™ ì—…ë°ì´íŠ¸**ê°€ í•µì‹¬ ìš”êµ¬ì‚¬í•­ì¼ ë•Œ\n- **SOPSë¡œ Secrets ê´€ë¦¬**í•  ë•Œ\n- **ê° í´ëŸ¬ìŠ¤í„°ì˜ ë…ë¦½ì„±**ì´ ì¤‘ìš”í•  ë•Œ\n- **Kubernetes Native ì ‘ê·¼**ì„ ì„ í˜¸í•  ë•Œ\n\n---\n\n## ì •ë¦¬\n\n| ì»´í¬ë„ŒíŠ¸ | ì—­í•  |\n|---------|------|\n| **Source Controller** | Git, Helm, OCIì—ì„œ ì•„í‹°íŒ©íŠ¸ ê°€ì ¸ì˜¤ê¸° |\n| **Kustomize Controller** | Kustomize ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì ìš©, ì˜ì¡´ì„± ê´€ë¦¬ |\n| **Helm Controller** | Helm ë¦´ë¦¬ìŠ¤ ìƒëª…ì£¼ê¸° ê´€ë¦¬ |\n| **Image Automation** | ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ìë™ ì—…ë°ì´íŠ¸ |\n| **Notification Controller** | ì´ë²¤íŠ¸ ì•Œë¦¼ ì†¡ìˆ˜ì‹  |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**4í¸: í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬**ì—ì„œëŠ” ë‹¤ìŒì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- Kustomize Base/Overlays íŒ¨í„´ ì‹¬í™”\n- Helm values ê´€ë¦¬ ì „ëµ\n- Kustomize vs Helm ì„ íƒ ê¸°ì¤€\n- í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼: Helm + Kustomize post-rendering\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Flux Official Documentation](https://fluxcd.io/flux/)\n- [Flux Components](https://fluxcd.io/flux/components/)\n- [GitOps Toolkit](https://fluxcd.io/flux/components/)\n- [Flux Image Automation](https://fluxcd.io/flux/guides/image-update/)\n- [Multi-tenancy with Flux](https://fluxcd.io/flux/components/kustomize/kustomizations/#multi-tenancy)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitOps",
      "Kubernetes",
      "Flux",
      "CNCF"
    ],
    "readingTime": 11,
    "wordCount": 2067,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitops",
    "date": "2026-01-05"
  },
  {
    "id": "gitops-02-argocd-deep-dive",
    "slug": "gitops-02-argocd-deep-dive",
    "path": "backend/gitops",
    "fullPath": "backend/gitops/gitops-02-argocd-deep-dive",
    "title": "GitOps ì‹¬í™” ì‹œë¦¬ì¦ˆ #2: ArgoCD Deep Dive - ì•„í‚¤í…ì²˜ì™€ ë™ì‘ ì›ë¦¬",
    "excerpt": "ArgoCDì˜ ë‚´ë¶€ ì•„í‚¤í…ì²˜, Application CRD ìƒì„¸ ë¶„ì„, Sync ì „ëµ, ê·¸ë¦¬ê³  ApplicationSetìœ¼ë¡œ ë©€í‹° í´ëŸ¬ìŠ¤í„°ë¥¼ ê´€ë¦¬í•˜ëŠ” ë°©ë²•ì„ ê¹Šì´ ìˆê²Œ ë‹¤ë£¹ë‹ˆë‹¤.",
    "content": "# GitOps ì‹¬í™” ì‹œë¦¬ì¦ˆ #2: ArgoCD Deep Dive - ì•„í‚¤í…ì²˜ì™€ ë™ì‘ ì›ë¦¬\n\n## ì‹œë¦¬ì¦ˆ ê°œìš”\n\n| # | ì£¼ì œ | í•µì‹¬ ë‚´ìš© |\n|---|------|----------|\n| 1 | GitOps ê°œìš” | ì² í•™ê³¼ ì›ì¹™, Push vs Pull ë°°í¬, Reconciliation |\n| **2** | **ArgoCD Deep Dive** | ì•„í‚¤í…ì²˜, Application CRD, Sync ì „ëµ |\n| 3 | Flux CD & GitOps Toolkit | ì»¨íŠ¸ë¡¤ëŸ¬ ì•„í‚¤í…ì²˜, GitRepository, Kustomization |\n| 4 | í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬ | Kustomize vs Helm, ì „ëµ ì„ íƒ ê¸°ì¤€ |\n| 5 | Secrets Management | Sealed Secrets, External Secrets, SOPS |\n| 6 | CI/CD íŒŒì´í”„ë¼ì¸ í†µí•© | Image Updater, Progressive Delivery |\n\n---\n\n## ArgoCDë€?\n\nArgoCDëŠ” Kubernetesë¥¼ ìœ„í•œ **ì„ ì–¸ì (Declarative) GitOps ì§€ì† ë°°í¬(Continuous Delivery) ë„êµ¬**ì…ë‹ˆë‹¤. Git ì €ì¥ì†Œì— ì •ì˜ëœ ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒíƒœë¥¼ ì§€ì†ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ê³ , Kubernetes í´ëŸ¬ìŠ¤í„°ì˜ ì‹¤ì œ ìƒíƒœì™€ ë™ê¸°í™”í•©ë‹ˆë‹¤.\n\n### í•µì‹¬ íŠ¹ì§•\n\n- **CNCF Graduated í”„ë¡œì íŠ¸**: 2022ë…„ CNCF ì¡¸ì—…, í”„ë¡œë•ì…˜ ê²€ì¦ ì™„ë£Œ\n- **ë‹¤ì–‘í•œ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì§€ì›**: Plain YAML, Helm, Kustomize, Jsonnet\n- **ê°•ë ¥í•œ Web UI**: ì‹¤ì‹œê°„ ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒíƒœ ì‹œê°í™”\n- **ë©€í‹° í´ëŸ¬ìŠ¤í„° ê´€ë¦¬**: ë‹¨ì¼ ArgoCD ì¸ìŠ¤í„´ìŠ¤ë¡œ ì—¬ëŸ¬ í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ ê°€ëŠ¥\n- **SSO í†µí•©**: OIDC, SAML, LDAP, GitHub, GitLab ë“± ë‹¤ì–‘í•œ ì¸ì¦ ì§€ì›\n\n---\n\n## ArgoCD ì•„í‚¤í…ì²˜\n\nArgoCDëŠ” 3ê°œì˜ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:\n\n```mermaid\nflowchart TB\n    subgraph External [ì™¸ë¶€]\n        User[ì‚¬ìš©ì]\n        Git[Git Repository]\n        Webhook[Webhook]\n    end\n    \n    subgraph ArgoCD [ArgoCD Namespace]\n        subgraph API [API Server]\n            REST[REST API]\n            GRPC[gRPC API]\n            WebUI[Web UI]\n        end\n        \n        subgraph Repo [Repo Server]\n            Clone[Git Clone]\n            Render[Manifest Rendering]\n            Cache[Cache]\n        end\n        \n        subgraph Controller [Application Controller]\n            Reconciler[Reconciliation Loop]\n            Diff[Diff Engine]\n            Sync[Sync Engine]\n        end\n        \n        Redis[(Redis)]\n    end\n    \n    subgraph Cluster [Kubernetes Cluster]\n        APIServer[K8s API Server]\n        Resources[Workloads]\n    end\n    \n    User --> WebUI & REST\n    Git --> Clone\n    Webhook --> API\n    \n    API <--> Repo\n    API <--> Controller\n    API <--> Redis\n    \n    Controller <--> APIServer\n    APIServer --> Resources\n    \n    Repo --> Clone\n    Clone --> Render\n    Render --> Cache\n```\n\n### 1. API Server\n\nArgoCDì˜ **í”„ë¡ íŠ¸ì—”ë“œ** ì—­í• ì„ í•©ë‹ˆë‹¤.\n\n| ê¸°ëŠ¥ | ì„¤ëª… |\n|-----|------|\n| **REST/gRPC API** | CLI, Web UI, ì™¸ë¶€ ì‹œìŠ¤í…œê³¼ í†µì‹  |\n| **Web UI** | React ê¸°ë°˜ ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ |\n| **ì¸ì¦/ì¸ê°€** | RBAC, SSO í†µí•© |\n| **Webhook ì²˜ë¦¬** | Git ë³€ê²½ ì‹œ ì¦‰ì‹œ Sync íŠ¸ë¦¬ê±° |\n\n```bash\n# ArgoCD CLIëŠ” gRPC API ì‚¬ìš©\nargocd app list\nargocd app sync my-app\n```\n\n### 2. Repo Server\n\nGit ì €ì¥ì†Œì™€ **ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë Œë”ë§**ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.\n\n```mermaid\nsequenceDiagram\n    participant AC as Application Controller\n    participant RS as Repo Server\n    participant Git as Git Repository\n    participant Cache as Cache\n    \n    AC->>RS: GetManifests(app, revision)\n    RS->>Cache: ìºì‹œ í™•ì¸\n    alt ìºì‹œ Hit\n        Cache-->>RS: ìºì‹œëœ ë§¤ë‹ˆí˜ìŠ¤íŠ¸\n    else ìºì‹œ Miss\n        RS->>Git: git clone/fetch\n        Git-->>RS: Repository\n        RS->>RS: Helm/Kustomize ë Œë”ë§\n        RS->>Cache: ìºì‹œ ì €ì¥\n    end\n    RS-->>AC: Rendered Manifests\n```\n\n**ì§€ì›í•˜ëŠ” ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë„êµ¬**:\n\n- Plain YAML/JSON\n- Helm Charts\n- Kustomize\n- Jsonnet\n- Custom Config Management Plugin (CMP)\n\n> [!TIP]\n> Repo ServerëŠ” **stateless**ì…ë‹ˆë‹¤. ë Œë”ë§ ê²°ê³¼ë¥¼ Redisì— ìºì‹±í•˜ë¯€ë¡œ, ìŠ¤ì¼€ì¼ ì•„ì›ƒì´ ìš©ì´í•©ë‹ˆë‹¤.\n\n### 3. Application Controller\n\nArgoCDì˜ **í•µì‹¬ ì—”ì§„**ì…ë‹ˆë‹¤. Kubernetes Controller íŒ¨í„´ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.\n\n```go\n// ê°„ëµí™”ëœ Application Controller ë¡œì§\nfunc (c *ApplicationController) Reconcile(app *Application) error {\n    // 1. Gitì—ì„œ Desired State ì¡°íšŒ (Repo Server í†µí•´)\n    desired, err := c.repoServer.GetManifests(app)\n    if err != nil {\n        return err\n    }\n    \n    // 2. í´ëŸ¬ìŠ¤í„°ì—ì„œ Current State ì¡°íšŒ\n    current, err := c.kubectl.GetResources(app.Destination)\n    if err != nil {\n        return err\n    }\n    \n    // 3. Diff ê³„ì‚°\n    diff := c.diffEngine.Compare(desired, current)\n    \n    // 4. ìƒíƒœ ì—…ë°ì´íŠ¸\n    app.Status.Sync.Status = calculateSyncStatus(diff)\n    app.Status.Health = calculateHealth(current)\n    \n    // 5. Auto Sync í™œì„±í™” ì‹œ ë™ê¸°í™”\n    if app.Spec.SyncPolicy.Automated != nil && diff.HasChanges() {\n        return c.syncEngine.Sync(app, desired)\n    }\n    \n    return nil\n}\n```\n\n**ì£¼ìš” ì±…ì„**:\n\n- Application ë¦¬ì†ŒìŠ¤ ê°ì‹œ\n- Git ìƒíƒœì™€ í´ëŸ¬ìŠ¤í„° ìƒíƒœ ë¹„êµ\n- Sync Status, Health Status ê³„ì‚°\n- Automated Sync ì‹¤í–‰\n\n---\n\n## Application CRD\n\nArgoCDì—ì„œ ê´€ë¦¬í•˜ëŠ” ëª¨ë“  ì• í”Œë¦¬ì¼€ì´ì…˜ì€ **Application Custom Resource**ë¡œ ì •ì˜ë©ë‹ˆë‹¤.\n\n### ê¸°ë³¸ êµ¬ì¡°\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: my-app\n  namespace: argocd  # ArgoCDê°€ ì„¤ì¹˜ëœ ë„¤ì„ìŠ¤í˜ì´ìŠ¤\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io  # ì‚­ì œ ì‹œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬\nspec:\n  # í”„ë¡œì íŠ¸ (RBAC ë‹¨ìœ„)\n  project: default\n  \n  # ì†ŒìŠ¤: ì–´ë””ì„œ ê°€ì ¸ì˜¬ ê²ƒì¸ê°€\n  source:\n    repoURL: https://github.com/myorg/myrepo.git\n    targetRevision: HEAD  # ë¸Œëœì¹˜, íƒœê·¸, ì»¤ë°‹ SHA\n    path: k8s/overlays/prod  # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ê²½ë¡œ\n  \n  # ëª©ì ì§€: ì–´ë””ì— ë°°í¬í•  ê²ƒì¸ê°€\n  destination:\n    server: https://kubernetes.default.svc  # í´ëŸ¬ìŠ¤í„° API Server\n    namespace: production  # íƒ€ê²Ÿ ë„¤ì„ìŠ¤í˜ì´ìŠ¤\n  \n  # ë™ê¸°í™” ì •ì±…\n  syncPolicy:\n    automated:\n      prune: true      # Gitì—ì„œ ì‚­ì œëœ ë¦¬ì†ŒìŠ¤ í´ëŸ¬ìŠ¤í„°ì—ì„œë„ ì‚­ì œ\n      selfHeal: true   # ìˆ˜ë™ ë³€ê²½ ì‹œ Git ìƒíƒœë¡œ ë³µêµ¬\n    syncOptions:\n      - CreateNamespace=true  # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìë™ ìƒì„±\n\nstatus:\n  sync:\n    status: Synced  # Synced, OutOfSync, Unknown\n    revision: abc123\n  health:\n    status: Healthy  # Healthy, Progressing, Degraded, Missing\n  resources:\n    - kind: Deployment\n      name: my-app\n      status: Synced\n      health: Healthy\n```\n\n### Source ì„¤ì •: Helm\n\n```yaml\nspec:\n  source:\n    repoURL: https://charts.bitnami.com/bitnami\n    chart: nginx\n    targetRevision: 15.0.0\n    helm:\n      releaseName: my-nginx\n      values: |\n        replicaCount: 3\n        service:\n          type: ClusterIP\n      parameters:\n        - name: image.tag\n          value: \"1.25\"\n      valueFiles:\n        - values-prod.yaml\n```\n\n### Source ì„¤ì •: Kustomize\n\n```yaml\nspec:\n  source:\n    repoURL: https://github.com/myorg/myrepo.git\n    path: k8s/overlays/prod\n    kustomize:\n      namePrefix: prod-\n      nameSuffix: -v1\n      images:\n        - name: my-app\n          newTag: v1.2.3\n      commonLabels:\n        environment: production\n```\n\n---\n\n## Sync ìƒíƒœì™€ Health ìƒíƒœ\n\nArgoCDëŠ” ë‘ ê°€ì§€ ìƒíƒœë¥¼ ì¶”ì í•©ë‹ˆë‹¤:\n\n### Sync Status\n\nGitì˜ Desired Stateì™€ í´ëŸ¬ìŠ¤í„°ì˜ Current State ë¹„êµ ê²°ê³¼\n\n```mermaid\nflowchart LR\n    subgraph SyncStatus [Sync Status]\n        Synced[Synced âœ“]\n        OutOfSync[OutOfSync âš ]\n        Unknown[Unknown ?]\n    end\n    \n    Git[Git Repository] --> |ë¹„êµ| Cluster[Kubernetes Cluster]\n    \n    Cluster --> |ì¼ì¹˜| Synced\n    Cluster --> |ë¶ˆì¼ì¹˜| OutOfSync\n    Cluster --> |í™•ì¸ ë¶ˆê°€| Unknown\n```\n\n| ìƒíƒœ | ì˜ë¯¸ |\n|-----|------|\n| **Synced** | Gitê³¼ í´ëŸ¬ìŠ¤í„° ìƒíƒœ ì¼ì¹˜ |\n| **OutOfSync** | ì°¨ì´ ì¡´ì¬ (Git ë³€ê²½ or ìˆ˜ë™ ìˆ˜ì •) |\n| **Unknown** | ìƒíƒœ í™•ì¸ ë¶ˆê°€ |\n\n### Health Status\n\ní´ëŸ¬ìŠ¤í„°ì— ë°°í¬ëœ ë¦¬ì†ŒìŠ¤ì˜ ì‹¤ì œ ìƒíƒœ\n\n| ìƒíƒœ | ì˜ë¯¸ | ì˜ˆì‹œ |\n|-----|------|-----|\n| **Healthy** | ì •ìƒ ë™ì‘ ì¤‘ | Deployment replicas ì¶©ì¡± |\n| **Progressing** | ì§„í–‰ ì¤‘ | ë¡¤ì•„ì›ƒ ì¤‘ |\n| **Degraded** | ë¬¸ì œ ë°œìƒ | Pod CrashLoopBackOff |\n| **Suspended** | ì¼ì‹œ ì¤‘ì§€ | HPA ì¼ì‹œ ì¤‘ì§€ |\n| **Missing** | ë¦¬ì†ŒìŠ¤ ì—†ìŒ | ì•„ì§ ìƒì„± ì•ˆë¨ |\n\n```mermaid\nflowchart TB\n    subgraph HealthCheck [Health Check]\n        Deploy[Deployment] --> |replicas í™•ì¸| Health{ìƒíƒœ}\n        Health --> |Available >= Desired| Healthy[Healthy]\n        Health --> |ë¡¤ì•„ì›ƒ ì¤‘| Progressing[Progressing]\n        Health --> |Available < Desired| Degraded[Degraded]\n    end\n```\n\n---\n\n## Sync ì „ëµ\n\n### Manual Sync vs Automated Sync\n\n```yaml\n# Manual Sync: ëª…ì‹œì  íŠ¸ë¦¬ê±° í•„ìš”\nspec:\n  syncPolicy: {}  # ë˜ëŠ” ìƒëµ\n\n# Automated Sync: Git ë³€ê²½ ì‹œ ìë™ ë™ê¸°í™”\nspec:\n  syncPolicy:\n    automated: {}\n```\n\n**Automated Sync ì˜µì…˜**:\n\n```yaml\nspec:\n  syncPolicy:\n    automated:\n      prune: true       # Gitì—ì„œ ì‚­ì œëœ ë¦¬ì†ŒìŠ¤ ì •ë¦¬\n      selfHeal: true    # Drift ë°œìƒ ì‹œ ìë™ ë³µêµ¬\n      allowEmpty: false # ë¹ˆ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ í—ˆìš© ì—¬ë¶€\n```\n\n> [!WARNING]\n> `prune: true`ëŠ” Gitì—ì„œ ë¦¬ì†ŒìŠ¤ ì •ì˜ë¥¼ ì‚­ì œí•˜ë©´ í´ëŸ¬ìŠ¤í„°ì—ì„œë„ **ì¦‰ì‹œ ì‚­ì œ**ë©ë‹ˆë‹¤. ì‹¤ìˆ˜ë¡œ íŒŒì¼ì„ ì‚­ì œí•˜ë©´ í”„ë¡œë•ì…˜ ë¦¬ì†ŒìŠ¤ê°€ ì‚¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n### Self-Heal ë™ì‘\n\n```mermaid\nsequenceDiagram\n    participant Ops as ìš´ì˜ì\n    participant K8s as Kubernetes\n    participant ArgoCD as ArgoCD Controller\n    participant Git as Git Repository\n    \n    Ops->>K8s: kubectl scale deployment --replicas=5\n    Note over K8s: ìˆ˜ë™ ë³€ê²½ (Drift)\n    \n    ArgoCD->>K8s: í˜„ì¬ ìƒíƒœ í™•ì¸ (replicas: 5)\n    ArgoCD->>Git: Git ìƒíƒœ í™•ì¸ (replicas: 3)\n    ArgoCD->>ArgoCD: Drift ê°ì§€!\n    \n    alt selfHeal: true\n        ArgoCD->>K8s: kubectl apply (replicas: 3)\n        Note over K8s: Git ìƒíƒœë¡œ ë³µêµ¬\n    else selfHeal: false\n        Note over ArgoCD: OutOfSync ìƒíƒœ ìœ ì§€\n        Note over K8s: ë³€ê²½ ìœ ì§€\n    end\n```\n\n### Sync Options\n\nì„¸ë°€í•œ ë™ê¸°í™” ì œì–´ë¥¼ ìœ„í•œ ì˜µì…˜ë“¤:\n\n```yaml\nspec:\n  syncPolicy:\n    syncOptions:\n      - CreateNamespace=true       # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìë™ ìƒì„±\n      - PrunePropagationPolicy=foreground  # ì‚­ì œ ì „íŒŒ ì •ì±…\n      - PruneLast=true             # ë‹¤ë¥¸ ë¦¬ì†ŒìŠ¤ ë¨¼ì € ì ìš© í›„ Prune\n      - Replace=true               # apply ëŒ€ì‹  replace ì‚¬ìš©\n      - ServerSideApply=true       # Server-Side Apply ì‚¬ìš©\n      - FailOnSharedResource=true  # ë‹¤ë¥¸ ì•±ê³¼ ë¦¬ì†ŒìŠ¤ ê³µìœ  ê¸ˆì§€\n      - RespectIgnoreDifferences=true  # ignoreDifferences ì¡´ì¤‘\n```\n\n---\n\n## Sync Wavesì™€ Hooks\n\në³µì¡í•œ ë°°í¬ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìœ„í•´ **ìˆœì„œ ì œì–´**ê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\n### Sync Waves\n\në¦¬ì†ŒìŠ¤ì— `argocd.argoproj.io/sync-wave` ì–´ë…¸í…Œì´ì…˜ìœ¼ë¡œ ìˆœì„œ ì§€ì •:\n\n```yaml\n# Wave 0: ë¨¼ì € ì‹¤í–‰ (ê¸°ë³¸ê°’)\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-app\n  annotations:\n    argocd.argoproj.io/sync-wave: \"0\"\n\n---\n# Wave 1: Namespace ìƒì„± í›„ ì‹¤í–‰\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\n  annotations:\n    argocd.argoproj.io/sync-wave: \"1\"\n\n---\n# Wave 2: ConfigMap ì´í›„ ì‹¤í–‰\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\n  annotations:\n    argocd.argoproj.io/sync-wave: \"2\"\n```\n\n```mermaid\ngantt\n    title Sync Waves ì‹¤í–‰ ìˆœì„œ\n    dateFormat X\n    axisFormat %s\n    \n    section Wave 0\n    Namespace    :0, 1\n    \n    section Wave 1\n    ConfigMap    :1, 2\n    Secret       :1, 2\n    \n    section Wave 2\n    Deployment   :2, 3\n    Service      :2, 3\n```\n\n### Sync Hooks\n\níŠ¹ì • ì‹œì ì— Jobì„ ì‹¤í–‰í•©ë‹ˆë‹¤:\n\n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migration\n  annotations:\n    argocd.argoproj.io/hook: PreSync           # ì–¸ì œ ì‹¤í–‰?\n    argocd.argoproj.io/hook-delete-policy: HookSucceeded  # ì–¸ì œ ì‚­ì œ?\nspec:\n  template:\n    spec:\n      containers:\n      - name: migrate\n        image: my-app:latest\n        command: [\"./migrate.sh\"]\n      restartPolicy: Never\n```\n\n**Hook Phases**:\n\n| Phase | ì‹œì  | ì‚¬ìš© ì‚¬ë¡€ |\n|-------|-----|----------|\n| `PreSync` | Sync ì´ì „ | DB ë§ˆì´ê·¸ë ˆì´ì…˜, ë°±ì—… |\n| `Sync` | ì¼ë°˜ ë¦¬ì†ŒìŠ¤ì™€ í•¨ê»˜ | íŠ¹ìˆ˜ ì²˜ë¦¬ í•„ìš” ë¦¬ì†ŒìŠ¤ |\n| `PostSync` | Sync ì™„ë£Œ í›„ | ì•Œë¦¼ ì „ì†¡, í…ŒìŠ¤íŠ¸ ì‹¤í–‰ |\n| `SyncFail` | Sync ì‹¤íŒ¨ ì‹œ | ë¡¤ë°±, ì•Œë¦¼ |\n\n**Delete Policy**:\n\n| ì •ì±… | ì„¤ëª… |\n|-----|------|\n| `HookSucceeded` | ì„±ê³µ ì‹œ ì‚­ì œ |\n| `HookFailed` | ì‹¤íŒ¨ ì‹œ ì‚­ì œ |\n| `BeforeHookCreation` | ë‹¤ìŒ Hook ìƒì„± ì „ ì‚­ì œ |\n\n### ì‹¤ì „ ì˜ˆì‹œ: Blue-Green ë°°í¬\n\n```yaml\n# PreSync: ìƒˆ ë²„ì „ ë°°í¬ ì „ ì¤€ë¹„\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pre-deploy-check\n  annotations:\n    argocd.argoproj.io/hook: PreSync\n    argocd.argoproj.io/sync-wave: \"-1\"\nspec:\n  template:\n    spec:\n      containers:\n      - name: check\n        image: curlimages/curl\n        command: [\"curl\", \"-f\", \"http://health-check-endpoint\"]\n      restartPolicy: Never\n\n---\n# Sync: ìƒˆ ë²„ì „ Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app-green\n  annotations:\n    argocd.argoproj.io/sync-wave: \"0\"\nspec:\n  replicas: 3\n  # ...\n\n---\n# PostSync: íŠ¸ë˜í”½ ì „í™˜\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: switch-traffic\n  annotations:\n    argocd.argoproj.io/hook: PostSync\n    argocd.argoproj.io/sync-wave: \"1\"\nspec:\n  template:\n    spec:\n      containers:\n      - name: switch\n        image: my-tool:latest\n        command: [\"./switch-service.sh\", \"my-app-green\"]\n      restartPolicy: Never\n```\n\n---\n\n## ApplicationSet\n\n**ApplicationSet**ì€ ë‹¨ì¼ í…œí”Œë¦¿ì—ì„œ ì—¬ëŸ¬ Applicationì„ ìë™ ìƒì„±í•©ë‹ˆë‹¤. ë©€í‹° í´ëŸ¬ìŠ¤í„°, ë©€í‹° í…Œë„ŒíŠ¸ í™˜ê²½ì—ì„œ í•„ìˆ˜ì…ë‹ˆë‹¤.\n\n### Generator ì¢…ë¥˜\n\n```mermaid\nflowchart TB\n    subgraph Generators [ApplicationSet Generators]\n        List[List Generator]\n        Cluster[Cluster Generator]\n        Git[Git Generator]\n        Matrix[Matrix Generator]\n        Merge[Merge Generator]\n    end\n    \n    List --> |ì •ì  ëª©ë¡| Apps1[Applications]\n    Cluster --> |ë“±ë¡ëœ í´ëŸ¬ìŠ¤í„°| Apps2[Applications]\n    Git --> |ë””ë ‰í† ë¦¬/íŒŒì¼ êµ¬ì¡°| Apps3[Applications]\n    Matrix --> |Generator ì¡°í•©| Apps4[Applications]\n```\n\n### List Generator\n\nì •ì  ëª©ë¡ì—ì„œ Application ìƒì„±:\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: my-apps\n  namespace: argocd\nspec:\n  goTemplate: true\n  generators:\n  - list:\n      elements:\n      - cluster: dev\n        url: https://dev.k8s.example.com\n        namespace: dev\n      - cluster: staging\n        url: https://staging.k8s.example.com\n        namespace: staging\n      - cluster: prod\n        url: https://prod.k8s.example.com\n        namespace: prod\n  template:\n    metadata:\n      name: 'my-app-{{.cluster}}'\n    spec:\n      project: default\n      source:\n        repoURL: https://github.com/myorg/myrepo.git\n        targetRevision: HEAD\n        path: 'k8s/overlays/{{.cluster}}'\n      destination:\n        server: '{{.url}}'\n        namespace: '{{.namespace}}'\n```\n\n### Cluster Generator\n\nArgoCDì— ë“±ë¡ëœ í´ëŸ¬ìŠ¤í„°ì—ì„œ ìë™ ìƒì„±:\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: cluster-addons\n  namespace: argocd\nspec:\n  goTemplate: true\n  generators:\n  - clusters:\n      selector:\n        matchLabels:\n          environment: production\n  template:\n    metadata:\n      name: '{{.name}}-monitoring'\n    spec:\n      project: default\n      source:\n        repoURL: https://github.com/myorg/cluster-addons.git\n        path: monitoring\n      destination:\n        server: '{{.server}}'\n        namespace: monitoring\n      syncPolicy:\n        automated:\n          prune: true\n          selfHeal: true\n```\n\n### Git Directory Generator\n\nGit ë””ë ‰í† ë¦¬ êµ¬ì¡°ì—ì„œ ìë™ ìƒì„±:\n\n```\napps/\nâ”œâ”€â”€ frontend/\nâ”‚   â””â”€â”€ kustomization.yaml\nâ”œâ”€â”€ backend/\nâ”‚   â””â”€â”€ kustomization.yaml\nâ””â”€â”€ database/\n    â””â”€â”€ kustomization.yaml\n```\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: apps\n  namespace: argocd\nspec:\n  goTemplate: true\n  generators:\n  - git:\n      repoURL: https://github.com/myorg/gitops.git\n      revision: HEAD\n      directories:\n      - path: apps/*\n  template:\n    metadata:\n      name: '{{.path.basename}}'\n    spec:\n      project: default\n      source:\n        repoURL: https://github.com/myorg/gitops.git\n        path: '{{.path.path}}'\n      destination:\n        server: https://kubernetes.default.svc\n        namespace: '{{.path.basename}}'\n```\n\n> [!TIP]\n> Git Directory Generatorë¥¼ ì‚¬ìš©í•˜ë©´ ë””ë ‰í† ë¦¬ë§Œ ì¶”ê°€í•˜ë©´ **ìë™ìœ¼ë¡œ Applicationì´ ìƒì„±**ë©ë‹ˆë‹¤. ì½”ë“œ ë³€ê²½ ì—†ì´ ìƒˆ ì•±ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n---\n\n## í”„ë¡œì íŠ¸(Project)ì™€ RBAC\n\n### AppProject\n\nAppProjectëŠ” Applicationì„ ê·¸ë£¹í™”í•˜ê³  ì ‘ê·¼ ì œì–´ë¥¼ ì ìš©í•©ë‹ˆë‹¤:\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: AppProject\nmetadata:\n  name: production\n  namespace: argocd\nspec:\n  description: Production applications\n  \n  # í—ˆìš©ëœ ì†ŒìŠ¤ ì €ì¥ì†Œ\n  sourceRepos:\n  - https://github.com/myorg/*\n  \n  # í—ˆìš©ëœ ëª©ì ì§€\n  destinations:\n  - namespace: 'prod-*'\n    server: https://prod.k8s.example.com\n  \n  # í—ˆìš©ëœ í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤\n  clusterResourceWhitelist:\n  - group: ''\n    kind: Namespace\n  \n  # ê±°ë¶€ëœ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë¦¬ì†ŒìŠ¤  \n  namespaceResourceBlacklist:\n  - group: ''\n    kind: ResourceQuota\n  \n  # RBAC ì—­í• \n  roles:\n  - name: developer\n    description: Developer access\n    policies:\n    - p, proj:production:developer, applications, get, production/*, allow\n    - p, proj:production:developer, applications, sync, production/*, allow\n    groups:\n    - developers\n```\n\n### RBAC ì •ì±…\n\n```csv\n# p, <role>, <resource>, <action>, <object>, <effect>\n\n# ê°œë°œì: ì½ê¸° + syncë§Œ ê°€ëŠ¥\np, role:developer, applications, get, */*, allow\np, role:developer, applications, sync, */*, allow\n\n# ìš´ì˜ì: ëª¨ë“  ê¶Œí•œ\np, role:operator, applications, *, */*, allow\np, role:operator, clusters, *, *, allow\n\n# íŠ¹ì • í”„ë¡œì íŠ¸ë§Œ ì ‘ê·¼\np, role:team-a, applications, *, team-a/*, allow\n```\n\n---\n\n## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…\n\n### OutOfSync ìƒíƒœê°€ í•´ê²°ë˜ì§€ ì•ŠìŒ\n\n```bash\n# ìƒì„¸ diff í™•ì¸\nargocd app diff my-app --local ./manifests\n\n# íŠ¹ì • ë¦¬ì†ŒìŠ¤ ë¬´ì‹œ ì„¤ì •\n```\n\n```yaml\nspec:\n  ignoreDifferences:\n  - group: apps\n    kind: Deployment\n    jsonPointers:\n    - /spec/replicas  # HPAê°€ ê´€ë¦¬í•˜ëŠ” í•„ë“œ ë¬´ì‹œ\n  - group: \"\"\n    kind: Service\n    jqPathExpressions:\n    - .spec.clusterIP  # ìë™ í• ë‹¹ í•„ë“œ ë¬´ì‹œ\n```\n\n### Sync ì‹¤íŒ¨\n\n```bash\n# ë™ê¸°í™” ìƒíƒœ í™•ì¸\nargocd app get my-app\n\n# ì´ë²¤íŠ¸ í™•ì¸\nkubectl describe application my-app -n argocd\n\n# ê°•ì œ ì¬ë™ê¸°í™”\nargocd app sync my-app --force\n```\n\n### Application ì‚­ì œ ì‹œ ë¦¬ì†ŒìŠ¤ê°€ ë‚¨ìŒ\n\n```yaml\n# Finalizer í™•ì¸\nmetadata:\n  finalizers:\n  - resources-finalizer.argocd.argoproj.io  # ì´ê²Œ ìˆì–´ì•¼ í•¨\n```\n\n```bash\n# ìˆ˜ë™ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ì‚­ì œ í›„ Application ì‚­ì œ\nargocd app delete my-app --cascade=false\nkubectl delete all -l app.kubernetes.io/instance=my-app\n```\n\n---\n\n## ì •ë¦¬\n\n| ê°œë… | ì„¤ëª… |\n|-----|------|\n| **API Server** | Web UI, CLI, Webhook ì²˜ë¦¬ |\n| **Repo Server** | Git clone, ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë Œë”ë§ |\n| **Application Controller** | Reconciliation Loop ì‹¤í–‰ |\n| **Application CRD** | ë°°í¬ ë‹¨ìœ„ ì •ì˜ (source, destination, syncPolicy) |\n| **Sync Waves** | ë¦¬ì†ŒìŠ¤ ì ìš© ìˆœì„œ ì œì–´ |\n| **Hooks** | PreSync, PostSync ë“± íŠ¹ì • ì‹œì  Job ì‹¤í–‰ |\n| **ApplicationSet** | í…œí”Œë¦¿ ê¸°ë°˜ ë‹¤ì¤‘ Application ìƒì„± |\n| **AppProject** | Application ê·¸ë£¹í™” ë° RBAC |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**3í¸: Flux CD & GitOps Toolkit**ì—ì„œëŠ” ë‹¤ìŒì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- Fluxì˜ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜\n- Source Controller, Kustomize Controller, Helm Controller\n- GitRepository, Kustomization, HelmRelease CRD\n- ArgoCD vs Flux ìƒì„¸ ë¹„êµ\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [ArgoCD Official Documentation](https://argo-cd.readthedocs.io/)\n- [ArgoCD Architecture](https://argo-cd.readthedocs.io/en/stable/operator-manual/architecture/)\n- [ApplicationSet Controller](https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/)\n- [Sync Waves and Hooks](https://argo-cd.readthedocs.io/en/stable/user-guide/sync-waves/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitOps",
      "Kubernetes",
      "ArgoCD"
    ],
    "readingTime": 11,
    "wordCount": 2179,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitops",
    "date": "2026-01-05"
  },
  {
    "id": "gitops-01-introduction",
    "slug": "gitops-01-introduction",
    "path": "backend/gitops",
    "fullPath": "backend/gitops/gitops-01-introduction",
    "title": "GitOps ì‹¬í™” ì‹œë¦¬ì¦ˆ #1: GitOpsë€ ë¬´ì—‡ì¸ê°€ - ì² í•™ê³¼ ì›ì¹™",
    "excerpt": "GitOpsì˜ 4ê°€ì§€ í•µì‹¬ ì›ì¹™, Push vs Pull ë°°í¬ ëª¨ë¸ì˜ ì°¨ì´, ê·¸ë¦¬ê³  Kubernetes Controller íŒ¨í„´ê³¼ì˜ ì—°ê²°ê³ ë¦¬ë¥¼ ê¹Šì´ ìˆê²Œ ì´í•´í•©ë‹ˆë‹¤.",
    "content": "# GitOps ì‹¬í™” ì‹œë¦¬ì¦ˆ #1: GitOpsë€ ë¬´ì—‡ì¸ê°€ - ì² í•™ê³¼ ì›ì¹™\n\n## ì‹œë¦¬ì¦ˆ ê°œìš”\n\n| # | ì£¼ì œ | í•µì‹¬ ë‚´ìš© |\n|---|------|----------|\n| **1** | **GitOps ê°œìš”** | ì² í•™ê³¼ ì›ì¹™, Push vs Pull ë°°í¬, Reconciliation |\n| 2 | ArgoCD Deep Dive | ì•„í‚¤í…ì²˜, Application CRD, Sync ì „ëµ |\n| 3 | Flux CD & GitOps Toolkit | ì»¨íŠ¸ë¡¤ëŸ¬ ì•„í‚¤í…ì²˜, GitRepository, Kustomization |\n| 4 | í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬ | Kustomize vs Helm, ì „ëµ ì„ íƒ ê¸°ì¤€ |\n| 5 | Secrets Management | Sealed Secrets, External Secrets, SOPS |\n| 6 | CI/CD íŒŒì´í”„ë¼ì¸ í†µí•© | Image Updater, Progressive Delivery |\n\n---\n\n## GitOps ì´ì „ì˜ ì„¸ê³„\n\nì „í†µì ì¸ CI/CD íŒŒì´í”„ë¼ì¸ì„ ë– ì˜¬ë ¤ë´…ì‹œë‹¤. Jenkinsë‚˜ GitHub Actionsê°€ ì½”ë“œë¥¼ ë¹Œë“œí•˜ê³ , í…ŒìŠ¤íŠ¸ë¥¼ í†µê³¼í•˜ë©´ `kubectl apply`ë‚˜ `helm upgrade` ëª…ë ¹ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ì— ì§ì ‘ ë°°í¬í•©ë‹ˆë‹¤.\n\n```mermaid\nsequenceDiagram\n    participant Dev as ê°œë°œì\n    participant Git as Git Repository\n    participant CI as CI Server\n    participant K8s as Kubernetes\n\n    Dev->>Git: git push\n    Git->>CI: Webhook íŠ¸ë¦¬ê±°\n    CI->>CI: Build & Test\n    CI->>K8s: kubectl apply / helm upgrade\n    Note over K8s: ë°°í¬ ì™„ë£Œ\n```\n\nì´ ë°©ì‹ì€ ë™ì‘í•˜ì§€ë§Œ, ëª‡ ê°€ì§€ ê·¼ë³¸ì ì¸ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤:\n\n| ë¬¸ì œ | ì„¤ëª… |\n|-----|------|\n| **Credential ë¶„ì‚°** | CI ì„œë²„ê°€ í”„ë¡œë•ì…˜ í´ëŸ¬ìŠ¤í„°ì— ì ‘ê·¼í•˜ëŠ” ê°•ë ¥í•œ ê¶Œí•œ ë³´ìœ  |\n| **Drift ê°ì§€ ë¶ˆê°€** | ëˆ„êµ°ê°€ `kubectl edit`ìœ¼ë¡œ ì§ì ‘ ìˆ˜ì •í•˜ë©´ Gitê³¼ ì‹¤ì œ ìƒíƒœ ë¶ˆì¼ì¹˜ |\n| **Audit Trail ë¶€ì¬** | ëˆ„ê°€, ì–¸ì œ, ì™œ ë³€ê²½í–ˆëŠ”ì§€ ì¶”ì  ì–´ë ¤ì›€ |\n| **ë¡¤ë°±ì˜ ë³µì¡ì„±** | ì´ì „ ìƒíƒœë¡œ ëŒì•„ê°€ë ¤ë©´ \"ì–´ë–¤ ë²„ì „ì´ ë°°í¬ë˜ì–´ ìˆì—ˆëŠ”ì§€\" ì°¾ì•„ì•¼ í•¨ |\n\n> [!WARNING]\n> **ì‹¤ì œ ì‚¬ê³  ì‚¬ë¡€**: CI ì„œë²„ê°€ í•´í‚¹ë‹¹í•˜ë©´ ê³µê²©ìê°€ í”„ë¡œë•ì…˜ í´ëŸ¬ìŠ¤í„°ì— ì„ì˜ ì½”ë“œë¥¼ ë°°í¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 2021ë…„ Codecov ì‚¬íƒœì—ì„œ CI íŒŒì´í”„ë¼ì¸ì´ ê³µê²© ë²¡í„°ê°€ ë˜ì–´ ìˆ˜ì²œ ê°œ ê¸°ì—…ì˜ credentialì´ ìœ ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.\n\n---\n\n## GitOpsì˜ í•µì‹¬ ì •ì˜\n\n**GitOps**ëŠ” Gitì„ **Single Source of Truth (SSOT)**ë¡œ ì‚¼ì•„, ì›í•˜ëŠ” ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ì„ ì–¸ì ìœ¼ë¡œ ì •ì˜í•˜ê³ , ìë™í™”ëœ í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤ì œ ìƒíƒœë¥¼ Gitì— ì •ì˜ëœ ìƒíƒœì™€ ì§€ì†ì ìœ¼ë¡œ ì¼ì¹˜ì‹œí‚¤ëŠ” ìš´ì˜ ë°©ì‹ì…ë‹ˆë‹¤.\n\n2021ë…„ CNCF ì‚°í•˜ [OpenGitOps í”„ë¡œì íŠ¸](https://opengitops.dev/)ì—ì„œ ê³µì‹í™”í•œ **4ê°€ì§€ ì›ì¹™**ì„ ì‚´í´ë´…ì‹œë‹¤:\n\n### 1. Declarative (ì„ ì–¸ì )\n\nì‹œìŠ¤í…œì˜ **ì›í•˜ëŠ” ìƒíƒœ(Desired State)**ë¥¼ ì„ ì–¸ì ìœ¼ë¡œ ê¸°ìˆ í•©ë‹ˆë‹¤.\n\n```yaml\n# \"3ê°œì˜ nginx Podê°€ ì‹¤í–‰ë˜ì–´ì•¼ í•œë‹¤\"ëŠ” ì„ ì–¸\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.25\n```\n\n- **ëª…ë ¹í˜•(Imperative)** ì ‘ê·¼: \"nginx Podë¥¼ 3ê°œ ë§Œë“¤ì–´ë¼\" (`kubectl scale`)\n- **ì„ ì–¸í˜•(Declarative)** ì ‘ê·¼: \"nginx Podê°€ 3ê°œì¸ ìƒíƒœê°€ ë˜ì–´ì•¼ í•œë‹¤\" (YAML manifest)\n\n> [!TIP]\n> Kubernetes ìì²´ê°€ ì„ ì–¸ì  ì‹œìŠ¤í…œì…ë‹ˆë‹¤. GitOpsëŠ” ì´ ì² í•™ì„ Gitê³¼ ê²°í•©í•œ ê²ƒì…ë‹ˆë‹¤.\n\n### 2. Versioned and Immutable (ë²„ì „ ê´€ë¦¬ ë° ë¶ˆë³€)\n\nGitì€ ë³¸ì§ˆì ìœ¼ë¡œ ëª¨ë“  ë³€ê²½ì‚¬í•­ì„ ë²„ì „ ê´€ë¦¬í•©ë‹ˆë‹¤. ê° ì»¤ë°‹ì€ ë¶ˆë³€(Immutable)í•˜ë©°, ì „ì²´ íˆìŠ¤í† ë¦¬ê°€ ë³´ì¡´ë©ë‹ˆë‹¤.\n\n```bash\n# ëª¨ë“  ë³€ê²½ ì´ë ¥ ì¶”ì  ê°€ëŠ¥\ngit log --oneline\n# a1b2c3d (HEAD) feat: scale nginx to 5 replicas\n# d4e5f6g fix: update nginx image to 1.25.3\n# g7h8i9j initial: deploy nginx with 3 replicas\n\n# íŠ¹ì • ì‹œì ìœ¼ë¡œ ë¡¤ë°±\ngit revert a1b2c3d\n```\n\nì´ê²ƒì´ ì™œ ì¤‘ìš”í•œê°€?\n\n- **Audit Trail**: ëˆ„ê°€, ì–¸ì œ, ì™œ ë³€ê²½í–ˆëŠ”ì§€ ëª¨ë“  ê¸°ë¡ì´ ë‚¨ìŒ\n- **Rollback**: `git revert`ë¡œ ì´ì „ ìƒíƒœë¡œ ì¦‰ì‹œ ë³µêµ¬ ê°€ëŠ¥\n- **Compliance**: ê¸ˆìœµ, í—¬ìŠ¤ì¼€ì–´ ë“± ê·œì œ ì‚°ì—…ì˜ ê°ì‚¬ ìš”êµ¬ì‚¬í•­ ì¶©ì¡±\n\n### 3. Pulled Automatically (ìë™ìœ¼ë¡œ Pull)\n\nì›í•˜ëŠ” ìƒíƒœì˜ ë³€ê²½ì‚¬í•­ì´ Gitì— í‘¸ì‹œë˜ë©´, **ì—ì´ì „íŠ¸ê°€ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ì ìš©**í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph GitRepo [Git Repository]\n        Manifest[Kubernetes Manifests]\n    end\n    \n    subgraph Cluster [Kubernetes Cluster]\n        Agent[GitOps Agent\\nArgoCD / Flux]\n        Workload[Running Workloads]\n    end\n    \n    Agent -->|1. Pull & Watch| GitRepo\n    Agent -->|2. Compare| Workload\n    Agent -->|3. Apply changes| Workload\n    \n    style Agent fill:#4CAF50,color:#fff\n```\n\nì—¬ê¸°ì„œ í•µì‹¬ì€ **Pull ëª¨ë¸**ì…ë‹ˆë‹¤. CI ì„œë²„ê°€ í´ëŸ¬ìŠ¤í„°ì— ì ‘ê·¼í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ì˜ ì—ì´ì „íŠ¸ê°€ Gitì„ ì£¼ê¸°ì ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.\n\n### 4. Continuously Reconciled (ì§€ì†ì  ì¡°ì •)\n\nì—ì´ì „íŠ¸ëŠ” ë‹¨ìˆœíˆ ë³€ê²½ì„ í•œ ë²ˆ ì ìš©í•˜ê³  ëë‚˜ëŠ” ê²ƒì´ ì•„ë‹™ë‹ˆë‹¤. **Reconciliation Loop**ë¥¼ í†µí•´ ì‹¤ì œ ìƒíƒœì™€ ì›í•˜ëŠ” ìƒíƒœë¥¼ ì§€ì†ì ìœ¼ë¡œ ë¹„êµí•˜ê³ , ì°¨ì´ê°€ ë°œìƒí•˜ë©´ ìë™ìœ¼ë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph ReconciliationLoop [Reconciliation Loop]\n        direction TB\n        Observe[1. í˜„ì¬ ìƒíƒœ ê´€ì°°\\nCurrent State]\n        Compare[2. ì›í•˜ëŠ” ìƒíƒœì™€ ë¹„êµ\\nDesired State in Git]\n        Drift{Drift\\në°œê²¬?}\n        Apply[3. ì°¨ì´ ì¡°ì •\\nReconcile]\n        Wait[4. ëŒ€ê¸°\\nInterval]\n    end\n    \n    Observe --> Compare --> Drift\n    Drift -->|Yes| Apply --> Wait\n    Drift -->|No| Wait\n    Wait --> Observe\n```\n\n> [!IMPORTANT]\n> **Drift Detection**: ëˆ„êµ°ê°€ `kubectl edit`ìœ¼ë¡œ replicasë¥¼ 5ê°œë¡œ ë³€ê²½í•˜ë©´? GitOps ì—ì´ì „íŠ¸ê°€ ê°ì§€í•˜ê³  Gitì— ì •ì˜ëœ 3ê°œë¡œ ìë™ ë³µêµ¬í•©ë‹ˆë‹¤. ì´ê²ƒì´ **Self-Healing**ì…ë‹ˆë‹¤.\n\n---\n\n## Push vs Pull ë°°í¬ ëª¨ë¸\n\nGitOpsë¥¼ ì´í•´í•˜ëŠ” í•µì‹¬ì€ **Push ëª¨ë¸ê³¼ Pull ëª¨ë¸ì˜ ì°¨ì´**ì…ë‹ˆë‹¤.\n\n### Push ëª¨ë¸ (ì „í†µì  CI/CD)\n\n```mermaid\nflowchart LR\n    subgraph External [í´ëŸ¬ìŠ¤í„° ì™¸ë¶€]\n        CI[CI Server]\n    end\n    \n    subgraph Cluster [Kubernetes Cluster]\n        API[API Server]\n        Pod[Pods]\n    end\n    \n    CI -->|1. kubectl apply\\nê°•ë ¥í•œ ê¶Œí•œ í•„ìš”| API\n    API --> Pod\n    \n    style CI fill:#f44336,color:#fff\n```\n\n**íŠ¹ì§•**:\n\n- CI ì„œë²„ê°€ í´ëŸ¬ìŠ¤í„°ì— **ì§ì ‘ ì ‘ê·¼**\n- CI ì„œë²„ì— admin ìˆ˜ì¤€ì˜ kubeconfig í•„ìš”\n- CI ì„œë²„ê°€ ì¹¨í•´ë˜ë©´ í´ëŸ¬ìŠ¤í„°ë„ ìœ„í—˜\n\n### Pull ëª¨ë¸ (GitOps)\n\n```mermaid\nflowchart LR\n    subgraph External [í´ëŸ¬ìŠ¤í„° ì™¸ë¶€]\n        Git[Git Repository]\n    end\n    \n    subgraph Cluster [Kubernetes Cluster]\n        Agent[GitOps Agent]\n        API[API Server]\n        Pod[Pods]\n    end\n    \n    Agent -->|1. Git clone/pull\\nì½ê¸° ê¶Œí•œë§Œ| Git\n    Agent -->|2. kubectl apply\\në‚´ë¶€ í†µì‹ | API\n    API --> Pod\n    \n    style Agent fill:#4CAF50,color:#fff\n```\n\n**íŠ¹ì§•**:\n\n- í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ì˜ ì—ì´ì „íŠ¸ê°€ Gitì„ **Pull**\n- Gitì— ëŒ€í•œ ì½ê¸° ì „ìš© ê¶Œí•œë§Œ í•„ìš”\n- ì™¸ë¶€ì—ì„œ í´ëŸ¬ìŠ¤í„°ë¡œì˜ ì¸ë°”ìš´ë“œ ì—°ê²° ì—†ìŒ\n\n### ë³´ì•ˆ ê´€ì ì—ì„œì˜ ë¹„êµ\n\n| ê´€ì  | Push ëª¨ë¸ | Pull ëª¨ë¸ |\n|-----|----------|----------|\n| **ë„¤íŠ¸ì›Œí¬ ë°©í–¥** | ì™¸ë¶€ â†’ í´ëŸ¬ìŠ¤í„° | í´ëŸ¬ìŠ¤í„° â†’ ì™¸ë¶€ (Git) |\n| **í•„ìš” ê¶Œí•œ** | CIì— cluster-admin | Agentì— ë‚´ë¶€ ê¶Œí•œ |\n| **ê³µê²© í‘œë©´** | CI ì„œë²„ ì¹¨í•´ ì‹œ í´ëŸ¬ìŠ¤í„° ìœ„í—˜ | Git ì €ì¥ì†Œ ë³´í˜¸ì— ì§‘ì¤‘ |\n| **ë°©í™”ë²½** | ì¸ë°”ìš´ë“œ í—ˆìš© í•„ìš” | ì•„ì›ƒë°”ìš´ë“œë§Œ í—ˆìš© |\n\n> [!NOTE]\n> ì‹¤ì œë¡œ ArgoCDë‚˜ Fluxë„ í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œ ê°•ë ¥í•œ ê¶Œí•œì„ ê°€ì§‘ë‹ˆë‹¤. í•˜ì§€ë§Œ **ê³µê²© ë²¡í„°ê°€ Git ì €ì¥ì†Œë¡œ ë‹¨ì¼í™”**ë˜ì–´ ë³´ì•ˆ ê´€ë¦¬ê°€ ë‹¨ìˆœí•´ì§‘ë‹ˆë‹¤.\n\n---\n\n## Kubernetes Controller Patternê³¼ì˜ ì—°ê²°\n\nGitOpsê°€ Kubernetesì—ì„œ íŠ¹íˆ ì˜ ì‘ë™í•˜ëŠ” ì´ìœ ëŠ” **Controller Pattern** ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ë¯¸ Kubernetes ì‹œë¦¬ì¦ˆì—ì„œ ë‹¤ë¤˜ë“¯ì´, ëª¨ë“  Kubernetes ì»¨íŠ¸ë¡¤ëŸ¬ëŠ” Reconciliation Loopë¡œ ë™ì‘í•©ë‹ˆë‹¤.\n\n```go\n// Kubernetes Controllerì˜ í•µì‹¬ ë¡œì§\nfunc (c *Controller) Reconcile(ctx context.Context, req Request) (Result, error) {\n    // 1. í˜„ì¬ ìƒíƒœ ì¡°íšŒ\n    current, err := c.Get(ctx, req.NamespacedName)\n    if err != nil {\n        return Result{}, err\n    }\n    \n    // 2. ì›í•˜ëŠ” ìƒíƒœì™€ ë¹„êµ\n    desired := c.calculateDesiredState(current)\n    \n    // 3. ì°¨ì´ê°€ ìˆìœ¼ë©´ ì¡°ì •\n    if !reflect.DeepEqual(current.Spec, desired) {\n        current.Spec = desired\n        return Result{}, c.Update(ctx, current)\n    }\n    \n    // 4. Requeue for next reconciliation\n    return Result{RequeueAfter: 30 * time.Second}, nil\n}\n```\n\nGitOps ì—ì´ì „íŠ¸(ArgoCD, Flux)ëŠ” ì´ íŒ¨í„´ì˜ **ìƒìœ„ ë ˆë²¨ êµ¬í˜„**ì…ë‹ˆë‹¤:\n\n| ê°œë… | Kubernetes Controller | GitOps Agent |\n|-----|----------------------|--------------|\n| Desired State | Spec in YAML | Git Repository |\n| Current State | Status in API Server | Live Kubernetes Objects |\n| Reconciliation | Controller Manager | ArgoCD/Flux Controller |\n| Watch mechanism | Informer (etcd watch) | Git polling / Webhook |\n\n```mermaid\nflowchart TB\n    subgraph GitOpsLevel [GitOps Level]\n        Git[Git Repository\\nDesired State]\n        ArgoCD[ArgoCD Controller]\n    end\n    \n    subgraph K8sLevel [Kubernetes Level]\n        API[API Server\\nDesired State]\n        DC[Deployment Controller]\n        RSC[ReplicaSet Controller]\n        Kubelet[kubelet]\n    end\n    \n    subgraph NodeLevel [Node Level]\n        Pod[Running Pods\\nCurrent State]\n    end\n    \n    Git --> ArgoCD\n    ArgoCD --> API\n    API --> DC --> RSC --> Kubelet --> Pod\n    \n    ArgoCD -.->|Reconcile| Git\n    DC -.->|Reconcile| API\n    RSC -.->|Reconcile| API\n    Kubelet -.->|Reconcile| API\n```\n\n> [!TIP]\n> GitOpsëŠ” Kubernetesì˜ ì„ ì–¸ì  ëª¨ë¸ì„ **Gitê¹Œì§€ í™•ì¥**í•œ ê²ƒì…ë‹ˆë‹¤. \"Infrastructure as Code\"ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì§„í™”ì…ë‹ˆë‹¤.\n\n---\n\n## GitOpsì˜ ì´ì \n\n### 1. ì™„ì „í•œ ê°ì‚¬ ì¶”ì  (Audit Trail)\n\nëª¨ë“  ë³€ê²½ì€ Git ì»¤ë°‹ìœ¼ë¡œ ê¸°ë¡ë©ë‹ˆë‹¤. `git log`, `git blame`ìœ¼ë¡œ ì™„ë²½í•œ íˆìŠ¤í† ë¦¬ ì¶”ì ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\n```bash\n# ëˆ„ê°€ replicasë¥¼ ë³€ê²½í–ˆëŠ”ê°€?\ngit log -p -- k8s/deployment.yaml | grep -A5 \"replicas:\"\n\n# íŠ¹ì • ì‹œì ì˜ í´ëŸ¬ìŠ¤í„° ìƒíƒœëŠ”?\ngit show abc123:k8s/deployment.yaml\n```\n\n### 2. ê°„í¸í•œ ë¡¤ë°±\n\në¬¸ì œê°€ ë°œìƒí•˜ë©´ `git revert`ë¡œ ì¦‰ì‹œ ì´ì „ ìƒíƒœë¡œ ë³µêµ¬í•©ë‹ˆë‹¤.\n\n```bash\n# ë§ˆì§€ë§‰ ë³€ê²½ ë¡¤ë°±\ngit revert HEAD\ngit push\n\n# GitOps ì—ì´ì „íŠ¸ê°€ ìë™ìœ¼ë¡œ ì´ì „ ìƒíƒœ ì ìš©\n```\n\n### 3. Drift Detectionê³¼ Self-Healing\n\n```mermaid\nsequenceDiagram\n    participant Dev as ìš´ì˜ì\n    participant K8s as Kubernetes\n    participant Agent as GitOps Agent\n    participant Git as Git Repository\n\n    Dev->>K8s: kubectl edit (ìˆ˜ë™ ë³€ê²½)\n    Note over K8s: Drift ë°œìƒ!\n    \n    Agent->>K8s: í˜„ì¬ ìƒíƒœ í™•ì¸\n    Agent->>Git: Git ìƒíƒœ í™•ì¸\n    Agent->>Agent: ì°¨ì´ ê°ì§€\n    Agent->>K8s: Git ìƒíƒœë¡œ ë³µêµ¬\n    Note over K8s: Self-Heal ì™„ë£Œ\n```\n\n### 4. Pull Request ê¸°ë°˜ ë³€ê²½ ê´€ë¦¬\n\nì¸í”„ë¼ ë³€ê²½ë„ ì½”ë“œ ë¦¬ë·°ì™€ ë™ì¼í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ë”°ë¦…ë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph PR [Pull Request Workflow]\n        Create[PR ìƒì„±] --> Review[ì½”ë“œ ë¦¬ë·°]\n        Review --> Approve[ìŠ¹ì¸]\n        Approve --> Merge[Merge to main]\n    end\n    \n    subgraph Deploy [ìë™ ë°°í¬]\n        Merge --> Agent[GitOps Agent]\n        Agent --> Apply[í´ëŸ¬ìŠ¤í„° ì ìš©]\n    end\n```\n\n---\n\n## GitOps ë„êµ¬: ArgoCD vs Flux\n\nGitOpsë¥¼ êµ¬í˜„í•˜ëŠ” ëŒ€í‘œì ì¸ ë‘ ë„êµ¬ê°€ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ í¸ì—ì„œ ê°ê°ì„ ê¹Šì´ ë‹¤ë£¨ê² ì§€ë§Œ, ê°„ëµíˆ ë¹„êµí•©ë‹ˆë‹¤.\n\n| íŠ¹ì„± | ArgoCD | Flux |\n|-----|--------|------|\n| **CNCF ë‹¨ê³„** | Graduated | Graduated |\n| **UI** | ê°•ë ¥í•œ Web UI ì œê³µ | CLI ì¤‘ì‹¬ (Web UIëŠ” ë³„ë„ í”„ë¡œì íŠ¸) |\n| **ì•„í‚¤í…ì²˜** | ë‹¨ì¼ ì• í”Œë¦¬ì¼€ì´ì…˜ | ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ (GitOps Toolkit) |\n| **ë©€í‹° í´ëŸ¬ìŠ¤í„°** | ApplicationSet | Kustomization + í´ëŸ¬ìŠ¤í„°ë³„ ë””ë ‰í† ë¦¬ |\n| **ì´ë¯¸ì§€ ìë™ ì—…ë°ì´íŠ¸** | Image Updater (ë³„ë„) | Image Automation Controller (ë‚´ì¥) |\n| **ì£¼ìš” ì‚¬ìš©ì** | Intuit, Red Hat | Weaveworks, AWS |\n\n```mermaid\nflowchart TB\n    subgraph ArgoCD [ArgoCD ì•„í‚¤í…ì²˜]\n        A_API[API Server]\n        A_Repo[Repo Server]\n        A_Controller[Application Controller]\n        A_API --> A_Repo\n        A_API --> A_Controller\n    end\n    \n    subgraph Flux [Flux ì•„í‚¤í…ì²˜]\n        F_Source[Source Controller]\n        F_Kustomize[Kustomize Controller]\n        F_Helm[Helm Controller]\n        F_Notification[Notification Controller]\n    end\n```\n\n> [!NOTE]\n> **ì„ íƒ ê¸°ì¤€**: UIê°€ ì¤‘ìš”í•˜ê³  ì¤‘ì•™ ì§‘ì¤‘ì‹ ê´€ë¦¬ë¥¼ ì›í•˜ë©´ ArgoCD, ê²½ëŸ‰í™”ì™€ ëª¨ë“ˆí™”ë¥¼ ì›í•˜ë©´ Fluxê°€ ì í•©í•©ë‹ˆë‹¤. ë‘˜ ë‹¤ ì„±ìˆ™í•œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.\n\n---\n\n## ì‹¤ë¬´ ë„ì… ì‹œ ê³ ë ¤ì‚¬í•­\n\n### 1. ë ˆí¬ì§€í† ë¦¬ êµ¬ì¡°\n\n**Monorepo vs Polyrepo**\n\n```\n# Monorepo ë°©ì‹\ngitops-repo/\nâ”œâ”€â”€ apps/\nâ”‚   â”œâ”€â”€ frontend/\nâ”‚   â”œâ”€â”€ backend/\nâ”‚   â””â”€â”€ database/\nâ”œâ”€â”€ infrastructure/\nâ”‚   â”œâ”€â”€ monitoring/\nâ”‚   â””â”€â”€ ingress/\nâ””â”€â”€ clusters/\n    â”œâ”€â”€ dev/\n    â”œâ”€â”€ staging/\n    â””â”€â”€ prod/\n```\n\n```\n# Polyrepo ë°©ì‹\nfrontend-gitops/      # í”„ë¡ íŠ¸ì—”ë“œ íŒ€ ê´€ë¦¬\nbackend-gitops/       # ë°±ì—”ë“œ íŒ€ ê´€ë¦¬\nplatform-gitops/      # í”Œë«í¼ íŒ€ ê´€ë¦¬\n```\n\n### 2. í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬\n\nKustomizeë‚˜ Helmìœ¼ë¡œ í™˜ê²½ë³„ ì„¤ì •ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤:\n\n```\nbase/\nâ”œâ”€â”€ deployment.yaml\nâ”œâ”€â”€ service.yaml\nâ””â”€â”€ kustomization.yaml\n\noverlays/\nâ”œâ”€â”€ dev/\nâ”‚   â”œâ”€â”€ kustomization.yaml\nâ”‚   â””â”€â”€ replica-patch.yaml\nâ”œâ”€â”€ staging/\nâ”‚   â””â”€â”€ kustomization.yaml\nâ””â”€â”€ prod/\n    â”œâ”€â”€ kustomization.yaml\n    â””â”€â”€ resource-patch.yaml\n```\n\n### 3. Secrets ì²˜ë¦¬\n\nGitì— í‰ë¬¸ Secretì„ ì €ì¥í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤. ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•©ë‹ˆë‹¤:\n\n- **Sealed Secrets**: ì•”í˜¸í™”ëœ í˜•íƒœë¡œ Gitì— ì €ì¥\n- **External Secrets Operator**: AWS Secrets Manager ë“± ì™¸ë¶€ ì°¸ì¡°\n- **SOPS**: íŒŒì¼ ë ˆë²¨ ì•”í˜¸í™”\n\n(5í¸ì—ì„œ ìì„¸íˆ ë‹¤ë£¹ë‹ˆë‹¤)\n\n---\n\n## ì •ë¦¬\n\n| ê°œë… | ì„¤ëª… |\n|-----|------|\n| **GitOps** | Gitì„ Single Source of Truthë¡œ ì‚¼ëŠ” ìš´ì˜ ë°©ì‹ |\n| **4ê°€ì§€ ì›ì¹™** | Declarative, Versioned, Pulled, Reconciled |\n| **Pull ëª¨ë¸** | í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ ì—ì´ì „íŠ¸ê°€ Gitì„ ê°ì‹œ |\n| **Reconciliation** | ì§€ì†ì ìœ¼ë¡œ í˜„ì¬ ìƒíƒœë¥¼ ì›í•˜ëŠ” ìƒíƒœë¡œ ì¡°ì • |\n| **Self-Healing** | ìˆ˜ë™ ë³€ê²½(Drift)ì„ ìë™ìœ¼ë¡œ Git ìƒíƒœë¡œ ë³µêµ¬ |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**2í¸: ArgoCD Deep Dive**ì—ì„œëŠ” ë‹¤ìŒì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- ArgoCD ì•„í‚¤í…ì²˜ ìƒì„¸ (API Server, Repo Server, Controller)\n- Application CRD ì™„ì „ ë¶„ì„\n- Sync ì „ëµ: Manual vs Automated, Self-Heal, Prune\n- Sync Wavesì™€ Hooks\n- ApplicationSetìœ¼ë¡œ ë©€í‹° í´ëŸ¬ìŠ¤í„° ê´€ë¦¬\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [OpenGitOps Principles](https://opengitops.dev/principles)\n- [ArgoCD - Declarative GitOps CD for Kubernetes](https://argo-cd.readthedocs.io/)\n- [Flux - The GitOps family of projects](https://fluxcd.io/)\n- [CNCF GitOps Working Group](https://github.com/cncf/tag-app-delivery/tree/main/gitops-wg)\n- [Guide to GitOps - Weaveworks](https://www.weave.works/technologies/gitops/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitOps",
      "Kubernetes",
      "ArgoCD",
      "Flux"
    ],
    "readingTime": 9,
    "wordCount": 1718,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitops",
    "date": "2026-01-05"
  },
  {
    "id": "gitlab-ci-06-external-integrations",
    "slug": "gitlab-ci-06-external-integrations",
    "path": "backend/devops",
    "fullPath": "backend/devops/gitlab-ci-06-external-integrations",
    "title": "GitLab CI/CD ì‹œë¦¬ì¦ˆ #6: ì™¸ë¶€ í†µí•© - Triggers, Webhooks, API",
    "excerpt": "Pipeline Triggers, Webhooks, APIë¥¼ í†µí•œ ì™¸ë¶€ ì‹œìŠ¤í…œ ì—°ë™, ChatOps, ê·¸ë¦¬ê³  GitOps ë„êµ¬ì™€ì˜ í†µí•©ì„ ë‹¤ë£¹ë‹ˆë‹¤.",
    "content": "# GitLab CI/CD ì‹œë¦¬ì¦ˆ #6: ì™¸ë¶€ í†µí•© - Triggers, Webhooks, API\n\n## ì‹œë¦¬ì¦ˆ ê°œìš”\n\n| # | ì£¼ì œ | í•µì‹¬ ë‚´ìš© |\n|---|------|----------|\n| 1 | ê¸°ì´ˆ | .gitlab-ci.yml êµ¬ì¡°, Stages, Jobs, Pipeline íë¦„ |\n| 2 | Variables & Secrets | ë³€ìˆ˜ ìœ í˜•, ìš°ì„ ìˆœìœ„, ì™¸ë¶€ Vault ì—°ë™ |\n| 3 | Runners & Executors | Docker, Kubernetes, Docker-in-Docker |\n| 4 | Pipeline ì•„í‚¤í…ì²˜ | Parent-Child, Multi-Project Pipeline |\n| 5 | ê³ ê¸‰ Job ì œì–´ | rules, needs, DAG, extends |\n| **6** | **ì™¸ë¶€ í†µí•©** | Triggers, Webhooks, API |\n\n---\n\n## Pipeline Triggers\n\nì™¸ë¶€ ì‹œìŠ¤í…œì—ì„œ **í† í° ê¸°ë°˜ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ì„ íŠ¸ë¦¬ê±°**í•©ë‹ˆë‹¤.\n\n### Trigger Token ìƒì„±\n\n1. `Settings > CI/CD > Pipeline trigger tokens`\n2. `Add trigger` í´ë¦­\n3. ì„¤ëª… ì…ë ¥ í›„ ìƒì„±\n4. ìƒì„±ëœ í† í° ë³µì‚¬\n\n### íŠ¸ë¦¬ê±° ì‹¤í–‰\n\n```bash\n# ê¸°ë³¸ íŠ¸ë¦¬ê±°\ncurl --request POST \\\n  --form \"token=YOUR_TRIGGER_TOKEN\" \\\n  --form \"ref=main\" \\\n  \"https://gitlab.com/api/v4/projects/PROJECT_ID/trigger/pipeline\"\n\n# ë³€ìˆ˜ì™€ í•¨ê»˜\ncurl --request POST \\\n  --form \"token=YOUR_TRIGGER_TOKEN\" \\\n  --form \"ref=main\" \\\n  --form \"variables[DEPLOY_ENV]=production\" \\\n  --form \"variables[VERSION]=1.2.3\" \\\n  \"https://gitlab.com/api/v4/projects/PROJECT_ID/trigger/pipeline\"\n```\n\n### íŒŒì´í”„ë¼ì¸ì—ì„œ íŠ¸ë¦¬ê±° ê°ì§€\n\n```yaml\ndeploy:\n  script:\n    - ./deploy.sh\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"trigger\"\n      variables:\n        DEPLOY_TOKEN: $DEPLOY_TOKEN  # íŠ¸ë¦¬ê±°ë¡œ ì „ë‹¬ëœ ë³€ìˆ˜\n```\n\n### Trigger ì „ìš© Job\n\n```yaml\ntriggered-deploy:\n  script:\n    - echo \"Deploying version $VERSION to $DEPLOY_ENV\"\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"trigger\"\n  needs: []  # ë‹¤ë¥¸ Job ëŒ€ê¸° ì—†ì´ ì¦‰ì‹œ ì‹¤í–‰\n```\n\n---\n\n## Webhooks\n\n**ì™¸ë¶€ ì´ë²¤íŠ¸ë¥¼ ìˆ˜ì‹ **í•˜ì—¬ íŒŒì´í”„ë¼ì¸ì„ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤.\n\n### Webhook URL í˜•ì‹\n\n```\nhttps://gitlab.com/api/v4/projects/PROJECT_ID/ref/REF_NAME/trigger/pipeline?token=TOKEN\n```\n\n### ì‹¤ì œ Webhook ì„¤ì •\n\n```bash\n# GitHub â†’ GitLab íŠ¸ë¦¬ê±°\n# GitHub ì €ì¥ì†Œì˜ Webhooksì— ë“±ë¡\nhttps://gitlab.com/api/v4/projects/12345/ref/main/trigger/pipeline?token=abc123\n\n# AWS SNS â†’ GitLab íŠ¸ë¦¬ê±°\n# Lambdaë¥¼ í†µí•´ ë³€í™˜ í›„ íŠ¸ë¦¬ê±°\n```\n\n### Webhook Payload ì ‘ê·¼\n\n```yaml\nprocess-webhook:\n  script:\n    - echo \"$TRIGGER_PAYLOAD\" | jq .\n    - export EVENT_TYPE=$(echo \"$TRIGGER_PAYLOAD\" | jq -r '.event_type')\n    - |\n      if [ \"$EVENT_TYPE\" = \"release\" ]; then\n        ./deploy-release.sh\n      fi\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"trigger\"\n```\n\n---\n\n## GitLab APIë¡œ íŒŒì´í”„ë¼ì¸ ì œì–´\n\n### íŒŒì´í”„ë¼ì¸ ìƒì„±\n\n```bash\n# Personal Access Token ì‚¬ìš©\ncurl --request POST \\\n  --header \"PRIVATE-TOKEN: $GITLAB_TOKEN\" \\\n  --header \"Content-Type: application/json\" \\\n  --data '{\n    \"ref\": \"main\",\n    \"variables\": [\n      {\"key\": \"DEPLOY_ENV\", \"value\": \"staging\"},\n      {\"key\": \"DEBUG\", \"value\": \"true\"}\n    ]\n  }' \\\n  \"https://gitlab.com/api/v4/projects/PROJECT_ID/pipeline\"\n```\n\n### íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¡°íšŒ\n\n```bash\n# íŠ¹ì • íŒŒì´í”„ë¼ì¸\ncurl --header \"PRIVATE-TOKEN: $GITLAB_TOKEN\" \\\n  \"https://gitlab.com/api/v4/projects/PROJECT_ID/pipelines/PIPELINE_ID\"\n\n# ìµœê·¼ íŒŒì´í”„ë¼ì¸\ncurl --header \"PRIVATE-TOKEN: $GITLAB_TOKEN\" \\\n  \"https://gitlab.com/api/v4/projects/PROJECT_ID/pipelines?per_page=5\"\n```\n\n### Job ì¬ì‹œë„\n\n```bash\ncurl --request POST \\\n  --header \"PRIVATE-TOKEN: $GITLAB_TOKEN\" \\\n  \"https://gitlab.com/api/v4/projects/PROJECT_ID/jobs/JOB_ID/retry\"\n```\n\n### íŒŒì´í”„ë¼ì¸ ì·¨ì†Œ\n\n```bash\ncurl --request POST \\\n  --header \"PRIVATE-TOKEN: $GITLAB_TOKEN\" \\\n  \"https://gitlab.com/api/v4/projects/PROJECT_ID/pipelines/PIPELINE_ID/cancel\"\n```\n\n---\n\n## CI Job Token\n\níŒŒì´í”„ë¼ì¸ ë‚´ì—ì„œ **GitLab APIë¥¼ í˜¸ì¶œ**í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ì„ì‹œ í† í°ì…ë‹ˆë‹¤.\n\n### CI_JOB_TOKEN ì‚¬ìš©\n\n```yaml\nnotify-other-project:\n  script:\n    # ë‹¤ë¥¸ í”„ë¡œì íŠ¸ íŒŒì´í”„ë¼ì¸ íŠ¸ë¦¬ê±°\n    - |\n      curl --request POST \\\n        --form \"token=$CI_JOB_TOKEN\" \\\n        --form \"ref=main\" \\\n        \"https://gitlab.com/api/v4/projects/OTHER_PROJECT_ID/trigger/pipeline\"\n```\n\n### ì•„í‹°íŒ©íŠ¸ ë‹¤ìš´ë¡œë“œ\n\n```yaml\ndownload-artifacts:\n  script:\n    - |\n      curl --header \"JOB-TOKEN: $CI_JOB_TOKEN\" \\\n        --output artifacts.zip \\\n        \"https://gitlab.com/api/v4/projects/PROJECT_ID/jobs/JOB_ID/artifacts\"\n    - unzip artifacts.zip\n```\n\n### ê¶Œí•œ ì„¤ì •\n\n`Settings > CI/CD > Token Access`ì—ì„œ í—ˆìš©í•  í”„ë¡œì íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph ProjectA [Project A]\n        PA_Job[Job]\n    end\n    \n    subgraph ProjectB [Project B]\n        PB_API[GitLab API]\n        PB_Token[Token Access ì„¤ì •]\n    end\n    \n    PA_Job -->|CI_JOB_TOKEN| PB_API\n    PB_Token -->|í—ˆìš©| PA_Job\n```\n\n---\n\n## ChatOps ì—°ë™\n\n### Slack ì•Œë¦¼\n\n```yaml\nnotify-slack:\n  stage: .post\n  script:\n    - |\n      curl -X POST -H 'Content-type: application/json' \\\n        --data '{\n          \"channel\": \"#deployments\",\n          \"username\": \"GitLab CI\",\n          \"text\": \"âœ… Pipeline succeeded for $CI_PROJECT_NAME\",\n          \"attachments\": [{\n            \"color\": \"good\",\n            \"fields\": [\n              {\"title\": \"Branch\", \"value\": \"'$CI_COMMIT_BRANCH'\", \"short\": true},\n              {\"title\": \"Commit\", \"value\": \"'$CI_COMMIT_SHORT_SHA'\", \"short\": true}\n            ]\n          }]\n        }' \\\n        $SLACK_WEBHOOK_URL\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      when: on_success\n```\n\n### ë°°í¬ ìŠ¹ì¸ (Slack â†’ GitLab)\n\n```yaml\n# Slack ë²„íŠ¼ í´ë¦­ â†’ Lambda â†’ GitLab API\nrequest-approval:\n  stage: deploy\n  script:\n    - |\n      curl -X POST -H 'Content-type: application/json' \\\n        --data '{\n          \"text\": \"ğŸš€ Production deployment pending\",\n          \"attachments\": [{\n            \"text\": \"Approve deployment?\",\n            \"callback_id\": \"deploy_'$CI_PIPELINE_ID'\",\n            \"actions\": [\n              {\"name\": \"approve\", \"text\": \"Approve\", \"type\": \"button\", \"style\": \"primary\"},\n              {\"name\": \"reject\", \"text\": \"Reject\", \"type\": \"button\", \"style\": \"danger\"}\n            ]\n          }]\n        }' \\\n        $SLACK_WEBHOOK_URL\n  environment:\n    name: production\n    action: prepare\n```\n\n---\n\n## ì™¸ë¶€ CI/CD ì‹œìŠ¤í…œ ì—°ë™\n\n### Jenkins â†’ GitLab\n\n```groovy\n// Jenkinsfile\npipeline {\n    agent any\n    stages {\n        stage('Build') {\n            steps {\n                sh 'make build'\n            }\n        }\n        stage('Trigger GitLab') {\n            steps {\n                sh '''\n                    curl --request POST \\\n                      --form \"token=${GITLAB_TRIGGER_TOKEN}\" \\\n                      --form \"ref=main\" \\\n                      --form \"variables[JENKINS_BUILD]=${BUILD_NUMBER}\" \\\n                      \"https://gitlab.com/api/v4/projects/${GITLAB_PROJECT_ID}/trigger/pipeline\"\n                '''\n            }\n        }\n    }\n}\n```\n\n### GitHub Actions â†’ GitLab\n\n```yaml\n# .github/workflows/trigger-gitlab.yml\nname: Trigger GitLab Pipeline\n\non:\n  push:\n    branches: [main]\n\njobs:\n  trigger:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Trigger GitLab\n        run: |\n          curl --request POST \\\n            --form \"token=${{ secrets.GITLAB_TRIGGER_TOKEN }}\" \\\n            --form \"ref=main\" \\\n            --form \"variables[GITHUB_SHA]=${{ github.sha }}\" \\\n            \"https://gitlab.com/api/v4/projects/${{ secrets.GITLAB_PROJECT_ID }}/trigger/pipeline\"\n```\n\n---\n\n## GitOps ë„êµ¬ ì—°ë™\n\n### GitLab CI â†’ ArgoCD\n\n```yaml\ndeploy-argocd:\n  stage: deploy\n  image: argoproj/argocd:latest\n  script:\n    # ArgoCD ë¡œê·¸ì¸\n    - argocd login $ARGOCD_SERVER --username admin --password $ARGOCD_PASSWORD --insecure\n    \n    # ì´ë¯¸ì§€ íƒœê·¸ ì—…ë°ì´íŠ¸\n    - argocd app set $APP_NAME --helm-set image.tag=$CI_COMMIT_SHA\n    \n    # Sync íŠ¸ë¦¬ê±°\n    - argocd app sync $APP_NAME --prune\n    \n    # ë°°í¬ ì™„ë£Œ ëŒ€ê¸°\n    - argocd app wait $APP_NAME --timeout 300\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n```\n\n### Image Updater íŒ¨í„´\n\n```yaml\n# CIëŠ” ì´ë¯¸ì§€ë§Œ ë¹Œë“œ & í‘¸ì‹œ\n# ArgoCD Image Updaterê°€ ìë™ìœ¼ë¡œ ê°ì§€í•˜ì—¬ ë°°í¬\n\nbuild-and-push:\n  stage: build\n  script:\n    - docker build -t $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA .\n    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n    \n    # SemVer íƒœê·¸ë„ í‘¸ì‹œ (Image Updaterê°€ ê°ì§€)\n    - docker tag $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA $CI_REGISTRY_IMAGE:v$VERSION\n    - docker push $CI_REGISTRY_IMAGE:v$VERSION\n```\n\n```mermaid\nflowchart LR\n    subgraph CI [GitLab CI]\n        Build[Build & Push Image]\n    end\n    \n    subgraph Registry [Container Registry]\n        Images[(Images)]\n    end\n    \n    subgraph GitOps [GitOps]\n        Updater[ArgoCD Image Updater]\n        ArgoCD[ArgoCD]\n        K8s[Kubernetes]\n    end\n    \n    Build --> Images\n    Updater -->|Scan| Images\n    Updater -->|Update| ArgoCD\n    ArgoCD -->|Sync| K8s\n```\n\n> [!TIP]\n> CI/CD ë¶„ë¦¬ ì›ì¹™: **CIëŠ” ì•„í‹°íŒ©íŠ¸ ìƒì„±**, **CDëŠ” GitOps Agent**ê°€ ë‹´ë‹¹í•©ë‹ˆë‹¤. GitOps ì‹œë¦¬ì¦ˆ 6í¸ì—ì„œ ìì„¸íˆ ë‹¤ë¤˜ìŠµë‹ˆë‹¤.\n\n---\n\n## Scheduled Pipelines (cron)\n\nì •ê¸°ì ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n\n### ì„¤ì •\n\n`Build > Pipeline schedules > New schedule`\n\n```yaml\nnightly-test:\n  script:\n    - npm run test:full\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"schedule\"\n      variables:\n        FULL_TEST: \"true\"\n\ndaily-backup:\n  script:\n    - ./backup.sh\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"schedule\" && $SCHEDULE_TYPE == \"backup\"\n```\n\n### Schedule ë³€ìˆ˜\n\n```yaml\n# Schedule ì„¤ì •ì—ì„œ SCHEDULE_TYPE=security ì§€ì •\n\nsecurity-scan:\n  script:\n    - trivy image $CI_REGISTRY_IMAGE:latest\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"schedule\" && $SCHEDULE_TYPE == \"security\"\n```\n\n---\n\n## ì‹¤ì „ ì˜ˆì œ: ì™„ì „í•œ ì™¸ë¶€ ì—°ë™\n\n```yaml\nstages:\n  - build\n  - deploy\n  - notify\n\nvariables:\n  DOCKER_IMAGE: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n\nbuild:\n  stage: build\n  script:\n    - docker build -t $DOCKER_IMAGE .\n    - docker push $DOCKER_IMAGE\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n\n# ArgoCD ë°°í¬ íŠ¸ë¦¬ê±°\ntrigger-argocd:\n  stage: deploy\n  image: curlimages/curl:latest\n  script:\n    - |\n      # GitOps ë ˆí¬ì— ì´ë¯¸ì§€ íƒœê·¸ ì—…ë°ì´íŠ¸ PR ìƒì„±\n      curl --request POST \\\n        --header \"PRIVATE-TOKEN: $GITOPS_TOKEN\" \\\n        --header \"Content-Type: application/json\" \\\n        --data '{\n          \"branch\": \"update-'$CI_COMMIT_SHORT_SHA'\",\n          \"commit_message\": \"Update image to '$DOCKER_IMAGE'\",\n          \"actions\": [{\n            \"action\": \"update\",\n            \"file_path\": \"apps/myapp/values.yaml\",\n            \"content\": \"image:\\n  tag: '$CI_COMMIT_SHA'\"\n          }]\n        }' \\\n        \"https://gitlab.com/api/v4/projects/GITOPS_PROJECT_ID/repository/commits\"\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n\n# Slack ì•Œë¦¼\nnotify-success:\n  stage: notify\n  script:\n    - |\n      curl -X POST -H 'Content-type: application/json' \\\n        --data '{\n          \"channel\": \"#deployments\",\n          \"text\": \"âœ… '$CI_PROJECT_NAME' deployed successfully\",\n          \"attachments\": [{\n            \"color\": \"good\",\n            \"fields\": [\n              {\"title\": \"Version\", \"value\": \"'$CI_COMMIT_SHORT_SHA'\"},\n              {\"title\": \"Pipeline\", \"value\": \"'$CI_PIPELINE_URL'\"}\n            ]\n          }]\n        }' \\\n        $SLACK_WEBHOOK_URL\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      when: on_success\n\nnotify-failure:\n  stage: notify\n  script:\n    - |\n      curl -X POST -H 'Content-type: application/json' \\\n        --data '{\n          \"channel\": \"#deployments\",\n          \"text\": \"âŒ '$CI_PROJECT_NAME' deployment failed!\",\n          \"attachments\": [{\n            \"color\": \"danger\",\n            \"fields\": [\n              {\"title\": \"Pipeline\", \"value\": \"'$CI_PIPELINE_URL'\"}\n            ]\n          }]\n        }' \\\n        $SLACK_WEBHOOK_URL\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      when: on_failure\n```\n\n---\n\n## ì •ë¦¬: ì‹œë¦¬ì¦ˆ ì™„ê²°\n\n6í¸ì˜ ì‹œë¦¬ì¦ˆë¥¼ í†µí•´ GitLab CI/CDì˜ í•µì‹¬ì„ ë‹¤ë¤˜ìŠµë‹ˆë‹¤.\n\n| í¸ | ì£¼ì œ | í•µì‹¬ ë©”ì‹œì§€ |\n|---|------|-----------|\n| 1 | ê¸°ì´ˆ | .gitlab-ci.yml, Stages, Jobs, Pipeline |\n| 2 | Variables | Predefined, Protected, Masked, dotenv |\n| 3 | Runners | Executors, DinD, Kubernetes |\n| 4 | ì•„í‚¤í…ì²˜ | Parent-Child, Multi-Project, ë™ì  ìƒì„± |\n| 5 | Job ì œì–´ | rules, needs, DAG, extends |\n| 6 | ì™¸ë¶€ í†µí•© | Triggers, Webhooks, API, GitOps |\n\n### ë‹¤ìŒ ë‹¨ê³„\n\n- **GitOps ì‹œë¦¬ì¦ˆ**: ArgoCD/Flux CDë¥¼ í™œìš©í•œ CD ìë™í™”\n- **Kubernetes ì‹œë¦¬ì¦ˆ**: í´ëŸ¬ìŠ¤í„° ìš´ì˜ ì‹¬í™”\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Pipeline Triggers](https://docs.gitlab.com/ee/ci/triggers/)\n- [CI/CD API](https://docs.gitlab.com/ee/api/pipelines.html)\n- [CI Job Token](https://docs.gitlab.com/ee/ci/jobs/ci_job_token.html)\n- [Scheduled Pipelines](https://docs.gitlab.com/ee/ci/pipelines/schedules.html)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitLab",
      "CI/CD",
      "Pipeline",
      "API",
      "Webhook"
    ],
    "readingTime": 7,
    "wordCount": 1302,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitlab-ci",
    "date": "2026-01-05"
  },
  {
    "id": "gitlab-ci-05-advanced-job-control",
    "slug": "gitlab-ci-05-advanced-job-control",
    "path": "backend/devops",
    "fullPath": "backend/devops/gitlab-ci-05-advanced-job-control",
    "title": "GitLab CI/CD ì‹œë¦¬ì¦ˆ #5: ê³ ê¸‰ Job ì œì–´ - rules, needs, DAG",
    "excerpt": "rules ì¡°ê±´ ë¶„ê¸°, needsë¥¼ í™œìš©í•œ DAG ì‹¤í–‰, Job í…œí”Œë¦¿ê³¼ extends, !reference íƒœê·¸ê¹Œì§€ ê³ ê¸‰ Job ì œì–´ ê¸°ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.",
    "content": "# GitLab CI/CD ì‹œë¦¬ì¦ˆ #5: ê³ ê¸‰ Job ì œì–´ - rules, needs, DAG\n\n## ì‹œë¦¬ì¦ˆ ê°œìš”\n\n| # | ì£¼ì œ | í•µì‹¬ ë‚´ìš© |\n|---|------|----------|\n| 1 | ê¸°ì´ˆ | .gitlab-ci.yml êµ¬ì¡°, Stages, Jobs, Pipeline íë¦„ |\n| 2 | Variables & Secrets | ë³€ìˆ˜ ìœ í˜•, ìš°ì„ ìˆœìœ„, ì™¸ë¶€ Vault ì—°ë™ |\n| 3 | Runners & Executors | Docker, Kubernetes, Docker-in-Docker |\n| 4 | Pipeline ì•„í‚¤í…ì²˜ | Parent-Child, Multi-Project Pipeline |\n| **5** | **ê³ ê¸‰ Job ì œì–´** | rules, needs, DAG, extends |\n| 6 | ì™¸ë¶€ í†µí•© | Triggers, Webhooks, API |\n\n---\n\n## rules: ì¡°ê±´ë¶€ Job ì‹¤í–‰\n\n`rules`ëŠ” `only/except`ë¥¼ ëŒ€ì²´í•˜ëŠ” **ê°•ë ¥í•œ ì¡°ê±´ë¶€ ì‹¤í–‰ í‚¤ì›Œë“œ**ì…ë‹ˆë‹¤.\n\n### ê¸°ë³¸ ë¬¸ë²•\n\n```yaml\njob:\n  script:\n    - echo \"Hello\"\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      when: always\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n      when: manual\n    - when: never  # ê¸°ë³¸ê°’\n```\n\n### rules í‰ê°€ ìˆœì„œ\n\n```mermaid\nflowchart TB\n    Start[rules í‰ê°€ ì‹œì‘] --> R1{Rule 1 ì¡°ê±´?}\n    R1 -->|ë§¤ì¹­| Apply1[Rule 1 ì ìš©, í‰ê°€ ì¢…ë£Œ]\n    R1 -->|ë¶ˆì¼ì¹˜| R2{Rule 2 ì¡°ê±´?}\n    R2 -->|ë§¤ì¹­| Apply2[Rule 2 ì ìš©, í‰ê°€ ì¢…ë£Œ]\n    R2 -->|ë¶ˆì¼ì¹˜| RN{... Rule N ì¡°ê±´?}\n    RN -->|ë§¤ì¹­| ApplyN[Rule N ì ìš©]\n    RN -->|ë¶ˆì¼ì¹˜| Never[Job ì‹¤í–‰ ì•ˆ í•¨]\n```\n\n> [!IMPORTANT]\n> rulesëŠ” **ìœ„ì—ì„œ ì•„ë˜ë¡œ ìˆœì°¨ í‰ê°€**í•˜ë©°, ì²« ë²ˆì§¸ ë§¤ì¹­ëœ ruleì´ ì ìš©ë©ë‹ˆë‹¤. ë§¤ì¹­ë˜ëŠ” ruleì´ ì—†ìœ¼ë©´ Jobì´ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n### rules ì¡°ê±´ ìœ í˜•\n\n#### if: í‘œí˜„ì‹ í‰ê°€\n\n```yaml\nrules:\n  # ë¸Œëœì¹˜ ì¡°ê±´\n  - if: $CI_COMMIT_BRANCH == \"main\"\n  \n  # Pipeline Source\n  - if: $CI_PIPELINE_SOURCE == \"push\"\n  - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n  \n  # ë³€ìˆ˜ ì¡´ì¬ ì—¬ë¶€\n  - if: $DEPLOY_TOKEN\n  \n  # ì •ê·œì‹\n  - if: $CI_COMMIT_TAG =~ /^v\\d+\\.\\d+\\.\\d+$/\n  \n  # ë³µí•© ì¡°ê±´\n  - if: $CI_COMMIT_BRANCH == \"main\" && $CI_PIPELINE_SOURCE == \"push\"\n```\n\n#### changes: íŒŒì¼ ë³€ê²½ ê°ì§€\n\n```yaml\nbuild-frontend:\n  rules:\n    - changes:\n        - frontend/**/*\n        - shared/**/*\n\nbuild-backend:\n  rules:\n    - changes:\n        paths:\n          - backend/**/*\n        compare_to: main  # main ë¸Œëœì¹˜ì™€ ë¹„êµ\n```\n\n#### exists: íŒŒì¼ ì¡´ì¬ í™•ì¸\n\n```yaml\ndocker-build:\n  rules:\n    - exists:\n        - Dockerfile\n        - docker-compose.yml\n\nnpm-build:\n  rules:\n    - exists:\n        - package.json\n```\n\n### when ì˜µì…˜\n\n| ê°’ | ë™ì‘ |\n|---|------|\n| `on_success` | ì´ì „ Stage ì„±ê³µ ì‹œ (ê¸°ë³¸ê°’) |\n| `always` | í•­ìƒ ì‹¤í–‰ |\n| `never` | ì‹¤í–‰ ì•ˆ í•¨ |\n| `manual` | ìˆ˜ë™ ìŠ¹ì¸ í•„ìš” |\n| `delayed` | ì§€ì—° í›„ ì‹¤í–‰ |\n\n```yaml\ndeploy-prod:\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      when: manual\n      allow_failure: false  # ë¸”ë¡œí‚¹ ìˆ˜ë™ Job\n```\n\n### only/exceptì—ì„œ ë§ˆì´ê·¸ë ˆì´ì…˜\n\n```yaml\n# ì´ì „ (only/except) - ë” ì´ìƒ ê¶Œì¥í•˜ì§€ ì•ŠìŒ\njob:\n  only:\n    - main\n  except:\n    - tags\n\n# í˜„ì¬ (rules) - ê¶Œì¥\njob:\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\" && $CI_COMMIT_TAG == null\n```\n\n---\n\n## workflow:rules: ì „ì—­ íŒŒì´í”„ë¼ì¸ ì œì–´\n\n**íŒŒì´í”„ë¼ì¸ ìì²´ì˜ ìƒì„± ì—¬ë¶€**ë¥¼ ì œì–´í•©ë‹ˆë‹¤.\n\n```yaml\nworkflow:\n  rules:\n    # MR íŒŒì´í”„ë¼ì¸: MR ì´ë²¤íŠ¸ì—ì„œë§Œ\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n    \n    # ë¸Œëœì¹˜ íŒŒì´í”„ë¼ì¸: main, developë§Œ\n    - if: $CI_COMMIT_BRANCH == \"main\"\n    - if: $CI_COMMIT_BRANCH == \"develop\"\n    \n    # íƒœê·¸ íŒŒì´í”„ë¼ì¸\n    - if: $CI_COMMIT_TAG\n    \n    # ê·¸ ì™¸: íŒŒì´í”„ë¼ì¸ ìƒì„± ì•ˆ í•¨\n\nstages:\n  - build\n  - test\n```\n\n### ì¤‘ë³µ íŒŒì´í”„ë¼ì¸ ë°©ì§€\n\n```yaml\nworkflow:\n  rules:\n    # MR ì´ë²¤íŠ¸ ì‹œ ë¸Œëœì¹˜ íŒŒì´í”„ë¼ì¸ ë°©ì§€ (MR íŒŒì´í”„ë¼ì¸ë§Œ ì‹¤í–‰)\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n    - if: $CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS\n      when: never  # MRì´ ì—´ë ¤ìˆìœ¼ë©´ ë¸Œëœì¹˜ íŒŒì´í”„ë¼ì¸ ìŠ¤í‚µ\n    - if: $CI_COMMIT_BRANCH\n```\n\n---\n\n## needs: DAG (Directed Acyclic Graph)\n\n`needs`ëŠ” **Stageë¥¼ ë¬´ì‹œí•˜ê³  Job ê°„ ì§ì ‘ ì˜ì¡´ì„±**ì„ ì •ì˜í•©ë‹ˆë‹¤.\n\n### Stage ê¸°ë°˜ vs DAG\n\n```mermaid\nflowchart LR\n    subgraph Stage-based [Stage ê¸°ë°˜]\n        direction TB\n        S1[Stage 1] --> S2[Stage 2] --> S3[Stage 3]\n    end\n    \n    subgraph DAG [DAG ê¸°ë°˜]\n        direction LR\n        A[Job A] --> C[Job C]\n        B[Job B] --> D[Job D]\n        C --> E[Job E]\n        D --> E\n    end\n```\n\n### ê¸°ë³¸ ì‚¬ìš©ë²•\n\n```yaml\nstages:\n  - build\n  - test\n  - deploy\n\nbuild-frontend:\n  stage: build\n  script: make build-frontend\n\nbuild-backend:\n  stage: build\n  script: make build-backend\n\ntest-frontend:\n  stage: test\n  needs: [build-frontend]  # build-frontend ì™„ë£Œ ì¦‰ì‹œ ì‹œì‘\n  script: make test-frontend\n\ntest-backend:\n  stage: test\n  needs: [build-backend]\n  script: make test-backend\n\ndeploy:\n  stage: deploy\n  needs: [test-frontend, test-backend]\n  script: make deploy\n```\n\n### DAG ì‹¤í–‰ íë¦„\n\n```mermaid\nflowchart LR\n    BF[build-frontend] --> TF[test-frontend]\n    BB[build-backend] --> TB[test-backend]\n    TF --> D[deploy]\n    TB --> D\n```\n\n`build-frontend`ê°€ ì™„ë£Œë˜ë©´ `build-backend`ë¥¼ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  **ì¦‰ì‹œ** `test-frontend`ê°€ ì‹œì‘ë©ë‹ˆë‹¤.\n\n### needs ì˜µì…˜\n\n```yaml\ntest:\n  needs:\n    - job: build\n      artifacts: true   # ì•„í‹°íŒ©íŠ¸ ë‹¤ìš´ë¡œë“œ (ê¸°ë³¸ê°’)\n      optional: false   # í•„ìˆ˜ ì˜ì¡´ì„± (ê¸°ë³¸ê°’)\n\ndeploy:\n  needs:\n    - job: test\n      artifacts: false  # ì•„í‹°íŒ©íŠ¸ ë¶ˆí•„ìš”\n    - job: security-scan\n      optional: true    # ì‹¤íŒ¨í•´ë„ ì§„í–‰\n```\n\n### parallelê³¼ needs\n\n```yaml\ntest:\n  parallel: 3\n  script: run-tests.sh\n\nreport:\n  needs:\n    - job: test\n      parallel:\n        matrix:\n          - RUNNER: [1, 2, 3]  # parallel ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ ëŒ€ê¸°\n```\n\n---\n\n## dependencies: ì•„í‹°íŒ©íŠ¸ ì œì–´\n\n`dependencies`ëŠ” **ì•„í‹°íŒ©íŠ¸ ë‹¤ìš´ë¡œë“œ**ë¥¼ ì œì–´í•©ë‹ˆë‹¤.\n\n```yaml\nbuild:\n  stage: build\n  script: make build\n  artifacts:\n    paths:\n      - dist/\n\ntest:\n  stage: test\n  dependencies:\n    - build  # buildì˜ ì•„í‹°íŒ©íŠ¸ë§Œ ë‹¤ìš´ë¡œë“œ\n  script: make test\n\ndeploy:\n  stage: deploy\n  dependencies: []  # ì•„í‹°íŒ©íŠ¸ ë‹¤ìš´ë¡œë“œ ì•ˆ í•¨\n  script: make deploy\n```\n\n### needs vs dependencies\n\n| íŠ¹ì„± | needs | dependencies |\n|-----|-------|--------------|\n| **ì‹¤í–‰ ìˆœì„œ** | ì œì–´í•¨ (DAG) | ì œì–´ ì•ˆ í•¨ |\n| **ì•„í‹°íŒ©íŠ¸** | ê¸°ë³¸ í¬í•¨ | ì „ìš© ì œì–´ |\n| **Stage ë¬´ì‹œ** | ê°€ëŠ¥ | ë¶ˆê°€ëŠ¥ |\n\n> [!TIP]\n> `needs`ë¥¼ ì‚¬ìš©í•˜ë©´ `dependencies`ê°€ í•„ìš” ì—†ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. `needs: [job]`ì€ í•´ë‹¹ Jobì˜ ì•„í‹°íŒ©íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.\n\n---\n\n## extends: Job í…œí”Œë¦¿\n\n`extends`ë¡œ **Job ì„¤ì •ì„ ìƒì†**í•©ë‹ˆë‹¤.\n\n### ê¸°ë³¸ ì‚¬ìš©ë²•\n\n```yaml\n.test-template:\n  stage: test\n  image: node:20\n  before_script:\n    - npm ci\n  cache:\n    paths:\n      - node_modules/\n\nunit-test:\n  extends: .test-template\n  script:\n    - npm run test:unit\n\nintegration-test:\n  extends: .test-template\n  script:\n    - npm run test:integration\n  services:\n    - postgres:15\n```\n\n### ë‹¤ì¤‘ ìƒì†\n\n```yaml\n.base:\n  tags:\n    - docker\n\n.node:\n  image: node:20\n\n.cache:\n  cache:\n    paths:\n      - node_modules/\n\nbuild:\n  extends:\n    - .base\n    - .node\n    - .cache\n  script:\n    - npm run build\n```\n\n### ìƒì† ìˆœì„œ\n\n```mermaid\nflowchart TB\n    Base[.base] --> Node[.node]\n    Node --> Cache[.cache]\n    Cache --> Job[ì‹¤ì œ Job]\n    \n    subgraph Merge [ë³‘í•© ê²°ê³¼]\n        M[tags + image + cache + script]\n    end\n    \n    Job --> M\n```\n\në‚˜ì¤‘ì— ì •ì˜ëœ ê°’ì´ **ì´ì „ ê°’ì„ ë®ì–´ì”ë‹ˆë‹¤**.\n\n---\n\n## !reference: ì„¸ë°€í•œ ì¬ì‚¬ìš©\n\n`!reference`ëŠ” **íŠ¹ì • í‚¤ë§Œ ì„ íƒì ìœ¼ë¡œ ì¬ì‚¬ìš©**í•©ë‹ˆë‹¤.\n\n```yaml\n.setup:\n  before_script:\n    - echo \"Setting up...\"\n  after_script:\n    - echo \"Cleaning up...\"\n  script:\n    - echo \"Default script\"\n\n.test-vars:\n  variables:\n    TEST_ENV: \"test\"\n\nbuild:\n  # .setupì˜ before_scriptë§Œ ê°€ì ¸ì˜´\n  before_script:\n    - !reference [.setup, before_script]\n    - echo \"Additional setup\"\n  script:\n    - npm run build\n  variables:\n    # .test-varsì˜ variables ë³‘í•©\n    !reference [.test-vars, variables]\n```\n\n### extends vs !reference\n\n```yaml\n# extends: ì „ì²´ ë³‘í•©\njob1:\n  extends: .template  # ëª¨ë“  í‚¤ ìƒì†\n\n# !reference: ì„ íƒì  ì¬ì‚¬ìš©\njob2:\n  script:\n    - !reference [.template, script]  # scriptë§Œ ê°€ì ¸ì˜´\n```\n\n---\n\n## ì‹¤ì „ ì˜ˆì œ: ì™„ì „í•œ íŒŒì´í”„ë¼ì¸\n\n```yaml\n# ì „ì—­ ì„¤ì •\ndefault:\n  image: node:20-alpine\n  interruptible: true\n\nvariables:\n  npm_config_cache: \"$CI_PROJECT_DIR/.npm\"\n\nworkflow:\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n    - if: $CI_COMMIT_BRANCH == \"main\"\n    - if: $CI_COMMIT_TAG\n\nstages:\n  - prepare\n  - build\n  - test\n  - deploy\n\n# í…œí”Œë¦¿\n.node-cache:\n  cache:\n    key:\n      files:\n        - package-lock.json\n    paths:\n      - .npm/\n      - node_modules/\n\n.deploy-template:\n  image: bitnami/kubectl:latest\n  before_script:\n    - kubectl config use-context $KUBE_CONTEXT\n\n# Jobs\ninstall:\n  stage: prepare\n  extends: .node-cache\n  script:\n    - npm ci\n  artifacts:\n    paths:\n      - node_modules/\n    expire_in: 1 hour\n\nlint:\n  stage: build\n  needs: [install]\n  script:\n    - npm run lint\n  allow_failure: true\n\nbuild:\n  stage: build\n  needs: [install]\n  script:\n    - npm run build\n  artifacts:\n    paths:\n      - dist/\n\nunit-test:\n  stage: test\n  needs:\n    - job: build\n      artifacts: true\n  script:\n    - npm run test:unit\n  coverage: '/Coverage: (\\d+\\.?\\d*)%/'\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n    - if: $CI_COMMIT_BRANCH == \"main\"\n\ne2e-test:\n  stage: test\n  needs: [build]\n  image: mcr.microsoft.com/playwright:v1.40.0\n  script:\n    - npm run test:e2e\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n    - when: manual\n      allow_failure: true\n\ndeploy-staging:\n  stage: deploy\n  extends: .deploy-template\n  needs: [unit-test]\n  environment:\n    name: staging\n    url: https://staging.example.com\n  script:\n    - kubectl apply -f k8s/staging/\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n\ndeploy-production:\n  stage: deploy\n  extends: .deploy-template\n  needs: [unit-test, e2e-test]\n  environment:\n    name: production\n    url: https://example.com\n  script:\n    - kubectl apply -f k8s/production/\n  rules:\n    - if: $CI_COMMIT_TAG =~ /^v\\d+\\.\\d+\\.\\d+$/\n      when: manual\n```\n\n---\n\n## ì •ë¦¬\n\n| í‚¤ì›Œë“œ | ìš©ë„ |\n|-------|------|\n| `rules` | ì¡°ê±´ë¶€ Job ì‹¤í–‰ (if, changes, exists) |\n| `workflow:rules` | ì „ì—­ íŒŒì´í”„ë¼ì¸ ìƒì„± ì œì–´ |\n| `needs` | DAG ì˜ì¡´ì„±, Stage ë¬´ì‹œ |\n| `dependencies` | ì•„í‹°íŒ©íŠ¸ ë‹¤ìš´ë¡œë“œ ì œì–´ |\n| `extends` | Job í…œí”Œë¦¿ ìƒì† |\n| `!reference` | ì„ íƒì  í‚¤ ì¬ì‚¬ìš© |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**6í¸: ì™¸ë¶€ í†µí•©**ì—ì„œëŠ” ë‹¤ìŒì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- Pipeline Triggers (í† í° ê¸°ë°˜)\n- Webhooksë¡œ íŒŒì´í”„ë¼ì¸ íŠ¸ë¦¬ê±°\n- APIë¥¼ í†µí•œ íŒŒì´í”„ë¼ì¸ ì œì–´\n- ChatOps ì—°ë™\n- GitOps ì‹œë¦¬ì¦ˆì™€ì˜ ì—°ê²°\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [rules Reference](https://docs.gitlab.com/ee/ci/yaml/#rules)\n- [needs - DAG](https://docs.gitlab.com/ee/ci/yaml/#needs)\n- [!reference Tag](https://docs.gitlab.com/ee/ci/yaml/yaml_optimization.html#reference-tags)\n- [Job Keywords Reference](https://docs.gitlab.com/ee/ci/yaml/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitLab",
      "CI/CD",
      "Pipeline",
      "DAG"
    ],
    "readingTime": 7,
    "wordCount": 1345,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitlab-ci",
    "date": "2026-01-05"
  },
  {
    "id": "gitlab-ci-04-pipeline-architectures",
    "slug": "gitlab-ci-04-pipeline-architectures",
    "path": "backend/devops",
    "fullPath": "backend/devops/gitlab-ci-04-pipeline-architectures",
    "title": "GitLab CI/CD ì‹œë¦¬ì¦ˆ #4: Pipeline ì•„í‚¤í…ì²˜ - Parent-Childì™€ Multi-Project",
    "excerpt": "ë³µì¡í•œ íŒŒì´í”„ë¼ì¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ Parent-Child Pipeline, Multi-Project Pipeline, ë™ì  íŒŒì´í”„ë¼ì¸ ìƒì„±ì„ ë‹¤ë£¹ë‹ˆë‹¤.",
    "content": "# GitLab CI/CD ì‹œë¦¬ì¦ˆ #4: Pipeline ì•„í‚¤í…ì²˜ - Parent-Childì™€ Multi-Project\n\n## ì‹œë¦¬ì¦ˆ ê°œìš”\n\n| # | ì£¼ì œ | í•µì‹¬ ë‚´ìš© |\n|---|------|----------|\n| 1 | ê¸°ì´ˆ | .gitlab-ci.yml êµ¬ì¡°, Stages, Jobs, Pipeline íë¦„ |\n| 2 | Variables & Secrets | ë³€ìˆ˜ ìœ í˜•, ìš°ì„ ìˆœìœ„, ì™¸ë¶€ Vault ì—°ë™ |\n| 3 | Runners & Executors | Docker, Kubernetes, Docker-in-Docker |\n| **4** | **Pipeline ì•„í‚¤í…ì²˜** | Parent-Child, Multi-Project Pipeline |\n| 5 | ê³ ê¸‰ Job ì œì–´ | rules, needs, DAG, extends |\n| 6 | ì™¸ë¶€ í†µí•© | Triggers, Webhooks, API |\n\n---\n\n## ì™œ Pipeline ì•„í‚¤í…ì²˜ê°€ í•„ìš”í•œê°€?\n\në‹¨ì¼ `.gitlab-ci.yml`ì´ ìˆ˜ë°± ì¤„ë¡œ ì»¤ì§€ë©´ ê´€ë¦¬ê°€ ì–´ë ¤ì›Œì§‘ë‹ˆë‹¤.\n\n### ë¬¸ì œì \n\n```yaml\n# ê±°ëŒ€í•œ ë‹¨ì¼ íŒŒì¼ì˜ ë¬¸ì œì \nstages:\n  - build\n  - test\n  - deploy\n\n# 50ê°œ ì´ìƒì˜ Jobs...\nbuild-frontend:\n  # ...\nbuild-backend:\n  # ...\nbuild-mobile-ios:\n  # ...\nbuild-mobile-android:\n  # ...\n# ... ìˆ˜ë°± ì¤„\n```\n\n### í•´ê²°ì±…\n\n| íŒ¨í„´ | ìš©ë„ |\n|------|------|\n| **include** | ì„¤ì • íŒŒì¼ ë¶„ë¦¬ (ë™ì¼ í”„ë¡œì íŠ¸ ë‚´) |\n| **Parent-Child** | ë™ì  íŒŒì´í”„ë¼ì¸, ì¡°ê±´ë¶€ ì‹¤í–‰ |\n| **Multi-Project** | í”„ë¡œì íŠ¸ ê°„ íŠ¸ë¦¬ê±° |\n| **DAG** | Stage ë¬´ì‹œí•˜ê³  Job ê°„ ì˜ì¡´ì„± ì§ì ‘ ì •ì˜ |\n\n---\n\n## include: ì„¤ì • íŒŒì¼ ë¶„ë¦¬\n\nê°€ì¥ ê¸°ë³¸ì ì¸ ëª¨ë“ˆí™” ë°©ë²•ì…ë‹ˆë‹¤.\n\n### include ìœ í˜•\n\n```yaml\ninclude:\n  # 1. ë¡œì»¬ íŒŒì¼\n  - local: '/templates/docker.yml'\n  \n  # 2. ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì˜ íŒŒì¼\n  - project: 'my-group/ci-templates'\n    ref: main\n    file: '/templates/node.yml'\n  \n  # 3. ì›ê²© URL\n  - remote: 'https://example.com/ci/template.yml'\n  \n  # 4. GitLab ì œê³µ í…œí”Œë¦¿\n  - template: 'Auto-DevOps.gitlab-ci.yml'\n```\n\n### ì‹¤ì „ êµ¬ì¡°\n\n```\nproject/\nâ”œâ”€â”€ .gitlab-ci.yml              # ë©”ì¸ íŒŒì¼\nâ”œâ”€â”€ .gitlab/\nâ”‚   â”œâ”€â”€ ci/\nâ”‚   â”‚   â”œâ”€â”€ build.yml           # ë¹Œë“œ Jobs\nâ”‚   â”‚   â”œâ”€â”€ test.yml            # í…ŒìŠ¤íŠ¸ Jobs\nâ”‚   â”‚   â””â”€â”€ deploy.yml          # ë°°í¬ Jobs\nâ”‚   â””â”€â”€ templates/\nâ”‚       â””â”€â”€ docker.yml          # ê³µí†µ í…œí”Œë¦¿\n```\n\n```yaml\n# .gitlab-ci.yml\nstages:\n  - build\n  - test\n  - deploy\n\ninclude:\n  - local: '.gitlab/ci/build.yml'\n  - local: '.gitlab/ci/test.yml'\n  - local: '.gitlab/ci/deploy.yml'\n```\n\n```yaml\n# .gitlab/ci/build.yml\nbuild-app:\n  stage: build\n  script:\n    - npm run build\n```\n\n---\n\n## Parent-Child Pipelines\n\n**Parent Pipeline**ì´ **Child Pipeline**ì„ íŠ¸ë¦¬ê±°í•˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.\n\n### ê¸°ë³¸ êµ¬ì¡°\n\n```mermaid\nflowchart TB\n    subgraph Parent [Parent Pipeline]\n        P1[build]\n        P2[trigger-child]\n    end\n    \n    subgraph Child [Child Pipeline]\n        C1[test-unit]\n        C2[test-e2e]\n        C3[deploy]\n    end\n    \n    P1 --> P2\n    P2 -->|trigger| C1 & C2\n    C1 & C2 --> C3\n```\n\n### ì •ì  Child Pipeline\n\n```yaml\n# .gitlab-ci.yml (Parent)\nstages:\n  - build\n  - trigger\n\nbuild:\n  stage: build\n  script:\n    - npm run build\n  artifacts:\n    paths:\n      - dist/\n\ntrigger-child:\n  stage: trigger\n  trigger:\n    include: .gitlab/child-pipeline.yml\n    strategy: depend  # Child ì™„ë£Œê¹Œì§€ ëŒ€ê¸°\n```\n\n```yaml\n# .gitlab/child-pipeline.yml (Child)\nstages:\n  - test\n  - deploy\n\nunit-test:\n  stage: test\n  script:\n    - npm run test:unit\n\ndeploy:\n  stage: deploy\n  script:\n    - ./deploy.sh\n```\n\n### strategy ì˜µì…˜\n\n| ì˜µì…˜ | ë™ì‘ |\n|------|------|\n| `depend` | Child ì™„ë£Œê¹Œì§€ Parent Job ëŒ€ê¸° |\n| (ì—†ìŒ) | Parent Job ì¦‰ì‹œ ì™„ë£Œ, Child ë¹„ë™ê¸° ì‹¤í–‰ |\n\n---\n\n## ë™ì  Child Pipeline\n\n**ëŸ°íƒ€ì„ì— Child Pipeline YAMLì„ ìƒì„±**í•©ë‹ˆë‹¤.\n\n### ë™ì  ìƒì„± ì˜ˆì œ\n\n```yaml\n# .gitlab-ci.yml\nstages:\n  - generate\n  - trigger\n\ngenerate-pipeline:\n  stage: generate\n  script:\n    - |\n      # ë³€ê²½ëœ ë””ë ‰í† ë¦¬ì— ë”°ë¼ ë™ì ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ ìƒì„±\n      cat > child-pipeline.yml <<EOF\n      stages:\n        - test\n      \n      $(for dir in $(git diff --name-only HEAD~1 | cut -d/ -f1 | sort -u); do\n        echo \"${dir}-test:\"\n        echo \"  stage: test\"\n        echo \"  script:\"\n        echo \"    - echo 'Testing $dir'\"\n        echo \"\"\n      done)\n      EOF\n  artifacts:\n    paths:\n      - child-pipeline.yml\n\ntrigger-tests:\n  stage: trigger\n  trigger:\n    include:\n      - artifact: child-pipeline.yml\n        job: generate-pipeline\n    strategy: depend\n```\n\n### Monorepo íŒ¨í„´\n\n```yaml\n# .gitlab-ci.yml\nstages:\n  - detect\n  - build\n\ndetect-changes:\n  stage: detect\n  script:\n    - |\n      # ë³€ê²½ëœ ì„œë¹„ìŠ¤ ê°ì§€\n      for service in frontend backend api; do\n        if git diff --name-only HEAD~1 | grep -q \"^$service/\"; then\n          echo \"$service\" >> changed_services.txt\n        fi\n      done\n      \n      # ë™ì  íŒŒì´í”„ë¼ì¸ ìƒì„±\n      echo \"stages:\" > child.yml\n      echo \"  - build\" >> child.yml\n      echo \"\" >> child.yml\n      \n      while read service; do\n        cat >> child.yml <<EOF\n      build-${service}:\n        stage: build\n        script:\n          - cd ${service} && make build\n      EOF\n      done < changed_services.txt\n  artifacts:\n    paths:\n      - child.yml\n\ntrigger-builds:\n  stage: build\n  trigger:\n    include:\n      - artifact: child.yml\n        job: detect-changes\n  rules:\n    - exists:\n        - changed_services.txt\n```\n\n---\n\n## ë³€ê²½ëœ íŒŒì¼ ê¸°ë°˜ íŠ¸ë¦¬ê±°\n\n`rules:changes`ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • íŒŒì¼ ë³€ê²½ ì‹œì—ë§Œ Childë¥¼ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤.\n\n```yaml\n# .gitlab-ci.yml\nstages:\n  - triggers\n\ntrigger-frontend:\n  stage: triggers\n  trigger:\n    include: frontend/.gitlab-ci.yml\n  rules:\n    - changes:\n        - frontend/**/*\n\ntrigger-backend:\n  stage: triggers\n  trigger:\n    include: backend/.gitlab-ci.yml\n  rules:\n    - changes:\n        - backend/**/*\n\ntrigger-shared:\n  stage: triggers\n  trigger:\n    include: shared/.gitlab-ci.yml\n  rules:\n    - changes:\n        - shared/**/*\n```\n\n```mermaid\nflowchart TB\n    subgraph Parent [Parent Pipeline]\n        Check[ë³€ê²½ ê°ì§€]\n    end\n    \n    subgraph Children [Child Pipelines]\n        F[Frontend Pipeline]\n        B[Backend Pipeline]\n        S[Shared Pipeline]\n    end\n    \n    Check -->|frontend/* ë³€ê²½| F\n    Check -->|backend/* ë³€ê²½| B\n    Check -->|shared/* ë³€ê²½| S\n```\n\n---\n\n## Multi-Project Pipelines\n\n**ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì˜ íŒŒì´í”„ë¼ì¸ì„ íŠ¸ë¦¬ê±°**í•©ë‹ˆë‹¤.\n\n### ê¸°ë³¸ ì‚¬ìš©ë²•\n\n```yaml\n# Project Aì˜ .gitlab-ci.yml\nstages:\n  - build\n  - trigger\n\nbuild:\n  stage: build\n  script:\n    - npm run build\n\ntrigger-project-b:\n  stage: trigger\n  trigger:\n    project: my-group/project-b\n    branch: main\n    strategy: depend\n```\n\n### ë³€ìˆ˜ ì „ë‹¬\n\n```yaml\ntrigger-deploy:\n  stage: trigger\n  trigger:\n    project: my-group/deployment\n    branch: main\n  variables:\n    DEPLOY_ENV: production\n    IMAGE_TAG: $CI_COMMIT_SHA\n```\n\n### ì–‘ë°©í–¥ íŠ¸ë¦¬ê±° (Upstream â†’ Downstream)\n\n```mermaid\nflowchart LR\n    subgraph Upstream [Project A - ë¼ì´ë¸ŒëŸ¬ë¦¬]\n        U1[build]\n        U2[publish]\n        U3[trigger]\n    end\n    \n    subgraph Downstream [Project B - ì•±]\n        D1[install]\n        D2[build]\n        D3[deploy]\n    end\n    \n    U1 --> U2 --> U3\n    U3 -->|trigger| D1\n    D1 --> D2 --> D3\n```\n\n---\n\n## ì‹¤ì „ ì˜ˆì œ: ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ë°°í¬\n\n```yaml\n# infrastructure/.gitlab-ci.yml (Parent)\nstages:\n  - build\n  - trigger-services\n  - deploy-infra\n\nbuild-base-images:\n  stage: build\n  script:\n    - docker build -t base-node:$CI_COMMIT_SHA ./base-images/node\n    - docker push base-node:$CI_COMMIT_SHA\n\ntrigger-user-service:\n  stage: trigger-services\n  trigger:\n    project: my-org/user-service\n    branch: main\n  variables:\n    BASE_IMAGE_TAG: $CI_COMMIT_SHA\n\ntrigger-order-service:\n  stage: trigger-services\n  trigger:\n    project: my-org/order-service\n    branch: main\n  variables:\n    BASE_IMAGE_TAG: $CI_COMMIT_SHA\n\ntrigger-payment-service:\n  stage: trigger-services\n  trigger:\n    project: my-org/payment-service\n    branch: main\n  variables:\n    BASE_IMAGE_TAG: $CI_COMMIT_SHA\n\ndeploy-kubernetes:\n  stage: deploy-infra\n  trigger:\n    project: my-org/k8s-manifests\n    branch: main\n  needs:\n    - trigger-user-service\n    - trigger-order-service\n    - trigger-payment-service\n  variables:\n    SERVICES_VERSION: $CI_COMMIT_SHA\n```\n\n---\n\n## Pipeline ê°„ ì•„í‹°íŒ©íŠ¸ ê³µìœ \n\n### Parent â†’ Child\n\n```yaml\n# Parent\nparent-job:\n  script:\n    - echo \"data\" > file.txt\n  artifacts:\n    paths:\n      - file.txt\n\ntrigger-child:\n  trigger:\n    include: child.yml\n  needs:\n    - parent-job\n```\n\n```yaml\n# Child (child.yml)\nchild-job:\n  script:\n    - cat file.txt  # Parentì˜ ì•„í‹°íŒ©íŠ¸ ì‚¬ìš© ê°€ëŠ¥\n```\n\n### Multi-Project ê°„ ì•„í‹°íŒ©íŠ¸\n\n```yaml\n# Project B\ndownstream-job:\n  script:\n    - |\n      # Project Aì˜ ì•„í‹°íŒ©íŠ¸ ë‹¤ìš´ë¡œë“œ\n      curl --header \"PRIVATE-TOKEN: $API_TOKEN\" \\\n        \"$CI_API_V4_URL/projects/123/jobs/$UPSTREAM_JOB_ID/artifacts\" \\\n        --output artifacts.zip\n      unzip artifacts.zip\n```\n\n---\n\n## íŒŒì´í”„ë¼ì¸ ì‹œê°í™”\n\n### Pipeline Graph\n\n```mermaid\nflowchart TB\n    subgraph MainPipeline [Main Pipeline]\n        direction LR\n        Build[Build Stage]\n        Test[Test Stage]\n        Deploy[Deploy Stage]\n    end\n    \n    subgraph ChildPipelines [Child Pipelines]\n        direction TB\n        Frontend[Frontend Pipeline]\n        Backend[Backend Pipeline]\n    end\n    \n    Build --> Frontend & Backend\n    Frontend & Backend --> Deploy\n```\n\nGitLab UIì—ì„œëŠ” ì´ëŸ¬í•œ ê´€ê³„ê°€ **ì‹œê°ì ìœ¼ë¡œ í‘œì‹œ**ë©ë‹ˆë‹¤.\n\n---\n\n## ì •ë¦¬\n\n| íŒ¨í„´ | ìš©ë„ | í‚¤ì›Œë“œ |\n|-----|------|--------|\n| **include** | ì„¤ì • íŒŒì¼ ë¶„ë¦¬ | `include: local/project/remote` |\n| **Parent-Child** | ë™ì  íŒŒì´í”„ë¼ì¸ | `trigger: include:` |\n| **Multi-Project** | í”„ë¡œì íŠ¸ ê°„ íŠ¸ë¦¬ê±° | `trigger: project:` |\n| **ë™ì  ìƒì„±** | ëŸ°íƒ€ì„ íŒŒì´í”„ë¼ì¸ | `artifact: + trigger` |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**5í¸: ê³ ê¸‰ Job ì œì–´**ì—ì„œëŠ” ë‹¤ìŒì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- `rules` vs `only/except` ë§ˆì´ê·¸ë ˆì´ì…˜\n- `needs`ì™€ DAG ì‹¤í–‰\n- `dependencies`ì™€ ì•„í‹°íŒ©íŠ¸ ì œì–´\n- `workflow:rules` ì „ì—­ ì œì–´\n- Job í…œí”Œë¦¿ê³¼ `extends`, `!reference`\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Parent-Child Pipelines](https://docs.gitlab.com/ee/ci/pipelines/downstream_pipelines.html)\n- [Multi-Project Pipelines](https://docs.gitlab.com/ee/ci/pipelines/multi_project_pipelines.html)\n- [Pipeline Architectures](https://docs.gitlab.com/ee/ci/pipelines/pipeline_architectures.html)\n- [Dynamic Child Pipelines](https://docs.gitlab.com/ee/ci/pipelines/downstream_pipelines.html#dynamic-child-pipelines)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitLab",
      "CI/CD",
      "Pipeline"
    ],
    "readingTime": 6,
    "wordCount": 1148,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitlab-ci",
    "date": "2026-01-05"
  },
  {
    "id": "gitlab-ci-03-runners-executors",
    "slug": "gitlab-ci-03-runners-executors",
    "path": "backend/devops",
    "fullPath": "backend/devops/gitlab-ci-03-runners-executors",
    "title": "GitLab CI/CD ì‹œë¦¬ì¦ˆ #3: Runnersì™€ Executors - Docker-in-Docker ì‹¬í™”",
    "excerpt": "GitLab Runnerì˜ ì•„í‚¤í…ì²˜ì™€ Executor ìœ í˜•, Docker-in-Docker(DinD) ì„¤ì •, Kubernetes Executor êµ¬ì„±ì„ ë‹¤ë£¹ë‹ˆë‹¤.",
    "content": "# GitLab CI/CD ì‹œë¦¬ì¦ˆ #3: Runnersì™€ Executors - Docker-in-Docker ì‹¬í™”\n\n## ì‹œë¦¬ì¦ˆ ê°œìš”\n\n| # | ì£¼ì œ | í•µì‹¬ ë‚´ìš© |\n|---|------|----------|\n| 1 | ê¸°ì´ˆ | .gitlab-ci.yml êµ¬ì¡°, Stages, Jobs, Pipeline íë¦„ |\n| 2 | Variables & Secrets | ë³€ìˆ˜ ìœ í˜•, ìš°ì„ ìˆœìœ„, ì™¸ë¶€ Vault ì—°ë™ |\n| **3** | **Runners & Executors** | Docker, Kubernetes, Docker-in-Docker |\n| 4 | Pipeline ì•„í‚¤í…ì²˜ | Parent-Child, Multi-Project Pipeline |\n| 5 | ê³ ê¸‰ Job ì œì–´ | rules, needs, DAG, extends |\n| 6 | ì™¸ë¶€ í†µí•© | Triggers, Webhooks, API |\n\n---\n\n## GitLab Runnerë€?\n\n**GitLab Runner**ëŠ” CI/CD Jobsë¥¼ ì‹¤ì œë¡œ ì‹¤í–‰í•˜ëŠ” ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. GitLab ì„œë²„ì™€ ë¶„ë¦¬ë˜ì–´ ë™ì‘í•˜ë©°, ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph GitLab\n        Server[GitLab Server]\n        Queue[Job Queue]\n    end\n    \n    subgraph Runners\n        R1[Runner 1 - Docker]\n        R2[Runner 2 - Kubernetes]\n        R3[Runner 3 - Shell]\n    end\n    \n    Server --> Queue\n    Queue <--> R1 & R2 & R3\n```\n\n### Runner ìœ í˜•\n\n| ìœ í˜• | ë²”ìœ„ | ì„¤ì • ìœ„ì¹˜ |\n|-----|------|----------|\n| **Shared** | ì¸ìŠ¤í„´ìŠ¤ ì „ì²´ | Admin Area |\n| **Group** | íŠ¹ì • ê·¸ë£¹ê³¼ í•˜ìœ„ í”„ë¡œì íŠ¸ | Group Settings |\n| **Project** | íŠ¹ì • í”„ë¡œì íŠ¸ë§Œ | Project Settings |\n\n### Runner ë“±ë¡\n\n```bash\n# Runner ì„¤ì¹˜ (Linux)\ncurl -L \"https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh\" | sudo bash\nsudo apt-get install gitlab-runner\n\n# Runner ë“±ë¡\nsudo gitlab-runner register \\\n  --url \"https://gitlab.com/\" \\\n  --registration-token \"PROJECT_REGISTRATION_TOKEN\" \\\n  --description \"My Docker Runner\" \\\n  --executor \"docker\" \\\n  --docker-image \"alpine:latest\"\n```\n\n---\n\n## Executor ìœ í˜•\n\nExecutorëŠ” Jobsê°€ **ì–´ë–¤ í™˜ê²½ì—ì„œ ì‹¤í–‰ë˜ëŠ”ì§€** ê²°ì •í•©ë‹ˆë‹¤.\n\n### ì£¼ìš” Executors\n\n| Executor | ê²©ë¦¬ ìˆ˜ì¤€ | ìš©ë„ |\n|----------|----------|------|\n| **Shell** | ì—†ìŒ | ê°„ë‹¨í•œ ìŠ¤í¬ë¦½íŠ¸, í˜¸ìŠ¤íŠ¸ ì§ì ‘ ì ‘ê·¼ |\n| **Docker** | ì»¨í…Œì´ë„ˆ | ì¼ë°˜ì ì¸ CI/CD |\n| **Docker Machine** | VM + ì»¨í…Œì´ë„ˆ | Auto-scaling |\n| **Kubernetes** | Pod | í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ |\n| **VirtualBox** | VM | ì™„ì „ ê²©ë¦¬ í•„ìš” ì‹œ |\n\n### Shell Executor\n\ní˜¸ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ëª…ë ¹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n\n```toml\n# /etc/gitlab-runner/config.toml\n[[runners]]\n  name = \"shell-runner\"\n  executor = \"shell\"\n  shell = \"bash\"\n```\n\n```yaml\njob:\n  script:\n    - whoami  # gitlab-runner ì‚¬ìš©ì\n    - ls /home\n```\n\n> [!WARNING]\n> Shell ExecutorëŠ” **ê²©ë¦¬ê°€ ì—†ì–´** ë³´ì•ˆì— ì·¨ì•½í•©ë‹ˆë‹¤. ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì½”ë“œë§Œ ì‹¤í–‰í•˜ì„¸ìš”.\n\n### Docker Executor\n\nê°€ì¥ ì¼ë°˜ì ì¸ Executorì…ë‹ˆë‹¤. ê° Jobì€ ë…ë¦½ëœ ì»¨í…Œì´ë„ˆì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.\n\n```toml\n[[runners]]\n  name = \"docker-runner\"\n  executor = \"docker\"\n  [runners.docker]\n    image = \"alpine:latest\"\n    privileged = false\n    volumes = [\"/cache\"]\n    shm_size = 0\n```\n\n```yaml\njob:\n  image: node:20-alpine\n  script:\n    - npm ci\n    - npm run build\n```\n\n---\n\n## Docker-in-Docker (DinD)\n\nCI/CD íŒŒì´í”„ë¼ì¸ì—ì„œ **Docker ì´ë¯¸ì§€ë¥¼ ë¹Œë“œ**í•´ì•¼ í•  ë•Œ DinDë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\n### ì™œ DinDê°€ í•„ìš”í•œê°€?\n\n```mermaid\nflowchart TB\n    subgraph Host [í˜¸ìŠ¤íŠ¸]\n        Docker[Docker Daemon]\n        subgraph Runner [GitLab Runner Container]\n            Job[CI Job]\n            Job -->|docker build| Docker\n        end\n    end\n```\n\nì¼ë°˜ Docker Executorì—ì„œëŠ” `docker` ëª…ë ¹ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì»¨í…Œì´ë„ˆ ì•ˆì— Docker ë°ëª¬ì´ ì—†ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n\n### DinD ì„¤ì •\n\n```yaml\ndefault:\n  image: docker:24.0.5\n  services:\n    - name: docker:24.0.5-dind\n      alias: docker\n\nvariables:\n  DOCKER_HOST: tcp://docker:2376\n  DOCKER_TLS_CERTDIR: \"/certs\"\n  DOCKER_TLS_VERIFY: 1\n  DOCKER_CERT_PATH: \"$DOCKER_TLS_CERTDIR/client\"\n\nbuild:\n  stage: build\n  script:\n    - docker info\n    - docker build -t myapp:$CI_COMMIT_SHA .\n    - docker push myapp:$CI_COMMIT_SHA\n```\n\n### DinD ì•„í‚¤í…ì²˜\n\n```mermaid\nflowchart TB\n    subgraph Pod/Container [Runner ì‹¤í–‰ í™˜ê²½]\n        subgraph JobContainer [Job Container]\n            CLI[docker CLI]\n        end\n        \n        subgraph DinD [DinD Service Container]\n            Daemon[Docker Daemon]\n            Images[(Images)]\n        end\n        \n        CLI -->|TCP :2376| Daemon\n        Daemon --> Images\n    end\n```\n\n### TLS í™œì„±í™” vs ë¹„í™œì„±í™”\n\n#### TLS í™œì„±í™” (ê¸°ë³¸, ê¶Œì¥)\n\n```yaml\nvariables:\n  DOCKER_HOST: tcp://docker:2376\n  DOCKER_TLS_CERTDIR: \"/certs\"\n  DOCKER_TLS_VERIFY: 1\n  DOCKER_CERT_PATH: \"$DOCKER_TLS_CERTDIR/client\"\n```\n\n#### TLS ë¹„í™œì„±í™” (í…ŒìŠ¤íŠ¸ìš©)\n\n```yaml\nvariables:\n  DOCKER_HOST: tcp://docker:2375\n  DOCKER_TLS_CERTDIR: \"\"\n```\n\n> [!CAUTION]\n> TLS ë¹„í™œì„±í™”ëŠ” **ì•”í˜¸í™”ë˜ì§€ ì•Šì€ í†µì‹ **ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í”„ë¡œë•ì…˜ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n\n### Runner ì„¤ì • (config.toml)\n\n```toml\n[[runners]]\n  name = \"docker-runner\"\n  executor = \"docker\"\n  [runners.docker]\n    tls_verify = false\n    image = \"docker:24.0.5\"\n    privileged = true  # DinD í•„ìˆ˜\n    disable_entrypoint_overwrite = false\n    oom_kill_disable = false\n    disable_cache = false\n    volumes = [\"/certs/client\", \"/cache\"]\n    shm_size = 0\n```\n\n---\n\n## Docker Socket Binding (ëŒ€ì•ˆ)\n\nDinD ëŒ€ì‹  **í˜¸ìŠ¤íŠ¸ Docker ì†Œì¼“ì„ ë§ˆìš´íŠ¸**í•˜ëŠ” ë°©ì‹ë„ ìˆìŠµë‹ˆë‹¤.\n\n```toml\n[[runners]]\n  [runners.docker]\n    volumes = [\"/var/run/docker.sock:/var/run/docker.sock\"]\n```\n\n```yaml\nbuild:\n  image: docker:24.0.5\n  script:\n    - docker build -t myapp .\n```\n\n### DinD vs Socket Binding\n\n| íŠ¹ì„± | DinD | Socket Binding |\n|-----|------|----------------|\n| **ê²©ë¦¬** | ì™„ì „ ê²©ë¦¬ | í˜¸ìŠ¤íŠ¸ì™€ ê³µìœ  |\n| **ë³´ì•ˆ** | ì•ˆì „ | í˜¸ìŠ¤íŠ¸ ì ‘ê·¼ ê°€ëŠ¥ |\n| **ì„±ëŠ¥** | ì•½ê°„ ëŠë¦¼ | ë¹ ë¦„ |\n| **ìºì‹œ** | Jobë§ˆë‹¤ ì´ˆê¸°í™” | í˜¸ìŠ¤íŠ¸ ìºì‹œ ê³µìœ  |\n| **ë¹Œë“œ ë ˆì´ì–´** | ë§¤ë²ˆ ë‹¤ìš´ë¡œë“œ | ìºì‹œ í™œìš© |\n\n> [!IMPORTANT]\n> **Socket Binding**ì€ í˜¸ìŠ¤íŠ¸ Dockerì— ì§ì ‘ ì ‘ê·¼í•˜ë¯€ë¡œ **ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì½”ë“œë§Œ** ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤. DinDê°€ ë” ì•ˆì „í•©ë‹ˆë‹¤.\n\n---\n\n## Kubernetes Executor\n\nKubernetes í´ëŸ¬ìŠ¤í„°ì—ì„œ Jobsë¥¼ Podë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.\n\n### ì„¤ì •\n\n```toml\n[[runners]]\n  name = \"k8s-runner\"\n  executor = \"kubernetes\"\n  [runners.kubernetes]\n    namespace = \"gitlab-runner\"\n    image = \"alpine:latest\"\n    privileged = false\n    \n    cpu_request = \"100m\"\n    cpu_limit = \"1\"\n    memory_request = \"128Mi\"\n    memory_limit = \"1Gi\"\n    \n    service_cpu_request = \"100m\"\n    service_memory_request = \"128Mi\"\n    \n    poll_interval = 5\n    poll_timeout = 3600\n```\n\n### Kubernetes DinD\n\n```toml\n[[runners]]\n  [runners.kubernetes]\n    privileged = true\n    \n    [[runners.kubernetes.services]]\n      name = \"docker:24.0.5-dind\"\n      alias = \"docker\"\n      command = [\"--storage-driver=overlay2\"]\n```\n\n```yaml\nbuild:\n  image: docker:24.0.5\n  services:\n    - docker:24.0.5-dind\n  variables:\n    DOCKER_HOST: tcp://docker:2376\n    DOCKER_TLS_CERTDIR: \"/certs\"\n  script:\n    - docker build -t myapp .\n```\n\n### Pod ìƒì„± íë¦„\n\n```mermaid\nsequenceDiagram\n    participant GitLab as GitLab Server\n    participant Runner as GitLab Runner\n    participant K8s as Kubernetes API\n    participant Pod as Job Pod\n    \n    GitLab->>Runner: Job í• ë‹¹\n    Runner->>K8s: Pod ìƒì„± ìš”ì²­\n    K8s->>Pod: Pod ìŠ¤ì¼€ì¤„ë§\n    Pod->>Pod: Job ì‹¤í–‰\n    Pod->>Runner: ê²°ê³¼ ì „ì†¡\n    Runner->>GitLab: Job ì™„ë£Œ ë³´ê³ \n    Runner->>K8s: Pod ì‚­ì œ\n```\n\n---\n\n## Runner íƒœê·¸ì™€ Job ë§¤ì¹­\n\nRunnerì— íƒœê·¸ë¥¼ ì§€ì •í•˜ì—¬ íŠ¹ì • Jobsë§Œ ì‹¤í–‰í•˜ë„ë¡ ì œí•œí•©ë‹ˆë‹¤.\n\n### Runner íƒœê·¸\n\n```toml\n[[runners]]\n  name = \"gpu-runner\"\n  tags = [\"gpu\", \"cuda\", \"ml\"]\n```\n\n### Job íƒœê·¸\n\n```yaml\ntrain-model:\n  tags:\n    - gpu\n    - ml\n  script:\n    - python train.py\n\ndeploy:\n  tags:\n    - docker\n  script:\n    - ./deploy.sh\n```\n\n### íƒœê·¸ ì—†ëŠ” Job ì²˜ë¦¬\n\n```toml\n[[runners]]\n  run_untagged = true  # íƒœê·¸ ì—†ëŠ” Jobë„ ì‹¤í–‰\n```\n\n---\n\n## Runner ì„±ëŠ¥ ìµœì í™”\n\n### ë™ì‹œ ì‹¤í–‰\n\n```toml\nconcurrent = 10  # ì „ì²´ Runnerê°€ ë™ì‹œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ Job ìˆ˜\n\n[[runners]]\n  limit = 5  # ì´ Runnerì˜ ìµœëŒ€ ë™ì‹œ Job ìˆ˜\n```\n\n### ìºì‹œ ì„¤ì •\n\n```yaml\ndefault:\n  cache:\n    key:\n      files:\n        - package-lock.json\n    paths:\n      - node_modules/\n    policy: pull-push\n\nbuild:\n  cache:\n    policy: pull  # ì½ê¸°ë§Œ\n```\n\n### ë¶„ì‚° ìºì‹œ (S3)\n\n```toml\n[[runners]]\n  [runners.cache]\n    Type = \"s3\"\n    Shared = true\n    [runners.cache.s3]\n      ServerAddress = \"s3.amazonaws.com\"\n      BucketName = \"gitlab-runner-cache\"\n      BucketLocation = \"ap-northeast-2\"\n```\n\n---\n\n## ì‹¤ì „ ì˜ˆì œ: ë©€í‹° ì•„í‚¤í…ì²˜ ë¹Œë“œ\n\n```yaml\nstages:\n  - build\n  - manifest\n\nvariables:\n  DOCKER_HOST: tcp://docker:2376\n  DOCKER_TLS_CERTDIR: \"/certs\"\n\n.docker-build:\n  image: docker:24.0.5\n  services:\n    - docker:24.0.5-dind\n  before_script:\n    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\n\nbuild-amd64:\n  extends: .docker-build\n  tags:\n    - amd64\n  script:\n    - docker build --platform linux/amd64 -t $CI_REGISTRY_IMAGE:amd64 .\n    - docker push $CI_REGISTRY_IMAGE:amd64\n\nbuild-arm64:\n  extends: .docker-build\n  tags:\n    - arm64\n  script:\n    - docker build --platform linux/arm64 -t $CI_REGISTRY_IMAGE:arm64 .\n    - docker push $CI_REGISTRY_IMAGE:arm64\n\ncreate-manifest:\n  extends: .docker-build\n  stage: manifest\n  script:\n    - docker manifest create $CI_REGISTRY_IMAGE:latest\n        $CI_REGISTRY_IMAGE:amd64\n        $CI_REGISTRY_IMAGE:arm64\n    - docker manifest push $CI_REGISTRY_IMAGE:latest\n```\n\n---\n\n## ì •ë¦¬\n\n| ê°œë… | ì„¤ëª… |\n|-----|------|\n| **Runner** | Jobsë¥¼ ì‹¤í–‰í•˜ëŠ” ì—ì´ì „íŠ¸ |\n| **Executor** | ì‹¤í–‰ í™˜ê²½ ê²°ì • (Shell, Docker, K8s) |\n| **DinD** | ì»¨í…Œì´ë„ˆ ì•ˆì—ì„œ Docker ë¹Œë“œ |\n| **TLS** | DinD í†µì‹  ì•”í˜¸í™” (ê¶Œì¥) |\n| **Socket Binding** | í˜¸ìŠ¤íŠ¸ Docker ê³µìœ  (ë³´ì•ˆ ì£¼ì˜) |\n| **Tags** | Runnerì™€ Job ë§¤ì¹­ |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**4í¸: Pipeline ì•„í‚¤í…ì²˜**ì—ì„œëŠ” ë‹¤ìŒì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- ê¸°ë³¸ Pipeline vs DAG Pipeline\n- **Parent-Child Pipelines** (ë™ì  íŒŒì´í”„ë¼ì¸)\n- **Multi-Project Pipelines** (í¬ë¡œìŠ¤ í”„ë¡œì íŠ¸)\n- `trigger` í‚¤ì›Œë“œ ì‹¬í™”\n- ë™ì  Child Pipeline ìƒì„±\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [GitLab Runner Documentation](https://docs.gitlab.com/runner/)\n- [Docker Executor](https://docs.gitlab.com/runner/executors/docker.html)\n- [Using Docker to Build Docker Images](https://docs.gitlab.com/ee/ci/docker/using_docker_build.html)\n- [Kubernetes Executor](https://docs.gitlab.com/runner/executors/kubernetes.html)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitLab",
      "CI/CD",
      "Docker",
      "Kubernetes",
      "Runner"
    ],
    "readingTime": 7,
    "wordCount": 1214,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitlab-ci",
    "date": "2026-01-05"
  },
  {
    "id": "gitlab-ci-01-fundamentals",
    "slug": "gitlab-ci-01-fundamentals",
    "path": "backend/devops",
    "fullPath": "backend/devops/gitlab-ci-01-fundamentals",
    "title": "GitLab CI/CD ì‹œë¦¬ì¦ˆ #1: ê¸°ì´ˆ - .gitlab-ci.ymlì˜ êµ¬ì¡°ì™€ Pipeline ì´í•´",
    "excerpt": "GitLab CI/CDì˜ í•µì‹¬ì¸ .gitlab-ci.yml íŒŒì¼ êµ¬ì¡°, Stagesì™€ Jobsì˜ ê´€ê³„, Pipeline ì‹¤í–‰ íë¦„ì„ ì²´ê³„ì ìœ¼ë¡œ ì´í•´í•©ë‹ˆë‹¤.",
    "content": "# GitLab CI/CD ì‹œë¦¬ì¦ˆ #1: ê¸°ì´ˆ - .gitlab-ci.ymlì˜ êµ¬ì¡°ì™€ Pipeline ì´í•´\n\n## ì‹œë¦¬ì¦ˆ ê°œìš”\n\n| # | ì£¼ì œ | í•µì‹¬ ë‚´ìš© |\n|---|------|----------|\n| **1** | **ê¸°ì´ˆ** | .gitlab-ci.yml êµ¬ì¡°, Stages, Jobs, Pipeline íë¦„ |\n| 2 | Variables & Secrets | ë³€ìˆ˜ ìœ í˜•, ìš°ì„ ìˆœìœ„, ì™¸ë¶€ Vault ì—°ë™ |\n| 3 | Runners & Executors | Docker, Kubernetes, Docker-in-Docker |\n| 4 | Pipeline ì•„í‚¤í…ì²˜ | Parent-Child, Multi-Project Pipeline |\n| 5 | ê³ ê¸‰ Job ì œì–´ | rules, needs, DAG, extends |\n| 6 | ì™¸ë¶€ í†µí•© | Triggers, Webhooks, API |\n\n---\n\n## GitLab CI/CDë€?\n\nGitLab CI/CDëŠ” GitLabì— ë‚´ì¥ëœ **ì§€ì†ì  í†µí•©(CI)** ë° **ì§€ì†ì  ë°°í¬(CD)** ë„êµ¬ì…ë‹ˆë‹¤. ì½”ë“œê°€ ì €ì¥ì†Œì— í‘¸ì‹œë  ë•Œë§ˆë‹¤ ìë™ìœ¼ë¡œ ë¹Œë“œ, í…ŒìŠ¤íŠ¸, ë°°í¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph Developer\n        Code[ì½”ë“œ ì‘ì„±]\n    end\n    \n    subgraph GitLab\n        Push[git push]\n        CI[CI/CD Pipeline]\n        Runner[GitLab Runner]\n    end\n    \n    subgraph Artifacts\n        Build[ë¹Œë“œ ê²°ê³¼ë¬¼]\n        Report[í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸]\n    end\n    \n    Code --> Push --> CI\n    CI --> Runner\n    Runner --> Build\n    Runner --> Report\n```\n\n### í•µì‹¬ êµ¬ì„± ìš”ì†Œ\n\n| êµ¬ì„± ìš”ì†Œ | ì—­í•  |\n|----------|------|\n| **`.gitlab-ci.yml`** | íŒŒì´í”„ë¼ì¸ ì •ì˜ íŒŒì¼ (í”„ë¡œì íŠ¸ ë£¨íŠ¸) |\n| **Pipeline** | Jobsì˜ ì§‘í•©, ì½”ë“œ ë³€ê²½ ì‹œ íŠ¸ë¦¬ê±° |\n| **Stage** | Jobsë¥¼ ê·¸ë£¹í™”í•˜ëŠ” ë‹¨ê³„ |\n| **Job** | ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë‹¨ìœ„ |\n| **Runner** | Jobsë¥¼ ì‹¤í–‰í•˜ëŠ” ì—ì´ì „íŠ¸ |\n\n---\n\n## .gitlab-ci.yml ê¸°ë³¸ êµ¬ì¡°\n\ní”„ë¡œì íŠ¸ ë£¨íŠ¸ì— `.gitlab-ci.yml` íŒŒì¼ì„ ìƒì„±í•˜ë©´ GitLabì´ ìë™ìœ¼ë¡œ ì¸ì‹í•©ë‹ˆë‹¤.\n\n### ìµœì†Œ ì˜ˆì œ\n\n```yaml\n# ê°€ì¥ ê°„ë‹¨í•œ .gitlab-ci.yml\nbuild-job:\n  script:\n    - echo \"Hello, GitLab CI!\"\n```\n\nì´ í•œ ì¤„ë§Œìœ¼ë¡œë„ íŒŒì´í”„ë¼ì¸ì´ ìƒì„±ë©ë‹ˆë‹¤. `script`ëŠ” í•„ìˆ˜ í‚¤ì›Œë“œì…ë‹ˆë‹¤.\n\n### ì™„ì „í•œ ê¸°ë³¸ êµ¬ì¡°\n\n```yaml\n# 1. ì „ì—­ ê¸°ë³¸ê°’ ì„¤ì •\ndefault:\n  image: node:20-alpine\n  before_script:\n    - npm ci\n\n# 2. Stages ì •ì˜ (ì‹¤í–‰ ìˆœì„œ)\nstages:\n  - build\n  - test\n  - deploy\n\n# 3. Jobs ì •ì˜\nbuild-job:\n  stage: build\n  script:\n    - npm run build\n  artifacts:\n    paths:\n      - dist/\n\ntest-job:\n  stage: test\n  script:\n    - npm run test\n\ndeploy-job:\n  stage: deploy\n  script:\n    - echo \"Deploying to production...\"\n  when: manual  # ìˆ˜ë™ ìŠ¹ì¸ í•„ìš”\n```\n\n---\n\n## Stages: ì‹¤í–‰ ìˆœì„œ ì •ì˜\n\n**Stages**ëŠ” Jobsë¥¼ ê·¸ë£¹í™”í•˜ê³  **ì‹¤í–‰ ìˆœì„œ**ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n\n```yaml\nstages:\n  - build      # 1ë‹¨ê³„: ëª¨ë“  build ìŠ¤í…Œì´ì§€ Jobs ë³‘ë ¬ ì‹¤í–‰\n  - test       # 2ë‹¨ê³„: build ì™„ë£Œ í›„ test Jobs ë³‘ë ¬ ì‹¤í–‰\n  - deploy     # 3ë‹¨ê³„: test ì™„ë£Œ í›„ deploy Jobs ì‹¤í–‰\n```\n\n### Stage ì‹¤í–‰ íë¦„\n\n```mermaid\nflowchart LR\n    subgraph Build [Stage: build]\n        B1[build-frontend]\n        B2[build-backend]\n    end\n    \n    subgraph Test [Stage: test]\n        T1[unit-test]\n        T2[integration-test]\n        T3[lint]\n    end\n    \n    subgraph Deploy [Stage: deploy]\n        D1[deploy-staging]\n        D2[deploy-prod]\n    end\n    \n    B1 --> T1 & T2 & T3\n    B2 --> T1 & T2 & T3\n    T1 & T2 & T3 --> D1\n    T1 & T2 & T3 --> D2\n```\n\n> [!IMPORTANT]\n> ê°™ì€ Stage ë‚´ì˜ JobsëŠ” **ë³‘ë ¬ë¡œ ì‹¤í–‰**ë©ë‹ˆë‹¤. ë‹¤ìŒ StageëŠ” ì´ì „ Stageì˜ ëª¨ë“  Jobsê°€ **ì„±ê³µí•´ì•¼** ì‹œì‘ë©ë‹ˆë‹¤.\n\n### ê¸°ë³¸ Stages\n\n`stages`ë¥¼ ëª…ì‹œí•˜ì§€ ì•Šìœ¼ë©´ ë‹¤ìŒ ê¸°ë³¸ê°’ì´ ì ìš©ë©ë‹ˆë‹¤:\n\n```yaml\nstages:\n  - .pre      # í•­ìƒ ì²« ë²ˆì§¸\n  - build\n  - test\n  - deploy\n  - .post     # í•­ìƒ ë§ˆì§€ë§‰\n```\n\n---\n\n## Jobs: ì‹¤ì œ ì‘ì—… ë‹¨ìœ„\n\n**Job**ì€ íŒŒì´í”„ë¼ì¸ì˜ ê¸°ë³¸ ì‹¤í–‰ ë‹¨ìœ„ì…ë‹ˆë‹¤. ê° Jobì€ ë…ë¦½ì ì¸ í™˜ê²½ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.\n\n### Job ê¸°ë³¸ ë¬¸ë²•\n\n```yaml\njob-name:                    # Job ì´ë¦„ (ììœ ë¡­ê²Œ ì§€ì •)\n  stage: test                # ì†Œì† Stage\n  image: python:3.12         # ì‹¤í–‰ í™˜ê²½ (Docker ì´ë¯¸ì§€)\n  script:                    # ì‹¤í–‰í•  ëª…ë ¹ì–´ (í•„ìˆ˜)\n    - pip install -r requirements.txt\n    - pytest\n  tags:                      # Runner ì„ íƒ íƒœê·¸\n    - docker\n```\n\n### ì˜ˆì•½ëœ Job ì´ë¦„\n\nì¼ë¶€ ì´ë¦„ì€ íŠ¹ë³„í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ë¯€ë¡œ **ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤**:\n\n| ì˜ˆì•½ì–´ | ìš©ë„ |\n|--------|------|\n| `image` | Docker ì´ë¯¸ì§€ ì§€ì • |\n| `services` | ì„œë¹„ìŠ¤ ì»¨í…Œì´ë„ˆ |\n| `stages` | Stage ì •ì˜ |\n| `include` | ì™¸ë¶€ íŒŒì¼ í¬í•¨ |\n| `variables` | ë³€ìˆ˜ ì •ì˜ |\n| `default` | ê¸°ë³¸ê°’ ì„¤ì • |\n\n### ìˆ¨ê²¨ì§„ Job (í…œí”Œë¦¿)\n\nì (`.`)ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” Jobì€ ì‹¤í–‰ë˜ì§€ ì•Šê³  **í…œí”Œë¦¿**ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤:\n\n```yaml\n.test-template:      # ì‹¤í–‰ë˜ì§€ ì•ŠìŒ, í…œí”Œë¦¿\n  stage: test\n  before_script:\n    - setup-test-env.sh\n\nunit-test:\n  extends: .test-template  # í…œí”Œë¦¿ ìƒì†\n  script:\n    - pytest unit/\n\nintegration-test:\n  extends: .test-template\n  script:\n    - pytest integration/\n```\n\n---\n\n## Script: ëª…ë ¹ì–´ ì‹¤í–‰\n\n### script, before_script, after_script\n\n```yaml\njob:\n  before_script:     # script ì´ì „ì— ì‹¤í–‰\n    - echo \"Setting up...\"\n    - apt-get update\n    \n  script:            # ë©”ì¸ ëª…ë ¹ì–´ (í•„ìˆ˜)\n    - echo \"Running main task...\"\n    - npm run build\n    \n  after_script:      # script ì´í›„ì— í•­ìƒ ì‹¤í–‰ (ì‹¤íŒ¨í•´ë„)\n    - echo \"Cleaning up...\"\n    - rm -rf temp/\n```\n\n### ì‹¤í–‰ ìˆœì„œ\n\n```mermaid\nflowchart TB\n    subgraph Job [Job ì‹¤í–‰]\n        BS[before_script] --> S[script]\n        S --> AS[after_script]\n    end\n    \n    S -->|ì‹¤íŒ¨í•´ë„| AS\n```\n\n> [!TIP]\n> `after_script`ëŠ” Jobì˜ ì„±ê³µ/ì‹¤íŒ¨ì™€ ê´€ê³„ì—†ì´ **í•­ìƒ ì‹¤í–‰**ë©ë‹ˆë‹¤. ë¦¬ì†ŒìŠ¤ ì •ë¦¬ì— ìœ ìš©í•©ë‹ˆë‹¤.\n\n### ì—¬ëŸ¬ ì¤„ ìŠ¤í¬ë¦½íŠ¸\n\n```yaml\njob:\n  script:\n    # ë°©ë²• 1: ë°°ì—´ë¡œ ë‚˜ì—´\n    - echo \"First command\"\n    - echo \"Second command\"\n    \n    # ë°©ë²• 2: ë¦¬í„°ëŸ´ ë¸”ë¡\n    - |\n      echo \"Multi-line\"\n      echo \"commands\"\n      if [ \"$DEBUG\" = \"true\" ]; then\n        echo \"Debug mode\"\n      fi\n    \n    # ë°©ë²• 3: í´ë”© ë¸”ë¡ (í•œ ì¤„ë¡œ ì—°ê²°)\n    - >\n      curl -X POST\n      -H \"Content-Type: application/json\"\n      -d '{\"key\": \"value\"}'\n      https://api.example.com\n```\n\n---\n\n## Pipeline íŠ¸ë¦¬ê±° ë°©ì‹\n\níŒŒì´í”„ë¼ì¸ì€ ë‹¤ì–‘í•œ ì´ë²¤íŠ¸ë¡œ íŠ¸ë¦¬ê±°ë©ë‹ˆë‹¤.\n\n### ê¸°ë³¸ íŠ¸ë¦¬ê±°\n\n| íŠ¸ë¦¬ê±° | ì„¤ëª… |\n|--------|------|\n| `push` | ë¸Œëœì¹˜ì— ì»¤ë°‹ í‘¸ì‹œ |\n| `merge_request_event` | MR ìƒì„±/ì—…ë°ì´íŠ¸ |\n| `schedule` | ìŠ¤ì¼€ì¤„ (cron) |\n| `web` | GitLab UIì—ì„œ ìˆ˜ë™ ì‹¤í–‰ |\n| `api` | API í˜¸ì¶œ |\n| `trigger` | ë‹¤ë¥¸ íŒŒì´í”„ë¼ì¸ì—ì„œ íŠ¸ë¦¬ê±° |\n\n### íŠ¸ë¦¬ê±°ë³„ ì¡°ê±´ ë¶„ê¸°\n\n```yaml\nbuild:\n  stage: build\n  script:\n    - npm run build\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"push\"\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n\ndeploy:\n  stage: deploy\n  script:\n    - deploy.sh\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      when: manual  # main ë¸Œëœì¹˜ëŠ” ìˆ˜ë™ ë°°í¬\n```\n\n### Predefined Variables\n\nGitLabì€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œ [ë‹¤ì–‘í•œ ë³€ìˆ˜](https://docs.gitlab.com/ee/ci/variables/predefined_variables.html)ë¥¼ ìë™ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤:\n\n```yaml\njob:\n  script:\n    - echo \"Branch: $CI_COMMIT_BRANCH\"\n    - echo \"Commit SHA: $CI_COMMIT_SHA\"\n    - echo \"Project: $CI_PROJECT_NAME\"\n    - echo \"Pipeline Source: $CI_PIPELINE_SOURCE\"\n    - echo \"MR IID: $CI_MERGE_REQUEST_IID\"\n```\n\n---\n\n## ì‹¤ì „ ì˜ˆì œ: Node.js í’€ìŠ¤íƒ í”„ë¡œì íŠ¸\n\n```yaml\ndefault:\n  image: node:20-alpine\n\nstages:\n  - install\n  - build\n  - test\n  - deploy\n\n# ìºì‹œ ì„¤ì • (ì˜ì¡´ì„± ì¬ì‚¬ìš©)\n.node-cache:\n  cache:\n    key:\n      files:\n        - package-lock.json\n    paths:\n      - node_modules/\n    policy: pull\n\ninstall-deps:\n  stage: install\n  extends: .node-cache\n  cache:\n    policy: pull-push  # ìºì‹œ ì €ì¥\n  script:\n    - npm ci\n  artifacts:\n    paths:\n      - node_modules/\n    expire_in: 1 hour\n\nbuild-app:\n  stage: build\n  extends: .node-cache\n  script:\n    - npm run build\n  artifacts:\n    paths:\n      - dist/\n    expire_in: 1 week\n\nlint:\n  stage: test\n  extends: .node-cache\n  script:\n    - npm run lint\n  allow_failure: true  # ì‹¤íŒ¨í•´ë„ íŒŒì´í”„ë¼ì¸ ê³„ì†\n\nunit-test:\n  stage: test\n  extends: .node-cache\n  script:\n    - npm run test:unit -- --coverage\n  coverage: '/All files\\s+\\|\\s+(\\d+\\.?\\d*)\\s*\\|/'\n  artifacts:\n    reports:\n      junit: junit.xml\n      coverage_report:\n        coverage_format: cobertura\n        path: coverage/cobertura-coverage.xml\n\ne2e-test:\n  stage: test\n  image: mcr.microsoft.com/playwright:v1.40.0\n  extends: .node-cache\n  script:\n    - npm run test:e2e\n  artifacts:\n    when: on_failure\n    paths:\n      - test-results/\n\ndeploy-staging:\n  stage: deploy\n  script:\n    - echo \"Deploying to staging...\"\n  environment:\n    name: staging\n    url: https://staging.example.com\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"develop\"\n\ndeploy-production:\n  stage: deploy\n  script:\n    - echo \"Deploying to production...\"\n  environment:\n    name: production\n    url: https://example.com\n  rules:\n    - if: $CI_COMMIT_BRANCH == \"main\"\n      when: manual\n```\n\n---\n\n## Pipeline ì‹œê°í™”ì™€ ë””ë²„ê¹…\n\n### GitLab UIì—ì„œ Pipeline í™•ì¸\n\n```mermaid\nflowchart TB\n    subgraph Pipeline [Pipeline ë·°]\n        direction LR\n        S1[install] --> S2[build] --> S3[test] --> S4[deploy]\n    end\n    \n    subgraph Jobs [Jobs ë·°]\n        J1[install-deps âœ“]\n        J2[build-app âœ“]\n        J3[lint âš ]\n        J4[unit-test âœ“]\n        J5[e2e-test âœ“]\n        J6[deploy-staging ğŸ”„]\n    end\n```\n\n### CI Lint\n\n`.gitlab-ci.yml` ë¬¸ë²•ì„ ê²€ì¦í•˜ë ¤ë©´:\n\n1. **GitLab UI**: `CI/CD > Pipelines > CI lint`\n2. **API**:\n\n   ```bash\n   curl --header \"PRIVATE-TOKEN: $GITLAB_TOKEN\" \\\n     --data \"content=$(cat .gitlab-ci.yml)\" \\\n     \"https://gitlab.com/api/v4/ci/lint\"\n   ```\n\n### ë””ë²„ê·¸ ë¡œê¹…\n\n```yaml\nvariables:\n  CI_DEBUG_TRACE: \"true\"  # ìƒì„¸ ë¡œê·¸ ì¶œë ¥\n\njob:\n  script:\n    - set -x  # ëª…ë ¹ì–´ ì¶œë ¥\n    - echo \"Debug info\"\n```\n\n> [!WARNING]\n> `CI_DEBUG_TRACE`ëŠ” **ì‹œí¬ë¦¿ì„ í¬í•¨í•œ ëª¨ë“  ë³€ìˆ˜**ë¥¼ ë¡œê·¸ì— ì¶œë ¥í•©ë‹ˆë‹¤. í”„ë¡œë•ì…˜ì—ì„œ ì‚¬ìš© ì‹œ ì£¼ì˜í•˜ì„¸ìš”.\n\n---\n\n## ì •ë¦¬\n\n| ê°œë… | ì„¤ëª… |\n|-----|------|\n| `.gitlab-ci.yml` | í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ìœ„ì¹˜í•œ íŒŒì´í”„ë¼ì¸ ì •ì˜ íŒŒì¼ |\n| **Stage** | Jobsë¥¼ ê·¸ë£¹í™”í•˜ê³  ì‹¤í–‰ ìˆœì„œ ì •ì˜ |\n| **Job** | ì‹¤ì œ ì‘ì—… ë‹¨ìœ„, ë…ë¦½ í™˜ê²½ì—ì„œ ì‹¤í–‰ |\n| **script** | Jobì—ì„œ ì‹¤í–‰í•  ëª…ë ¹ì–´ (í•„ìˆ˜) |\n| **Pipeline** | íŠ¸ë¦¬ê±°ì— ì˜í•´ ìƒì„±ë˜ëŠ” Stages/Jobsì˜ ì§‘í•© |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**2í¸: Variablesì™€ Secrets ê´€ë¦¬**ì—ì„œëŠ” ë‹¤ìŒì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- CI/CD Variablesì˜ ì¢…ë¥˜ (Predefined, Custom, Protected, Masked)\n- í”„ë¡œì íŠ¸/ê·¸ë£¹/ì¸ìŠ¤í„´ìŠ¤ ë ˆë²¨ ë³€ìˆ˜\n- ë³€ìˆ˜ ìš°ì„ ìˆœìœ„ (Precedence)\n- `.env` íŒŒì¼ê³¼ dotenv artifacts\n- Vault, AWS Secrets Manager ì—°ë™\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [GitLab CI/CD Documentation](https://docs.gitlab.com/ee/ci/)\n- [.gitlab-ci.yml Reference](https://docs.gitlab.com/ee/ci/yaml/)\n- [Predefined Variables](https://docs.gitlab.com/ee/ci/variables/predefined_variables.html)\n- [CI/CD Pipeline Configuration](https://docs.gitlab.com/ee/ci/pipelines/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "GitLab",
      "CI/CD",
      "Pipeline"
    ],
    "readingTime": 7,
    "wordCount": 1308,
    "isFeatured": false,
    "isPublic": true,
    "series": "gitlab-ci",
    "date": "2026-01-05"
  },
  {
    "id": "k8s-06-security",
    "slug": "k8s-06-security",
    "path": "backend/kubernetes",
    "fullPath": "backend/kubernetes/k8s-06-security",
    "title": "Kubernetes ì‹¬í™” ì‹œë¦¬ì¦ˆ #6: ë³´ì•ˆ ì™„ì „ ê°€ì´ë“œ",
    "excerpt": "RBAC, NetworkPolicy, Pod Security Standards(PSS)ê¹Œì§€ Kubernetes ë³´ì•ˆì˜ ëª¨ë“  ê²ƒì„ ê¹Šì´ ìˆê²Œ ë‹¤ë£¹ë‹ˆë‹¤.",
    "content": "# Kubernetes ì‹¬í™” ì‹œë¦¬ì¦ˆ #6: ë³´ì•ˆ ì™„ì „ ê°€ì´ë“œ\n\n## ì‹œë¦¬ì¦ˆ ê°œìš”\n\n| # | ì£¼ì œ | í•µì‹¬ ë‚´ìš© |\n|---|------|----------|\n| 1 | ì›Œí¬ë¡œë“œ ì»¨íŠ¸ë¡¤ëŸ¬ ì‹¬í™” | Deployment, StatefulSet, DaemonSet, CronJob |\n| 2 | ì„œë¹„ìŠ¤ ë„¤íŠ¸ì›Œí‚¹ ì‹¬í™” | Service íƒ€ì…, kube-proxy, AWS ALB/NLB |\n| 3 | ì„¤ì • ë° ì‹œí¬ë¦¿ ê´€ë¦¬ | ConfigMap, Secrets, AWS Secrets Manager CSI Driver |\n| 4 | Istio ì„œë¹„ìŠ¤ ë©”ì‹œ | VirtualService, DestinationRule, ì™€ì¼ë“œì¹´ë“œ ì„œë¸Œë„ë©”ì¸ |\n| 5 | ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì‹¬í™” | HPA, VPA, Cluster Autoscaler, Karpenter, KEDA |\n| **6** | **ë³´ì•ˆ ì‹¬í™”** | RBAC, NetworkPolicy, Pod Security Standards |\n\n---\n\n## Kubernetes ë³´ì•ˆì˜ 4ê°€ì§€ ë ˆì´ì–´\n\n```mermaid\nflowchart TB\n    subgraph Layer1 [\"1. í´ëŸ¬ìŠ¤í„° ë³´ì•ˆ\"]\n        API[\"API Server ì¸ì¦/ì¸ê°€\"]\n        ETCD[\"etcd ì•”í˜¸í™”\"]\n    end\n    \n    subgraph Layer2 [\"2. ì›Œí¬ë¡œë“œ ë³´ì•ˆ\"]\n        PSS[\"Pod Security Standards\"]\n        SA[\"ServiceAccount\"]\n    end\n    \n    subgraph Layer3 [\"3. ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ\"]\n        NP[\"NetworkPolicy\"]\n        mTLS[\"mTLS (Istio)\"]\n    end\n    \n    subgraph Layer4 [\"4. ëŸ°íƒ€ì„ ë³´ì•ˆ\"]\n        Falco[\"Falco\"]\n        Seccomp[\"Seccomp\"]\n    end\n    \n    Layer1 --> Layer2 --> Layer3 --> Layer4\n```\n\n---\n\n## RBAC (Role-Based Access Control)\n\n### RBACì˜ 4ê°€ì§€ í•µì‹¬ ë¦¬ì†ŒìŠ¤\n\n```mermaid\nflowchart LR\n    subgraph Namespace [\"ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë²”ìœ„\"]\n        Role[\"Role\"]\n        RB[\"RoleBinding\"]\n    end\n    \n    subgraph Cluster [\"í´ëŸ¬ìŠ¤í„° ë²”ìœ„\"]\n        CR[\"ClusterRole\"]\n        CRB[\"ClusterRoleBinding\"]\n    end\n    \n    Role --> RB\n    CR --> CRB\n    CR --> RB\n```\n\n| ë¦¬ì†ŒìŠ¤ | ë²”ìœ„ | ìš©ë„ |\n|-------|-----|-----|\n| `Role` | ë„¤ì„ìŠ¤í˜ì´ìŠ¤ | íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë‚´ ê¶Œí•œ ì •ì˜ |\n| `ClusterRole` | í´ëŸ¬ìŠ¤í„° | í´ëŸ¬ìŠ¤í„° ì „ì²´ ê¶Œí•œ ì •ì˜ |\n| `RoleBinding` | ë„¤ì„ìŠ¤í˜ì´ìŠ¤ | Role/ClusterRoleì„ ì£¼ì²´ì— ë°”ì¸ë”© |\n| `ClusterRoleBinding` | í´ëŸ¬ìŠ¤í„° | ClusterRoleì„ í´ëŸ¬ìŠ¤í„° ì „ì²´ì— ë°”ì¸ë”© |\n\n### Role ì •ì˜\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: prod\n  name: pod-reader\nrules:\n- apiGroups: [\"\"]           # \"\" = core API group\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n  \n- apiGroups: [\"\"]\n  resources: [\"pods/log\"]   # í•˜ìœ„ ë¦¬ì†ŒìŠ¤\n  verbs: [\"get\"]\n```\n\n### Verbs ì™„ì „ ì •ë¦¬\n\n| Verb | ì„¤ëª… | HTTP ë©”ì„œë“œ |\n|------|-----|------------|\n| `get` | ë‹¨ì¼ ë¦¬ì†ŒìŠ¤ ì¡°íšŒ | GET |\n| `list` | ë¦¬ì†ŒìŠ¤ ëª©ë¡ ì¡°íšŒ | GET |\n| `watch` | ë³€ê²½ ê°ì‹œ | GET (watch=true) |\n| `create` | ë¦¬ì†ŒìŠ¤ ìƒì„± | POST |\n| `update` | ë¦¬ì†ŒìŠ¤ ì „ì²´ ì—…ë°ì´íŠ¸ | PUT |\n| `patch` | ë¦¬ì†ŒìŠ¤ ë¶€ë¶„ ì—…ë°ì´íŠ¸ | PATCH |\n| `delete` | ë‹¨ì¼ ë¦¬ì†ŒìŠ¤ ì‚­ì œ | DELETE |\n| `deletecollection` | ì—¬ëŸ¬ ë¦¬ì†ŒìŠ¤ ì‚­ì œ | DELETE |\n\n### ClusterRole ì •ì˜\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: secret-reader\nrules:\n# ëª¨ë“  ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ secrets ì½ê¸°\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n  \n# íŠ¹ì • ì´ë¦„ì˜ secretsë§Œ (resourceNames)\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  resourceNames: [\"db-credentials\", \"api-key\"]\n  verbs: [\"get\"]\n  \n# í´ëŸ¬ìŠ¤í„° ë²”ìœ„ ë¦¬ì†ŒìŠ¤\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"get\", \"list\"]\n  \n# ë¹„-ë¦¬ì†ŒìŠ¤ URL\n- nonResourceURLs: [\"/healthz\", \"/healthz/*\"]\n  verbs: [\"get\"]\n```\n\n### RoleBinding\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: prod\nsubjects:\n# ì‚¬ìš©ì\n- kind: User\n  name: jane\n  apiGroup: rbac.authorization.k8s.io\n  \n# ê·¸ë£¹\n- kind: Group\n  name: developers\n  apiGroup: rbac.authorization.k8s.io\n  \n# ServiceAccount\n- kind: ServiceAccount\n  name: app-sa\n  namespace: prod\n  \nroleRef:\n  kind: Role  # ë˜ëŠ” ClusterRole\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### ê¸°ë³¸ ClusterRole í™œìš©\n\nKubernetesëŠ” ê¸°ë³¸ ClusterRoleì„ ì œê³µí•©ë‹ˆë‹¤:\n\n| ClusterRole | ì„¤ëª… |\n|-------------|-----|\n| `cluster-admin` | ëª¨ë“  ê¶Œí•œ (ìŠˆí¼ìœ ì €) |\n| `admin` | ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë‚´ ëª¨ë“  ê¶Œí•œ (RBAC í¬í•¨) |\n| `edit` | ëŒ€ë¶€ë¶„ì˜ ë¦¬ì†ŒìŠ¤ CRUD (RBAC ì œì™¸) |\n| `view` | ì½ê¸° ì „ìš© (Secrets ì œì™¸) |\n\n```yaml\n# ê°œë°œìì—ê²Œ íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ edit ê¶Œí•œ ë¶€ì—¬\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: developer-edit\n  namespace: dev\nsubjects:\n- kind: Group\n  name: developers\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: edit\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### ìµœì†Œ ê¶Œí•œ ì›ì¹™ ì„¤ê³„ íŒ¨í„´\n\n```mermaid\nflowchart TB\n    subgraph Prod [\"prod ë„¤ì„ìŠ¤í˜ì´ìŠ¤\"]\n        ProdRole[\"Role: pod-viewer\\n(read-only)\"]\n        DevRB[\"RoleBinding\\nâ†’ developers\"]\n    end\n    \n    subgraph Dev [\"dev ë„¤ì„ìŠ¤í˜ì´ìŠ¤\"]\n        DevRole[\"ClusterRole: edit\\n(CRUD)\"]\n        DevRB2[\"RoleBinding\\nâ†’ developers\"]\n    end\n    \n    subgraph Staging [\"staging ë„¤ì„ìŠ¤í˜ì´ìŠ¤\"]\n        StagingRole[\"ClusterRole: edit\\n(CRUD)\"]\n        DevRB3[\"RoleBinding\\nâ†’ developers\"]\n    end\n```\n\n```yaml\n# 1. í”„ë¡œë•ì…˜: ì½ê¸° ì „ìš©\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: developers-view\n  namespace: prod\nsubjects:\n- kind: Group\n  name: developers\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: view\n  apiGroup: rbac.authorization.k8s.io\n---\n# 2. ê°œë°œ/ìŠ¤í…Œì´ì§•: í¸ì§‘ ê¶Œí•œ\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: developers-edit\n  namespace: dev\nsubjects:\n- kind: Group\n  name: developers\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: edit\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### ê¶Œí•œ í™•ì¸ ëª…ë ¹ì–´\n\n```bash\n# í˜„ì¬ ì‚¬ìš©ì ê¶Œí•œ í™•ì¸\nkubectl auth can-i create pods --namespace=prod\n# yes ë˜ëŠ” no\n\n# íŠ¹ì • ì‚¬ìš©ì/SA ê¶Œí•œ í™•ì¸\nkubectl auth can-i create pods --namespace=prod --as=system:serviceaccount:prod:app-sa\n\n# ëª¨ë“  ê¶Œí•œ ë‚˜ì—´\nkubectl auth can-i --list --namespace=prod\n```\n\n---\n\n## NetworkPolicy\n\n### NetworkPolicy ê¸°ë³¸ ê°œë…\n\n> [!IMPORTANT]\n> NetworkPolicyëŠ” **CNI í”ŒëŸ¬ê·¸ì¸ ì§€ì›ì´ í•„ìš”**í•©ë‹ˆë‹¤. Calico, Cilium, Weave Net ë“±ì´ ì§€ì›í•˜ë©°, AWS VPC CNI ë‹¨ë…ìœ¼ë¡œëŠ” ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph Default [\"ê¸°ë³¸ ìƒíƒœ (ì •ì±… ì—†ìŒ)\"]\n        A1[\"Pod A\"] <--> B1[\"Pod B\"]\n        A1 <--> C1[\"Pod C\"]\n        B1 <--> C1\n    end\n    \n    subgraph WithPolicy [\"NetworkPolicy ì ìš© í›„\"]\n        A2[\"Pod A\\n(app: frontend)\"]\n        B2[\"Pod B\\n(app: backend)\"]\n        C2[\"Pod C\\n(app: db)\"]\n        A2 --> B2\n        B2 --> C2\n        A2 -.-x C2\n    end\n```\n\n### ê¸°ë³¸ NetworkPolicy êµ¬ì¡°\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: backend-policy\n  namespace: prod\nspec:\n  # ì´ ì •ì±…ì´ ì ìš©ë  Pod\n  podSelector:\n    matchLabels:\n      app: backend\n  \n  # ì •ì±… ìœ í˜• (Ingress, Egress, ë˜ëŠ” ë‘˜ ë‹¤)\n  policyTypes:\n  - Ingress\n  - Egress\n  \n  # ì¸ë°”ìš´ë“œ íŠ¸ë˜í”½ ê·œì¹™\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n  \n  # ì•„ì›ƒë°”ìš´ë“œ íŠ¸ë˜í”½ ê·œì¹™\n  egress:\n  - to:\n    - podSelector:\n        matchLabels:\n          app: database\n    ports:\n    - protocol: TCP\n      port: 5432\n```\n\n### from/to ì…€ë ‰í„° ìœ í˜•\n\n```mermaid\nflowchart TB\n    subgraph Selectors [\"íŠ¸ë˜í”½ ì†ŒìŠ¤/ëŒ€ìƒ\"]\n        PS[\"podSelector\\n(ê°™ì€ ë„¤ì„ìŠ¤í˜ì´ìŠ¤)\"]\n        NS[\"namespaceSelector\\n(ë‹¤ë¥¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤)\"]\n        IP[\"ipBlock\\n(CIDR ë¸”ë¡)\"]\n        COMBO[\"podSelector +\\nnamespaceSelector\\n(ì¡°í•©)\"]\n    end\n```\n\n| ì…€ë ‰í„° | ì„¤ëª… |\n|-------|-----|\n| `podSelector` | ê°™ì€ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ Pod |\n| `namespaceSelector` | ë‹¤ë¥¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ ëª¨ë“  Pod |\n| `podSelector` + `namespaceSelector` | íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ íŠ¹ì • Pod |\n| `ipBlock` | CIDR ê¸°ë°˜ IP ë²”ìœ„ |\n\n### ì…€ë ‰í„° ì¡°í•© ì˜ˆì‹œ\n\n```yaml\ningress:\n# OR ê´€ê³„ (ë°°ì—´ì˜ ê° í•­ëª©)\n- from:\n  # 1. ê°™ì€ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ frontend Pod\n  - podSelector:\n      matchLabels:\n        app: frontend\n  \n  # 2. monitoring ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ ëª¨ë“  Pod\n  - namespaceSelector:\n      matchLabels:\n        name: monitoring\n  \n  # 3. íŠ¹ì • IP ëŒ€ì—­\n  - ipBlock:\n      cidr: 10.0.0.0/8\n      except:\n      - 10.1.0.0/16\n\n# AND ê´€ê³„ (ê°™ì€ í•­ëª© ë‚´)\n- from:\n  - namespaceSelector:\n      matchLabels:\n        env: prod\n    podSelector:\n      matchLabels:\n        role: client\n```\n\n> [!WARNING]\n> `podSelector`ì™€ `namespaceSelector`ê°€ **ê°™ì€ `-` ì•„ë˜**ì— ìˆìœ¼ë©´ **AND** ì¡°ê±´ì…ë‹ˆë‹¤. **ë³„ë„ì˜ `-`**ë¡œ ë¶„ë¦¬í•˜ë©´ **OR** ì¡°ê±´ì…ë‹ˆë‹¤.\n\n### Default Deny ì •ì±… (Zero Trust ê¸°ë°˜)\n\n```yaml\n# 1. ëª¨ë“  ì¸ë°”ìš´ë“œ íŠ¸ë˜í”½ ì°¨ë‹¨ (ê¸°ë³¸)\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-ingress\n  namespace: prod\nspec:\n  podSelector: {}  # ëª¨ë“  Podì— ì ìš©\n  policyTypes:\n  - Ingress\n  # ingress ê·œì¹™ ì—†ìŒ = ëª¨ë“  ì¸ë°”ìš´ë“œ ì°¨ë‹¨\n---\n# 2. ëª¨ë“  ì•„ì›ƒë°”ìš´ë“œ íŠ¸ë˜í”½ ì°¨ë‹¨\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-egress\n  namespace: prod\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n  # DNSëŠ” í—ˆìš© (í•„ìˆ˜)\n  egress:\n  - to:\n    - namespaceSelector: {}\n      podSelector:\n        matchLabels:\n          k8s-app: kube-dns\n    ports:\n    - protocol: UDP\n      port: 53\n```\n\n### ì‹¤ì „ ì˜ˆì‹œ: 3-tier ì•± NetworkPolicy\n\n```mermaid\nflowchart LR\n    Internet[\"ì¸í„°ë„·\"] --> FE[\"Frontend\\n(app: frontend)\"]\n    FE --> BE[\"Backend\\n(app: backend)\"]\n    BE --> DB[\"Database\\n(app: database)\"]\n    \n    FE -.-x DB\n    Internet -.-x BE\n    Internet -.-x DB\n```\n\n```yaml\n# Frontend: ì™¸ë¶€ì—ì„œë§Œ ì ‘ê·¼ ê°€ëŠ¥\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: frontend-policy\n  namespace: prod\nspec:\n  podSelector:\n    matchLabels:\n      app: frontend\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  # ì™¸ë¶€(Ingress Controller)ì—ì„œ ì ‘ê·¼\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: ingress-nginx\n    ports:\n    - port: 80\n  egress:\n  # Backendë¡œë§Œ ë‚˜ê°\n  - to:\n    - podSelector:\n        matchLabels:\n          app: backend\n    ports:\n    - port: 8080\n  # DNS í—ˆìš©\n  - to:\n    - namespaceSelector: {}\n      podSelector:\n        matchLabels:\n          k8s-app: kube-dns\n    ports:\n    - protocol: UDP\n      port: 53\n---\n# Backend: Frontendì—ì„œë§Œ ì ‘ê·¼, Databaseë¡œë§Œ ë‚˜ê°\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: backend-policy\n  namespace: prod\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n    ports:\n    - port: 8080\n  egress:\n  - to:\n    - podSelector:\n        matchLabels:\n          app: database\n    ports:\n    - port: 5432\n  - to:\n    - namespaceSelector: {}\n      podSelector:\n        matchLabels:\n          k8s-app: kube-dns\n    ports:\n    - protocol: UDP\n      port: 53\n---\n# Database: Backendì—ì„œë§Œ ì ‘ê·¼, ì™¸ë¶€ ë‚˜ê° ì°¨ë‹¨\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: database-policy\n  namespace: prod\nspec:\n  podSelector:\n    matchLabels:\n      app: database\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: backend\n    ports:\n    - port: 5432\n  egress: []  # ì•„ì›ƒë°”ìš´ë“œ ì—†ìŒ\n```\n\n---\n\n## Pod Security Standards (PSS)\n\n### PSS ê°œìš”\n\n> [!NOTE]\n> **Pod Security Admission**ì€ Kubernetes 1.25ì—ì„œ GAê°€ ë˜ì—ˆìœ¼ë©°, ë” ì´ìƒ PodSecurityPolicy(PSP)ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n### 3ê°€ì§€ ë³´ì•ˆ ë ˆë²¨\n\n```mermaid\nflowchart LR\n    subgraph Levels [\"Pod Security Standards\"]\n        P[\"Privileged\\n(ì œí•œ ì—†ìŒ)\"]\n        B[\"Baseline\\n(ê¸°ë³¸ ë³´ì•ˆ)\"]\n        R[\"Restricted\\n(ìµœëŒ€ ë³´ì•ˆ)\"]\n    end\n    \n    P --> |ì™„í™”| B --> |ì™„í™”| R\n```\n\n| ë ˆë²¨ | ì„¤ëª… | ì°¨ë‹¨ í•­ëª© |\n|-----|-----|----------|\n| `privileged` | ì œí•œ ì—†ìŒ | ì—†ìŒ |\n| `baseline` | ì•Œë ¤ì§„ ê¶Œí•œ ìƒìŠ¹ ì°¨ë‹¨ | hostNetwork, hostPID, privileged ì»¨í…Œì´ë„ˆ |\n| `restricted` | ìµœëŒ€ ë³´ì•ˆ | root ì‚¬ìš©ì, ì“°ê¸° ê°€ëŠ¥ ë£¨íŠ¸ íŒŒì¼ì‹œìŠ¤í…œ ë“± |\n\n### 3ê°€ì§€ ì ìš© ëª¨ë“œ\n\n| ëª¨ë“œ | ì„¤ëª… |\n|-----|-----|\n| `enforce` | ì •ì±… ìœ„ë°˜ ì‹œ Pod ìƒì„± ì°¨ë‹¨ |\n| `audit` | ì •ì±… ìœ„ë°˜ ì‹œ ê°ì‚¬ ë¡œê·¸ ê¸°ë¡ |\n| `warn` | ì •ì±… ìœ„ë°˜ ì‹œ ê²½ê³  ë©”ì‹œì§€ í‘œì‹œ |\n\n### ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë¼ë²¨ë¡œ ì ìš©\n\n```bash\n# baseline ê°•ì œ, restricted ê²½ê³ /ê°ì‚¬\nkubectl label namespace prod \\\n  pod-security.kubernetes.io/enforce=baseline \\\n  pod-security.kubernetes.io/enforce-version=latest \\\n  pod-security.kubernetes.io/warn=restricted \\\n  pod-security.kubernetes.io/warn-version=latest \\\n  pod-security.kubernetes.io/audit=restricted \\\n  pod-security.kubernetes.io/audit-version=latest\n```\n\n```yaml\n# ë˜ëŠ” YAMLë¡œ ì •ì˜\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: prod\n  labels:\n    pod-security.kubernetes.io/enforce: baseline\n    pod-security.kubernetes.io/enforce-version: latest\n    pod-security.kubernetes.io/warn: restricted\n    pod-security.kubernetes.io/warn-version: latest\n    pod-security.kubernetes.io/audit: restricted\n    pod-security.kubernetes.io/audit-version: latest\n```\n\n### í´ëŸ¬ìŠ¤í„° ì „ì²´ ê¸°ë³¸ê°’ ì„¤ì •\n\n```yaml\n# /etc/kubernetes/pss-config.yaml\napiVersion: apiserver.config.k8s.io/v1\nkind: AdmissionConfiguration\nplugins:\n- name: PodSecurity\n  configuration:\n    apiVersion: pod-security.admission.config.k8s.io/v1\n    kind: PodSecurityConfiguration\n    defaults:\n      enforce: \"baseline\"\n      enforce-version: \"latest\"\n      audit: \"restricted\"\n      audit-version: \"latest\"\n      warn: \"restricted\"\n      warn-version: \"latest\"\n    exemptions:\n      usernames: []\n      runtimeClasses: []\n      namespaces:\n      - kube-system\n      - kube-public\n```\n\n### Restricted ë ˆë²¨ Pod ì˜ˆì‹œ\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secure-pod\nspec:\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  \n  containers:\n  - name: app\n    image: my-app:latest\n    securityContext:\n      allowPrivilegeEscalation: false\n      readOnlyRootFilesystem: true\n      runAsNonRoot: true\n      runAsUser: 1000\n      capabilities:\n        drop:\n        - ALL\n    \n    # ì“°ê¸° í•„ìš”í•œ ë””ë ‰í† ë¦¬ëŠ” emptyDir ì‚¬ìš©\n    volumeMounts:\n    - name: tmp\n      mountPath: /tmp\n    - name: cache\n      mountPath: /app/cache\n  \n  volumes:\n  - name: tmp\n    emptyDir: {}\n  - name: cache\n    emptyDir: {}\n```\n\n---\n\n## ServiceAccount ë³´ì•ˆ\n\n### ìë™ ë§ˆìš´íŠ¸ ë¹„í™œì„±í™”\n\nëª¨ë“  Podì—ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ServiceAccount í† í°ì´ ë§ˆìš´íŠ¸ë©ë‹ˆë‹¤. í•„ìš” ì—†ë‹¤ë©´ ë¹„í™œì„±í™”í•˜ì„¸ìš”.\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  automountServiceAccountToken: false\n  containers:\n  - name: app\n    image: my-app:latest\n```\n\n### ì „ìš© ServiceAccount ì‚¬ìš©\n\n```yaml\n# ìµœì†Œ ê¶Œí•œ ServiceAccount\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: app-sa\n  namespace: prod\nautomountServiceAccountToken: false  # ê¸°ë³¸ ë¹„í™œì„±í™”\n---\n# í•„ìš”í•œ ê¶Œí•œë§Œ ë¶€ì—¬\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: app-role\n  namespace: prod\nrules:\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  resourceNames: [\"app-config\"]\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: app-rolebinding\n  namespace: prod\nsubjects:\n- kind: ServiceAccount\n  name: app-sa\n  namespace: prod\nroleRef:\n  kind: Role\n  name: app-role\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### AWS IRSA (IAM Roles for Service Accounts)\n\n```yaml\n# ServiceAccountì— IAM Role ì—°ê²°\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: s3-reader\n  namespace: prod\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/S3ReaderRole\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: s3-reader-pod\n  namespace: prod\nspec:\n  serviceAccountName: s3-reader\n  containers:\n  - name: app\n    image: amazon/aws-cli\n    command: [\"aws\", \"s3\", \"ls\"]\n    # AWS SDKê°€ ìë™ìœ¼ë¡œ IRSA í† í° ì‚¬ìš©\n```\n\n```mermaid\nsequenceDiagram\n    participant Pod as Pod\n    participant IRSA as AWS STS\n    participant S3 as AWS S3\n    \n    Pod->>Pod: SA í† í° ë§ˆìš´íŠ¸ë¨\\n(/var/run/secrets/eks.amazonaws.com/serviceaccount/token)\n    Pod->>IRSA: AssumeRoleWithWebIdentity\\n(SA í† í° ì „ë‹¬)\n    IRSA-->>Pod: ì„ì‹œ AWS ìê²© ì¦ëª…\n    Pod->>S3: S3 API í˜¸ì¶œ\\n(ì„ì‹œ ìê²© ì¦ëª… ì‚¬ìš©)\n    S3-->>Pod: ì‘ë‹µ\n```\n\n---\n\n## íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ\n\n### RBAC ê¶Œí•œ ì˜¤ë¥˜\n\n```bash\n# ì—ëŸ¬: User \"jane\" cannot list pods in namespace \"prod\"\n\n# 1. ê¶Œí•œ í™•ì¸\nkubectl auth can-i list pods --namespace=prod --as=jane\n\n# 2. RoleBinding í™•ì¸\nkubectl get rolebindings -n prod -o wide\n\n# 3. ClusterRoleBinding í™•ì¸\nkubectl get clusterrolebindings -o wide | grep jane\n\n# 4. ìì„¸í•œ ê¶Œí•œ ì¡°íšŒ\nkubectl auth can-i --list --namespace=prod --as=jane\n```\n\n### NetworkPolicyê°€ ì‘ë™í•˜ì§€ ì•ŠìŒ\n\n```bash\n# 1. CNI í”ŒëŸ¬ê·¸ì¸ í™•ì¸ (Calico, Cilium ë“±)\nkubectl get pods -n kube-system | grep calico\n\n# 2. NetworkPolicy í™•ì¸\nkubectl get networkpolicy -n prod\n\n# 3. Pod ë¼ë²¨ í™•ì¸\nkubectl get pods -n prod --show-labels\n\n# 4. ì •ì±… ìƒì„¸ í™•ì¸\nkubectl describe networkpolicy backend-policy -n prod\n```\n\n**í”í•œ ì›ì¸**:\n\n1. **CNI ë¯¸ì§€ì›**: AWS VPC CNI ë‹¨ë…ì€ NetworkPolicy ë¯¸ì§€ì›\n2. **ë¼ë²¨ ë¶ˆì¼ì¹˜**: podSelector/namespaceSelector ë¼ë²¨ ì˜¤íƒ€\n3. **DNS ì°¨ë‹¨**: egressì—ì„œ DNS(UDP 53) ë¯¸í—ˆìš©\n\n### Pod Security ìœ„ë°˜\n\n```bash\n# ê²½ê³  ë©”ì‹œì§€ ì˜ˆì‹œ\n# Warning: would violate PodSecurity \"restricted:latest\"\n\n# 1. ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë¼ë²¨ í™•ì¸\nkubectl get ns prod --show-labels\n\n# 2. Pod ë³´ì•ˆ ì»¨í…ìŠ¤íŠ¸ í™•ì¸\nkubectl get pod my-pod -o yaml | grep -A 20 securityContext\n\n# 3. ìœ„ë°˜ í•­ëª© í™•ì¸ (dry-run)\nkubectl apply -f pod.yaml --dry-run=server\n```\n\n**í”í•œ ì›ì¸**:\n\n1. **runAsNonRoot ë¯¸ì„¤ì •**\n2. **allowPrivilegeEscalation: true**\n3. **capabilities.drop: ALL ë¯¸ì„¤ì •**\n\n---\n\n## ì •ë¦¬\n\n| ë³´ì•ˆ ë ˆì´ì–´ | ë„êµ¬ | í•µì‹¬ ê¸°ëŠ¥ |\n|-----------|-----|----------|\n| **ì¸ì¦/ì¸ê°€** | RBAC | Role, ClusterRole, ìµœì†Œ ê¶Œí•œ |\n| **ë„¤íŠ¸ì›Œí¬** | NetworkPolicy | ingress/egress ì œì–´, Zero Trust |\n| **ì›Œí¬ë¡œë“œ** | PSS | Privileged/Baseline/Restricted |\n| **ID ê´€ë¦¬** | ServiceAccount | IRSA, í† í° ë§ˆìš´íŠ¸ ì œì–´ |\n\n---\n\n## ì‹œë¦¬ì¦ˆ ë§ˆë¬´ë¦¬\n\nì´ ì‹œë¦¬ì¦ˆì—ì„œ ë‹¤ë£¬ í•µì‹¬ ë‚´ìš©:\n\n| í¸ | ì£¼ì œ | í•µì‹¬ í‚¤ì›Œë“œ |\n|---|------|----------|\n| 1í¸ | ì›Œí¬ë¡œë“œ ì»¨íŠ¸ë¡¤ëŸ¬ | Reconciliation Loop, Rolling Update, StatefulSet |\n| 2í¸ | ì„œë¹„ìŠ¤ ë„¤íŠ¸ì›Œí‚¹ | ClusterIP/NodePort/LoadBalancer, nftables |\n| 3í¸ | ì„¤ì •/ì‹œí¬ë¦¿ ê´€ë¦¬ | ConfigMap, Secrets at Rest, CSI Driver |\n| 4í¸ | Istio ì„œë¹„ìŠ¤ ë©”ì‹œ | VirtualService, DestinationRule, mTLS |\n| 5í¸ | ì˜¤í† ìŠ¤ì¼€ì¼ë§ | HPA v2, VPA, Karpenter, KEDA |\n| 6í¸ | ë³´ì•ˆ | RBAC, NetworkPolicy, PSS, IRSA |\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Kubernetes RBAC](https://kubernetes.io/docs/reference/access-authn-authz/rbac/)\n- [Network Policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/)\n- [Pod Security Standards](https://kubernetes.io/docs/concepts/security/pod-security-standards/)\n- [Pod Security Admission](https://kubernetes.io/docs/concepts/security/pod-security-admission/)\n- [AWS IRSA](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "Kubernetes",
      "AWS",
      "Security"
    ],
    "readingTime": 11,
    "wordCount": 2159,
    "isFeatured": false,
    "isPublic": true,
    "series": "kubernetes",
    "date": "2026-01-04"
  },
  {
    "id": "k8s-05-autoscaling",
    "slug": "k8s-05-autoscaling",
    "path": "backend/kubernetes",
    "fullPath": "backend/kubernetes/k8s-05-autoscaling",
    "title": "Kubernetes ì‹¬í™” ì‹œë¦¬ì¦ˆ #5: ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì™„ì „ ê°€ì´ë“œ",
    "excerpt": "HPA v2, VPA, Cluster Autoscaler, Karpenter, KEDAê¹Œì§€ Kubernetes ì˜¤í† ìŠ¤ì¼€ì¼ë§ì˜ ëª¨ë“  ê²ƒì„ ê¹Šì´ ìˆê²Œ ë‹¤ë£¹ë‹ˆë‹¤.",
    "content": "# Kubernetes ì‹¬í™” ì‹œë¦¬ì¦ˆ #5: ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì™„ì „ ê°€ì´ë“œ\n\n## ì‹œë¦¬ì¦ˆ ê°œìš”\n\n| # | ì£¼ì œ | í•µì‹¬ ë‚´ìš© |\n|---|------|----------|\n| 1 | ì›Œí¬ë¡œë“œ ì»¨íŠ¸ë¡¤ëŸ¬ ì‹¬í™” | Deployment, StatefulSet, DaemonSet, CronJob |\n| 2 | ì„œë¹„ìŠ¤ ë„¤íŠ¸ì›Œí‚¹ ì‹¬í™” | Service íƒ€ì…, kube-proxy, AWS ALB/NLB |\n| 3 | ì„¤ì • ë° ì‹œí¬ë¦¿ ê´€ë¦¬ | ConfigMap, Secrets, AWS Secrets Manager CSI Driver |\n| 4 | Istio ì„œë¹„ìŠ¤ ë©”ì‹œ | VirtualService, DestinationRule, ì™€ì¼ë“œì¹´ë“œ ì„œë¸Œë„ë©”ì¸ |\n| **5** | **ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì‹¬í™”** | HPA, VPA, Cluster Autoscaler, Karpenter, KEDA |\n| 6 | ë³´ì•ˆ ì‹¬í™” | RBAC, NetworkPolicy, Pod Security Standards |\n\n---\n\n## ì˜¤í† ìŠ¤ì¼€ì¼ë§ì˜ 3ê°€ì§€ ì¶•\n\nKubernetes ì˜¤í† ìŠ¤ì¼€ì¼ë§ì€ **ë¬´ì—‡ì„ ìŠ¤ì¼€ì¼ë§í•˜ëŠëƒ**ì— ë”°ë¼ ì„¸ ê°€ì§€ë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph Horizontal [\"ìˆ˜í‰ ìŠ¤ì¼€ì¼ë§ (HPA)\"]\n        H1[\"Pod 1\"]\n        H2[\"Pod 2\"]\n        H3[\"Pod 3\"]\n        H4[\"Pod 4 (ì‹ ê·œ)\"]\n    end\n    \n    subgraph Vertical [\"ìˆ˜ì§ ìŠ¤ì¼€ì¼ë§ (VPA)\"]\n        V1[\"CPU: 100m â†’ 500m\\nMemory: 128Mi â†’ 512Mi\"]\n    end\n    \n    subgraph Cluster [\"í´ëŸ¬ìŠ¤í„° ìŠ¤ì¼€ì¼ë§\"]\n        N1[\"Node 1\"]\n        N2[\"Node 2\"]\n        N3[\"Node 3 (ì‹ ê·œ)\"]\n    end\n```\n\n| ìŠ¤ì¼€ì¼ë§ ìœ í˜• | ëŒ€ìƒ | ë„êµ¬ |\n|-------------|-----|------|\n| **ìˆ˜í‰ (Horizontal)** | Pod ê°œìˆ˜ | HPA, KEDA |\n| **ìˆ˜ì§ (Vertical)** | Pod ë¦¬ì†ŒìŠ¤ (CPU/Memory) | VPA |\n| **í´ëŸ¬ìŠ¤í„°** | Node ê°œìˆ˜ | Cluster Autoscaler, Karpenter |\n\n---\n\n## HPA (HorizontalPodAutoscaler)\n\n### HPA v2: í˜„ì¬ Stable API\n\n> [!IMPORTANT]\n> HPAëŠ” í˜„ì¬ `autoscaling/v2`ê°€ stableì…ë‹ˆë‹¤. `v2beta2`ëŠ” deprecatedë˜ì—ˆìŠµë‹ˆë‹¤.\n\n### ê¸°ë³¸ ì„¤ì •: CPU ê¸°ë°˜\n\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: api-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: api-server\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70  # í‰ê·  CPU ì‚¬ìš©ë¥  70%\n```\n\n### ë‹¤ì¤‘ ë©”íŠ¸ë¦­: CPU + Memory\n\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: api-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: api-server\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  # CPU ê¸°ë°˜\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  # Memory ê¸°ë°˜\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: AverageValue\n        averageValue: 500Mi\n```\n\n> [!NOTE]\n> ë‹¤ì¤‘ ë©”íŠ¸ë¦­ ì‚¬ìš© ì‹œ HPAëŠ” **ê°€ì¥ í° replica ìˆ˜ë¥¼ ìš”êµ¬í•˜ëŠ” ë©”íŠ¸ë¦­**ì„ ê¸°ì¤€ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§í•©ë‹ˆë‹¤.\n\n### ë©”íŠ¸ë¦­ íƒ€ì… ì™„ì „ ì •ë¦¬\n\n```mermaid\nflowchart LR\n    subgraph Metrics [\"HPA ë©”íŠ¸ë¦­ íƒ€ì…\"]\n        R[\"Resource\\n(CPU, Memory)\"]\n        P[\"Pods\\n(ì»¤ìŠ¤í…€ Pod ë©”íŠ¸ë¦­)\"]\n        O[\"Object\\n(ë‹¤ë¥¸ K8s ì˜¤ë¸Œì íŠ¸)\"]\n        E[\"External\\n(ì™¸ë¶€ ì‹œìŠ¤í…œ)\"]\n    end\n    \n    R --> MS[\"Metrics Server\"]\n    P --> PA[\"Prometheus Adapter\"]\n    O --> PA\n    E --> PA\n```\n\n| íƒ€ì… | ì„¤ëª… | ì˜ˆì‹œ |\n|-----|-----|-----|\n| `Resource` | CPU, Memory (Metrics Server í•„ìš”) | í‰ê·  CPU 70% |\n| `Pods` | Podë‹¹ ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ | requests-per-second |\n| `Object` | ë‹¤ë¥¸ K8s ì˜¤ë¸Œì íŠ¸ì˜ ë©”íŠ¸ë¦­ | Ingressì˜ requests-per-second |\n| `External` | ì™¸ë¶€ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ | SQS í ë©”ì‹œì§€ ìˆ˜ |\n\n### External ë©”íŠ¸ë¦­ ì˜ˆì‹œ: AWS SQS\n\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: worker-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: worker\n  minReplicas: 1\n  maxReplicas: 50\n  metrics:\n  - type: External\n    external:\n      metric:\n        name: sqs_messages_visible\n        selector:\n          matchLabels:\n            queue_name: \"order-processing\"\n      target:\n        type: AverageValue\n        averageValue: \"30\"  # Podë‹¹ 30ê°œ ë©”ì‹œì§€ ì²˜ë¦¬\n```\n\n### Scaling Behavior: ê¸‰ê²©í•œ ìŠ¤ì¼€ì¼ë§ ë°©ì§€\n\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: api-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: api-server\n  minReplicas: 2\n  maxReplicas: 100\n  \n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300  # 5ë¶„ê°„ ì•ˆì •í™”\n      policies:\n      - type: Percent\n        value: 10        # ìµœëŒ€ 10%ì”© ê°ì†Œ\n        periodSeconds: 60\n      selectPolicy: Max\n      \n    scaleUp:\n      stabilizationWindowSeconds: 0  # ì¦‰ì‹œ ìŠ¤ì¼€ì¼ ì—…\n      policies:\n      - type: Percent\n        value: 100       # ìµœëŒ€ 100%ì”© ì¦ê°€\n        periodSeconds: 15\n      - type: Pods\n        value: 4         # ë˜ëŠ” ìµœëŒ€ 4ê°œì”© ì¦ê°€\n        periodSeconds: 15\n      selectPolicy: Max\n  \n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n```\n\n```mermaid\nsequenceDiagram\n    participant HPA as HPA Controller\n    participant Metrics as Metrics Server\n    participant Deploy as Deployment\n    \n    loop ë§¤ 15ì´ˆ\n        HPA->>Metrics: ë©”íŠ¸ë¦­ ì¡°íšŒ\n        Metrics-->>HPA: CPU ì‚¬ìš©ë¥  ë°˜í™˜\n        \n        alt CPU > 70%\n            HPA->>Deploy: Scale Up (behavior ì •ì±… ì ìš©)\n        else CPU < 70%\n            Note over HPA: stabilizationWindow ëŒ€ê¸°\n            HPA->>Deploy: Scale Down (10%ì”©)\n        end\n    end\n```\n\n---\n\n## VPA (VerticalPodAutoscaler)\n\n### VPA ì„¤ì¹˜ (ë³„ë„ ì„¤ì¹˜ í•„ìš”)\n\n> [!CAUTION]\n> VPAëŠ” Kubernetes ê¸°ë³¸ ì œê³µì´ ì•„ë‹™ë‹ˆë‹¤. ë³„ë„ ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n\n```bash\n# VPA ì„¤ì¹˜ (ê³µì‹ ë¦¬í¬ì§€í† ë¦¬)\ngit clone https://github.com/kubernetes/autoscaler.git\ncd autoscaler/vertical-pod-autoscaler\n./hack/vpa-up.sh\n```\n\n### VPA 4ê°€ì§€ ëª¨ë“œ\n\n```mermaid\nflowchart TB\n    subgraph Modes [\"VPA ëª¨ë“œ\"]\n        Auto[\"Auto / Recreate\\nPod ì¬ìƒì„±ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ë³€ê²½\"]\n        Initial[\"Initial\\nìƒì„± ì‹œì—ë§Œ ì ìš©\"]\n        Off[\"Off\\nì¶”ì²œë§Œ ì œê³µ\"]\n    end\n    \n    Auto --> |Pod ì‚­ì œ í›„ ì¬ìƒì„±| Pod1[\"ìƒˆ Pod\\n(í° ë¦¬ì†ŒìŠ¤)\"]\n    Initial --> |ìµœì´ˆ ìƒì„± ì‹œ| Pod2[\"Pod\\n(ì¶”ì²œ ë¦¬ì†ŒìŠ¤)\"]\n    Off --> |ë³€ê²½ ì—†ìŒ| Pod3[\"ê¸°ì¡´ Pod\"]\n```\n\n| ëª¨ë“œ | ë™ì‘ | ì‚¬ìš© ì‚¬ë¡€ |\n|-----|-----|----------|\n| `Auto` | Pod ì¬ìƒì„±ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ë³€ê²½ | ì¼ë°˜ì ì¸ ì›Œí¬ë¡œë“œ |\n| `Recreate` | Autoì™€ ë™ì¼ | Autoì˜ ë³„ì¹­ |\n| `Initial` | ìƒì„± ì‹œì—ë§Œ ì ìš© | StatefulSet, ì¤‘ë‹¨ ë¯¼ê° ì›Œí¬ë¡œë“œ |\n| `Off` | ì¶”ì²œë§Œ ì œê³µ, ë³€ê²½ ì—†ìŒ | ëª¨ë‹ˆí„°ë§ ìš©ë„ |\n\n### VPA ì„¤ì • ì˜ˆì‹œ\n\n```yaml\napiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: api-vpa\nspec:\n  targetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: api-server\n  \n  updatePolicy:\n    updateMode: \"Auto\"  # Auto, Recreate, Initial, Off\n  \n  resourcePolicy:\n    containerPolicies:\n    - containerName: api\n      # ë¦¬ì†ŒìŠ¤ ë²”ìœ„ ì œí•œ\n      minAllowed:\n        cpu: 100m\n        memory: 128Mi\n      maxAllowed:\n        cpu: 2\n        memory: 4Gi\n      # íŠ¹ì • ë¦¬ì†ŒìŠ¤ë§Œ ì¡°ì •\n      controlledResources: [\"cpu\", \"memory\"]\n```\n\n### VPA ì¶”ì²œ í™•ì¸\n\n```bash\nkubectl describe vpa api-vpa\n\n# Status:\n#   Recommendation:\n#     Container Recommendations:\n#       Container Name: api\n#       Lower Bound:\n#         Cpu:     25m\n#         Memory:  128Mi\n#       Target:\n#         Cpu:     100m    # ì¶”ì²œ CPU\n#         Memory:  256Mi   # ì¶”ì²œ Memory\n#       Upper Bound:\n#         Cpu:     1\n#         Memory:  1Gi\n```\n\n### In-place Pod Resizing (K8s 1.33+)\n\n> [!TIP]\n> Kubernetes 1.33ë¶€í„° **In-place Pod Resizing**ì´ betaë¡œ ê¸°ë³¸ í™œì„±í™”ë©ë‹ˆë‹¤. Podë¥¼ ì¬ìƒì„±í•˜ì§€ ì•Šê³  ë¦¬ì†ŒìŠ¤ë¥¼ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¨, 2024ë…„ ê¸°ì¤€ VPAëŠ” ì•„ì§ In-place Resizingì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë©°, í†µí•©ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.\n\n```yaml\n# ìˆ˜ë™ In-place Resizing ì˜ˆì‹œ\napiVersion: v1\nkind: Pod\nmetadata:\n  name: api-pod\nspec:\n  containers:\n  - name: api\n    image: api:latest\n    resources:\n      requests:\n        cpu: 100m\n        memory: 128Mi\n      limits:\n        cpu: 500m\n        memory: 512Mi\n    resizePolicy:        # K8s 1.33+ í•„ìš”\n    - resourceName: cpu\n      restartPolicy: NotRequired  # ì¬ì‹œì‘ ì—†ì´ ë³€ê²½\n    - resourceName: memory\n      restartPolicy: RestartContainer  # ë©”ëª¨ë¦¬ëŠ” ì¬ì‹œì‘ í•„ìš”\n```\n\n### HPA + VPA ë™ì‹œ ì‚¬ìš© ì‹œ ì£¼ì˜\n\n> [!WARNING]\n> HPAì™€ VPAë¥¼ **ê°™ì€ ë©”íŠ¸ë¦­(CPU/Memory)**ìœ¼ë¡œ ë™ì‹œì— ì‚¬ìš©í•˜ë©´ ì¶©ëŒì´ ë°œìƒí•©ë‹ˆë‹¤. HPAê°€ Pod ìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ VPAê°€ ë¦¬ì†ŒìŠ¤ë¥¼ ì¤„ì´ê³ , ë‹¤ì‹œ HPAê°€ Podë¥¼ ëŠ˜ë¦¬ëŠ” ì•…ìˆœí™˜ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n**ê¶Œì¥ íŒ¨í„´**:\n\n- HPA: Custom/External ë©”íŠ¸ë¦­ (requests-per-second, í ê¸¸ì´)\n- VPA: CPU/Memory (Off ëª¨ë“œë¡œ ì¶”ì²œë§Œ í™œìš©)\n\n---\n\n## Cluster Autoscaler vs Karpenter\n\n### ê³µì‹ ì§€ì› Node Autoscaler\n\n> [!NOTE]\n> Kubernetes SIG Autoscalingì€ **Cluster Autoscaler**ì™€ **Karpenter** ë‘ ê°€ì§€ë¥¼ ê³µì‹ ì§€ì›í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph CA [\"Cluster Autoscaler\"]\n        NG1[\"Node Group 1\\n(m5.large)\"]\n        NG2[\"Node Group 2\\n(m5.xlarge)\"]\n        NG3[\"Node Group 3\\n(c5.large)\"]\n    end\n    \n    subgraph KP [\"Karpenter\"]\n        NP[\"NodePool\\n(constraintsë§Œ ì •ì˜)\"]\n        N1[\"m5.large\\n(ìë™ ì„ íƒ)\"]\n        N2[\"c5.xlarge\\n(ìë™ ì„ íƒ)\"]\n        N3[\"r5.2xlarge\\n(ìë™ ì„ íƒ)\"]\n    end\n```\n\n### í•µì‹¬ ì°¨ì´ì \n\n| íŠ¹ì„± | Cluster Autoscaler | Karpenter |\n|-----|-------------------|-----------|\n| **í”„ë¡œë¹„ì €ë‹** | Node Group ì‚¬ì „ ì •ì˜ í•„ìš” | ìë™ í”„ë¡œë¹„ì €ë‹ (constraintsë§Œ) |\n| **ì¸ìŠ¤í„´ìŠ¤ ì„ íƒ** | Node Groupë‹¹ ê³ ì • | ì›Œí¬ë¡œë“œì— ë§ê²Œ ìë™ ì„ íƒ |\n| **ìŠ¤ì¼€ì¼ ì†ë„** | ëŠë¦¼ (Node Group í™•ì¸) | ë¹ ë¦„ (ì§ì ‘ EC2 API í˜¸ì¶œ) |\n| **Node Lifecycle** | ìŠ¤ì¼€ì¼ë§ë§Œ | Disruption, ìë™ ì—…ê·¸ë ˆì´ë“œ |\n| **í´ë¼ìš°ë“œ ì§€ì›** | ë‹¤ì–‘ (AWS, GCP, Azure ë“±) | AWS, Azure |\n| **Spot í†µí•©** | ë³„ë„ Node Group í•„ìš” | NodePoolì—ì„œ ë°”ë¡œ ì„¤ì • |\n\n### Karpenter ì„¤ì • ì˜ˆì‹œ (AWS)\n\n```yaml\n# NodePool ì •ì˜\napiVersion: karpenter.sh/v1\nkind: NodePool\nmetadata:\n  name: default\nspec:\n  template:\n    spec:\n      requirements:\n      - key: kubernetes.io/arch\n        operator: In\n        values: [\"amd64\", \"arm64\"]\n      - key: karpenter.sh/capacity-type\n        operator: In\n        values: [\"spot\", \"on-demand\"]  # Spot ìš°ì„ \n      - key: karpenter.k8s.aws/instance-category\n        operator: In\n        values: [\"c\", \"m\", \"r\"]\n      - key: karpenter.k8s.aws/instance-size\n        operator: In\n        values: [\"medium\", \"large\", \"xlarge\", \"2xlarge\"]\n      \n      nodeClassRef:\n        group: karpenter.k8s.aws\n        kind: EC2NodeClass\n        name: default\n  \n  # ìë™ ì •ë¦¬ (Disruption)\n  disruption:\n    consolidationPolicy: WhenEmptyOrUnderutilized\n    consolidateAfter: 1m\n  \n  limits:\n    cpu: 1000\n    memory: 1000Gi\n---\n# EC2NodeClass ì •ì˜\napiVersion: karpenter.k8s.aws/v1\nkind: EC2NodeClass\nmetadata:\n  name: default\nspec:\n  amiFamily: AL2023\n  subnetSelectorTerms:\n  - tags:\n      karpenter.sh/discovery: my-cluster\n  securityGroupSelectorTerms:\n  - tags:\n      karpenter.sh/discovery: my-cluster\n  role: KarpenterNodeRole-my-cluster\n```\n\n### Karpenter Disruption (ìë™ ìµœì í™”)\n\n```mermaid\nsequenceDiagram\n    participant KP as Karpenter\n    participant Node as Under-utilized Node\n    participant Pod as Pods\n    participant NewNode as Optimal Node\n    \n    Note over KP: consolidateAfter: 1m ê²½ê³¼\n    KP->>Node: Node í™œìš©ë„ ë¶„ì„\n    Note over KP: ì‚¬ìš©ë¥  ë‚®ìŒ ê°ì§€\n    \n    KP->>NewNode: ìµœì  í¬ê¸° Node í”„ë¡œë¹„ì €ë‹\n    KP->>Pod: Pod ì´ë™ (drain)\n    Pod->>NewNode: Pod ìŠ¤ì¼€ì¤„ë§\n    KP->>Node: Node ì¢…ë£Œ\n    Note over KP: ë¹„ìš© ìµœì í™” ì™„ë£Œ\n```\n\n---\n\n## KEDA (Kubernetes Event-driven Autoscaling)\n\n### KEDAë€?\n\nKEDAëŠ” HPAë¥¼ í™•ì¥í•˜ì—¬ **ì´ë²¤íŠ¸ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§**ì„ ì œê³µí•©ë‹ˆë‹¤. 60ê°œ ì´ìƒì˜ íŠ¸ë¦¬ê±°ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph Triggers [\"KEDA íŠ¸ë¦¬ê±°\"]\n        SQS[\"AWS SQS\"]\n        Kafka[\"Kafka\"]\n        Prometheus[\"Prometheus\"]\n        Cron[\"Cron\"]\n        HTTP[\"HTTP\"]\n    end\n    \n    subgraph KEDA\n        SO[\"ScaledObject\"]\n        HPA[\"HPA (ìë™ ìƒì„±)\"]\n    end\n    \n    subgraph Target\n        Deploy[\"Deployment\"]\n    end\n    \n    Triggers --> SO --> HPA --> Deploy\n```\n\n### KEDA ì„¤ì¹˜\n\n```bash\n# Helmìœ¼ë¡œ ì„¤ì¹˜\nhelm repo add kedacore https://kedacore.github.io/charts\nhelm install keda kedacore/keda --namespace keda --create-namespace\n```\n\n### ScaledObject: AWS SQS ì˜ˆì‹œ\n\n```yaml\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: order-processor-scaler\nspec:\n  scaleTargetRef:\n    name: order-processor\n  \n  minReplicaCount: 0   # 0ê¹Œì§€ ìŠ¤ì¼€ì¼ ë‹¤ìš´ ê°€ëŠ¥!\n  maxReplicaCount: 100\n  \n  pollingInterval: 30  # 30ì´ˆë§ˆë‹¤ í™•ì¸\n  cooldownPeriod: 300  # 5ë¶„ ëŒ€ê¸° í›„ ìŠ¤ì¼€ì¼ ë‹¤ìš´\n  \n  triggers:\n  - type: aws-sqs-queue\n    metadata:\n      queueURL: https://sqs.ap-northeast-2.amazonaws.com/123456789012/orders\n      queueLength: \"10\"  # ë©”ì‹œì§€ 10ê°œë‹¹ 1ê°œ Pod\n      awsRegion: ap-northeast-2\n    authenticationRef:\n      name: keda-aws-credentials\n---\n# ì¸ì¦ ì„¤ì •\napiVersion: keda.sh/v1alpha1\nkind: TriggerAuthentication\nmetadata:\n  name: keda-aws-credentials\nspec:\n  podIdentity:\n    provider: aws-eks  # IRSA ì‚¬ìš©\n```\n\n### ScaledObject: Kafka ì˜ˆì‹œ\n\n```yaml\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: kafka-consumer-scaler\nspec:\n  scaleTargetRef:\n    name: kafka-consumer\n  minReplicaCount: 1\n  maxReplicaCount: 50\n  \n  triggers:\n  - type: kafka\n    metadata:\n      bootstrapServers: kafka.default.svc:9092\n      consumerGroup: my-consumer-group\n      topic: events\n      lagThreshold: \"100\"  # lag 100 ì´ˆê³¼ ì‹œ ìŠ¤ì¼€ì¼ ì—…\n```\n\n### ScaledObject: Cron (ì‹œê°„ ê¸°ë°˜)\n\n```yaml\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: business-hours-scaler\nspec:\n  scaleTargetRef:\n    name: api-server\n  minReplicaCount: 2\n  maxReplicaCount: 20\n  \n  triggers:\n  # ì—…ë¬´ ì‹œê°„ (9ì‹œ-18ì‹œ í‰ì¼)\n  - type: cron\n    metadata:\n      timezone: Asia/Seoul\n      start: 0 9 * * 1-5   # ì›”-ê¸ˆ 9ì‹œ\n      end: 0 18 * * 1-5    # ì›”-ê¸ˆ 18ì‹œ\n      desiredReplicas: \"10\"\n  \n  # ë¹„ì—…ë¬´ ì‹œê°„\n  - type: cron\n    metadata:\n      timezone: Asia/Seoul\n      start: 0 18 * * 1-5\n      end: 0 9 * * 2-6\n      desiredReplicas: \"2\"\n```\n\n### KEDAì˜ í•µì‹¬ ì¥ì \n\n| ê¸°ëŠ¥ | HPA | KEDA |\n|-----|-----|------|\n| **0ìœ¼ë¡œ ìŠ¤ì¼€ì¼ ë‹¤ìš´** | âŒ (min 1) | âœ… |\n| **ì™¸ë¶€ ë©”íŠ¸ë¦­ ì—°ë™** | ë³µì¡ (Adapter í•„ìš”) | ê°„ë‹¨ (ë‚´ì¥) |\n| **ì´ë²¤íŠ¸ ê¸°ë°˜** | âŒ | âœ… |\n| **Cron ìŠ¤ì¼€ì¤„ë§** | âŒ | âœ… |\n\n---\n\n## íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ\n\n### HPAê°€ ìŠ¤ì¼€ì¼ë§í•˜ì§€ ì•ŠìŒ\n\n```bash\n# 1. HPA ìƒíƒœ í™•ì¸\nkubectl get hpa api-hpa\n\n# TARGETSê°€ <unknown>ì¸ ê²½ìš°\n# NAME      REFERENCE             TARGETS         MINPODS   MAXPODS\n# api-hpa   Deployment/api-server <unknown>/70%   2         10\n\n# 2. Metrics Server í™•ì¸\nkubectl get deployment metrics-server -n kube-system\n\n# 3. Podì˜ resource requests í™•ì¸ (í•„ìˆ˜!)\nkubectl get pod api-xxx -o yaml | grep -A 5 resources\n```\n\n**í”í•œ ì›ì¸**:\n\n1. **Metrics Server ë¯¸ì„¤ì¹˜**\n2. **Podì— resource requests ë¯¸ì„¤ì •** (HPA ì‘ë™ ë¶ˆê°€)\n3. **ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì§€ì—°** (15ì´ˆ ì£¼ê¸°)\n\n### VPA ì¶”ì²œì´ ì ìš©ë˜ì§€ ì•ŠìŒ\n\n```bash\n# 1. VPA ìƒíƒœ í™•ì¸\nkubectl describe vpa api-vpa\n\n# 2. updateMode í™•ì¸\n# Off ëª¨ë“œëŠ” ì¶”ì²œë§Œ ì œê³µ\n\n# 3. Pod ì¬ìƒì„± ì—¬ë¶€ í™•ì¸\nkubectl get pods -w\n```\n\n**í”í•œ ì›ì¸**:\n\n1. **updateMode: Off** ì„¤ì •\n2. **minAllowed/maxAllowed ë²”ìœ„ ë°–**\n3. **PDB(PodDisruptionBudget)ë¡œ ì¸í•œ ì¬ìƒì„± ì°¨ë‹¨**\n\n### Karpenter Nodeê°€ í”„ë¡œë¹„ì €ë‹ë˜ì§€ ì•ŠìŒ\n\n```bash\n# 1. Karpenter ë¡œê·¸ í™•ì¸\nkubectl logs -n karpenter -l app.kubernetes.io/name=karpenter\n\n# 2. NodePool ìƒíƒœ í™•ì¸\nkubectl describe nodepool default\n\n# 3. Pending Pod í™•ì¸\nkubectl get pods --field-selector=status.phase=Pending\n```\n\n**í”í•œ ì›ì¸**:\n\n1. **requirements ì¶©ì¡± ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ì—†ìŒ**\n2. **ì„œë¸Œë„·/ë³´ì•ˆ ê·¸ë£¹ íƒœê·¸ ëˆ„ë½**\n3. **IAM ê¶Œí•œ ë¶€ì¡±**\n\n---\n\n## ì •ë¦¬\n\n| ë„êµ¬ | ìŠ¤ì¼€ì¼ë§ ëŒ€ìƒ | í•µì‹¬ ê¸°ëŠ¥ |\n|-----|-------------|----------|\n| **HPA** | Pod ìˆ˜ | Resource/Custom/External ë©”íŠ¸ë¦­ |\n| **VPA** | Pod ë¦¬ì†ŒìŠ¤ | ë¦¬ì†ŒìŠ¤ ì¶”ì²œ ë° ìë™ ì¡°ì • |\n| **Cluster Autoscaler** | Node ìˆ˜ | Node Group ê¸°ë°˜ |\n| **Karpenter** | Node ìˆ˜ | ìë™ í”„ë¡œë¹„ì €ë‹, Disruption |\n| **KEDA** | Pod ìˆ˜ | ì´ë²¤íŠ¸ ê¸°ë°˜, 0 ìŠ¤ì¼€ì¼ ë‹¤ìš´ |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**6í¸: ë³´ì•ˆ ì‹¬í™”**ì—ì„œëŠ” ë‹¤ìŒì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- RBAC ì„¤ê³„ íŒ¨í„´ (ìµœì†Œ ê¶Œí•œ ì›ì¹™)\n- NetworkPolicyë¡œ Zero Trust êµ¬í˜„\n- Pod Security Standards (PSS)\n- ServiceAccount ë³´ì•ˆê³¼ IRSA\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Kubernetes HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)\n- [Kubernetes VPA](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler)\n- [Cluster Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler)\n- [Karpenter](https://karpenter.sh/)\n- [KEDA](https://keda.sh/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "Kubernetes",
      "AWS",
      "Autoscaling"
    ],
    "readingTime": 10,
    "wordCount": 1929,
    "isFeatured": false,
    "isPublic": true,
    "series": "kubernetes",
    "date": "2026-01-04"
  },
  {
    "id": "k8s-04-istio-service-mesh",
    "slug": "k8s-04-istio-service-mesh",
    "path": "backend/kubernetes",
    "fullPath": "backend/kubernetes/k8s-04-istio-service-mesh",
    "title": "Kubernetes ì‹¬í™” ì‹œë¦¬ì¦ˆ #4: Istio ì„œë¹„ìŠ¤ ë©”ì‹œ ì™„ì „ ê°€ì´ë“œ",
    "excerpt": "Istioì˜ VirtualService, DestinationRule, Gatewayë¥¼ ê¹Šì´ ìˆê²Œ ë‹¤ë£¨ë©°, ì™€ì¼ë“œì¹´ë“œ ì„œë¸Œë„ë©”ì¸ + AWS Route 53/ACM ì—°ë™ê¹Œì§€ ì‹¤ì „ íŒ¨í„´ì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# Kubernetes ì‹¬í™” ì‹œë¦¬ì¦ˆ #4: Istio ì„œë¹„ìŠ¤ ë©”ì‹œ ì™„ì „ ê°€ì´ë“œ\n\n## ì‹œë¦¬ì¦ˆ ê°œìš”\n\n| # | ì£¼ì œ | í•µì‹¬ ë‚´ìš© |\n|---|------|----------|\n| 1 | ì›Œí¬ë¡œë“œ ì»¨íŠ¸ë¡¤ëŸ¬ ì‹¬í™” | Deployment, StatefulSet, DaemonSet, CronJob |\n| 2 | ì„œë¹„ìŠ¤ ë„¤íŠ¸ì›Œí‚¹ ì‹¬í™” | Service íƒ€ì…, kube-proxy, AWS ALB/NLB |\n| 3 | ì„¤ì • ë° ì‹œí¬ë¦¿ ê´€ë¦¬ | ConfigMap, Secrets, AWS Secrets Manager CSI Driver |\n| **4** | **Istio ì„œë¹„ìŠ¤ ë©”ì‹œ** | VirtualService, DestinationRule, ì™€ì¼ë“œì¹´ë“œ ì„œë¸Œë„ë©”ì¸ |\n| 5 | ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì‹¬í™” | HPA, VPA, Cluster Autoscaler, Karpenter, KEDA |\n| 6 | ë³´ì•ˆ ì‹¬í™” | RBAC, NetworkPolicy, Pod Security Standards |\n\n---\n\n## ì™œ ì„œë¹„ìŠ¤ ë©”ì‹œì¸ê°€?\n\në§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ì—ì„œëŠ” ì„œë¹„ìŠ¤ ê°„ í†µì‹ ì´ í•µì‹¬ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ê° ì„œë¹„ìŠ¤ì— ë‹¤ìŒì„ êµ¬í˜„í•˜ë©´:\n\n```\nâŒ ì¬ì‹œë„/íƒ€ì„ì•„ì›ƒ ë¡œì§ ì¤‘ë³µ\nâŒ ì„œí‚· ë¸Œë ˆì´ì»¤ ê°ê° êµ¬í˜„\nâŒ mTLS ì§ì ‘ ê´€ë¦¬\nâŒ íŠ¸ë˜í”½ ë¶„í• /ì¹´ë‚˜ë¦¬ ë°°í¬ ì–´ë ¤ì›€\nâŒ ë¶„ì‚° ì¶”ì  ì—°ë™ ë³µì¡\n```\n\n**ì„œë¹„ìŠ¤ ë©”ì‹œ**ëŠ” ì´ëŸ¬í•œ **íš¡ë‹¨ ê´€ì‹¬ì‚¬(Cross-cutting Concerns)**ë¥¼ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë¶„ë¦¬í•˜ì—¬ ì¸í”„ë¼ ë ˆì´ì–´ì—ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph Without [ì„œë¹„ìŠ¤ ë©”ì‹œ ì—†ì´]\n        A1[Service A<br/>+ ì¬ì‹œë„ ë¡œì§<br/>+ ì„œí‚· ë¸Œë ˆì´ì»¤<br/>+ TLS ê´€ë¦¬]\n        B1[Service B<br/>+ ì¬ì‹œë„ ë¡œì§<br/>+ ì„œí‚· ë¸Œë ˆì´ì»¤<br/>+ TLS ê´€ë¦¬]\n        A1 <--> B1\n    end\n    \n    subgraph With [ì„œë¹„ìŠ¤ ë©”ì‹œ ì‚¬ìš©]\n        A2[Service A]\n        EA[Envoy Proxy]\n        B2[Service B]\n        EB[Envoy Proxy]\n        A2 --- EA <-->|mTLS, ì¬ì‹œë„, ì„œí‚· ë¸Œë ˆì´ì»¤| EB --- B2\n    end\n```\n\n---\n\n## Istio ì•„í‚¤í…ì²˜\n\n### Control Plane vs Data Plane\n\n```mermaid\nflowchart TB\n    subgraph ControlPlane [Control Plane]\n        Istiod[istiod<br/>- Pilot: ì„¤ì • ë°°í¬<br/>- Citadel: ì¸ì¦ì„œ ê´€ë¦¬<br/>- Galley: ì„¤ì • ê²€ì¦]\n    end\n    \n    subgraph DataPlane [Data Plane]\n        subgraph PodA [Service A Pod]\n            AppA[Application]\n            EnvoyA[Envoy Sidecar]\n        end\n        \n        subgraph PodB [Service B Pod]\n            AppB[Application]\n            EnvoyB[Envoy Sidecar]\n        end\n        \n        subgraph PodC [Service C Pod]\n            AppC[Application]\n            EnvoyC[Envoy Sidecar]\n        end\n    end\n    \n    subgraph IngressEgress [Ingress/Egress]\n        IG[Ingress Gateway<br/>Envoy]\n        EG[Egress Gateway<br/>Envoy]\n    end\n    \n    Istiod -->|xDS í”„ë¡œí† ì½œ| EnvoyA & EnvoyB & EnvoyC & IG & EG\n    \n    External[ì™¸ë¶€ íŠ¸ë˜í”½] --> IG --> EnvoyA\n    EnvoyA <--> EnvoyB <--> EnvoyC\n    EnvoyC --> EG --> ExtService[ì™¸ë¶€ ì„œë¹„ìŠ¤]\n```\n\n### Envoy Sidecar ìë™ ì£¼ì…\n\n```yaml\n# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ë¼ë²¨ ì¶”ê°€\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-app\n  labels:\n    istio-injection: enabled  # ìë™ sidecar ì£¼ì…\n```\n\nëª¨ë“  Podì— Envoy ì»¨í…Œì´ë„ˆê°€ ìë™ìœ¼ë¡œ ì¶”ê°€ë©ë‹ˆë‹¤:\n\n```yaml\n# ì‹¤ì œ ìƒì„±ëœ Pod í™•ì¸\nkubectl get pod my-app-xxx -o yaml\n\n# ì»¨í…Œì´ë„ˆ 2ê°œ: app, istio-proxy\ncontainers:\n- name: my-app\n  image: my-app:v1\n- name: istio-proxy\n  image: docker.io/istio/proxyv2:1.20.0\n```\n\n---\n\n## Gateway: ì™¸ë¶€ íŠ¸ë˜í”½ ì§„ì…ì \n\n### ê¸°ë³¸ Gateway ì„¤ì •\n\n```yaml\napiVersion: networking.istio.io/v1\nkind: Gateway\nmetadata:\n  name: my-gateway\n  namespace: istio-system\nspec:\n  selector:\n    istio: ingressgateway  # Istio Ingress Gateway Pod ì„ íƒ\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"api.example.com\"\n    - \"dashboard.example.com\"\n```\n\n### HTTPS + AWS ACM ì—°ë™\n\n```yaml\napiVersion: networking.istio.io/v1\nkind: Gateway\nmetadata:\n  name: secure-gateway\n  namespace: istio-system\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: my-tls-secret  # Kubernetes TLS Secret\n    hosts:\n    - \"*.example.com\"  # ì™€ì¼ë“œì¹´ë“œ!\n```\n\n---\n\n## ì™€ì¼ë“œì¹´ë“œ ì„œë¸Œë„ë©”ì¸ ë¼ìš°íŒ… (ì‹¤ì „ íŒ¨í„´)\n\nì—¬ëŸ¬ ì„œë¹„ìŠ¤ë¥¼ `api.example.com`, `dashboard.example.com`, `admin.example.com` í˜•íƒœë¡œ ì‰½ê²Œ ë°°í¬í•  ìˆ˜ ìˆëŠ” ì´ìœ ëŠ” **Istio + AWS Route 53 + ACM í†µí•©** ë•ë¶„ì…ë‹ˆë‹¤. ì´ íŒ¨í„´ì˜ í•µì‹¬ êµ¬ì¡°ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n\n### ì „ì²´ ì•„í‚¤í…ì²˜\n\n```mermaid\nflowchart TB\n    subgraph Internet [ì¸í„°ë„·]\n        User[ì‚¬ìš©ì]\n    end\n    \n    subgraph AWS [AWS]\n        R53[Route 53<br/>*.example.com â†’ ALB]\n        ACM[ACM ì¸ì¦ì„œ<br/>*.example.com]\n        ALB[Application Load Balancer]\n    end\n    \n    subgraph EKS [EKS í´ëŸ¬ìŠ¤í„°]\n        IG[Istio Ingress Gateway<br/>istio-ingressgateway]\n        \n        GW[Gateway<br/>hosts: '*.example.com']\n        \n        VS1[VirtualService<br/>api.example.com]\n        VS2[VirtualService<br/>dashboard.example.com]\n        VS3[VirtualService<br/>admin.example.com]\n        \n        SVC1[api-service]\n        SVC2[dashboard-service]\n        SVC3[admin-service]\n    end\n    \n    User --> R53 --> ALB --> IG\n    IG --> GW\n    GW --> VS1 & VS2 & VS3\n    VS1 --> SVC1\n    VS2 --> SVC2\n    VS3 --> SVC3\n```\n\n### Step 1: AWS Route 53 ì™€ì¼ë“œì¹´ë“œ ë ˆì½”ë“œ\n\n```bash\n# Route 53ì— ì™€ì¼ë“œì¹´ë“œ A ë ˆì½”ë“œ ìƒì„±\naws route53 change-resource-record-sets \\\n  --hosted-zone-id Z1234567890 \\\n  --change-batch '{\n    \"Changes\": [{\n      \"Action\": \"CREATE\",\n      \"ResourceRecordSet\": {\n        \"Name\": \"*.example.com\",\n        \"Type\": \"A\",\n        \"AliasTarget\": {\n          \"HostedZoneId\": \"Z35SXDOTRQ7X7K\",\n          \"DNSName\": \"my-alb-123456789.ap-northeast-2.elb.amazonaws.com\",\n          \"EvaluateTargetHealth\": true\n        }\n      }\n    }]\n  }'\n```\n\n### Step 2: AWS ACM ì™€ì¼ë“œì¹´ë“œ ì¸ì¦ì„œ\n\n```bash\n# ì™€ì¼ë“œì¹´ë“œ ì¸ì¦ì„œ ìš”ì²­\naws acm request-certificate \\\n  --domain-name \"*.example.com\" \\\n  --subject-alternative-names \"example.com\" \\\n  --validation-method DNS\n```\n\n### Step 3: ALB + Istio Ingress Gateway ì—°ê²°\n\n```yaml\n# Istio Ingress Gateway Service (LoadBalancer)\napiVersion: v1\nkind: Service\nmetadata:\n  name: istio-ingressgateway\n  namespace: istio-system\n  annotations:\n    # ALB ì‚¬ìš©\n    service.beta.kubernetes.io/aws-load-balancer-type: \"external\"\n    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: \"ip\"\n    service.beta.kubernetes.io/aws-load-balancer-scheme: \"internet-facing\"\n    \n    # HTTPS ì¢…ë£ŒëŠ” ALBì—ì„œ (ACM ì¸ì¦ì„œ ì‚¬ìš©)\n    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \"arn:aws:acm:ap-northeast-2:123456789012:certificate/xxx\"\n    service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \"443\"\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 443\n    targetPort: 8080  # Istio GatewayëŠ” 8080ì—ì„œ ìˆ˜ì‹ \n    name: https\n```\n\n### Step 4: Istio Gateway (ì™€ì¼ë“œì¹´ë“œ)\n\n```yaml\napiVersion: networking.istio.io/v1\nkind: Gateway\nmetadata:\n  name: wildcard-gateway\n  namespace: istio-system\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 8080      # ALBì—ì„œ TLS ì¢…ë£Œ í›„ 8080ìœ¼ë¡œ ì „ë‹¬\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"*.example.com\"   # ì™€ì¼ë“œì¹´ë“œë¡œ ëª¨ë“  ì„œë¸Œë„ë©”ì¸ ìˆ˜ì‹ \n```\n\n### Step 5: ì„œë¹„ìŠ¤ë³„ VirtualService\n\n```yaml\n# API ì„œë¹„ìŠ¤\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: api-vs\n  namespace: my-app\nspec:\n  hosts:\n  - \"api.example.com\"\n  gateways:\n  - istio-system/wildcard-gateway\n  http:\n  - route:\n    - destination:\n        host: api-service\n        port:\n          number: 80\n---\n# Dashboard ì„œë¹„ìŠ¤\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: dashboard-vs\n  namespace: my-app\nspec:\n  hosts:\n  - \"dashboard.example.com\"\n  gateways:\n  - istio-system/wildcard-gateway\n  http:\n  - route:\n    - destination:\n        host: dashboard-service\n        port:\n          number: 80\n---\n# Admin ì„œë¹„ìŠ¤\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: admin-vs\n  namespace: my-app\nspec:\n  hosts:\n  - \"admin.example.com\"\n  gateways:\n  - istio-system/wildcard-gateway\n  http:\n  - route:\n    - destination:\n        host: admin-service\n        port:\n          number: 80\n```\n\n> [!TIP]\n> ì´ êµ¬ì¡°ì—ì„œ **ìƒˆ ì„œë¹„ìŠ¤ ì¶”ê°€**ëŠ” VirtualService í•˜ë‚˜ë§Œ ì¶”ê°€í•˜ë©´ ë©ë‹ˆë‹¤. DNSë‚˜ ì¸ì¦ì„œ ì‘ì—…ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤!\n\n---\n\n## VirtualService: L7 ë¼ìš°íŒ… ê·œì¹™\n\n### ê²½ë¡œ ê¸°ë°˜ ë¼ìš°íŒ…\n\n```yaml\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: api-routing\nspec:\n  hosts:\n  - \"api.example.com\"\n  gateways:\n  - istio-system/wildcard-gateway\n  http:\n  # /v1/* â†’ v1 ì„œë¹„ìŠ¤\n  - match:\n    - uri:\n        prefix: /v1/\n    route:\n    - destination:\n        host: api-v1\n        \n  # /v2/* â†’ v2 ì„œë¹„ìŠ¤\n  - match:\n    - uri:\n        prefix: /v2/\n    route:\n    - destination:\n        host: api-v2\n        \n  # ê¸°ë³¸ â†’ latest\n  - route:\n    - destination:\n        host: api-latest\n```\n\n### ê°€ì¤‘ì¹˜ ê¸°ë°˜ íŠ¸ë˜í”½ ë¶„í•  (ì¹´ë‚˜ë¦¬ ë°°í¬)\n\n```yaml\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: canary-rollout\nspec:\n  hosts:\n  - api-service\n  http:\n  - route:\n    - destination:\n        host: api-service\n        subset: stable\n      weight: 90\n    - destination:\n        host: api-service\n        subset: canary\n      weight: 10\n```\n\n```mermaid\nflowchart LR\n    Client[í´ë¼ì´ì–¸íŠ¸] --> VS[VirtualService]\n    VS -->|90%| Stable[stable\\nv1.0.0]\n    VS -->|10%| Canary[canary\\nv1.1.0]\n```\n\n### í—¤ë” ê¸°ë°˜ ë¼ìš°íŒ… (A/B í…ŒìŠ¤íŒ…)\n\n```yaml\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: ab-testing\nspec:\n  hosts:\n  - api-service\n  http:\n  # Beta ì‚¬ìš©ì â†’ ìƒˆ ë²„ì „\n  - match:\n    - headers:\n        x-user-group:\n          exact: beta\n    route:\n    - destination:\n        host: api-service\n        subset: v2\n        \n  # ë‚˜ë¨¸ì§€ â†’ ê¸°ì¡´ ë²„ì „\n  - route:\n    - destination:\n        host: api-service\n        subset: v1\n```\n\n### íƒ€ì„ì•„ì›ƒê³¼ ì¬ì‹œë„\n\n```yaml\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: timeout-retry\nspec:\n  hosts:\n  - api-service\n  http:\n  - route:\n    - destination:\n        host: api-service\n    timeout: 10s  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ\n    retries:\n      attempts: 3          # ìµœëŒ€ 3íšŒ ì¬ì‹œë„\n      perTryTimeout: 2s    # ê° ì‹œë„ë‹¹ 2ì´ˆ íƒ€ì„ì•„ì›ƒ\n      retryOn: \"5xx,reset,connect-failure\"\n```\n\n---\n\n## DestinationRule: íŠ¸ë˜í”½ ì •ì±…\n\n### Subset ì •ì˜\n\n```yaml\napiVersion: networking.istio.io/v1\nkind: DestinationRule\nmetadata:\n  name: api-service-dr\nspec:\n  host: api-service\n  subsets:\n  - name: stable\n    labels:\n      version: v1\n  - name: canary\n    labels:\n      version: v2\n```\n\n### ë¡œë“œë°¸ëŸ°ì‹± ì•Œê³ ë¦¬ì¦˜\n\n```yaml\napiVersion: networking.istio.io/v1\nkind: DestinationRule\nmetadata:\n  name: lb-policy\nspec:\n  host: api-service\n  trafficPolicy:\n    loadBalancer:\n      simple: LEAST_CONN  # ìµœì†Œ ì—°ê²° ìˆ˜\n      # ROUND_ROBIN (ê¸°ë³¸ê°’)\n      # RANDOM\n      # PASSTHROUGH (ì›ë˜ IPë¡œ ì „ë‹¬)\n```\n\n### ì„œí‚· ë¸Œë ˆì´ì»¤\n\n```yaml\napiVersion: networking.istio.io/v1\nkind: DestinationRule\nmetadata:\n  name: circuit-breaker\nspec:\n  host: api-service\n  trafficPolicy:\n    connectionPool:\n      tcp:\n        maxConnections: 100      # ìµœëŒ€ ì—°ê²° ìˆ˜\n      http:\n        h2UpgradePolicy: UPGRADE\n        http1MaxPendingRequests: 100\n        http2MaxRequests: 1000\n        \n    outlierDetection:\n      consecutive5xxErrors: 5    # ì—°ì† 5xx ì—ëŸ¬ 5ë²ˆ\n      interval: 30s              # 30ì´ˆ ê°„ê²©ìœ¼ë¡œ ì²´í¬\n      baseEjectionTime: 30s      # 30ì´ˆê°„ íŠ¸ë˜í”½ ì œì™¸\n      maxEjectionPercent: 50     # ìµœëŒ€ 50%ê¹Œì§€ ì œì™¸\n```\n\n```mermaid\nsequenceDiagram\n    participant Client as í´ë¼ì´ì–¸íŠ¸\n    participant Envoy as Envoy Proxy\n    participant PodA as Pod A (ì •ìƒ)\n    participant PodB as Pod B (ì¥ì• )\n    \n    Client->>Envoy: ìš”ì²­ 1\n    Envoy->>PodB: ì „ë‹¬\n    PodB-->>Envoy: 500 Error\n    \n    Client->>Envoy: ìš”ì²­ 2-5\n    Envoy->>PodB: ì „ë‹¬\n    PodB-->>Envoy: 500 Error (5íšŒ ì—°ì†)\n    \n    Note over Envoy,PodB: ì„œí‚· ì˜¤í”ˆ! Pod B ì œì™¸\n    \n    Client->>Envoy: ìš”ì²­ 6\n    Envoy->>PodA: Pod Aë¡œë§Œ ì „ë‹¬\n    PodA-->>Envoy: 200 OK\n    \n    Note over Envoy,PodB: 30ì´ˆ í›„ ì¬ì‹œë„\n    Envoy->>PodB: í—¬ìŠ¤ì²´í¬\n    PodB-->>Envoy: 200 OK\n    Note over Envoy,PodB: ì„œí‚· ë‹«í˜, íŠ¸ë˜í”½ ë³µêµ¬\n```\n\n---\n\n## mTLS: ì„œë¹„ìŠ¤ ê°„ ì•”í˜¸í™”\n\n### PeerAuthentication\n\n```yaml\n# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì „ì²´ì— mTLS ê°•ì œ\napiVersion: security.istio.io/v1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: my-app\nspec:\n  mtls:\n    mode: STRICT  # mTLS í•„ìˆ˜\n```\n\n| ëª¨ë“œ | ì„¤ëª… |\n|------|-----|\n| `DISABLE` | mTLS ë¹„í™œì„±í™” |\n| `PERMISSIVE` | mTLSì™€ í‰ë¬¸ ëª¨ë‘ í—ˆìš© (ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œ ìœ ìš©) |\n| `STRICT` | mTLSë§Œ í—ˆìš© |\n\n### AuthorizationPolicy (ì ‘ê·¼ ì œì–´)\n\n```yaml\napiVersion: security.istio.io/v1\nkind: AuthorizationPolicy\nmetadata:\n  name: api-access-policy\n  namespace: my-app\nspec:\n  selector:\n    matchLabels:\n      app: api-service\n  rules:\n  # frontendì—ì„œë§Œ ì ‘ê·¼ í—ˆìš©\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/my-app/sa/frontend\"]\n    to:\n    - operation:\n        methods: [\"GET\", \"POST\"]\n        paths: [\"/api/*\"]\n```\n\n---\n\n## Observability ì—°ë™\n\nIstioëŠ” ìë™ìœ¼ë¡œ ë©”íŠ¸ë¦­, ë¡œê·¸, íŠ¸ë ˆì´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n\n### Kiali: ì„œë¹„ìŠ¤ ë©”ì‹œ ì‹œê°í™”\n\n```bash\n# Kiali ì„¤ì¹˜\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.20/samples/addons/kiali.yaml\n\n# ëŒ€ì‹œë³´ë“œ ì ‘ê·¼\nkubectl port-forward -n istio-system svc/kiali 20001:20001\n```\n\n### Jaeger: ë¶„ì‚° ì¶”ì \n\n```bash\n# Jaeger ì„¤ì¹˜\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.20/samples/addons/jaeger.yaml\n\n# ëŒ€ì‹œë³´ë“œ ì ‘ê·¼\nkubectl port-forward -n istio-system svc/tracing 16686:80\n```\n\n```mermaid\nflowchart LR\n    subgraph Trace [ë¶„ì‚° ì¶”ì ]\n        A[Frontend<br/>trace-id: abc123] --> B[API Gateway<br/>span: 1]\n        B --> C[User Service<br/>span: 2]\n        B --> D[Order Service<br/>span: 3]\n        D --> E[Payment Service<br/>span: 4]\n    end\n```\n\n---\n\n## íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ\n\n### VirtualServiceê°€ ì ìš©ë˜ì§€ ì•ŠìŒ\n\n```bash\n# 1. Gatewayì™€ VirtualService ì—°ê²° í™•ì¸\nkubectl get vs my-vs -o yaml | grep gateways -A 5\n\n# 2. Proxy ì„¤ì • ë™ê¸°í™” í™•ì¸\nistioctl proxy-status\n\n# 3. Envoy ì„¤ì • í™•ì¸\nistioctl proxy-config routes deploy/my-app\n```\n\n**í”í•œ ì›ì¸**:\n\n1. Gateway ì´ë¦„/ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë¶ˆì¼ì¹˜\n2. hosts ì„¤ì • ë¶ˆì¼ì¹˜\n3. ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— `istio-injection` ë¼ë²¨ ëˆ„ë½\n\n### 503 Service Unavailable\n\n```bash\n# 1. ëŒ€ìƒ ì„œë¹„ìŠ¤ í™•ì¸\nkubectl get svc api-service\nkubectl get endpoints api-service\n\n# 2. DestinationRule subset í™•ì¸\nkubectl get dr api-service-dr -o yaml\n\n# 3. Pod ë¼ë²¨ í™•ì¸\nkubectl get pods --show-labels\n```\n\n**í”í•œ ì›ì¸**:\n\n1. DestinationRuleì˜ subset ë¼ë²¨ê³¼ Pod ë¼ë²¨ ë¶ˆì¼ì¹˜\n2. ì„œë¹„ìŠ¤ í¬íŠ¸ ì„¤ì • ì˜¤ë¥˜\n3. mTLS ëª¨ë“œ ë¶ˆì¼ì¹˜\n\n### mTLS ê´€ë ¨ ì—°ê²° ì‹¤íŒ¨\n\n```bash\n# 1. PeerAuthentication í™•ì¸\nkubectl get peerauthentication -A\n\n# 2. íŠ¹ì • ì›Œí¬ë¡œë“œì˜ mTLS ìƒíƒœ í™•ì¸\nistioctl authn tls-check deploy/my-app\n\n# 3. ì¸ì¦ì„œ ìƒíƒœ í™•ì¸\nistioctl proxy-config secret deploy/my-app -o json\n```\n\n---\n\n## ì •ë¦¬\n\n| êµ¬ì„±ìš”ì†Œ | ì—­í•  |\n|---------|-----|\n| **Gateway** | ì™¸ë¶€ íŠ¸ë˜í”½ ì§„ì…ì , í¬íŠ¸/í”„ë¡œí† ì½œ/TLS ì„¤ì • |\n| **VirtualService** | L7 ë¼ìš°íŒ… ê·œì¹™ (ê²½ë¡œ, í—¤ë”, ê°€ì¤‘ì¹˜) |\n| **DestinationRule** | íŠ¸ë˜í”½ ì •ì±… (LB, ì„œí‚· ë¸Œë ˆì´ì»¤, subset) |\n| **PeerAuthentication** | mTLS ëª¨ë“œ ì„¤ì • |\n| **AuthorizationPolicy** | ì ‘ê·¼ ì œì–´ (RBAC) |\n\n### ì™€ì¼ë“œì¹´ë“œ ì„œë¸Œë„ë©”ì¸ íŒ¨í„´ ìš”ì•½\n\n```\nRoute 53: *.example.com â†’ ALB\nACM: *.example.com ì™€ì¼ë“œì¹´ë“œ ì¸ì¦ì„œ\nALB â†’ Istio Ingress Gateway (TLS ì¢…ë£Œ)\nGateway: hosts: [\"*.example.com\"]\nVirtualService: ê° ì„œë¹„ìŠ¤ë³„ ì„œë¸Œë„ë©”ì¸ ë¼ìš°íŒ…\n```\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**5í¸: ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì‹¬í™”**ì—ì„œëŠ” ë‹¤ìŒì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- HPA v2: ë‹¤ì¤‘ ë©”íŠ¸ë¦­ê³¼ Scaling Behavior\n- VPA: ìˆ˜ì§ ìŠ¤ì¼€ì¼ë§ê³¼ In-place Resizing\n- Cluster Autoscaler vs Karpenter\n- KEDA: ì´ë²¤íŠ¸ ê¸°ë°˜ ì˜¤í† ìŠ¤ì¼€ì¼ë§\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Istio Documentation](https://istio.io/latest/docs/)\n- [Istio Traffic Management](https://istio.io/latest/docs/concepts/traffic-management/)\n- [Istio Security](https://istio.io/latest/docs/concepts/security/)\n- [AWS Load Balancer Controller + Istio](https://aws.amazon.com/blogs/containers/using-aws-load-balancer-controller-with-istio/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "Kubernetes",
      "Istio",
      "AWS"
    ],
    "readingTime": 9,
    "wordCount": 1740,
    "isFeatured": false,
    "isPublic": true,
    "series": "kubernetes",
    "date": "2026-01-03"
  },
  {
    "id": "k8s-03-config-secrets",
    "slug": "k8s-03-config-secrets",
    "path": "backend/kubernetes",
    "fullPath": "backend/kubernetes/k8s-03-config-secrets",
    "title": "Kubernetes ì‹¬í™” ì‹œë¦¬ì¦ˆ #3: ì„¤ì • ë° ì‹œí¬ë¦¿ ê´€ë¦¬ ì™„ì „ ê°€ì´ë“œ",
    "excerpt": "ConfigMapê³¼ Secretsì˜ ë‚´ë¶€ ë™ì‘ë¶€í„° AWS Secrets Manager CSI Driver í†µí•©ê¹Œì§€, í”„ë¡œë•ì…˜ í™˜ê²½ì˜ ì„¤ì • ê´€ë¦¬ ì „ëµì„ ê¹Šì´ ìˆê²Œ ë‹¤ë£¹ë‹ˆë‹¤.",
    "content": "# Kubernetes ì‹¬í™” ì‹œë¦¬ì¦ˆ #3: ì„¤ì • ë° ì‹œí¬ë¦¿ ê´€ë¦¬ ì™„ì „ ê°€ì´ë“œ\n\n## ì‹œë¦¬ì¦ˆ ê°œìš”\n\n| # | ì£¼ì œ | í•µì‹¬ ë‚´ìš© |\n|---|------|----------|\n| 1 | ì›Œí¬ë¡œë“œ ì»¨íŠ¸ë¡¤ëŸ¬ ì‹¬í™” | Deployment, StatefulSet, DaemonSet, CronJob |\n| 2 | ì„œë¹„ìŠ¤ ë„¤íŠ¸ì›Œí‚¹ ì‹¬í™” | Service íƒ€ì…, kube-proxy, AWS ALB/NLB |\n| **3** | **ì„¤ì • ë° ì‹œí¬ë¦¿ ê´€ë¦¬** | ConfigMap, Secrets, AWS Secrets Manager CSI Driver |\n| 4 | Istio ì„œë¹„ìŠ¤ ë©”ì‹œ | VirtualService, DestinationRule, ì™€ì¼ë“œì¹´ë“œ ì„œë¸Œë„ë©”ì¸ |\n| 5 | ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì‹¬í™” | HPA, VPA, Cluster Autoscaler, Karpenter, KEDA |\n| 6 | ë³´ì•ˆ ì‹¬í™” | RBAC, NetworkPolicy, Pod Security Standards |\n\n---\n\n## ConfigMap: ì„¤ì •ì˜ ì™¸ë¶€í™”\n\n### ì™œ ConfigMapì¸ê°€?\n\nì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ì— ì„¤ì •ì„ í•˜ë“œì½”ë”©í•˜ë©´:\n\n```\nâŒ í™˜ê²½ë³„ ì´ë¯¸ì§€ ë¹Œë“œ í•„ìš” (dev, staging, prod)\nâŒ ì„¤ì • ë³€ê²½ ì‹œ ì¬ë°°í¬ í•„ìš”\nâŒ ì„¤ì • ì¬ì‚¬ìš© ë¶ˆê°€\n```\n\nConfigMapìœ¼ë¡œ **ì„¤ì •ê³¼ ì½”ë“œë¥¼ ë¶„ë¦¬**í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph Before [í•˜ë“œì½”ë”©ëœ ì„¤ì •]\n        IMG1[ì´ë¯¸ì§€-dev]\n        IMG2[ì´ë¯¸ì§€-staging]\n        IMG3[ì´ë¯¸ì§€-prod]\n    end\n    \n    subgraph After [ConfigMap ì‚¬ìš©]\n        IMG[ë‹¨ì¼ ì´ë¯¸ì§€]\n        CM1[ConfigMap-dev]\n        CM2[ConfigMap-staging]\n        CM3[ConfigMap-prod]\n        IMG --> CM1 & CM2 & CM3\n    end\n```\n\n### ConfigMap ìƒì„± ë°©ë²•\n\n```yaml\n# ì§ì ‘ ì •ì˜\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  # í‚¤-ê°’ ìŒ\n  DATABASE_HOST: mysql.default.svc.cluster.local\n  DATABASE_PORT: \"3306\"\n  LOG_LEVEL: info\n  \n  # íŒŒì¼ ë‚´ìš©\n  nginx.conf: |\n    server {\n      listen 80;\n      location / {\n        proxy_pass http://backend:8080;\n      }\n    }\n```\n\n```bash\n# íŒŒì¼ì—ì„œ ìƒì„±\nkubectl create configmap nginx-config --from-file=nginx.conf\n\n# ë¦¬í„°ëŸ´ì—ì„œ ìƒì„±\nkubectl create configmap app-config \\\n  --from-literal=DATABASE_HOST=mysql \\\n  --from-literal=LOG_LEVEL=debug\n```\n\n### ConfigMap ì‚¬ìš© íŒ¨í„´\n\n#### 1. í™˜ê²½ë³€ìˆ˜ë¡œ ì£¼ì…\n\n```yaml\nspec:\n  containers:\n  - name: app\n    env:\n    # ê°œë³„ í‚¤ ì„ íƒ\n    - name: DB_HOST\n      valueFrom:\n        configMapKeyRef:\n          name: app-config\n          key: DATABASE_HOST\n    \n    # ì „ì²´ ConfigMapì„ í™˜ê²½ë³€ìˆ˜ë¡œ\n    envFrom:\n    - configMapRef:\n        name: app-config\n```\n\n#### 2. ë³¼ë¥¨ìœ¼ë¡œ ë§ˆìš´íŠ¸\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    volumeMounts:\n    - name: config-volume\n      mountPath: /etc/nginx/nginx.conf\n      subPath: nginx.conf  # íŠ¹ì • í‚¤ë§Œ íŒŒì¼ë¡œ\n      \n  volumes:\n  - name: config-volume\n    configMap:\n      name: nginx-config\n```\n\n### ë¶ˆë³€(Immutable) ConfigMap\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\nimmutable: true  # K8s 1.21+\ndata:\n  LOG_LEVEL: info\n```\n\n**ì¥ì **:\n\n- API Server ë¶€í•˜ ê°ì†Œ (watch ì œê±°)\n- ì‹¤ìˆ˜ë¡œ ì¸í•œ ë³€ê²½ ë°©ì§€\n\n**ë‹¨ì **:\n\n- ë³€ê²½í•˜ë ¤ë©´ ìƒˆ ConfigMap ìƒì„± í•„ìš”\n- Pod ì¬ë°°í¬ í•„ìš”\n\n---\n\n## Secrets: ë¯¼ê° ì •ë³´ ê´€ë¦¬\n\n### ConfigMap vs Secrets\n\n| íŠ¹ì„± | ConfigMap | Secrets |\n|------|-----------|---------|\n| ìš©ë„ | ì¼ë°˜ ì„¤ì • | ë¯¼ê° ì •ë³´ |\n| ì €ì¥ | í‰ë¬¸ | Base64 ì¸ì½”ë”© |\n| í¬ê¸° ì œí•œ | 1MB | 1MB |\n| etcd ì €ì¥ | í‰ë¬¸ | ì•”í˜¸í™” ê°€ëŠ¥ (ë³„ë„ ì„¤ì •) |\n| kubectl ì¶œë ¥ | ê·¸ëŒ€ë¡œ í‘œì‹œ | ê¸°ë³¸ì ìœ¼ë¡œ ìˆ¨ê¹€ |\n\n> [!CAUTION]\n> Base64ëŠ” **ì¸ì½”ë”©**ì´ì§€ **ì•”í˜¸í™”**ê°€ ì•„ë‹™ë‹ˆë‹¤! ëˆ„êµ¬ë‚˜ ì‰½ê²Œ ë””ì½”ë”©í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë°˜ë“œì‹œ **Secrets at Rest Encryption**ì„ í™œì„±í™”í•˜ì„¸ìš”.\n\n### Secret íƒ€ì…\n\n```yaml\n# Opaque (ê¸°ë³¸ê°’) - ì¼ë°˜ ì‹œí¬ë¦¿\napiVersion: v1\nkind: Secret\nmetadata:\n  name: db-credentials\ntype: Opaque\ndata:\n  username: YWRtaW4=      # echo -n 'admin' | base64\n  password: cGFzc3dvcmQ=  # echo -n 'password' | base64\n\n# ë˜ëŠ” stringData ì‚¬ìš© (ìë™ ì¸ì½”ë”©)\nstringData:\n  username: admin\n  password: password\n```\n\n```yaml\n# TLS ì¸ì¦ì„œ\napiVersion: v1\nkind: Secret\nmetadata:\n  name: tls-secret\ntype: kubernetes.io/tls\ndata:\n  tls.crt: <base64-encoded-cert>\n  tls.key: <base64-encoded-key>\n```\n\n```yaml\n# Docker Registry ì¸ì¦\napiVersion: v1\nkind: Secret\nmetadata:\n  name: regcred\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: <base64-encoded-docker-config>\n```\n\n### Secrets ì‚¬ìš© íŒ¨í„´\n\n```yaml\nspec:\n  containers:\n  - name: app\n    # í™˜ê²½ë³€ìˆ˜ë¡œ ì£¼ì…\n    env:\n    - name: DB_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: db-credentials\n          key: password\n    \n    # ë³¼ë¥¨ìœ¼ë¡œ ë§ˆìš´íŠ¸ (íŒŒì¼)\n    volumeMounts:\n    - name: secrets-volume\n      mountPath: /etc/secrets\n      readOnly: true\n      \n  volumes:\n  - name: secrets-volume\n    secret:\n      secretName: db-credentials\n```\n\n### ServiceAccount Token ìë™ ë§ˆìš´íŠ¸ ë¹„í™œì„±í™”\n\nëª¨ë“  Podì— ServiceAccount í† í°ì´ ìë™ ë§ˆìš´íŠ¸ë©ë‹ˆë‹¤. í•„ìš” ì—†ë‹¤ë©´ ë¹„í™œì„±í™”í•˜ì„¸ìš”.\n\n```yaml\nspec:\n  automountServiceAccountToken: false\n  containers:\n  - name: app\n```\n\n---\n\n## Secrets at Rest Encryption\n\netcdì— ì €ì¥ëœ Secretsë¥¼ ì•”í˜¸í™”í•©ë‹ˆë‹¤.\n\n### EncryptionConfiguration\n\n```yaml\n# /etc/kubernetes/encryption-config.yaml\napiVersion: apiserver.config.k8s.io/v1\nkind: EncryptionConfiguration\nresources:\n  - resources:\n      - secrets\n    providers:\n      # AWS KMS ì‚¬ìš©\n      - kms:\n          apiVersion: v2\n          name: aws-encryption-provider\n          endpoint: unix:///var/run/kmsplugin/socket.sock\n          cachesize: 1000\n          timeout: 3s\n      # í´ë°±: identity (ì•”í˜¸í™” ì•ˆ í•¨)\n      - identity: {}\n```\n\n### EKSì—ì„œ Envelope Encryption\n\nEKSëŠ” **AWS KMS**ë¥¼ ì‚¬ìš©í•œ Envelope Encryptionì„ ì§€ì›í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph K8s [Kubernetes]\n        Secret[Secret ë°ì´í„°]\n        DEK[Data Encryption Key<br/>ëœë¤ ìƒì„±]\n    end\n    \n    subgraph KMS [AWS KMS]\n        CMK[Customer Master Key]\n    end\n    \n    subgraph etcd [etcd]\n        Encrypted[ì•”í˜¸í™”ëœ Secret +<br/>ì•”í˜¸í™”ëœ DEK]\n    end\n    \n    Secret -->|DEKë¡œ ì•”í˜¸í™”| Encrypted\n    DEK -->|CMKë¡œ ì•”í˜¸í™”| Encrypted\n    CMK -.->|ë³µí˜¸í™” ì‹œ ì‚¬ìš©| DEK\n```\n\n```bash\n# EKS í´ëŸ¬ìŠ¤í„°ì— KMS í‚¤ ì—°ê²°\naws eks associate-encryption-config \\\n  --cluster-name my-cluster \\\n  --encryption-config '[{\n    \"resources\": [\"secrets\"],\n    \"provider\": {\n      \"keyArn\": \"arn:aws:kms:ap-northeast-2:123456789012:key/12345678-1234-1234-1234-123456789012\"\n    }\n  }]'\n```\n\n---\n\n## AWS Secrets Manager + CSI Driver\n\n### ë¬¸ì œ: Kubernetes Secretsì˜ í•œê³„\n\n```\nâŒ ì‹œí¬ë¦¿ì´ etcdì— ì €ì¥ë¨ (í´ëŸ¬ìŠ¤í„° ì¹¨í•´ ì‹œ ë…¸ì¶œ)\nâŒ ì‹œí¬ë¦¿ ë¡œí…Œì´ì…˜ ì–´ë ¤ì›€\nâŒ ê°ì‚¬(Audit) ê¸°ëŠ¥ ì œí•œì \nâŒ ë²„ì „ ê´€ë¦¬ ì—†ìŒ\n```\n\n### í•´ê²°: External Secrets Store\n\n```mermaid\nflowchart TB\n    subgraph AWS [AWS]\n        SM[\"Secrets Manager\"]\n    end\n    \n    subgraph K8s [\"Kubernetes í´ëŸ¬ìŠ¤í„°\"]\n        CSI[\"Secrets Store CSI Driver\"]\n        ASCP[\"AWS Secrets & Config Provider\"]\n        Pod[\"Application Pod\"]\n        \n        subgraph Mount [\"Pod ë‚´ë¶€\"]\n            File[\"íŒŒì¼: db-password\"]\n            Env[\"ENV: DB_PASSWORD\"]\n        end\n    end\n    \n    SM --> ASCP --> CSI --> Pod\n    Pod --> Mount\n```\n\n### ì„¤ì¹˜\n\n```bash\n# Secrets Store CSI Driver ì„¤ì¹˜\nhelm repo add secrets-store-csi-driver \\\n  https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts\n\nhelm install csi-secrets-store \\\n  secrets-store-csi-driver/secrets-store-csi-driver \\\n  --namespace kube-system \\\n  --set syncSecret.enabled=true \\\n  --set enableSecretRotation=true\n\n# AWS Provider ì„¤ì¹˜\nkubectl apply -f https://raw.githubusercontent.com/aws/secrets-store-csi-driver-provider-aws/main/deployment/aws-provider-installer.yaml\n```\n\n### IRSA (IAM Roles for Service Accounts) ì„¤ì •\n\n```bash\n# OIDC Provider ì—°ê²° (EKS í´ëŸ¬ìŠ¤í„° ìƒì„± ì‹œ ìë™)\neksctl utils associate-iam-oidc-provider \\\n  --cluster my-cluster \\\n  --approve\n\n# ServiceAccountìš© IAM Role ìƒì„±\neksctl create iamserviceaccount \\\n  --cluster my-cluster \\\n  --namespace default \\\n  --name my-app-sa \\\n  --attach-policy-arn arn:aws:iam::123456789012:policy/SecretsManagerReadPolicy \\\n  --approve\n```\n\n### SecretProviderClass ì •ì˜\n\n```yaml\napiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n  name: aws-secrets\nspec:\n  provider: aws\n  parameters:\n    objects: |\n      - objectName: \"prod/myapp/db-credentials\"\n        objectType: \"secretsmanager\"\n        jmesPath:\n          - path: username\n            objectAlias: db-username\n          - path: password\n            objectAlias: db-password\n      - objectName: \"prod/myapp/api-key\"\n        objectType: \"secretsmanager\"\n        \n  # Kubernetes Secretìœ¼ë¡œë„ ë™ê¸°í™” (optional)\n  secretObjects:\n  - secretName: db-credentials-k8s\n    type: Opaque\n    data:\n    - objectName: db-username\n      key: username\n    - objectName: db-password\n      key: password\n```\n\n### Podì—ì„œ ì‚¬ìš©\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: my-app\nspec:\n  serviceAccountName: my-app-sa  # IRSA ì—°ê²°ëœ SA\n  \n  containers:\n  - name: app\n    image: my-app:latest\n    \n    # íŒŒì¼ë¡œ ë§ˆìš´íŠ¸ëœ ì‹œí¬ë¦¿ ê²½ë¡œ\n    volumeMounts:\n    - name: secrets\n      mountPath: /mnt/secrets\n      readOnly: true\n    \n    # í™˜ê²½ë³€ìˆ˜ë¡œë„ ì‚¬ìš© ê°€ëŠ¥ (secretObjects ì‚¬ìš© ì‹œ)\n    env:\n    - name: DB_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: db-credentials-k8s\n          key: password\n          \n  volumes:\n  - name: secrets\n    csi:\n      driver: secrets-store.csi.k8s.io\n      readOnly: true\n      volumeAttributes:\n        secretProviderClass: aws-secrets\n```\n\n### ì‹œí¬ë¦¿ ë¡œí…Œì´ì…˜\n\n```yaml\n# Secrets Store CSI Driver ì„¤ì¹˜ ì‹œ ì˜µì…˜\nhelm upgrade csi-secrets-store \\\n  secrets-store-csi-driver/secrets-store-csi-driver \\\n  --namespace kube-system \\\n  --set enableSecretRotation=true \\\n  --set rotationPollInterval=2m  # 2ë¶„ë§ˆë‹¤ ì²´í¬\n```\n\n```mermaid\nsequenceDiagram\n    participant SM as AWS Secrets Manager\n    participant CSI as CSI Driver\n    participant Vol as Volume (íŒŒì¼)\n    participant App as Application\n    \n    Note over SM: ì‹œí¬ë¦¿ ë¡œí…Œì´ì…˜ ë°œìƒ\n    SM->>SM: ìƒˆ ë²„ì „ ìƒì„±\n    \n    loop ë§¤ 2ë¶„\n        CSI->>SM: í˜„ì¬ ì‹œí¬ë¦¿ ì¡°íšŒ\n        SM-->>CSI: ìƒˆ ê°’ ë°˜í™˜\n        CSI->>Vol: íŒŒì¼ ì—…ë°ì´íŠ¸\n    end\n    \n    App->>Vol: íŒŒì¼ ë‹¤ì‹œ ì½ê¸°\n    Note over App: ìƒˆ ì‹œí¬ë¦¿ ì‚¬ìš©\n```\n\n> [!TIP]\n> ì• í”Œë¦¬ì¼€ì´ì…˜ì´ íŒŒì¼ ë³€ê²½ì„ ê°ì§€í•˜ê±°ë‚˜ ì£¼ê¸°ì ìœ¼ë¡œ íŒŒì¼ì„ ë‹¤ì‹œ ì½ë„ë¡ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ë¡œ ë™ê¸°í™”í•˜ëŠ” ê²½ìš°ì—ëŠ” Pod ì¬ì‹œì‘ì´ í•„ìš”í•©ë‹ˆë‹¤.\n\n---\n\n## External Secrets Operator (ëŒ€ì•ˆ)\n\nCSI Driverì˜ ëŒ€ì•ˆìœ¼ë¡œ, ì™¸ë¶€ ì‹œí¬ë¦¿ ì €ì¥ì†Œì˜ ê°’ì„ **Kubernetes Secretìœ¼ë¡œ ì§ì ‘ ë™ê¸°í™”**í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph AWS [AWS]\n        SM[Secrets Manager]\n    end\n    \n    subgraph K8s [Kubernetes]\n        ESO[External Secrets Operator]\n        ES[ExternalSecret CR]\n        Secret[Kubernetes Secret]\n        Pod[Pod]\n    end\n    \n    ES --> ESO\n    ESO --> SM\n    SM --> ESO --> Secret --> Pod\n```\n\n### CSI Driver vs External Secrets Operator\n\n| íŠ¹ì„± | CSI Driver | External Secrets Operator |\n|------|-----------|--------------------------|\n| ì‹œí¬ë¦¿ ì €ì¥ ìœ„ì¹˜ | Pod ë¡œì»¬ ë³¼ë¥¨ (tmpfs) | Kubernetes Secret (etcd) |\n| ì„¤ì¹˜ ë³µì¡ë„ | DaemonSet + Provider | Deployment |\n| GitOps ì¹œí™”ì„± | ë‚®ìŒ | ë†’ìŒ (ExternalSecret CRD) |\n| etcdì— ì €ì¥ | ì•ˆ ë¨ | ë¨ |\n| í˜¸í™˜ì„± | CSI ì§€ì› í•„ìš” | ëª¨ë“  í™˜ê²½ |\n\n---\n\n## ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤\n\n### 1. í™˜ê²½ë³„ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë¶„ë¦¬\n\n```yaml\n# dev ë„¤ì„ìŠ¤í˜ì´ìŠ¤\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\n  namespace: dev\ndata:\n  LOG_LEVEL: debug\n---\n# prod ë„¤ì„ìŠ¤í˜ì´ìŠ¤\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\n  namespace: prod\ndata:\n  LOG_LEVEL: warn\n```\n\n### 2. RBACë¡œ ì‹œí¬ë¦¿ ì ‘ê·¼ ì œí•œ\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: secret-reader\n  namespace: prod\nrules:\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  resourceNames: [\"db-credentials\"]  # íŠ¹ì • ì‹œí¬ë¦¿ë§Œ\n  verbs: [\"get\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: app-secret-binding\n  namespace: prod\nsubjects:\n- kind: ServiceAccount\n  name: my-app-sa\n  namespace: prod\nroleRef:\n  kind: Role\n  name: secret-reader\n  apiGroup: rbac.authorization.k8s.io\n```\n\n### 3. ê°ì‚¬ ë¡œê¹… í™œì„±í™”\n\n```yaml\n# Audit Policy\napiVersion: audit.k8s.io/v1\nkind: Policy\nrules:\n- level: Metadata\n  resources:\n  - group: \"\"\n    resources: [\"secrets\"]\n```\n\n### 4. ì‹œí¬ë¦¿ ê°’ ë³€ê²½ ì‹œ Pod ì¬ì‹œì‘\n\n```yaml\n# Deployment í…œí”Œë¦¿ì— checksum ì¶”ê°€\nspec:\n  template:\n    metadata:\n      annotations:\n        checksum/secret: {{ include (print $.Template.BasePath \"/secret.yaml\") . | sha256sum }}\n```\n\n---\n\n## íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ\n\n### CSI Driver Podê°€ ì‹œí¬ë¦¿ì„ ì½ì§€ ëª»í•¨\n\n```bash\n# 1. Provider Pod ë¡œê·¸ í™•ì¸\nkubectl logs -n kube-system -l app=csi-secrets-store-provider-aws\n\n# 2. IRSA ì„¤ì • í™•ì¸\nkubectl describe sa my-app-sa\n\n# 3. IAM Roleì˜ Trust Policy í™•ì¸\naws iam get-role --role-name MyAppRole --query 'Role.AssumeRolePolicyDocument'\n```\n\n**í”í•œ ì›ì¸**:\n\n1. OIDC Provider ë¯¸ì„¤ì •\n2. ServiceAccountì™€ IAM Role ì—°ê²° ëˆ„ë½\n3. Secrets Manager ì ‘ê·¼ ê¶Œí•œ ëˆ„ë½\n\n### ConfigMap ë³€ê²½ì´ Podì— ë°˜ì˜ë˜ì§€ ì•ŠìŒ\n\n```bash\n# 1. í™˜ê²½ë³€ìˆ˜ë¡œ ì£¼ì…í•œ ê²½ìš°: Pod ì¬ì‹œì‘ í•„ìš”\nkubectl rollout restart deployment/my-app\n\n# 2. ë³¼ë¥¨ìœ¼ë¡œ ë§ˆìš´íŠ¸í•œ ê²½ìš°: kubelet sync ëŒ€ê¸° (ê¸°ë³¸ 1ë¶„)\n# ì¦‰ì‹œ ë°˜ì˜í•˜ë ¤ë©´:\nkubectl exec -it my-pod -- cat /etc/config/my-key\n```\n\n> [!IMPORTANT]\n> ë³¼ë¥¨ ë§ˆìš´íŠ¸ ì‹œ íŒŒì¼ì€ ìë™ ì—…ë°ì´íŠ¸ë˜ì§€ë§Œ, ì• í”Œë¦¬ì¼€ì´ì…˜ì´ íŒŒì¼ì„ ìºì‹±í•˜ê³  ìˆë‹¤ë©´ ì¬ì‹œì‘ì´ í•„ìš”í•©ë‹ˆë‹¤.\n\n---\n\n## ì •ë¦¬\n\n| êµ¬ì„±ìš”ì†Œ | ìš©ë„ | ë³´ì•ˆ ìˆ˜ì¤€ |\n|---------|-----|---------|\n| **ConfigMap** | ì¼ë°˜ ì„¤ì • | í‰ë¬¸ |\n| **Secrets** | ë¯¼ê° ì •ë³´ (etcd) | Base64 + (ì•”í˜¸í™” ê°€ëŠ¥) |\n| **Secrets at Rest** | etcd ì•”í˜¸í™” | KMS ì•”í˜¸í™” |\n| **CSI Driver** | ì™¸ë¶€ ì €ì¥ì†Œ ì—°ë™ | ì €ì¥ì†Œ ì™¸ë¶€í™” |\n| **External Secrets** | K8s Secret ë™ê¸°í™” | GitOps ì¹œí™”ì  |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**4í¸: Istio ì„œë¹„ìŠ¤ ë©”ì‹œ**ì—ì„œëŠ” ë‹¤ìŒì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- Istio ì•„í‚¤í…ì²˜ (istiod, Envoy Sidecar)\n- VirtualServiceì™€ DestinationRule\n- Gateway + ì™€ì¼ë“œì¹´ë“œ ì„œë¸Œë„ë©”ì¸ + Route 53 í†µí•©\n- mTLSì™€ ë³´ì•ˆ ì •ì±…\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Kubernetes ConfigMaps](https://kubernetes.io/docs/concepts/configuration/configmap/)\n- [Kubernetes Secrets](https://kubernetes.io/docs/concepts/configuration/secret/)\n- [Secrets Store CSI Driver](https://secrets-store-csi-driver.sigs.k8s.io/)\n- [AWS Secrets Manager Provider](https://github.com/aws/secrets-store-csi-driver-provider-aws)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "Kubernetes",
      "AWS",
      "Security"
    ],
    "readingTime": 9,
    "wordCount": 1627,
    "isFeatured": false,
    "isPublic": true,
    "series": "kubernetes",
    "date": "2026-01-03"
  },
  {
    "id": "k8s-02-service-networking",
    "slug": "k8s-02-service-networking",
    "path": "backend/kubernetes",
    "fullPath": "backend/kubernetes/k8s-02-service-networking",
    "title": "Kubernetes ì‹¬í™” ì‹œë¦¬ì¦ˆ #2: ì„œë¹„ìŠ¤ ë„¤íŠ¸ì›Œí‚¹ì˜ ë‚´ë¶€ ë™ì‘ ì›ë¦¬",
    "excerpt": "ClusterIP, NodePort, LoadBalancerì˜ ë‚´ë¶€ ë™ì‘ ì›ë¦¬ë¥¼ kube-proxy(iptables/IPVS), AWS ALB/NLB í†µí•©ê³¼ í•¨ê»˜ ê¹Šì´ ìˆê²Œ ì´í•´í•©ë‹ˆë‹¤.",
    "content": "# Kubernetes ì‹¬í™” ì‹œë¦¬ì¦ˆ #2: ì„œë¹„ìŠ¤ ë„¤íŠ¸ì›Œí‚¹ì˜ ë‚´ë¶€ ë™ì‘ ì›ë¦¬\n\n## ì‹œë¦¬ì¦ˆ ê°œìš”\n\n| # | ì£¼ì œ | í•µì‹¬ ë‚´ìš© |\n|---|------|----------|\n| 1 | ì›Œí¬ë¡œë“œ ì»¨íŠ¸ë¡¤ëŸ¬ ì‹¬í™” | Deployment, StatefulSet, DaemonSet, CronJob |\n| **2** | **ì„œë¹„ìŠ¤ ë„¤íŠ¸ì›Œí‚¹ ì‹¬í™”** | Service íƒ€ì…, kube-proxy, AWS ALB/NLB |\n| 3 | ì„¤ì • ë° ì‹œí¬ë¦¿ ê´€ë¦¬ | ConfigMap, Secrets, AWS Secrets Manager CSI Driver |\n| 4 | Istio ì„œë¹„ìŠ¤ ë©”ì‹œ | VirtualService, DestinationRule, ì™€ì¼ë“œì¹´ë“œ ì„œë¸Œë„ë©”ì¸ |\n| 5 | ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì‹¬í™” | HPA, VPA, Cluster Autoscaler, Karpenter, KEDA |\n| 6 | ë³´ì•ˆ ì‹¬í™” | RBAC, NetworkPolicy, Pod Security Standards |\n\n---\n\n## Serviceì˜ ë³¸ì§ˆ: Pod ì¶”ìƒí™”\n\nPodëŠ” ìƒì„±ë  ë•Œë§ˆë‹¤ **IPê°€ ë°”ë€ë‹ˆë‹¤**. ServiceëŠ” ì´ ë³€í™”í•˜ëŠ” Podë“¤ ì•ì— **ì•ˆì •ì ì¸ ì—”ë“œí¬ì¸íŠ¸**ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph Before [Pod IP ì§ì ‘ ì‚¬ìš©]\n        Client1[í´ë¼ì´ì–¸íŠ¸]\n        P1[Pod 10.0.1.5]\n        P2[Pod 10.0.1.6]\n        Client1 --> P1\n        Client1 -.->|Pod ì¬ì‹œì‘<br/>IP ë³€ê²½!| P2\n    end\n    \n    subgraph After [Service ì‚¬ìš©]\n        Client2[í´ë¼ì´ì–¸íŠ¸]\n        SVC[Service<br/>10.96.0.100]\n        P3[Pod 10.0.1.5]\n        P4[Pod 10.0.1.6]\n        P5[Pod 10.0.1.7]\n        Client2 --> SVC --> P3 & P4 & P5\n    end\n```\n\n---\n\n## Service íƒ€ì…ë³„ ë™ì‘ ì›ë¦¬\n\n### ClusterIP (ê¸°ë³¸ê°’): í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ í†µì‹ \n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: backend-api\nspec:\n  type: ClusterIP  # ê¸°ë³¸ê°’, ìƒëµ ê°€ëŠ¥\n  selector:\n    app: backend\n  ports:\n  - port: 80         # Service í¬íŠ¸\n    targetPort: 8080 # Pod í¬íŠ¸\n```\n\n**ë™ì‘ ì›ë¦¬**:\n\n1. Service ìƒì„± ì‹œ **ClusterIP** í• ë‹¹ (ì˜ˆ: 10.96.0.100)\n2. **Endpoints** ê°ì²´ ìë™ ìƒì„± (selectorì™€ ì¼ì¹˜í•˜ëŠ” Pod IP ëª©ë¡)\n3. kube-proxyê°€ **iptables/IPVS ê·œì¹™** ìƒì„±\n4. ClusterIPë¡œ ì˜¤ëŠ” íŠ¸ë˜í”½ì„ Pod IPë¡œ ë¡œë“œë°¸ëŸ°ì‹±\n\n```bash\n# Endpoints í™•ì¸ (ì‹¤ì œ Pod IP ëª©ë¡)\nkubectl get endpoints backend-api\n\n# ì¶œë ¥ ì˜ˆì‹œ:\n# NAME          ENDPOINTS                                   AGE\n# backend-api   10.0.1.5:8080,10.0.1.6:8080,10.0.1.7:8080   1h\n```\n\n### NodePort: ì™¸ë¶€ì—ì„œ ë…¸ë“œ IPë¡œ ì ‘ê·¼\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: backend-api\nspec:\n  type: NodePort\n  selector:\n    app: backend\n  ports:\n  - port: 80\n    targetPort: 8080\n    nodePort: 30080  # 30000-32767 ë²”ìœ„, ìƒëµ ì‹œ ìë™ í• ë‹¹\n```\n\n```mermaid\nflowchart LR\n    subgraph External [ì™¸ë¶€ ë„¤íŠ¸ì›Œí¬]\n        Client[í´ë¼ì´ì–¸íŠ¸]\n    end\n    \n    subgraph Cluster [Kubernetes í´ëŸ¬ìŠ¤í„°]\n        subgraph Node1 [Node 1 - 192.168.1.10]\n            NP1[NodePort :30080]\n            P1[Pod :8080]\n        end\n        \n        subgraph Node2 [Node 2 - 192.168.1.11]\n            NP2[NodePort :30080]\n            P2[Pod :8080]\n        end\n        \n        subgraph Node3 [Node 3 - 192.168.1.12]\n            NP3[NodePort :30080]\n        end\n    end\n    \n    Client -->|192.168.1.10:30080| NP1\n    Client -->|192.168.1.11:30080| NP2\n    Client -->|192.168.1.12:30080| NP3\n    \n    NP1 --> P1 & P2\n    NP2 --> P1 & P2\n    NP3 --> P1 & P2\n```\n\n> [!IMPORTANT]\n> NodePortëŠ” **ëª¨ë“  ë…¸ë“œ**ì—ì„œ ì—´ë¦½ë‹ˆë‹¤. Node3ì— Podê°€ ì—†ì–´ë„ 30080 í¬íŠ¸ë¡œ ì ‘ê·¼í•˜ë©´ ë‹¤ë¥¸ ë…¸ë“œì˜ Podë¡œ íŠ¸ë˜í”½ì´ ì „ë‹¬ë©ë‹ˆë‹¤.\n\n### externalTrafficPolicy: íŠ¸ë˜í”½ ê²½ë¡œ ì œì–´\n\n```yaml\nspec:\n  type: NodePort\n  externalTrafficPolicy: Local  # ê¸°ë³¸ê°’: Cluster\n```\n\n| ì •ì±… | ë™ì‘ | ì¥ì  | ë‹¨ì  |\n|------|-----|------|------|\n| `Cluster` | ë‹¤ë¥¸ ë…¸ë“œ Podë¡œë„ ì „ë‹¬ | ê· ë“±í•œ ë¡œë“œë°¸ëŸ°ì‹± | ì¶”ê°€ í™‰, í´ë¼ì´ì–¸íŠ¸ IP ì†ì‹¤ |\n| `Local` | í•´ë‹¹ ë…¸ë“œì˜ Podë¡œë§Œ ì „ë‹¬ | í´ë¼ì´ì–¸íŠ¸ IP ë³´ì¡´, ë‚®ì€ ì§€ì—° | ë¶ˆê· ë“± ë¶„ë°° ê°€ëŠ¥ |\n\n```mermaid\nflowchart TB\n    subgraph Cluster_Policy [externalTrafficPolicy: Cluster]\n        C1[Client â†’ Node1:30080]\n        N1_1[Node1]\n        N2_1[Node2]\n        P1_1[Pod on Node2]\n        C1 --> N1_1 -->|SNAT| N2_1 --> P1_1\n        Note1[í´ë¼ì´ì–¸íŠ¸ IPê°€ Node1 IPë¡œ ë³€ê²½ë¨]\n    end\n    \n    subgraph Local_Policy [externalTrafficPolicy: Local]\n        C2[Client â†’ Node2:30080]\n        N2_2[Node2]\n        P1_2[Pod on Node2]\n        C2 --> N2_2 --> P1_2\n        Note2[í´ë¼ì´ì–¸íŠ¸ IP ë³´ì¡´ë¨]\n    end\n```\n\n### LoadBalancer: í´ë¼ìš°ë“œ ë¡œë“œë°¸ëŸ°ì„œ í†µí•©\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: backend-api\n  annotations:\n    # AWS NLB ì‚¬ìš© (ê¸°ë³¸ì€ CLB)\n    service.beta.kubernetes.io/aws-load-balancer-type: nlb\n    # ì¸í„°ë„ LB\n    service.beta.kubernetes.io/aws-load-balancer-internal: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: backend\n  ports:\n  - port: 80\n    targetPort: 8080\n```\n\n```mermaid\nflowchart LR\n    subgraph AWS [AWS]\n        Internet[ì¸í„°ë„·]\n        NLB[Network Load Balancer<br/>abc123.elb.amazonaws.com]\n    end\n    \n    subgraph K8s [Kubernetes í´ëŸ¬ìŠ¤í„°]\n        subgraph Node1 [Node 1]\n            NP1[NodePort :30080]\n            P1[Pod]\n        end\n        subgraph Node2 [Node 2]\n            NP2[NodePort :30080]\n            P2[Pod]\n        end\n    end\n    \n    Internet --> NLB --> NP1 & NP2\n    NP1 --> P1\n    NP2 --> P2\n```\n\n**LoadBalancer = NodePort + í´ë¼ìš°ë“œ LB ìë™ í”„ë¡œë¹„ì €ë‹**\n\n### ExternalName: ì™¸ë¶€ ì„œë¹„ìŠ¤ CNAME\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: external-db\nspec:\n  type: ExternalName\n  externalName: mydb.us-east-1.rds.amazonaws.com\n```\n\ní´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œ `external-db`ë¡œ DNS ì¡°íšŒí•˜ë©´ RDS ì£¼ì†Œê°€ ë°˜í™˜ë©ë‹ˆë‹¤.\n\n```bash\n# í´ëŸ¬ìŠ¤í„° í¬ë“œ ë‚´ì—ì„œ\nnslookup external-db.default.svc.cluster.local\n# â†’ mydb.us-east-1.rds.amazonaws.com\n```\n\n---\n\n## kube-proxy: Service êµ¬í˜„ì²´\n\n### ëª¨ë“œ ë¹„êµ: iptables vs IPVS vs nftables\n\n```mermaid\nflowchart TB\n    subgraph IptablesMode [\"iptables ëª¨ë“œ\"]\n        IT[\"iptables ê·œì¹™\"]\n        Rule1[\"ê·œì¹™ 1\"]\n        Rule2[\"ê·œì¹™ 2\"]\n        RuleN[\"ê·œì¹™ N\"]\n        Target[\"Pod\"]\n        IT --> Rule1 --> Rule2 --> RuleN --> Target\n    end\n    \n    subgraph IPVSMode [\"IPVS ëª¨ë“œ\"]\n        IV[\"IPVS í•´ì‹œ í…Œì´ë¸”\"]\n        Target2[\"Pod\"]\n        IV --> Target2\n    end\n    \n    subgraph NftablesMode [\"nftables ëª¨ë“œ (ê¶Œì¥)\"]\n        NFT[\"nftables ê·œì¹™\"]\n        Target3[\"Pod\"]\n        NFT --> Target3\n    end\n```\n\n| íŠ¹ì„± | iptables | IPVS | **nftables** |\n|------|---------|------|-------------|\n| **ì¡°íšŒ ì‹œê°„** | O(n) | O(1) | O(1) |\n| **ê¶Œì¥ ì—¬ë¶€** | ì•ˆì •ì  | âš ï¸ Deprecated ì˜ˆì • | âœ… **ê¶Œì¥** |\n| **ë¡œë“œë°¸ëŸ°ì‹±** | ëœë¤ | rr, lc, dh ë“± | ë‹¤ì–‘ |\n| **ì„±ëŠ¥** | ê°œì„ ë¨ | ì–‘í˜¸ | **ìµœê³ ** |\n| **ìµœì†Œ ë²„ì „** | ëª¨ë“  ë²„ì „ | - | K8s 1.31+ (1.33 Stable) |\n\n> [!WARNING]\n> **IPVS ëª¨ë“œ Deprecated ì˜ˆì •**: Kubernetes ê³µì‹ ë¬¸ì„œì— ë”°ë¥´ë©´ IPVS ëª¨ë“œëŠ” Kubernetes Services APIì™€ì˜ ë¶ˆì¼ì¹˜ë¡œ ì¸í•´ í–¥í›„ deprecatedë  ì˜ˆì •ì…ë‹ˆë‹¤. ìƒˆë¡œìš´ í´ëŸ¬ìŠ¤í„°ì—ì„œëŠ” **nftables ëª¨ë“œ**(K8s 1.33+ stable) ë˜ëŠ” **iptables ëª¨ë“œ**ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.\n\n### kube-proxy ëª¨ë“œ ì„¤ì •\n\n```yaml\n# nftables ëª¨ë“œ (K8s 1.31+, ê¶Œì¥)\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kube-proxy\n  namespace: kube-system\ndata:\n  config.conf: |\n    mode: \"nftables\"\n```\n\n```yaml\n# IPVS ëª¨ë“œ (âš ï¸ deprecated ì˜ˆì •)\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kube-proxy\n  namespace: kube-system\ndata:\n  config.conf: |\n    mode: \"ipvs\"\n    ipvs:\n      scheduler: \"rr\"  # round-robin\n```\n\n---\n\n## AWS Load Balancer Controller\n\n### ê°œìš”\n\nAWS Load Balancer ControllerëŠ” Ingressì™€ Serviceë¥¼ **ALB/NLB**ë¡œ í”„ë¡œë¹„ì €ë‹í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph K8s [Kubernetes]\n        Ingress[Ingress ë¦¬ì†ŒìŠ¤]\n        SVC[Service type: LoadBalancer]\n        CTRL[AWS Load Balancer Controller]\n    end\n    \n    subgraph AWS [AWS]\n        ALB[Application Load Balancer]\n        NLB[Network Load Balancer]\n        TG[Target Group]\n        R53[Route 53]\n    end\n    \n    Ingress --> CTRL\n    SVC --> CTRL\n    CTRL --> ALB & NLB\n    ALB & NLB --> TG\n    TG --> |Pod IP ì§ì ‘ ë“±ë¡| Pods[Pods]\n```\n\n### Ingressë¡œ ALB ìƒì„±\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-ingress\n  annotations:\n    # ALB Ingress Controller\n    kubernetes.io/ingress.class: alb\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/target-type: ip  # Pod IP ì§ì ‘ ì—°ê²°\n    \n    # SSL/TLS\n    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:...\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTPS\":443}]'\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\n    \n    # í—¬ìŠ¤ì²´í¬\n    alb.ingress.kubernetes.io/healthcheck-path: /health\n    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '15'\nspec:\n  rules:\n  - host: api.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: backend-api\n            port:\n              number: 80\n```\n\n### target-type: ip vs instance\n\n```mermaid\nflowchart LR\n    subgraph Instance [target-type: instance]\n        ALB1[ALB] --> NP[NodePort] --> Pod1[Pod]\n    end\n    \n    subgraph IP [target-type: ip]\n        ALB2[ALB] --> Pod2[Pod]\n    end\n```\n\n| target-type | ê²½ë¡œ | ì¥ì  | ë‹¨ì  |\n|-------------|-----|------|------|\n| `instance` | ALB â†’ NodePort â†’ Pod | ê°„ë‹¨, ëª¨ë“  ë„¤íŠ¸ì›Œí¬ì—ì„œ ë™ì‘ | ì¶”ê°€ í™‰, ì§€ì—° |\n| `ip` | ALB â†’ Pod (ì§ì ‘) | ë‚®ì€ ì§€ì—°, íš¨ìœ¨ì  | VPC CNI í•„ìš”, ENI ì œí•œ |\n\n### NLB for TCP/UDP\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: tcp-service\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: \"external\"\n    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: \"ip\"\n    service.beta.kubernetes.io/aws-load-balancer-scheme: \"internet-facing\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: my-app\n  ports:\n  - port: 9000\n    targetPort: 9000\n    protocol: TCP\n```\n\n---\n\n## Headless Service ì‹¬í™”\n\nSelectorê°€ ìˆì§€ë§Œ ClusterIPê°€ ì—†ëŠ” Serviceì…ë‹ˆë‹¤. DNS ì¡°íšŒ ì‹œ Service IPê°€ ì•„ë‹Œ Pod IP ëª©ë¡ì„ ì§ì ‘ ë°˜í™˜í•©ë‹ˆë‹¤.\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\nspec:\n  clusterIP: None  # Headless!\n  selector:\n    app: mysql\n  ports:\n  - port: 3306\n```\n\n### DNS í•´ì„ ì°¨ì´\n\n```bash\n# ì¼ë°˜ Service: ClusterIP ë°˜í™˜\nnslookup backend-api.default.svc.cluster.local\n# â†’ 10.96.0.100\n\n# Headless Service: ëª¨ë“  Pod IP ë°˜í™˜\nnslookup mysql.default.svc.cluster.local\n# â†’ 10.0.1.5\n# â†’ 10.0.1.6\n# â†’ 10.0.1.7\n```\n\n```mermaid\nflowchart TB\n    subgraph Normal [ì¼ë°˜ Service]\n        DNS1[DNS ì¡°íšŒ] --> ClusterIP[10.96.0.100]\n        ClusterIP --> |kube-proxy ë¡œë“œë°¸ëŸ°ì‹±| P1 & P2 & P3\n    end\n    \n    subgraph Headless [Headless Service]\n        DNS2[DNS ì¡°íšŒ] --> PodIPs[10.0.1.5, 10.0.1.6, 10.0.1.7]\n        PodIPs --> |í´ë¼ì´ì–¸íŠ¸ê°€ ì§ì ‘ ì„ íƒ| P4[Pod]\n    end\n```\n\n### ì‚¬ìš© ì‚¬ë¡€\n\n| ì‚¬ìš© ì‚¬ë¡€ | ì´ìœ  |\n|----------|-----|\n| StatefulSet | ê° Podì— ê³ ìœ  DNS í•„ìš” (`mysql-0.mysql.svc`) |\n| í´ë¼ì´ì–¸íŠ¸ ì¸¡ ë¡œë“œë°¸ëŸ°ì‹± | gRPC, ì»¤ìŠ¤í…€ LB ì•Œê³ ë¦¬ì¦˜ |\n| ì„œë¹„ìŠ¤ ë””ìŠ¤ì»¤ë²„ë¦¬ | Consul, etcd ê°™ì€ í´ëŸ¬ìŠ¤í„° |\n\n---\n\n## EndpointSlices: ëŒ€ê·œëª¨ í´ëŸ¬ìŠ¤í„° ìµœì í™”\n\nEndpoints ê°ì²´ëŠ” Serviceë‹¹ í•˜ë‚˜ì´ë©°, **ëª¨ë“  Pod IP**ë¥¼ ë‹´ìŠµë‹ˆë‹¤. Podê°€ ìˆ˜ì²œ ê°œë©´ ë¬¸ì œê°€ ë©ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph Endpoints [ê¸°ì¡´ Endpoints]\n        EP[Endpoints ê°ì²´<br/>5000ê°œ Pod IP<br/>ë§¤ìš° í¼!]\n    end\n    \n    subgraph EndpointSlices [EndpointSlices]\n        ES1[EndpointSlice 1<br/>100ê°œ Pod]\n        ES2[EndpointSlice 2<br/>100ê°œ Pod]\n        ES3[...<br/>...]\n        ES50[EndpointSlice 50<br/>100ê°œ Pod]\n    end\n```\n\n**EndpointSlices ì¥ì **:\n\n- ìµœëŒ€ 100ê°œ ì—”ë“œí¬ì¸íŠ¸ì”© ë¶„í• \n- ë³€ê²½ ì‹œ í•´ë‹¹ ìŠ¬ë¼ì´ìŠ¤ë§Œ ì—…ë°ì´íŠ¸\n- etcd ë¶€í•˜ ê°ì†Œ\n\n```bash\n# EndpointSlices í™•ì¸\nkubectl get endpointslices -l kubernetes.io/service-name=my-service\n```\n\n---\n\n## íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ\n\n### Serviceì— ì—°ê²°ë˜ì§€ ì•ŠìŒ\n\n```bash\n# 1. Endpoints í™•ì¸ (ë¹„ì–´ìˆìœ¼ë©´ selector ë¬¸ì œ)\nkubectl get endpoints my-service\n\n# 2. Pod label í™•ì¸\nkubectl get pods --show-labels\n\n# 3. Pod readiness í™•ì¸\nkubectl get pods -o wide\n```\n\n### NodePortë¡œ ì™¸ë¶€ ì ‘ì† ë¶ˆê°€\n\n```bash\n# 1. NodePort í™•ì¸\nkubectl get svc my-service\n\n# 2. ë°©í™”ë²½/Security Group í™•ì¸\n# AWS: EC2 Security Groupì—ì„œ NodePort ë²”ìœ„ í—ˆìš©\n\n# 3. iptables ê·œì¹™ í™•ì¸\nsudo iptables -t nat -L KUBE-SERVICES -n\n```\n\n### LoadBalancer EXTERNAL-IPê°€ `<pending>`\n\n```bash\n# 1. AWS Load Balancer Controller ë¡œê·¸ í™•ì¸\nkubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller\n\n# 2. ì„œë¹„ìŠ¤ ì´ë²¤íŠ¸ í™•ì¸\nkubectl describe svc my-service\n\n# 3. IAM ê¶Œí•œ í™•ì¸ (IRSA)\n```\n\n**í”í•œ ì›ì¸**:\n\n1. Cloud Controller Manager ë¯¸ì„¤ì¹˜ (On-prem)\n2. IAM ê¶Œí•œ ë¶€ì¡±\n3. ì„œë¸Œë„· íƒœê·¸ ëˆ„ë½ (`kubernetes.io/role/elb`)\n\n---\n\n## ì •ë¦¬\n\n| êµ¬ì„±ìš”ì†Œ | ì—­í•  |\n|---------|-----|\n| **ClusterIP** | í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ ê°€ìƒ IP |\n| **NodePort** | ëª¨ë“  ë…¸ë“œì—ì„œ ê³ ì • í¬íŠ¸ ë…¸ì¶œ |\n| **LoadBalancer** | í´ë¼ìš°ë“œ LB ìë™ í”„ë¡œë¹„ì €ë‹ |\n| **ExternalName** | ì™¸ë¶€ ì„œë¹„ìŠ¤ DNS CNAME |\n| **kube-proxy** | iptables/IPVSë¡œ íŒ¨í‚· ë¼ìš°íŒ… |\n| **AWS LB Controller** | Ingress â†’ ALB, Service â†’ NLB |\n| **Headless Service** | ê° Podì— ì§ì ‘ DNS ì ‘ê·¼ |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**3í¸: ì„¤ì • ë° ì‹œí¬ë¦¿ ê´€ë¦¬**ì—ì„œëŠ” ë‹¤ìŒì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- ConfigMapê³¼ Secretsì˜ ë‚´ë¶€ ë™ì‘\n- Secrets at Rest Encryption\n- AWS Secrets Manager + CSI Driver í†µí•©\n- External Secrets Operator\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Kubernetes Services](https://kubernetes.io/docs/concepts/services-networking/service/)\n- [kube-proxy Modes](https://kubernetes.io/docs/reference/networking/virtual-ips/)\n- [AWS Load Balancer Controller](https://kubernetes-sigs.github.io/aws-load-balancer-controller/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "Kubernetes",
      "Networking",
      "AWS"
    ],
    "readingTime": 8,
    "wordCount": 1566,
    "isFeatured": false,
    "isPublic": true,
    "series": "kubernetes",
    "date": "2026-01-03"
  },
  {
    "id": "k8s-01-workload-controllers",
    "slug": "k8s-01-workload-controllers",
    "path": "backend/kubernetes",
    "fullPath": "backend/kubernetes/k8s-01-workload-controllers",
    "title": "Kubernetes ì‹¬í™” ì‹œë¦¬ì¦ˆ #1: ì›Œí¬ë¡œë“œ ì»¨íŠ¸ë¡¤ëŸ¬ì˜ ë‚´ë¶€ ë™ì‘ ì›ë¦¬",
    "excerpt": "Deployment, StatefulSet, DaemonSet, CronJobì˜ ë‚´ë¶€ ë™ì‘ ì›ë¦¬ë¥¼ ê¹Šì´ ìˆê²Œ ì´í•´í•©ë‹ˆë‹¤. Reconciliation Loop, ì»¨íŠ¸ë¡¤ëŸ¬ íŒ¨í„´, ê·¸ë¦¬ê³  ì‹¤ë¬´ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…ê¹Œì§€.",
    "content": "# Kubernetes ì‹¬í™” ì‹œë¦¬ì¦ˆ #1: ì›Œí¬ë¡œë“œ ì»¨íŠ¸ë¡¤ëŸ¬ì˜ ë‚´ë¶€ ë™ì‘ ì›ë¦¬\n\n## ì‹œë¦¬ì¦ˆ ê°œìš”\n\n| # | ì£¼ì œ | í•µì‹¬ ë‚´ìš© |\n|---|------|----------|\n| **1** | **ì›Œí¬ë¡œë“œ ì»¨íŠ¸ë¡¤ëŸ¬ ì‹¬í™”** | Deployment, StatefulSet, DaemonSet, CronJob |\n| 2 | ì„œë¹„ìŠ¤ ë„¤íŠ¸ì›Œí‚¹ ì‹¬í™” | Service íƒ€ì…, kube-proxy, AWS ALB/NLB |\n| 3 | ì„¤ì • ë° ì‹œí¬ë¦¿ ê´€ë¦¬ | ConfigMap, Secrets, AWS Secrets Manager CSI Driver |\n| 4 | Istio ì„œë¹„ìŠ¤ ë©”ì‹œ | VirtualService, DestinationRule, ì™€ì¼ë“œì¹´ë“œ ì„œë¸Œë„ë©”ì¸ |\n| 5 | ì˜¤í† ìŠ¤ì¼€ì¼ë§ ì‹¬í™” | HPA, VPA, Cluster Autoscaler, Karpenter, KEDA |\n| 6 | ë³´ì•ˆ ì‹¬í™” | RBAC, NetworkPolicy, Pod Security Standards |\n\n---\n\n## ì»¨íŠ¸ë¡¤ëŸ¬ íŒ¨í„´: Kubernetesì˜ í•µì‹¬ ì² í•™\n\nKubernetesì˜ ëª¨ë“  ê²ƒì€ **ì»¨íŠ¸ë¡¤ëŸ¬ íŒ¨í„´**ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤. ìš°ë¦¬ê°€ `kubectl apply`ë¡œ Deploymentë¥¼ ìƒì„±í•˜ë©´, ì‹¤ì œ Podë¥¼ ë§Œë“œëŠ” ê²ƒì€ Deployment Controllerì…ë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph User [ì‚¬ìš©ì]\n        YAML[Deployment YAML]\n    end\n    \n    subgraph ControlPlane [Control Plane]\n        API[API Server]\n        ETCD[(etcd)]\n        CM[Controller Manager]\n        \n        subgraph Controllers [ì»¨íŠ¸ë¡¤ëŸ¬ë“¤]\n            DC[Deployment Controller]\n            RSC[ReplicaSet Controller]\n            STC[StatefulSet Controller]\n            DSC[DaemonSet Controller]\n            JC[Job Controller]\n            CJC[CronJob Controller]\n        end\n    end\n    \n    subgraph Node [Worker Node]\n        Kubelet[kubelet]\n        Pod1[Pod]\n        Pod2[Pod]\n    end\n    \n    YAML --> API --> ETCD\n    API <--> CM --> Controllers\n    DC --> API\n    RSC --> API\n    API <--> Kubelet --> Pod1 & Pod2\n```\n\n### Reconciliation Loop (ì¡°ì • ë£¨í”„)\n\nëª¨ë“  ì»¨íŠ¸ë¡¤ëŸ¬ëŠ” ë™ì¼í•œ íŒ¨í„´ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤:\n\n```\në¬´í•œ ë£¨í”„:\n  1. í˜„ì¬ ìƒíƒœ(Current State) ê´€ì°°\n  2. ì›í•˜ëŠ” ìƒíƒœ(Desired State)ì™€ ë¹„êµ\n  3. ì°¨ì´ê°€ ìˆìœ¼ë©´ ì¡°ì •(Reconcile)\n  4. ë‹¤ìŒ ì´ë²¤íŠ¸ ëŒ€ê¸°\n```\n\n```go\n// ì‹¤ì œ Kubernetes ì»¨íŠ¸ë¡¤ëŸ¬ì˜ í•µì‹¬ êµ¬ì¡° (ê°„ëµí™”)\nfunc (c *Controller) Run(ctx context.Context) {\n    for {\n        select {\n        case <-ctx.Done():\n            return\n        default:\n            // 1. ì‘ì—… íì—ì„œ ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸°\n            key, shutdown := c.workqueue.Get()\n            if shutdown {\n                return\n            }\n            \n            // 2. Reconcile ì‹¤í–‰\n            err := c.reconcile(key.(string))\n            if err != nil {\n                // ì¬ì‹œë„ íì— ì¶”ê°€\n                c.workqueue.AddRateLimited(key)\n            } else {\n                c.workqueue.Forget(key)\n            }\n            c.workqueue.Done(key)\n        }\n    }\n}\n```\n\n> [!IMPORTANT]\n> **Level-triggered vs Edge-triggered**: Kubernetes ì»¨íŠ¸ë¡¤ëŸ¬ëŠ” **Level-triggered** ë°©ì‹ì…ë‹ˆë‹¤. \"ìƒíƒœê°€ ë³€í–ˆë‹¤\"(Edge)ê°€ ì•„ë‹ˆë¼ \"í˜„ì¬ ìƒíƒœê°€ ì´ê²ƒì´ë‹¤\"(Level)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤. ë•ë¶„ì— ì»¨íŠ¸ë¡¤ëŸ¬ê°€ ì¬ì‹œì‘ë˜ì–´ë„ í˜„ì¬ ìƒíƒœë¥¼ ë‹¤ì‹œ ì½ì–´ì„œ ì •ìƒì ìœ¼ë¡œ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n---\n\n## Deployment: ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” ì›Œí¬ë¡œë“œ\n\n### ë‚´ë¶€ êµ¬ì¡°: Deployment â†’ ReplicaSet â†’ Pod\n\nDeploymentëŠ” Podë¥¼ ì§ì ‘ ê´€ë¦¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. **ReplicaSet**ì„ í†µí•´ ê°„ì ‘ì ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph Deployment [Deployment: my-app]\n        D[spec.replicas: 3<br/>spec.strategy: RollingUpdate]\n    end\n    \n    subgraph RS [ReplicaSet: my-app-7d4f5b8c9]\n        RS1[spec.replicas: 3<br/>ownerReference: my-app]\n    end\n    \n    subgraph Pods [Pods]\n        P1[my-app-7d4f5b8c9-abc12]\n        P2[my-app-7d4f5b8c9-def34]\n        P3[my-app-7d4f5b8c9-ghi56]\n    end\n    \n    Deployment --> RS --> Pods\n```\n\n### Rolling Update ë™ì‘ ì›ë¦¬\n\nDeploymentë¥¼ ì—…ë°ì´íŠ¸í•˜ë©´ ìƒˆ ReplicaSetì´ ìƒì„±ë˜ê³ , ì ì§„ì ìœ¼ë¡œ íŠ¸ë˜í”½ì´ ì´ë™í•©ë‹ˆë‹¤.\n\n```mermaid\nsequenceDiagram\n    participant User as ì‚¬ìš©ì\n    participant API as API Server\n    participant DC as Deployment Controller\n    participant RSC as ReplicaSet Controller\n    \n    User->>API: kubectl apply (ìƒˆ ì´ë¯¸ì§€)\n    API->>DC: Deployment ë³€ê²½ ê°ì§€\n    \n    DC->>API: ìƒˆ ReplicaSet ìƒì„± (replicas: 1)\n    DC->>API: ê¸°ì¡´ ReplicaSet scale down (replicas: 2)\n    \n    loop maxSurge/maxUnavailable ì¡°ê±´ ì¶©ì¡±ê¹Œì§€\n        RSC->>API: ìƒˆ Pod ìƒì„±\n        RSC->>API: ê¸°ì¡´ Pod ì‚­ì œ\n        DC->>API: ReplicaSet replicas ì¡°ì •\n    end\n    \n    DC->>API: ê¸°ì¡´ ReplicaSet replicas: 0\n    Note over API: ë¡¤ì•„ì›ƒ ì™„ë£Œ\n```\n\n### ì—…ë°ì´íŠ¸ ì „ëµ ë¹„êµ\n\n```yaml\n# RollingUpdate (ê¸°ë³¸ê°’) - ë¬´ì¤‘ë‹¨ ë°°í¬\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 25%        # ìµœëŒ€ ì¶”ê°€ Pod ìˆ˜\n      maxUnavailable: 25%  # ìµœëŒ€ ì‚¬ìš© ë¶ˆê°€ Pod ìˆ˜\n```\n\n```yaml\n# Recreate - ëª¨ë“  Pod ì‚­ì œ í›„ ì¬ìƒì„±\nspec:\n  strategy:\n    type: Recreate\n    # ì£¼ì˜: ë‹¤ìš´íƒ€ì„ ë°œìƒ!\n    # ì‚¬ìš© ì‚¬ë¡€: DB ë§ˆì´ê·¸ë ˆì´ì…˜, ì‹±ê¸€ ì¸ìŠ¤í„´ìŠ¤ ì œì•½\n```\n\n| ì „ëµ | ë‹¤ìš´íƒ€ì„ | ë¦¬ì†ŒìŠ¤ ì‚¬ìš© | ì‚¬ìš© ì‚¬ë¡€ |\n|------|---------|-----------|----------|\n| RollingUpdate | ì—†ìŒ | ì¼ì‹œì  ì¦ê°€ | ëŒ€ë¶€ë¶„ì˜ ìƒí™© |\n| Recreate | ìˆìŒ | ë™ì¼ | ì‹±ê¸€ ì¸ìŠ¤í„´ìŠ¤, ë³¼ë¥¨ ê³µìœ  ë¶ˆê°€ ì‹œ |\n\n### ë¡¤ë°± ë™ì‘ ì›ë¦¬\n\n```bash\n# ë¡¤ì•„ì›ƒ íˆìŠ¤í† ë¦¬ í™•ì¸\nkubectl rollout history deployment/my-app\n\n# íŠ¹ì • ë¦¬ë¹„ì „ìœ¼ë¡œ ë¡¤ë°±\nkubectl rollout undo deployment/my-app --to-revision=2\n```\n\në¡¤ë°±ì€ ìƒˆ ReplicaSetì„ ìƒì„±í•˜ëŠ” ê²ƒì´ ì•„ë‹™ë‹ˆë‹¤. **ê¸°ì¡´ ReplicaSetì„ ë‹¤ì‹œ Scale Up**í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph Before [ë¡¤ë°± ì „]\n        RS1_B[RS v1: 0ê°œ]\n        RS2_B[RS v2: 0ê°œ]\n        RS3_B[RS v3: 3ê°œ âœ“]\n    end\n    \n    subgraph After [v2ë¡œ ë¡¤ë°± í›„]\n        RS1_A[RS v1: 0ê°œ]\n        RS2_A[RS v2: 3ê°œ âœ“]\n        RS3_A[RS v3: 0ê°œ]\n    end\n    \n    Before --> |kubectl rollout undo<br/>--to-revision=2| After\n```\n\n> [!TIP]\n> `revisionHistoryLimit` (ê¸°ë³¸ê°’ 10)ìœ¼ë¡œ ìœ ì§€í•  ReplicaSet ìˆ˜ë¥¼ ì œí•œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë„ˆë¬´ ë§ìœ¼ë©´ etcd ë¶€ë‹´ì´ ì¦ê°€í•©ë‹ˆë‹¤.\n\n---\n\n## StatefulSet: ìƒíƒœ ìœ ì§€ê°€ í•„ìš”í•œ ì›Œí¬ë¡œë“œ\n\n### Deploymentì™€ì˜ í•µì‹¬ ì°¨ì´\n\n| íŠ¹ì„± | Deployment | StatefulSet |\n|------|-----------|-------------|\n| Pod ì´ë¦„ | ëœë¤ (`app-7d4f5b8c9-abc12`) | ìˆœì°¨ì  (`app-0`, `app-1`, `app-2`) |\n| ìƒì„±/ì‚­ì œ ìˆœì„œ | ë³‘ë ¬ | ìˆœì°¨ì  (0â†’1â†’2, ì‚­ì œëŠ” ì—­ìˆœ) |\n| ë„¤íŠ¸ì›Œí¬ ID | ì—†ìŒ | Headless Serviceë¡œ ê³ ì • DNS |\n| ìŠ¤í† ë¦¬ì§€ | Pod ì‚­ì œ ì‹œ PVCë„ ì‚­ì œ ê°€ëŠ¥ | Pod ì‚­ì œí•´ë„ PVC ìœ ì§€ |\n\n### ìˆœì°¨ì  ìƒì„±ê³¼ ì‚­ì œ\n\n```mermaid\nsequenceDiagram\n    participant API as API Server\n    participant STC as StatefulSet Controller\n    \n    Note over STC: ìƒì„± (ìˆœì°¨ì )\n    STC->>API: Pod-0 ìƒì„±\n    API-->>STC: Pod-0 Ready\n    STC->>API: Pod-1 ìƒì„±\n    API-->>STC: Pod-1 Ready\n    STC->>API: Pod-2 ìƒì„±\n    API-->>STC: Pod-2 Ready\n    \n    Note over STC: ì‚­ì œ (ì—­ìˆœ)\n    STC->>API: Pod-2 ì‚­ì œ\n    API-->>STC: Pod-2 Terminated\n    STC->>API: Pod-1 ì‚­ì œ\n    API-->>STC: Pod-1 Terminated\n    STC->>API: Pod-0 ì‚­ì œ\n```\n\n### Headless Serviceì™€ DNS\n\n```yaml\n# Headless Service (ClusterIP: None)\napiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\nspec:\n  clusterIP: None  # Headless!\n  selector:\n    app: mysql\n  ports:\n    - port: 3306\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  serviceName: mysql  # Headless Service ì´ë¦„\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: mysql\n        image: mysql:8.0\n        ports:\n        - containerPort: 3306\n```\n\nì´ë ‡ê²Œ ì„¤ì •í•˜ë©´ ê° Podì— **ê³ ì • DNS**ê°€ ë¶€ì—¬ë©ë‹ˆë‹¤:\n\n```\nmysql-0.mysql.default.svc.cluster.local\nmysql-1.mysql.default.svc.cluster.local\nmysql-2.mysql.default.svc.cluster.local\n```\n\n> [!IMPORTANT]\n> ì¼ë°˜ ServiceëŠ” Pod IPê°€ ë³€í•´ë„ Service IPë¡œ ë¡œë“œë°¸ëŸ°ì‹±ë©ë‹ˆë‹¤. StatefulSet + Headless ServiceëŠ” **ê° Podë¥¼ ì§ì ‘ ì§€ì •**í•  ìˆ˜ ìˆì–´ MySQL ë ˆí”Œë¦¬ì¼€ì´ì…˜, Kafka ë¸Œë¡œì»¤ ë“±ì— í•„ìˆ˜ì…ë‹ˆë‹¤.\n\n### VolumeClaimTemplates\n\n```yaml\nspec:\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      storageClassName: gp3\n      resources:\n        requests:\n          storage: 100Gi\n```\n\nPodê°€ ì‚­ì œë˜ì–´ë„ PVCëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ë©ë‹ˆë‹¤. `mysql-0`ì´ ë‹¤ì‹œ ìƒì„±ë˜ë©´ **ê°™ì€ ë°ì´í„°**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph StatefulSet [StatefulSet: mysql]\n        P0[mysql-0]\n        P1[mysql-1]\n        P2[mysql-2]\n    end\n    \n    subgraph PVCs [PersistentVolumeClaims]\n        PVC0[data-mysql-0<br/>100Gi]\n        PVC1[data-mysql-1<br/>100Gi]\n        PVC2[data-mysql-2<br/>100Gi]\n    end\n    \n    subgraph PVs [PersistentVolumes - AWS EBS]\n        PV0[vol-abc...]\n        PV1[vol-def...]\n        PV2[vol-ghi...]\n    end\n    \n    P0 --> PVC0 --> PV0\n    P1 --> PVC1 --> PV1\n    P2 --> PVC2 --> PV2\n```\n\n### podManagementPolicy\n\n```yaml\nspec:\n  podManagementPolicy: Parallel  # ê¸°ë³¸ê°’: OrderedReady\n```\n\n| ì •ì±… | ì„¤ëª… | ì‚¬ìš© ì‚¬ë¡€ |\n|------|-----|----------|\n| `OrderedReady` | ìˆœì°¨ì  ìƒì„±/ì‚­ì œ, ì´ì „ Pod Ready ëŒ€ê¸° | MySQL, ZooKeeper |\n| `Parallel` | ë³‘ë ¬ ìƒì„±/ì‚­ì œ, ìˆœì„œ ë¬´ê´€ | Elasticsearch (ë¹ ë¥¸ ìŠ¤ì¼€ì¼ë§ í•„ìš”) |\n\n---\n\n## DaemonSet: ëª¨ë“  ë…¸ë“œì— Pod ë°°í¬\n\n### ë™ì‘ ì›ë¦¬\n\nDaemonSet ControllerëŠ” ê° ë…¸ë“œì— ì •í™•íˆ í•˜ë‚˜ì˜ Podê°€ ì‹¤í–‰ë˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph Cluster [Kubernetes í´ëŸ¬ìŠ¤í„°]\n        subgraph Node1 [Node 1]\n            P1[fluentd]\n        end\n        subgraph Node2 [Node 2]\n            P2[fluentd]\n        end\n        subgraph Node3 [Node 3]\n            P3[fluentd]\n        end\n        subgraph Node4 [Node 4 - ì‹ ê·œ]\n            P4[fluentd]:::new\n        end\n    end\n    \n    DS[DaemonSet: fluentd]\n    DS --> P1 & P2 & P3 & P4\n    \n    classDef new fill:#90EE90\n```\n\n- **ë…¸ë“œ ì¶”ê°€ ì‹œ**: ìë™ìœ¼ë¡œ í•´ë‹¹ ë…¸ë“œì— Pod ìƒì„±\n- **ë…¸ë“œ ì‚­ì œ ì‹œ**: í•´ë‹¹ ë…¸ë“œì˜ Podë„ í•¨ê»˜ ì‚­ì œ\n\n### nodeSelectorì™€ tolerations\n\níŠ¹ì • ë…¸ë“œì—ë§Œ ë°°í¬í•˜ê±°ë‚˜, taintê°€ ìˆëŠ” ë…¸ë“œì—ë„ ë°°í¬í•˜ë ¤ë©´:\n\n```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nvidia-driver\nspec:\n  selector:\n    matchLabels:\n      app: nvidia-driver\n  template:\n    spec:\n      # íŠ¹ì • ë…¸ë“œì—ë§Œ ë°°í¬\n      nodeSelector:\n        hardware: gpu\n      \n      # taintë¥¼ í—ˆìš©\n      tolerations:\n      - key: nvidia.com/gpu\n        operator: Exists\n        effect: NoSchedule\n      \n      containers:\n      - name: nvidia-driver\n        image: nvidia/driver:latest\n```\n\n### ì£¼ìš” ì‚¬ìš© ì‚¬ë¡€\n\n| ì‚¬ìš© ì‚¬ë¡€ | ì˜ˆì‹œ |\n|----------|-----|\n| **ë¡œê·¸ ìˆ˜ì§‘** | Fluentd, Fluent Bit, Filebeat |\n| **ëª¨ë‹ˆí„°ë§** | Node Exporter, cAdvisor |\n| **ë„¤íŠ¸ì›Œí¬** | Calico, Cilium (CNI í”ŒëŸ¬ê·¸ì¸) |\n| **ìŠ¤í† ë¦¬ì§€** | CSI ë“œë¼ì´ë²„, Rook-Ceph |\n| **ë³´ì•ˆ** | Falco, Sysdig |\n\n---\n\n## CronJob & Job: ë°°ì¹˜ ì‘ì—…\n\n### Job: ì¼íšŒì„± ì‘ì—…\n\n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migration\nspec:\n  backoffLimit: 3           # ì‹¤íŒ¨ ì‹œ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜\n  activeDeadlineSeconds: 600  # ìµœëŒ€ ì‹¤í–‰ ì‹œê°„ (10ë¶„)\n  ttlSecondsAfterFinished: 3600  # ì™„ë£Œ í›„ 1ì‹œê°„ ë’¤ ìë™ ì‚­ì œ\n  template:\n    spec:\n      restartPolicy: Never   # Jobì—ì„œëŠ” OnFailure ë˜ëŠ” Neverë§Œ ê°€ëŠ¥\n      containers:\n      - name: migration\n        image: my-app:latest\n        command: [\"./migrate.sh\"]\n```\n\n### CronJob: ì •ê¸°ì  ì‘ì—…\n\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: daily-report\nspec:\n  schedule: \"0 2 * * *\"      # ë§¤ì¼ ìƒˆë²½ 2ì‹œ (UTC)\n  timeZone: \"Asia/Seoul\"     # K8s 1.27+ ì§€ì›\n  \n  # ë™ì‹œ ì‹¤í–‰ ì •ì±…\n  concurrencyPolicy: Forbid  # ì´ì „ Jobì´ ì‹¤í–‰ ì¤‘ì´ë©´ ìƒˆ Job ìƒì„± ì•ˆí•¨\n  \n  # íˆìŠ¤í† ë¦¬ ì œí•œ\n  successfulJobsHistoryLimit: 3\n  failedJobsHistoryLimit: 1\n  \n  # ìŠ¤ì¼€ì¤„ ë†“ì³¤ì„ ë•Œ ì •ì±…\n  startingDeadlineSeconds: 300  # 5ë¶„ ë‚´ ì‹œì‘ ëª»í•˜ë©´ ìŠ¤í‚µ\n  \n  jobTemplate:\n    spec:\n      template:\n        spec:\n          restartPolicy: OnFailure\n          containers:\n          - name: report\n            image: report-generator:latest\n```\n\n### concurrencyPolicy ë¹„êµ\n\n```mermaid\ngantt\n    title CronJob concurrencyPolicy ë¹„êµ\n    dateFormat HH:mm\n    axisFormat %H:%M\n    \n    section Allow\n    Job 1    :active, 00:00, 30m\n    Job 2    :active, 00:30, 30m\n    Job 3 (ë™ì‹œì‹¤í–‰ë¨) :active, 00:45, 30m\n    \n    section Forbid\n    Job 1    :active, 00:00, 60m\n    Job 2 (ìŠ¤í‚µë¨) :crit, 00:30, 5m\n    Job 2 (ì‹¤í–‰) :active, 01:00, 30m\n    \n    section Replace\n    Job 1    :active, 00:00, 30m\n    Job 2 (Job 1 ì¤‘ë‹¨) :active, 00:30, 30m\n```\n\n| ì •ì±… | ë™ì‘ | ì‚¬ìš© ì‚¬ë¡€ |\n|------|-----|----------|\n| `Allow` | ë™ì‹œ ì‹¤í–‰ í—ˆìš© (ê¸°ë³¸ê°’) | ë…ë¦½ì ì¸ ì‘ì—… |\n| `Forbid` | ì´ì „ Job ì‹¤í–‰ ì¤‘ì´ë©´ ìŠ¤í‚µ | ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ í•„ìš” ì‹œ |\n| `Replace` | ì´ì „ Job ì¤‘ë‹¨í•˜ê³  ìƒˆë¡œ ì‹œì‘ | ìµœì‹  ë°ì´í„°ë§Œ ì¤‘ìš”í•  ë•Œ |\n\n---\n\n## íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ\n\n### Deployment ë¡¤ì•„ì›ƒì´ ë©ˆì¶¤\n\n```bash\n# ìƒíƒœ í™•ì¸\nkubectl rollout status deployment/my-app\n\n# ì´ë²¤íŠ¸ í™•ì¸\nkubectl describe deployment my-app\n\n# ReplicaSet ìƒíƒœ í™•ì¸\nkubectl get rs -l app=my-app\n```\n\n**í”í•œ ì›ì¸**:\n\n1. **ì´ë¯¸ì§€ Pull ì‹¤íŒ¨**: ImagePullBackOff\n2. **ë¦¬ì†ŒìŠ¤ ë¶€ì¡±**: Pending ìƒíƒœ\n3. **Liveness/Readiness Probe ì‹¤íŒ¨**: CrashLoopBackOff\n4. **maxUnavailable 0**: ê¸°ì¡´ Pod ì‚­ì œ ì•ˆ ë¨\n\n### StatefulSet Podê°€ Pending ìƒíƒœ\n\n```bash\n# PVC ìƒíƒœ í™•ì¸\nkubectl get pvc\n\n# ìŠ¤í† ë¦¬ì§€ ì´ë²¤íŠ¸ í™•ì¸\nkubectl describe pvc data-mysql-0\n```\n\n**í”í•œ ì›ì¸**:\n\n1. **StorageClass ì—†ìŒ**: ê¸°ë³¸ StorageClass ë¯¸ì„¤ì •\n2. **ê°€ìš© ì˜ì—­(AZ) ë¶ˆì¼ì¹˜**: EBSëŠ” ê°™ì€ AZì—ì„œë§Œ ë§ˆìš´íŠ¸\n3. **ìš©ëŸ‰ ë¶€ì¡±**: AWS EBS í•œë„ ì´ˆê³¼\n\n### CronJobì´ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ\n\n```bash\n# CronJob ìƒíƒœ í™•ì¸\nkubectl get cronjob\n\n# ìµœê·¼ Job í™•ì¸\nkubectl get jobs --sort-by=.metadata.creationTimestamp\n\n# ë§ˆì§€ë§‰ ìŠ¤ì¼€ì¤„ ì‹œê°„ í™•ì¸\nkubectl describe cronjob daily-report | grep \"Last Schedule\"\n```\n\n**í”í•œ ì›ì¸**:\n\n1. **startingDeadlineSeconds ì´ˆê³¼**: ìŠ¤ì¼€ì¤„ ì‹œê°„ ë†“ì¹¨\n2. **concurrencyPolicy: Forbid + ê¸´ ì‹¤í–‰ ì‹œê°„**: ì´ì „ Jobì´ ê³„ì† ì‹¤í–‰ ì¤‘\n3. **suspend: true**: CronJobì´ ì¼ì‹œ ì¤‘ì§€ë¨\n\n---\n\n## ì •ë¦¬\n\n| ì›Œí¬ë¡œë“œ | í•µì‹¬ íŠ¹ì§• | ì‚¬ìš© ì‚¬ë¡€ |\n|---------|----------|----------|\n| **Deployment** | ReplicaSet ê´€ë¦¬, Rolling Update | ë¬´ìƒíƒœ ì›¹ ì„œë¹„ìŠ¤ |\n| **StatefulSet** | ìˆœì°¨ì  ìƒì„±, ê³ ì • ë„¤íŠ¸ì›Œí¬ ID, PVC ìœ ì§€ | DB, ë©”ì‹œì§€ í |\n| **DaemonSet** | ë…¸ë“œë‹¹ 1ê°œ Pod ë³´ì¥ | ë¡œê·¸/ëª¨ë‹ˆí„°ë§ ì—ì´ì „íŠ¸ |\n| **Job** | ì¼íšŒì„± ì™„ë£Œ ì‘ì—… | ë§ˆì´ê·¸ë ˆì´ì…˜, ë°°ì¹˜ |\n| **CronJob** | ì •ê¸°ì  Job ìŠ¤ì¼€ì¤„ë§ | ë¦¬í¬íŠ¸, ì •ë¦¬ ì‘ì—… |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**2í¸: ì„œë¹„ìŠ¤ ë„¤íŠ¸ì›Œí‚¹ ì‹¬í™”**ì—ì„œëŠ” ë‹¤ìŒì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- Service íƒ€ì…ë³„ ë‚´ë¶€ ë™ì‘ (ClusterIP, NodePort, LoadBalancer)\n- kube-proxyì˜ iptables vs IPVS ëª¨ë“œ\n- AWS ALB/NLB Ingress Controller\n- ExternalDNSì™€ Route 53 í†µí•©\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Kubernetes Controllers](https://kubernetes.io/docs/concepts/architecture/controller/)\n- [Deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)\n- [StatefulSets](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/)\n- [CronJob](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "Kubernetes",
      "Controller"
    ],
    "readingTime": 9,
    "wordCount": 1721,
    "isFeatured": false,
    "isPublic": true,
    "series": "kubernetes",
    "date": "2026-01-03"
  },
  {
    "id": "asyncio-concurrency-guide",
    "slug": "asyncio-concurrency-guide",
    "path": "languages/python",
    "fullPath": "languages/python/asyncio-concurrency-guide",
    "title": "Python asyncio ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë° ì™„ë²½ ê°€ì´ë“œ",
    "excerpt": "Python asyncioë¥¼ í™œìš©í•œ ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì˜ í•µì‹¬ ê°œë…ê³¼ ì‹¤ì „ ë™ì‹œì„± íŒ¨í„´ì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# Python asyncio ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë° ì™„ë²½ ê°€ì´ë“œ\n\n## ê°œìš”\n\n**asyncio**ëŠ” Pythonì˜ ë¹„ë™ê¸° I/O í”„ë ˆì„ì›Œí¬ë¡œ, ì½”ë£¨í‹´(coroutine)ì„ ê¸°ë°˜ìœ¼ë¡œ ë™ì‹œì„±ì„ êµ¬í˜„í•©ë‹ˆë‹¤. I/O ë°”ìš´ë“œ ì‘ì—…(ë„¤íŠ¸ì›Œí¬, íŒŒì¼)ì—ì„œ ìŠ¤ë ˆë“œë³´ë‹¤ íš¨ìœ¨ì ì¸ ë™ì‹œ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\n## í•µì‹¬ ê°œë…\n\n### ì½”ë£¨í‹´ (Coroutine)\n\n```python\nimport asyncio\n\n# async defë¡œ ì •ì˜ëœ í•¨ìˆ˜ = ì½”ë£¨í‹´ í•¨ìˆ˜\nasync def fetch_data(url: str) -> dict:\n    print(f\"Fetching {url}\")\n    await asyncio.sleep(1)  # ë¹„ë™ê¸° ëŒ€ê¸°\n    return {\"url\": url, \"data\": \"...\"}\n\n# ì½”ë£¨í‹´ ì‹¤í–‰\nasync def main():\n    result = await fetch_data(\"https://api.example.com\")\n    print(result)\n\nasyncio.run(main())  # ì§„ì…ì \n```\n\n### ì´ë²¤íŠ¸ ë£¨í”„ (Event Loop)\n\n```python\n# asyncio.run()ì´ ë‚´ë¶€ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ì‘ì—…\nloop = asyncio.new_event_loop()\nasyncio.set_event_loop(loop)\ntry:\n    loop.run_until_complete(main())\nfinally:\n    loop.close()\n```\n\n> [!NOTE]\n> Python 3.10+ì—ì„œëŠ” `asyncio.run()`ë§Œ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤. ì§ì ‘ ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ ê´€ë¦¬í•  í•„ìš”ê°€ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤.\n\n## ë™ì‹œ ì‹¤í–‰ íŒ¨í„´\n\n### asyncio.gather - ë³‘ë ¬ ì‹¤í–‰\n\n```python\nasync def main():\n    # ë™ì‹œì— 3ê°œ ìš”ì²­ ì‹¤í–‰\n    results = await asyncio.gather(\n        fetch_data(\"https://api1.example.com\"),\n        fetch_data(\"https://api2.example.com\"),\n        fetch_data(\"https://api3.example.com\"),\n    )\n    print(results)  # [result1, result2, result3]\n```\n\n**ì—ëŸ¬ ì²˜ë¦¬**:\n\n```python\nasync def main():\n    results = await asyncio.gather(\n        fetch_data(\"url1\"),\n        fetch_data(\"url2\"),\n        return_exceptions=True,  # ì˜ˆì™¸ë¥¼ ê²°ê³¼ë¡œ ë°˜í™˜\n    )\n    for result in results:\n        if isinstance(result, Exception):\n            print(f\"Error: {result}\")\n        else:\n            print(f\"Success: {result}\")\n```\n\n### asyncio.TaskGroup - êµ¬ì¡°ì  ë™ì‹œì„± (3.11+)\n\n```python\nasync def main():\n    async with asyncio.TaskGroup() as tg:\n        task1 = tg.create_task(fetch_data(\"url1\"))\n        task2 = tg.create_task(fetch_data(\"url2\"))\n        task3 = tg.create_task(fetch_data(\"url3\"))\n    \n    # ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ í›„ ê²°ê³¼ ì ‘ê·¼\n    print(task1.result(), task2.result(), task3.result())\n```\n\n> [!IMPORTANT]\n> `TaskGroup`ì€ í•˜ë‚˜ì˜ íƒœìŠ¤í¬ê°€ ì‹¤íŒ¨í•˜ë©´ ë‚˜ë¨¸ì§€ë¥¼ ìë™ ì·¨ì†Œí•©ë‹ˆë‹¤. ì—ëŸ¬ ì „íŒŒê°€ ëª…í™•í•©ë‹ˆë‹¤.\n\n### asyncio.create_task - ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰\n\n```python\nasync def background_job():\n    while True:\n        print(\"Background running...\")\n        await asyncio.sleep(5)\n\nasync def main():\n    # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì‹œì‘\n    task = asyncio.create_task(background_job())\n    \n    # ë©”ì¸ ë¡œì§ ì‹¤í–‰\n    await do_main_work()\n    \n    # íƒœìŠ¤í¬ ì·¨ì†Œ\n    task.cancel()\n    try:\n        await task\n    except asyncio.CancelledError:\n        print(\"Background job cancelled\")\n```\n\n### as_completed - ì™„ë£Œ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬\n\n```python\nasync def main():\n    tasks = [\n        asyncio.create_task(fetch_data(f\"url{i}\"))\n        for i in range(5)\n    ]\n    \n    # ì™„ë£Œë˜ëŠ” ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ì²˜ë¦¬\n    for coro in asyncio.as_completed(tasks):\n        result = await coro\n        print(f\"Got: {result}\")\n```\n\n## ë™ê¸°í™” í”„ë¦¬ë¯¸í‹°ë¸Œ\n\n### Semaphore - ë™ì‹œ ì‹¤í–‰ ì œí•œ\n\n```python\nasync def fetch_with_limit(sem: asyncio.Semaphore, url: str):\n    async with sem:  # ì„¸ë§ˆí¬ì–´ íšë“\n        return await fetch_data(url)\n\nasync def main():\n    sem = asyncio.Semaphore(5)  # ìµœëŒ€ 5ê°œ ë™ì‹œ ì‹¤í–‰\n    urls = [f\"url{i}\" for i in range(100)]\n    \n    tasks = [fetch_with_limit(sem, url) for url in urls]\n    results = await asyncio.gather(*tasks)\n```\n\n### Lock - ìƒí˜¸ ë°°ì œ\n\n```python\nclass Counter:\n    def __init__(self):\n        self.value = 0\n        self._lock = asyncio.Lock()\n    \n    async def increment(self):\n        async with self._lock:\n            current = self.value\n            await asyncio.sleep(0.01)  # ì‹œë®¬ë ˆì´ì…˜\n            self.value = current + 1\n```\n\n### Event - ì´ë²¤íŠ¸ ì•Œë¦¼\n\n```python\nasync def waiter(event: asyncio.Event):\n    print(\"Waiting for event...\")\n    await event.wait()\n    print(\"Event received!\")\n\nasync def setter(event: asyncio.Event):\n    await asyncio.sleep(2)\n    event.set()\n    print(\"Event set!\")\n\nasync def main():\n    event = asyncio.Event()\n    await asyncio.gather(waiter(event), setter(event))\n```\n\n### Queue - ìƒì‚°ì-ì†Œë¹„ì íŒ¨í„´\n\n```python\nasync def producer(queue: asyncio.Queue):\n    for i in range(10):\n        await queue.put(f\"item-{i}\")\n        print(f\"Produced: item-{i}\")\n        await asyncio.sleep(0.1)\n\nasync def consumer(queue: asyncio.Queue, name: str):\n    while True:\n        item = await queue.get()\n        print(f\"{name} consumed: {item}\")\n        queue.task_done()\n\nasync def main():\n    queue = asyncio.Queue(maxsize=5)\n    \n    producers = [asyncio.create_task(producer(queue))]\n    consumers = [\n        asyncio.create_task(consumer(queue, f\"consumer-{i}\"))\n        for i in range(3)\n    ]\n    \n    await asyncio.gather(*producers)\n    await queue.join()  # ëª¨ë“  ì•„ì´í…œ ì²˜ë¦¬ ëŒ€ê¸°\n    \n    for c in consumers:\n        c.cancel()\n```\n\n## íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬\n\n### asyncio.timeout (3.11+)\n\n```python\nasync def main():\n    try:\n        async with asyncio.timeout(5.0):  # 5ì´ˆ ì œí•œ\n            await long_running_task()\n    except TimeoutError:\n        print(\"Task timed out!\")\n```\n\n### asyncio.wait_for (ë ˆê±°ì‹œ)\n\n```python\nasync def main():\n    try:\n        result = await asyncio.wait_for(\n            long_running_task(),\n            timeout=5.0\n        )\n    except asyncio.TimeoutError:\n        print(\"Task timed out!\")\n```\n\n## ë™ê¸° ì½”ë“œì™€ í†µí•©\n\n### run_in_executor - ë¸”ë¡œí‚¹ í•¨ìˆ˜ ì‹¤í–‰\n\n```python\nimport concurrent.futures\n\ndef blocking_io():\n    \"\"\"ë™ê¸° ë¸”ë¡œí‚¹ í•¨ìˆ˜\"\"\"\n    import time\n    time.sleep(2)\n    return \"IO completed\"\n\nasync def main():\n    loop = asyncio.get_event_loop()\n    \n    # ThreadPoolExecutor (I/O ë°”ìš´ë“œ)\n    with concurrent.futures.ThreadPoolExecutor() as pool:\n        result = await loop.run_in_executor(pool, blocking_io)\n        print(result)\n    \n    # ProcessPoolExecutor (CPU ë°”ìš´ë“œ)\n    with concurrent.futures.ProcessPoolExecutor() as pool:\n        result = await loop.run_in_executor(pool, cpu_intensive_task)\n```\n\n### asyncio.to_thread (3.9+)\n\n```python\nasync def main():\n    # ê°„í¸í•œ ë°©ë²•\n    result = await asyncio.to_thread(blocking_io)\n    print(result)\n```\n\n## ì‹¤ì „ íŒ¨í„´\n\n### Retry with Exponential Backoff\n\n```python\nasync def fetch_with_retry(\n    url: str,\n    max_retries: int = 3,\n    base_delay: float = 1.0\n) -> dict:\n    for attempt in range(max_retries):\n        try:\n            return await fetch_data(url)\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise\n            delay = base_delay * (2 ** attempt)\n            print(f\"Retry {attempt + 1} after {delay}s: {e}\")\n            await asyncio.sleep(delay)\n```\n\n### Rate Limiter\n\n```python\nclass RateLimiter:\n    def __init__(self, rate: int, per: float):\n        self.rate = rate\n        self.per = per\n        self.tokens = rate\n        self.updated_at = asyncio.get_event_loop().time()\n        self._lock = asyncio.Lock()\n    \n    async def acquire(self):\n        async with self._lock:\n            now = asyncio.get_event_loop().time()\n            elapsed = now - self.updated_at\n            self.tokens = min(self.rate, self.tokens + elapsed * (self.rate / self.per))\n            self.updated_at = now\n            \n            if self.tokens < 1:\n                wait_time = (1 - self.tokens) * (self.per / self.rate)\n                await asyncio.sleep(wait_time)\n                self.tokens = 0\n            else:\n                self.tokens -= 1\n```\n\n### Graceful Shutdown\n\n```python\nimport signal\n\nasync def shutdown(loop, signal=None):\n    if signal:\n        print(f\"Received signal {signal.name}\")\n    \n    tasks = [t for t in asyncio.all_tasks() if t is not asyncio.current_task()]\n    [task.cancel() for task in tasks]\n    \n    await asyncio.gather(*tasks, return_exceptions=True)\n    loop.stop()\n\ndef main():\n    loop = asyncio.new_event_loop()\n    \n    for sig in (signal.SIGTERM, signal.SIGINT):\n        loop.add_signal_handler(\n            sig,\n            lambda s=sig: asyncio.create_task(shutdown(loop, s))\n        )\n    \n    try:\n        loop.run_until_complete(run_server())\n    finally:\n        loop.close()\n```\n\n## ë””ë²„ê¹… íŒ\n\n### ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”\n\n```python\n# í™˜ê²½ë³€ìˆ˜\n# PYTHONASYNCIODEBUG=1\n\n# ë˜ëŠ” ì½”ë“œì—ì„œ\nasyncio.run(main(), debug=True)\n```\n\n### ëŠë¦° ì½œë°± ê°ì§€\n\n```python\nloop = asyncio.get_event_loop()\nloop.slow_callback_duration = 0.1  # 100ms ì´ìƒ ê²½ê³ \n```\n\n## ì£¼ì˜ì‚¬í•­\n\n1. âš ï¸ **async í•¨ìˆ˜ ë‚´ì—ì„œ `time.sleep()` ê¸ˆì§€** - `await asyncio.sleep()` ì‚¬ìš©\n2. âš ï¸ **CPU ë°”ìš´ë“œ ì‘ì—…ì€ ProcessPoolExecutor** - ì´ë²¤íŠ¸ ë£¨í”„ ë¸”ë¡œí‚¹ ë°©ì§€\n3. âš ï¸ **ì½”ë£¨í‹´ í˜¸ì¶œ í›„ await í•„ìˆ˜** - `await` ì—†ì´ í˜¸ì¶œí•˜ë©´ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ\n4. âš ï¸ **íƒœìŠ¤í¬ ì·¨ì†Œ ì‹œ CancelledError ì²˜ë¦¬** - ë¦¬ì†ŒìŠ¤ ì •ë¦¬ í•„ìš”\n\n## ì°¸ê³  ìë£Œ\n\n- [asyncio ê³µì‹ ë¬¸ì„œ](https://docs.python.org/3/library/asyncio.html)\n- [Real Python: Async IO in Python](https://realpython.com/async-io-python/)\n- [PEP 492 â€“ Coroutines](https://peps.python.org/pep-0492/)",
    "docType": "original",
    "category": "Languages",
    "tags": [
      "Concurrency",
      "Python"
    ],
    "readingTime": 5,
    "wordCount": 917,
    "isFeatured": false,
    "isPublic": true,
    "date": "2026-01-02"
  },
  {
    "id": "postgresql-query-tuning-guide",
    "slug": "postgresql-query-tuning-guide",
    "path": "database/postgresql",
    "fullPath": "database/postgresql/postgresql-query-tuning-guide",
    "title": "PostgreSQL ì¿¼ë¦¬ ì„±ëŠ¥ íŠœë‹ ì™„ë²½ ê°€ì´ë“œ",
    "excerpt": "EXPLAIN ANALYZEë¥¼ í™œìš©í•œ ì¿¼ë¦¬ ë¶„ì„ê³¼ ì¸ë±ìŠ¤ ì „ëµìœ¼ë¡œ PostgreSQL ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# PostgreSQL ì¿¼ë¦¬ ì„±ëŠ¥ íŠœë‹ ì™„ë²½ ê°€ì´ë“œ\n\n## ê°œìš”\n\nPostgreSQLì˜ ì¿¼ë¦¬ ì„±ëŠ¥ íŠœë‹ì€ **EXPLAIN ANALYZEë¥¼ í†µí•œ ë¬¸ì œ ì§„ë‹¨**ê³¼ **ì ì ˆí•œ ì¸ë±ìŠ¤ ì„¤ê³„**ê°€ í•µì‹¬ì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” ì‹¤ì „ì—ì„œ ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” íŠœë‹ ì „ëµì„ ë‹¤ë£¹ë‹ˆë‹¤.\n\n## EXPLAIN ANALYZE ê¸°ì´ˆ\n\n### ê¸°ë³¸ ì‚¬ìš©ë²•\n\n```sql\n-- ê¸°ë³¸ ì‹¤í–‰ ê³„íš\nEXPLAIN SELECT * FROM users WHERE email = 'test@example.com';\n\n-- ì‹¤ì œ ì‹¤í–‰ í†µê³„ í¬í•¨ (ê¶Œì¥)\nEXPLAIN ANALYZE SELECT * FROM users WHERE email = 'test@example.com';\n\n-- ìƒì„¸ ì •ë³´ í¬í•¨\nEXPLAIN (ANALYZE, BUFFERS, FORMAT TEXT)\nSELECT * FROM users WHERE email = 'test@example.com';\n```\n\n### ì¶œë ¥ í•´ì„\n\n```\nSeq Scan on users  (cost=0.00..1520.00 rows=1 width=256) (actual time=12.345..45.678 rows=1 loops=1)\n  Filter: (email = 'test@example.com'::text)\n  Rows Removed by Filter: 49999\n  Buffers: shared hit=520 read=200\nPlanning Time: 0.150 ms\nExecution Time: 45.720 ms\n```\n\n| í•­ëª© | ì„¤ëª… |\n|-----|------|\n| `cost` | ì˜ˆìƒ ë¹„ìš© (ì‹œì‘..ì´) |\n| `rows` | ì˜ˆìƒ ë°˜í™˜ í–‰ ìˆ˜ |\n| `actual time` | ì‹¤ì œ ì‹¤í–‰ ì‹œê°„ (ms) |\n| `Buffers` | ë²„í¼ ìºì‹œ hit/read í†µê³„ |\n| `Rows Removed by Filter` | í•„í„°ë§ìœ¼ë¡œ ì œê±°ëœ í–‰ ìˆ˜ âš ï¸ |\n\n> [!WARNING]\n> `Rows Removed by Filter`ê°€ í¬ë©´ ì¸ë±ìŠ¤ê°€ ì—†ê±°ë‚˜ ë¹„íš¨ìœ¨ì ì¸ ìƒíƒœì…ë‹ˆë‹¤.\n\n### ì£¼ìš” ìŠ¤ìº” íƒ€ì…\n\n| ìŠ¤ìº” íƒ€ì… | ì„¤ëª… | ì„±ëŠ¥ |\n|---------|------|-----|\n| `Seq Scan` | ì „ì²´ í…Œì´ë¸” ìŠ¤ìº” | ğŸ”´ ëŠë¦¼ |\n| `Index Scan` | ì¸ë±ìŠ¤ + í…Œì´ë¸” ì ‘ê·¼ | ğŸŸ¢ ë¹ ë¦„ |\n| `Index Only Scan` | ì¸ë±ìŠ¤ë§Œìœ¼ë¡œ í•´ê²° | ğŸŸ¢ğŸŸ¢ ê°€ì¥ ë¹ ë¦„ |\n| `Bitmap Index Scan` | ì—¬ëŸ¬ ì¡°ê±´ ë³‘í•© | ğŸŸ¡ ì¡°ê±´ë¶€ |\n\n## ì¸ë±ìŠ¤ ì „ëµ\n\n### B-Tree ì¸ë±ìŠ¤ (ê¸°ë³¸)\n\n```sql\n-- ë‹¨ì¼ ì»¬ëŸ¼ ì¸ë±ìŠ¤\nCREATE INDEX idx_users_email ON users(email);\n\n-- ë³µí•© ì¸ë±ìŠ¤ (ìˆœì„œ ì¤‘ìš”!)\nCREATE INDEX idx_orders_user_created ON orders(user_id, created_at DESC);\n```\n\n> [!IMPORTANT]\n> ë³µí•© ì¸ë±ìŠ¤ì˜ ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ WHERE ì¡°ê±´ì— ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ë¥¼ íƒ€ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n### ë³µí•© ì¸ë±ìŠ¤ ì„¤ê³„ ì›ì¹™\n\n```sql\n-- âŒ ì˜ëª»ëœ ì˜ˆ: ë‘ ë²ˆì§¸ ì»¬ëŸ¼ë§Œ ì¡°íšŒ\nSELECT * FROM orders WHERE created_at > '2025-01-01';  -- idx_orders_user_created ì‚¬ìš© ë¶ˆê°€\n\n-- âœ… ì˜¬ë°”ë¥¸ ì˜ˆ: ì²« ë²ˆì§¸ ì»¬ëŸ¼ í¬í•¨\nSELECT * FROM orders WHERE user_id = 123 AND created_at > '2025-01-01';\n```\n\n**ë³µí•© ì¸ë±ìŠ¤ ì»¬ëŸ¼ ìˆœì„œ ê°€ì´ë“œë¼ì¸**:\n\n1. **ë“±í˜¸(=) ì¡°ê±´** â†’ ì•ìª½\n2. **ë²”ìœ„(>, <, BETWEEN)** â†’ ë’¤ìª½\n3. **ë†’ì€ ì¹´ë””ë„ë¦¬í‹°** â†’ ì•ìª½\n\n### Covering Index (í¬í•¨ ì¸ë±ìŠ¤)\n\n```sql\n-- INCLUDEë¡œ ì¶”ê°€ ì»¬ëŸ¼ í¬í•¨ â†’ Index Only Scan ê°€ëŠ¥\nCREATE INDEX idx_users_email_include ON users(email) INCLUDE (name, created_at);\n\n-- ì¿¼ë¦¬\nSELECT email, name, created_at FROM users WHERE email = 'test@example.com';\n-- â†’ Index Only Scan ë°œìƒ!\n```\n\n### Partial Index (ë¶€ë¶„ ì¸ë±ìŠ¤)\n\n```sql\n-- íŠ¹ì • ì¡°ê±´ì˜ í–‰ë§Œ ì¸ë±ì‹±\nCREATE INDEX idx_orders_active ON orders(user_id)\nWHERE status = 'PENDING';\n\n-- í™œì„± ì£¼ë¬¸ë§Œ ìì£¼ ì¡°íšŒí•  ë•Œ íš¨ìœ¨ì \nSELECT * FROM orders WHERE user_id = 123 AND status = 'PENDING';\n```\n\n### Expression Index (í‘œí˜„ì‹ ì¸ë±ìŠ¤)\n\n```sql\n-- í•¨ìˆ˜ ê²°ê³¼ì— ì¸ë±ìŠ¤ ìƒì„±\nCREATE INDEX idx_users_lower_email ON users(LOWER(email));\n\n-- ì‚¬ìš©\nSELECT * FROM users WHERE LOWER(email) = 'test@example.com';\n```\n\n## ì¡°ì¸ ìµœì í™”\n\n### ì¡°ì¸ íƒ€ì… ì´í•´\n\n```\nEXPLAIN ANALYZE\nSELECT u.name, o.total FROM users u\nJOIN orders o ON u.id = o.user_id;\n```\n\n| ì¡°ì¸ íƒ€ì… | ì„¤ëª… | ìµœì  ìƒí™© |\n|---------|------|---------|\n| `Nested Loop` | ì¤‘ì²© ë°˜ë³µ | ì‘ì€ í…Œì´ë¸” ì¡°ì¸ |\n| `Hash Join` | í•´ì‹œ í…Œì´ë¸” ìƒì„± | í° í…Œì´ë¸” ë“±ê°€ ì¡°ì¸ |\n| `Merge Join` | ì •ë ¬ í›„ ë³‘í•© | ì •ë ¬ëœ ëŒ€ëŸ‰ ë°ì´í„° |\n\n### ì¡°ì¸ ìˆœì„œ íŒíŠ¸\n\n```sql\n-- ì¡°ì¸ ìˆœì„œ ê³ ì • (ë“œë¬¼ê²Œ ì‚¬ìš©)\nSET join_collapse_limit = 1;\n\n-- ë˜ëŠ” ì¿¼ë¦¬ì—ì„œ ìˆœì„œ ì§€ì •\nSELECT /*+ Leading(users orders) */ ...\n```\n\n## í†µê³„ ê´€ë¦¬\n\n### í†µê³„ ì—…ë°ì´íŠ¸\n\n```sql\n-- í…Œì´ë¸” í†µê³„ ê°±ì‹  (í•„ìˆ˜!)\nANALYZE users;\n\n-- ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤\nANALYZE;\n\n-- í†µê³„ ì •ë³´ í™•ì¸\nSELECT \n    tablename,\n    n_live_tup,\n    n_dead_tup,\n    last_analyze,\n    last_autoanalyze\nFROM pg_stat_user_tables;\n```\n\n### í†µê³„ ìƒ˜í”Œ í¬ê¸° ì¡°ì •\n\n```sql\n-- íŠ¹ì • ì»¬ëŸ¼ì˜ í†µê³„ ì •ë°€ë„ ì¦ê°€ (ê¸°ë³¸ê°’: 100)\nALTER TABLE users ALTER COLUMN email SET STATISTICS 1000;\nANALYZE users;\n```\n\n## ì‹¤ì „ ë¬¸ì œ ì§„ë‹¨\n\n### ëŠë¦° ì¿¼ë¦¬ ì°¾ê¸°\n\n```sql\n-- pg_stat_statements í™•ì¥ í™œì„±í™” í•„ìš”\nSELECT \n    query,\n    calls,\n    mean_exec_time,\n    total_exec_time,\n    rows\nFROM pg_stat_statements\nORDER BY mean_exec_time DESC\nLIMIT 10;\n```\n\n### ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ì¸ë±ìŠ¤ íƒì§€\n\n```sql\nSELECT \n    schemaname,\n    tablename,\n    indexname,\n    idx_scan,\n    idx_tup_read\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0\nAND indexname NOT LIKE 'pg_%';\n```\n\n### ëˆ„ë½ëœ ì¸ë±ìŠ¤ íŒíŠ¸\n\n```sql\n-- ìˆœì°¨ ìŠ¤ìº” ë¹„ìœ¨ì´ ë†’ì€ í…Œì´ë¸”\nSELECT \n    relname,\n    seq_scan,\n    idx_scan,\n    ROUND(100.0 * seq_scan / NULLIF(seq_scan + idx_scan, 0), 2) AS seq_scan_pct\nFROM pg_stat_user_tables\nWHERE seq_scan + idx_scan > 100\nORDER BY seq_scan_pct DESC;\n```\n\n## ì„±ëŠ¥ ì„¤ì • íŠœë‹\n\n### ë©”ëª¨ë¦¬ ê´€ë ¨\n\n```sql\n-- ì •ë ¬/í•´ì‹œ ì‘ì—…ì— ì‚¬ìš©í•  ë©”ëª¨ë¦¬ (ì„¸ì…˜ë³„)\nSET work_mem = '256MB';\n\n-- shared_buffers (postgresql.conf, RAMì˜ 25%)\nshared_buffers = '4GB'\n\n-- íš¨ê³¼ì ì¸ ìºì‹œ í¬ê¸° (ì¿¼ë¦¬ í”Œë˜ë„ˆ íŒíŠ¸)\neffective_cache_size = '12GB'\n```\n\n### í”Œë˜ë„ˆ ê´€ë ¨\n\n```sql\n-- ìˆœì°¨ ìŠ¤ìº” ë¹„ìš© ì¡°ì • (SSDëŠ” ë‚®ê²Œ)\nSET random_page_cost = 1.1;  -- ê¸°ë³¸ê°’ 4.0\n\n-- ë³‘ë ¬ ì¿¼ë¦¬ í™œì„±í™”\nSET max_parallel_workers_per_gather = 4;\n```\n\n## ëª¨ë²” ì‚¬ë¡€ ì²´í¬ë¦¬ìŠ¤íŠ¸\n\n1. âœ… **ì •ê¸°ì ìœ¼ë¡œ ANALYZE ì‹¤í–‰** - autovacuum ì„¤ì • í™•ì¸\n2. âœ… **ë³µí•© ì¸ë±ìŠ¤ ì»¬ëŸ¼ ìˆœì„œ** - ë“±í˜¸ ì¡°ê±´ â†’ ë²”ìœ„ ì¡°ê±´\n3. âœ… **Covering Index í™œìš©** - Index Only Scan ìœ ë„\n4. âœ… **Partial Index** - ìì£¼ ì¡°íšŒí•˜ëŠ” ì„œë¸Œì…‹ì— ì ìš©\n5. âœ… **pg_stat_statements í™œìš©** - ëŠë¦° ì¿¼ë¦¬ ëª¨ë‹ˆí„°ë§\n6. âœ… **ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì¸ë±ìŠ¤ ì œê±°** - ì“°ê¸° ì„±ëŠ¥ ì €í•˜ ë°©ì§€\n\n## ì°¸ê³  ìë£Œ\n\n- [PostgreSQL EXPLAIN ê³µì‹ ë¬¸ì„œ](https://www.postgresql.org/docs/current/sql-explain.html)\n- [Using EXPLAIN](https://www.postgresql.org/docs/current/using-explain.html)\n- [Index Types](https://www.postgresql.org/docs/current/indexes-types.html)",
    "docType": "original",
    "category": "Database",
    "tags": [
      "Database",
      "Performance",
      "PostgreSQL"
    ],
    "readingTime": 5,
    "wordCount": 856,
    "isFeatured": false,
    "isPublic": true,
    "date": "2026-01-02"
  },
  {
    "id": "de-12-data-quality",
    "slug": "de-12-data-quality",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-12-data-quality",
    "title": "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #12: ë°ì´í„° í’ˆì§ˆ - í…ŒìŠ¤íŠ¸, ëª¨ë‹ˆí„°ë§, ê´€ì¸¡ì„±",
    "excerpt": "ë°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ í’ˆì§ˆì„ ë³´ì¥í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤. dbt í…ŒìŠ¤íŠ¸, Great Expectations, ë°ì´í„° ê³„ë³´, ê´€ì¸¡ì„±ê¹Œì§€.",
    "content": "# ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #12: ë°ì´í„° í’ˆì§ˆ - í…ŒìŠ¤íŠ¸, ëª¨ë‹ˆí„°ë§, ê´€ì¸¡ì„±\n\n> **ëŒ€ìƒ ë…ì**: ì¶©ë¶„í•œ ê²½í—˜ì„ ê°€ì§„ ë°±ì—”ë“œ/í’€ìŠ¤íƒ ì—”ì§€ë‹ˆì–´ë¡œ, ì†Œí”„íŠ¸ì›¨ì–´ í…ŒìŠ¤íŠ¸ì— ìµìˆ™í•˜ì§€ë§Œ ë°ì´í„° í…ŒìŠ¤íŠ¸ëŠ” ì²˜ìŒì¸ ë¶„\n\n## ì´ í¸ì—ì„œ ë‹¤ë£¨ëŠ” ê²ƒ\n\n\"ëŒ€ì‹œë³´ë“œ ìˆ«ìê°€ ì™œ ì–´ì œì™€ ë‹¬ë¼ìš”?\" ì´ëŸ° ì§ˆë¬¸ì— ì²´ê³„ì ìœ¼ë¡œ ë‹µí•  ìˆ˜ ìˆëŠ” **ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì²´ê³„**ë¥¼ ë°°ì›ë‹ˆë‹¤.\n\n---\n\n## ë°ì´í„° í’ˆì§ˆì´ë€?\n\n### í’ˆì§ˆì˜ ë‹¤ì„¯ ê°€ì§€ ì°¨ì›\n\n```mermaid\nflowchart TB\n    subgraph Dimensions [\"ë°ì´í„° í’ˆì§ˆ ì°¨ì›\"]\n        C[\"ì™„ì „ì„±<br/>(Completeness)<br/>NULLì´ ì—†ëŠ”ê°€?\"]\n        A[\"ì •í™•ì„±<br/>(Accuracy)<br/>ê°’ì´ ì˜¬ë°”ë¥¸ê°€?\"]\n        Con[\"ì¼ê´€ì„±<br/>(Consistency)<br/>ê·œì¹™ì— ë§ëŠ”ê°€?\"]\n        T[\"ì ì‹œì„±<br/>(Timeliness)<br/>ìµœì‹ ì¸ê°€?\"]\n        V[\"ìœ íš¨ì„±<br/>(Validity)<br/>í˜•ì‹ì´ ë§ëŠ”ê°€?\"]\n    end\n```\n\n### ì†Œí”„íŠ¸ì›¨ì–´ í…ŒìŠ¤íŠ¸ì™€ì˜ ë¹„êµ\n\n| íŠ¹ì„± | ì†Œí”„íŠ¸ì›¨ì–´ í…ŒìŠ¤íŠ¸ | ë°ì´í„° í…ŒìŠ¤íŠ¸ |\n|------|------------------|--------------|\n| **ëŒ€ìƒ** | ì½”ë“œ | ë°ì´í„° |\n| **ì‹œì ** | ë°°í¬ ì „ | íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘/í›„ |\n| **ì…ë ¥** | ê³ ì • (mock) | ë³€ë™ (ì‹¤ì œ ë°ì´í„°) |\n| **ì‹¤íŒ¨ ëŒ€ì‘** | ë°°í¬ ì¤‘ë‹¨ | ì•Œë¦¼/ì¬ì²˜ë¦¬/ê²©ë¦¬ |\n| **ë„êµ¬** | JUnit, Jest | dbt, Great Expectations |\n\n---\n\n## dbt: ë³€í™˜ê³¼ í…ŒìŠ¤íŠ¸ì˜ í†µí•©\n\n### dbtë€?\n\n```mermaid\nflowchart LR\n    subgraph dbt [\"dbt (data build tool)\"]\n        Models[\"SQL ëª¨ë¸\"]\n        Tests[\"í…ŒìŠ¤íŠ¸\"]\n        Docs[\"ë¬¸ì„œí™”\"]\n        \n        Models --> Tests --> Docs\n    end\n    \n    subgraph Workflow [\"ì›Œí¬í”Œë¡œìš°\"]\n        Source[\"ì›ë³¸ ë°ì´í„°\"]\n        Transform[\"ë³€í™˜\"]\n        Target[\"ê²°ê³¼ í…Œì´ë¸”\"]\n        \n        Source --> Transform --> Target\n    end\n    \n    dbt --> Workflow\n```\n\n### í•µì‹¬ ì² í•™\n\n1. **SQL ê¸°ë°˜**: ë³µì¡í•œ ì½”ë“œ ì—†ì´ SQLë§Œìœ¼ë¡œ ë³€í™˜\n2. **ë²„ì „ ê´€ë¦¬**: Gitìœ¼ë¡œ ëª¨ë¸ ê´€ë¦¬\n3. **í…ŒìŠ¤íŠ¸ ë‚´ì¥**: ìŠ¤í‚¤ë§ˆì— í…ŒìŠ¤íŠ¸ ì •ì˜\n4. **ë¬¸ì„œ ìë™í™”**: ëª¨ë¸ ì •ë³´ ìë™ ìƒì„±\n5. **ì˜ì¡´ì„± ê´€ë¦¬**: ref() í•¨ìˆ˜ë¡œ ëª¨ë¸ ê°„ ì˜ì¡´ì„±\n\n### í”„ë¡œì íŠ¸ êµ¬ì¡°\n\n```\nmy_dbt_project/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ staging/\nâ”‚   â”‚   â”œâ”€â”€ stg_orders.sql\nâ”‚   â”‚   â””â”€â”€ schema.yml\nâ”‚   â”œâ”€â”€ marts/\nâ”‚   â”‚   â”œâ”€â”€ fct_orders.sql\nâ”‚   â”‚   â””â”€â”€ dim_customers.sql\nâ”‚   â””â”€â”€ schema.yml\nâ”œâ”€â”€ tests/\nâ”‚   â””â”€â”€ custom_tests.sql\nâ”œâ”€â”€ macros/\nâ”œâ”€â”€ dbt_project.yml\nâ””â”€â”€ profiles.yml\n```\n\n---\n\n## dbt í…ŒìŠ¤íŠ¸\n\n### í…ŒìŠ¤íŠ¸ ì¢…ë¥˜\n\n```mermaid\nflowchart TB\n    subgraph Tests [\"dbt í…ŒìŠ¤íŠ¸ ìœ í˜•\"]\n        subgraph Schema [\"ìŠ¤í‚¤ë§ˆ í…ŒìŠ¤íŠ¸\"]\n            S1[\"unique<br/>ì¤‘ë³µ ì—†ìŒ\"]\n            S2[\"not_null<br/>NULL ì—†ìŒ\"]\n            S3[\"accepted_values<br/>í—ˆìš©ê°’ ëª©ë¡\"]\n            S4[\"relationships<br/>ì°¸ì¡° ë¬´ê²°ì„±\"]\n        end\n        \n        subgraph Custom [\"ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸\"]\n            C1[\"SQL ê¸°ë°˜\"]\n            C2[\"ë³µì¡í•œ ë¡œì§\"]\n            C3[\"ë§¤í¬ë¡œ ì¬ì‚¬ìš©\"]\n        end\n    end\n```\n\n### schema.yml ì‘ì„±\n\n```yaml\n# models/marts/schema.yml\nversion: 2\n\nmodels:\n  - name: fct_orders\n    description: \"ì£¼ë¬¸ Fact í…Œì´ë¸”\"\n    columns:\n      - name: order_id\n        description: \"ì£¼ë¬¸ ê³ ìœ  ID\"\n        data_tests:\n          - unique\n          - not_null\n      \n      - name: customer_id\n        description: \"ê³ ê° ID\"\n        data_tests:\n          - not_null\n          - relationships:\n              to: ref('dim_customers')\n              field: customer_id\n      \n      - name: order_status\n        description: \"ì£¼ë¬¸ ìƒíƒœ\"\n        data_tests:\n          - accepted_values:\n              values: ['pending', 'shipped', 'delivered', 'cancelled']\n      \n      - name: total_amount\n        description: \"ì£¼ë¬¸ ì´ì•¡\"\n        data_tests:\n          - not_null\n          # dbt_utils íŒ¨í‚¤ì§€ ì‚¬ìš©\n          - dbt_utils.expression_is_true:\n              expression: \">= 0\"\n```\n\n### ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸\n\n```sql\n-- tests/assert_positive_revenue.sql\n-- ì´ ë§¤ì¶œì´ ì–‘ìˆ˜ì¸ì§€ í™•ì¸\n\nSELECT \n    order_date,\n    SUM(total_amount) as daily_revenue\nFROM {{ ref('fct_orders') }}\nGROUP BY order_date\nHAVING SUM(total_amount) < 0\n```\n\n### í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n\n```bash\n# ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰\ndbt test\n\n# íŠ¹ì • ëª¨ë¸ í…ŒìŠ¤íŠ¸\ndbt test --select fct_orders\n\n# ì‹¤íŒ¨ ì‹œ ìƒì„¸ ì •ë³´\ndbt test --store-failures\n```\n\n---\n\n## ë°ì´í„° Freshness\n\n### Source Freshness\n\n```yaml\n# models/staging/sources.yml\nversion: 2\n\nsources:\n  - name: raw\n    database: production\n    schema: public\n    freshness:\n      warn_after: {count: 12, period: hour}\n      error_after: {count: 24, period: hour}\n    \n    tables:\n      - name: orders\n        loaded_at_field: _etl_loaded_at\n        \n      - name: customers\n        loaded_at_field: updated_at\n```\n\n```bash\n# Freshness ì²´í¬\ndbt source freshness\n```\n\n### Freshness ê²°ê³¼\n\n```mermaid\nflowchart TB\n    subgraph Status [\"Freshness ìƒíƒœ\"]\n        Pass[\"âœ… Pass<br/>12ì‹œê°„ ì´ë‚´\"]\n        Warn[\"âš ï¸ Warn<br/>12~24ì‹œê°„\"]\n        Error[\"âŒ Error<br/>24ì‹œê°„ ì´ˆê³¼\"]\n    end\n```\n\n---\n\n## Great Expectations\n\n### dbtì™€ì˜ ë¹„êµ\n\n| íŠ¹ì„± | dbt | Great Expectations |\n|------|-----|-------------------|\n| **ì–¸ì–´** | SQL | Python |\n| **ì í•©í•œ ê²½ìš°** | SQL ë³€í™˜ í›„ í…ŒìŠ¤íŠ¸ | ì›ë³¸ ë°ì´í„° ê²€ì¦ |\n| **í•™ìŠµ ê³¡ì„ ** | ë‚®ìŒ | ì¤‘ê°„ |\n| **ìœ ì—°ì„±** | ì œí•œì  | ë†’ìŒ |\n| **ë¬¸ì„œí™”** | ìë™ | ìë™ (Data Docs) |\n\n### ê¸°ë³¸ ì‚¬ìš©ë²•\n\n```python\nimport great_expectations as gx\n\n# Context ìƒì„±\ncontext = gx.get_context()\n\n# ë°ì´í„° ì†ŒìŠ¤ ì—°ê²°\ndatasource = context.sources.add_pandas(\"my_datasource\")\ndata_asset = datasource.add_dataframe_asset(\"orders\")\n\n# Expectation Suite ì •ì˜\nsuite = context.add_expectation_suite(\"orders_suite\")\n\n# Expectations ì¶”ê°€\nsuite.add_expectation(\n    gx.expectations.ExpectColumnValuesToBeUnique(column=\"order_id\")\n)\nsuite.add_expectation(\n    gx.expectations.ExpectColumnValuesToNotBeNull(column=\"customer_id\")\n)\nsuite.add_expectation(\n    gx.expectations.ExpectColumnValuesToBeBetween(\n        column=\"total_amount\",\n        min_value=0,\n        max_value=1000000\n    )\n)\nsuite.add_expectation(\n    gx.expectations.ExpectColumnValuesToMatchRegex(\n        column=\"email\",\n        regex=r\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n    )\n)\n\n# Validation ì‹¤í–‰\nbatch = data_asset.build_batch_request()\nresults = context.run_checkpoint(\n    checkpoint_name=\"orders_checkpoint\",\n    batch_request=batch,\n    expectation_suite_name=\"orders_suite\"\n)\n\n# ê²°ê³¼ í™•ì¸\nprint(f\"Success: {results.success}\")\n```\n\n### Airflow ì—°ë™\n\n```python\nfrom airflow.decorators import dag, task\nfrom datetime import datetime\n\n@dag(\n    dag_id=\"data_quality_pipeline\",\n    schedule=\"@daily\",\n    start_date=datetime(2024, 1, 1)\n)\ndef quality_pipeline():\n    \n    @task\n    def run_great_expectations(**context):\n        import great_expectations as gx\n        \n        gx_context = gx.get_context()\n        results = gx_context.run_checkpoint(\n            checkpoint_name=\"orders_checkpoint\"\n        )\n        \n        if not results.success:\n            raise ValueError(\"Data quality check failed!\")\n        \n        return {\"success\": True, \"statistics\": results.statistics}\n    \n    @task\n    def run_dbt_tests():\n        import subprocess\n        result = subprocess.run([\"dbt\", \"test\"], capture_output=True)\n        \n        if result.returncode != 0:\n            raise ValueError(f\"dbt tests failed: {result.stderr}\")\n    \n    @task\n    def load_to_warehouse(quality_result):\n        # í’ˆì§ˆ ê²€ì¦ í†µê³¼ í›„ì—ë§Œ ë¡œë“œ\n        print(\"Loading data to warehouse...\")\n    \n    quality = run_great_expectations()\n    dbt = run_dbt_tests()\n    load_to_warehouse(quality)\n    \n    # dbtë„ í†µê³¼í•´ì•¼ í•¨\n    dbt >> load_to_warehouse\n\nquality_pipeline()\n```\n\n---\n\n## ë°ì´í„° ê³„ë³´ (Lineage)\n\n### ì™œ ê³„ë³´ê°€ ì¤‘ìš”í•œê°€?\n\n```mermaid\nflowchart TB\n    subgraph Problem [\"ë¬¸ì œ ìƒí™©\"]\n        P1[\"ëŒ€ì‹œë³´ë“œ ìˆ«ìê°€ ì´ìƒí•´ìš”\"]\n        P2[\"ì–´ë””ì„œ ì˜ëª»ëœ ê±°ì£ ?\"]\n        P3[\"ì–´ë–¤ í…Œì´ë¸”ì„ ë´ì•¼ í•˜ì£ ?\"]\n    end\n    \n    subgraph Lineage [\"Lineageë¡œ í•´ê²°\"]\n        L1[\"ë°ì´í„° íë¦„ ì¶”ì \"]\n        L2[\"ì˜í–¥ ë²”ìœ„ íŒŒì•…\"]\n        L3[\"ì›ì¸ ë¶„ì„\"]\n    end\n    \n    Problem --> Lineage\n```\n\n### dbtì˜ ìë™ Lineage\n\n```mermaid\nflowchart LR\n    subgraph Sources [\"Sources\"]\n        S1[(\"raw.orders\")]\n        S2[(\"raw.customers\")]\n    end\n    \n    subgraph Staging [\"Staging\"]\n        ST1[\"stg_orders\"]\n        ST2[\"stg_customers\"]\n    end\n    \n    subgraph Marts [\"Marts\"]\n        M1[\"fct_orders\"]\n        M2[\"dim_customers\"]\n    end\n    \n    S1 --> ST1 --> M1\n    S2 --> ST2 --> M2\n    ST2 --> M1\n```\n\n```bash\n# Lineage ë¬¸ì„œ ìƒì„±\ndbt docs generate\ndbt docs serve\n```\n\n### OpenLineage í‘œì¤€\n\n```mermaid\nflowchart TB\n    subgraph Tools [\"ë‹¤ì–‘í•œ ë„êµ¬\"]\n        Airflow[\"Airflow\"]\n        Spark[\"Spark\"]\n        dbt[\"dbt\"]\n        Flink[\"Flink\"]\n    end\n    \n    subgraph Standard [\"OpenLineage\"]\n        OL[\"í†µí•© í¬ë§·\"]\n    end\n    \n    subgraph Catalog [\"ë°ì´í„° ì¹´íƒˆë¡œê·¸\"]\n        Marquez[\"Marquez\"]\n        DataHub[\"DataHub\"]\n        Atlan[\"Atlan\"]\n    end\n    \n    Tools --> Standard --> Catalog\n```\n\n---\n\n## ëª¨ë‹ˆí„°ë§ê³¼ ê´€ì¸¡ì„±\n\n### í•µì‹¬ ì§€í‘œ\n\n```mermaid\nflowchart TB\n    subgraph Metrics [\"ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì§€í‘œ\"]\n        subgraph Availability [\"ê°€ìš©ì„±\"]\n            A1[\"íŒŒì´í”„ë¼ì¸ ì„±ê³µë¥ \"]\n            A2[\"SLA ì¤€ìˆ˜ìœ¨\"]\n        end\n        \n        subgraph Quality [\"í’ˆì§ˆ\"]\n            Q1[\"í…ŒìŠ¤íŠ¸ í†µê³¼ìœ¨\"]\n            Q2[\"ì´ìƒì¹˜ ë¹„ìœ¨\"]\n        end\n        \n        subgraph Freshness [\"ì‹ ì„ ë„\"]\n            F1[\"ë°ì´í„° ì§€ì—°\"]\n            F2[\"ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸\"]\n        end\n        \n        subgraph Volume [\"ë³¼ë¥¨\"]\n            V1[\"í–‰ ìˆ˜ ë³€í™”\"]\n            V2[\"íŒŒì¼ í¬ê¸°\"]\n        end\n    end\n```\n\n### ì´ìƒ íƒì§€\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import count, avg, stddev\n\nspark = SparkSession.builder.getOrCreate()\n\n# ì˜¤ëŠ˜ ë°ì´í„°\ntoday = spark.read.parquet(\"/data/today\")\ntoday_stats = today.agg(\n    count(\"*\").alias(\"row_count\"),\n    avg(\"amount\").alias(\"avg_amount\")\n).collect()[0]\n\n# íˆìŠ¤í† ë¦¬ (ìµœê·¼ 30ì¼ í‰ê· )\nhistory = spark.read.parquet(\"/data/history_30d\")\nhistory_stats = history.agg(\n    avg(\"daily_count\").alias(\"avg_count\"),\n    stddev(\"daily_count\").alias(\"std_count\"),\n    avg(\"daily_avg_amount\").alias(\"avg_amount\")\n).collect()[0]\n\n# ì´ìƒ íƒì§€ (3-sigma)\nif abs(today_stats[\"row_count\"] - history_stats[\"avg_count\"]) > 3 * history_stats[\"std_count\"]:\n    alert(\"Row count anomaly detected!\")\n```\n\n### ëŒ€ì‹œë³´ë“œ êµ¬ì„±\n\n```mermaid\nflowchart TB\n    subgraph Dashboard [\"ë°ì´í„° í’ˆì§ˆ ëŒ€ì‹œë³´ë“œ\"]\n        subgraph Overview [\"ê°œìš”\"]\n            O1[\"ğŸŸ¢ 95% íŒŒì´í”„ë¼ì¸ ì„±ê³µ\"]\n            O2[\"ğŸŸ¡ 2ê°œ SLA ê²½ê³ \"]\n            O3[\"ğŸ”´ 1ê°œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨\"]\n        end\n        \n        subgraph Details [\"ìƒì„¸\"]\n            D1[\"íŒŒì´í”„ë¼ì¸ë³„ ìƒíƒœ\"]\n            D2[\"í…ŒìŠ¤íŠ¸ ê²°ê³¼\"]\n            D3[\"ë°ì´í„° ë³¼ë¥¨ íŠ¸ë Œë“œ\"]\n        end\n        \n        subgraph Alerts [\"ì•Œë¦¼\"]\n            AL1[\"ì‹¤íŒ¨ ì•Œë¦¼\"]\n            AL2[\"ì´ìƒ íƒì§€ ì•Œë¦¼\"]\n        end\n    end\n```\n\n---\n\n## í”„ë¡œë•ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸\n\n### ë°°í¬ ì „ í™•ì¸\n\n```mermaid\nflowchart TB\n    subgraph Checklist [\"í”„ë¡œë•ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸\"]\n        C1[\"âœ… ëª¨ë“  dbt í…ŒìŠ¤íŠ¸ í†µê³¼\"]\n        C2[\"âœ… Source freshness í™•ì¸\"]\n        C3[\"âœ… Lineage ë¬¸ì„œí™”\"]\n        C4[\"âœ… ì•Œë¦¼ ì„¤ì • ì™„ë£Œ\"]\n        C5[\"âœ… ë¡¤ë°± ê³„íš ìˆ˜ë¦½\"]\n        C6[\"âœ… ë‹´ë‹¹ì ì§€ì •\"]\n    end\n```\n\n### ì¼ì¼ ìš´ì˜\n\n| ì‹œê°„ | ì‘ì—… | ë‹´ë‹¹ |\n|------|------|------|\n| 09:00 | ì•¼ê°„ ë°°ì¹˜ ê²°ê³¼ í™•ì¸ | ì˜¨ì½œ |\n| 09:30 | í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ê²€í†  | ë°ì´í„° íŒ€ |\n| 10:00 | ì´ìƒ ì•Œë¦¼ ì²˜ë¦¬ | í•´ë‹¹ ë‹´ë‹¹ì |\n| ë§¤ì‹œ | ìë™ freshness ì²´í¬ | ìë™í™” |\n\n---\n\n## ì •ë¦¬\n\n```mermaid\nmindmap\n  root((ë°ì´í„°<br/>í’ˆì§ˆ))\n    í’ˆì§ˆ ì°¨ì›\n      ì™„ì „ì„±\n      ì •í™•ì„±\n      ì¼ê´€ì„±\n      ì ì‹œì„±\n      ìœ íš¨ì„±\n    dbt\n      SQL ê¸°ë°˜\n      ìŠ¤í‚¤ë§ˆ í…ŒìŠ¤íŠ¸\n      Freshness\n      Lineage\n    Great Expectations\n      Python ê¸°ë°˜\n      ì»¤ìŠ¤í…€ ê²€ì¦\n      Data Docs\n    Lineage\n      ë°ì´í„° íë¦„\n      ì˜í–¥ ë¶„ì„\n      OpenLineage\n    ëª¨ë‹ˆí„°ë§\n      ì„±ê³µë¥ \n      ì§€ì—° ì‹œê°„\n      ì´ìƒ íƒì§€\n      ëŒ€ì‹œë³´ë“œ\n```\n\n---\n\n## ì‹œë¦¬ì¦ˆ ë§ˆë¬´ë¦¬\n\n12í¸ì— ê±¸ì³ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ì˜ í•µì‹¬ì„ ë‹¤ë¤˜ìŠµë‹ˆë‹¤:\n\n| Part | ì£¼ì œ | í•µì‹¬ ê¸°ìˆ  |\n|------|------|----------|\n| 1-2 | ê°œë… | OLTP/OLAP, ì•„í‚¤í…ì²˜ |\n| 3-5 | Spark | RDD, DataFrame, ìµœì í™” |\n| 6-7 | Airflow | DAG, TaskFlow, ìš´ì˜ |\n| 8-9 | ìŠ¤íŠ¸ë¦¬ë° | Kafka, Spark Streaming |\n| 10-11 | ì €ì¥ì†Œ | Lakehouse, ëª¨ë¸ë§ |\n| 12 | í’ˆì§ˆ | í…ŒìŠ¤íŠ¸, ëª¨ë‹ˆí„°ë§ |\n\nì´ì œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ ì „ì²´ ê·¸ë¦¼ì„ ì´í•´í•˜ì…¨ì„ ê²ë‹ˆë‹¤. ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì ìš©í•˜ë©´ì„œ ê¹Šì´ë¥¼ ë”í•´ ê°€ì‹œê¸¸ ë°”ëë‹ˆë‹¤!\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [dbt Documentation](https://docs.getdbt.com/)\n- [Great Expectations Documentation](https://docs.greatexpectations.io/)\n- [OpenLineage](https://openlineage.io/)\n- Monte Carlo, \"Data Observability Explained\"\n- \"Fundamentals of Data Engineering\" (O'Reilly)",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Data Engineering",
      "Data Quality",
      "Great Expectations",
      "Lineage",
      "Observability",
      "Testing",
      "dbt"
    ],
    "readingTime": 7,
    "wordCount": 1269,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-11-dimensional-modeling",
    "slug": "de-11-dimensional-modeling",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-11-dimensional-modeling",
    "title": "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #11: ë°ì´í„° ëª¨ë¸ë§ - Star Schemaì™€ Slowly Changing Dimensions",
    "excerpt": "ë¶„ì„ìš© ë°ì´í„° ëª¨ë¸ë§ì˜ í•µì‹¬ì„ ë°°ì›ë‹ˆë‹¤. Star Schema, Fact/Dimension í…Œì´ë¸”, SCD Type 2 íŒ¨í„´ì„ Delta Lake MERGEë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.",
    "content": "# ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #11: ë°ì´í„° ëª¨ë¸ë§ - Star Schemaì™€ Slowly Changing Dimensions\n\n> **ëŒ€ìƒ ë…ì**: ì¶©ë¶„í•œ ê²½í—˜ì„ ê°€ì§„ ë°±ì—”ë“œ/í’€ìŠ¤íƒ ì—”ì§€ë‹ˆì–´ë¡œ, RDBMS ì •ê·œí™”ì— ìµìˆ™í•˜ì§€ë§Œ ë¶„ì„ìš© ëª¨ë¸ë§ì€ ì²˜ìŒì¸ ë¶„\n\n## ì´ í¸ì—ì„œ ë‹¤ë£¨ëŠ” ê²ƒ\n\në°±ì—”ë“œì—ì„œì˜ DB ì„¤ê³„ì™€ **ë¶„ì„ìš© ë°ì´í„° ëª¨ë¸ë§ì€ ì™„ì „íˆ ë‹¤ë¦…ë‹ˆë‹¤**. ì™œ ë‹¤ë¥¸ì§€, ì–´ë–»ê²Œ ì„¤ê³„í•˜ëŠ”ì§€ ë°°ì›ë‹ˆë‹¤.\n\n---\n\n## OLTP vs OLAP ëª¨ë¸ë§ì˜ ì°¨ì´\n\n### ì„¤ê³„ ëª©í‘œê°€ ë‹¤ë¥´ë‹¤\n\n```mermaid\nflowchart TB\n    subgraph OLTP [\"OLTP (ìš´ì˜ DB)\"]\n        T1[\"ëª©í‘œ: ë°ì´í„° ë¬´ê²°ì„±\"]\n        T2[\"ì •ê·œí™” (3NF)\"]\n        T3[\"ì¤‘ë³µ ìµœì†Œí™”\"]\n        T4[\"ë¹ ë¥¸ ë‹¨ê±´ ì¡°íšŒ/ìˆ˜ì •\"]\n    end\n    \n    subgraph OLAP [\"OLAP (ë¶„ì„ DB)\"]\n        A1[\"ëª©í‘œ: ì¿¼ë¦¬ ì„±ëŠ¥\"]\n        A2[\"ë¹„ì •ê·œí™”\"]\n        A3[\"ì¤‘ë³µ í—ˆìš©\"]\n        A4[\"ë¹ ë¥¸ ì§‘ê³„ ì¿¼ë¦¬\"]\n    end\n```\n\n### ì˜ˆì‹œë¡œ ë¹„êµ\n\n**OLTP (ì •ê·œí™”)**:\n\n```sql\n-- ì£¼ë¬¸ ì¡°íšŒ: 3ê°œ í…Œì´ë¸” ì¡°ì¸ í•„ìš”\nSELECT o.id, c.name, p.product_name\nFROM orders o\nJOIN customers c ON o.customer_id = c.id\nJOIN products p ON o.product_id = p.id\nWHERE o.id = 12345;\n```\n\n**OLAP (ë¹„ì •ê·œí™”)**:\n\n```sql\n-- ì´ë¯¸ ì¡°ì¸ëœ ìƒíƒœë¡œ ì €ì¥\nSELECT order_id, customer_name, product_name\nFROM fact_orders\nWHERE order_id = 12345;\n```\n\n### ì™œ ë¹„ì •ê·œí™”í•˜ëŠ”ê°€?\n\n```mermaid\nflowchart TB\n    subgraph Problem [\"ì¡°ì¸ì˜ ë¹„ìš©\"]\n        P1[\"100ë§Œ ì£¼ë¬¸ Ã— 10ë§Œ ê³ ê° Ã— 1000 ìƒí’ˆ\"]\n        P2[\"3-way ì¡°ì¸ = ë§¤ìš° ëŠë¦¼\"]\n        P3[\"ë¶„ì„ ì¿¼ë¦¬ë§ˆë‹¤ ë°˜ë³µ\"]\n    end\n    \n    subgraph Solution [\"ë¹„ì •ê·œí™”\"]\n        S1[\"í•œ ë²ˆ ì¡°ì¸í•´ì„œ ì €ì¥\"]\n        S2[\"ì´í›„ ì¿¼ë¦¬ëŠ” ìŠ¤ìº”ë§Œ\"]\n        S3[\"ì €ì¥ ê³µê°„ â†” ì¿¼ë¦¬ ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„\"]\n    end\n    \n    Problem --> Solution\n```\n\n---\n\n## Kimball vs Inmon\n\n### ë‘ ê°€ì§€ ë°©ë²•ë¡ \n\n```mermaid\nflowchart TB\n    subgraph Kimball [\"Kimball (Bottom-Up)\"]\n        K1[\"ë¹„ì¦ˆë‹ˆìŠ¤ í”„ë¡œì„¸ìŠ¤ë³„ ì„¤ê³„\"]\n        K2[\"Data Mart ë¨¼ì €\"]\n        K3[\"Star Schema\"]\n        K4[\"ë¹ ë¥¸ êµ¬í˜„\"]\n    end\n    \n    subgraph Inmon [\"Inmon (Top-Down)\"]\n        I1[\"ì „ì‚¬ ë°ì´í„° ëª¨ë¸ ë¨¼ì €\"]\n        I2[\"Enterprise DW\"]\n        I3[\"3NF ìœ ì§€\"]\n        I4[\"Data MartëŠ” ë‚˜ì¤‘ì—\"]\n    end\n```\n\n| íŠ¹ì„± | Kimball | Inmon |\n|------|---------|-------|\n| **ì ‘ê·¼** | Bottom-Up | Top-Down |\n| **ì‹œì‘ì ** | ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ | ì „ì‚¬ ëª¨ë¸ |\n| **êµ¬í˜„ ì†ë„** | ë¹ ë¦„ | ëŠë¦¼ |\n| **ì¼ê´€ì„±** | Martë³„ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ | ë†’ìŒ |\n| **ë³µì¡ë„** | ë‚®ìŒ | ë†’ìŒ |\n| **í˜„ëŒ€ íŠ¸ë Œë“œ** | âœ… ì„ í˜¸ | ì¼ë¶€ ì±„íƒ |\n\n> **í˜„ëŒ€ íŠ¸ë Œë“œ**: Kimball ë°©ì‹ì´ **ë” ì‹¤ìš©ì **ìœ¼ë¡œ í‰ê°€. ë¹ ë¥´ê²Œ ê°€ì¹˜ ì œê³µ í›„ ì ì§„ì  í™•ì¥.\n\n---\n\n## Star Schema\n\n### êµ¬ì¡°\n\n```mermaid\nflowchart TB\n    subgraph Star [\"Star Schema\"]\n        Fact[\"ğŸ“Š fact_orders<br/>(Fact Table)\"]\n        \n        DimCustomer[\"ğŸ‘¤ dim_customers\"]\n        DimProduct[\"ğŸ“¦ dim_products\"]\n        DimDate[\"ğŸ“… dim_date\"]\n        DimStore[\"ğŸª dim_stores\"]\n        \n        Fact --> DimCustomer\n        Fact --> DimProduct\n        Fact --> DimDate\n        Fact --> DimStore\n    end\n```\n\n### Fact Table (ì‚¬ì‹¤ í…Œì´ë¸”)\n\n```mermaid\nflowchart TB\n    subgraph FactTable [\"Fact Table: fact_orders\"]\n        direction TB\n        Keys[\"ğŸ”‘ Foreign Keys<br/>customer_key, product_key,<br/>date_key, store_key\"]\n        Measures[\"ğŸ“ Measures (ì¸¡ì •ê°’)<br/>quantity, amount,<br/>discount, tax\"]\n        Grain[\"ğŸ“ Grain: ì£¼ë¬¸ 1ê±´\"]\n    end\n```\n\n```sql\nCREATE TABLE fact_orders (\n    -- Surrogate Keys (FK)\n    order_key       BIGINT,\n    customer_key    BIGINT,\n    product_key     BIGINT,\n    date_key        INT,\n    store_key       BIGINT,\n    \n    -- Measures\n    quantity        INT,\n    unit_price      DECIMAL(10,2),\n    discount        DECIMAL(5,2),\n    total_amount    DECIMAL(12,2),\n    \n    -- Degenerate Dimension (ì›ë³¸ í‚¤)\n    order_id        VARCHAR(50)\n);\n```\n\n### Dimension Table (ì°¨ì› í…Œì´ë¸”)\n\n```mermaid\nflowchart TB\n    subgraph DimTable [\"Dimension Table: dim_customers\"]\n        direction TB\n        SK[\"ğŸ”‘ Surrogate Key<br/>customer_key (ìë™ ìƒì„±)\"]\n        NK[\"ğŸ·ï¸ Natural Key<br/>customer_id (ì›ë³¸ ID)\"]\n        Attrs[\"ğŸ“‹ Attributes<br/>name, email, segment,<br/>city, country\"]\n    end\n```\n\n```sql\nCREATE TABLE dim_customers (\n    -- Surrogate Key\n    customer_key    BIGINT PRIMARY KEY,\n    \n    -- Natural Key\n    customer_id     VARCHAR(50),\n    \n    -- Attributes\n    name            VARCHAR(200),\n    email           VARCHAR(200),\n    segment         VARCHAR(50),\n    city            VARCHAR(100),\n    country         VARCHAR(50),\n    \n    -- Metadata\n    created_at      TIMESTAMP,\n    updated_at      TIMESTAMP\n);\n```\n\n### ì™œ Surrogate Keyì¸ê°€?\n\n```mermaid\nflowchart TB\n    subgraph Problem [\"Natural Key ë¬¸ì œ\"]\n        P1[\"customer_id ë³€ê²½ë˜ë©´?\"]\n        P2[\"ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ì¤‘ë³µ?\"]\n        P3[\"ë°ì´í„° íƒ€ì…ì´ ë‹¤ë¥´ë©´?\"]\n    end\n    \n    subgraph Solution [\"Surrogate Key í•´ê²°\"]\n        S1[\"ë‚´ë¶€ ìƒì„± ì •ìˆ˜ í‚¤\"]\n        S2[\"ë³€ê²½ ì—†ì´ ì•ˆì •ì \"]\n        S3[\"ì¡°ì¸ ì„±ëŠ¥ ìš°ìˆ˜\"]\n    end\n    \n    Problem --> Solution\n```\n\n---\n\n## Slowly Changing Dimensions (SCD)\n\n### ë¬¸ì œ ìƒí™©\n\n```mermaid\nflowchart TB\n    subgraph Problem [\"ì°¨ì› ë°ì´í„° ë³€ê²½\"]\n        P1[\"ê³ ê° 'ê¹€ì² ìˆ˜'ê°€<br/>ì„œìš¸ â†’ ë¶€ì‚° ì´ì‚¬\"]\n        P2[\"ê¸°ì¡´ ì£¼ë¬¸ì€<br/>ì–´ëŠ ë„ì‹œë¡œ ë³´ì—¬ì•¼ í• ê¹Œ?\"]\n    end\n    \n    subgraph Options [\"ì„ íƒì§€\"]\n        O1[\"í•­ìƒ 'ë¶€ì‚°' (í˜„ì¬ ê°’)\"]\n        O2[\"ì£¼ë¬¸ ë‹¹ì‹œ 'ì„œìš¸' (íˆìŠ¤í† ë¦¬)\"]\n    end\n```\n\n### SCD ìœ í˜•\n\n| Type | ì „ëµ | ì„¤ëª… | íˆìŠ¤í† ë¦¬ |\n|------|------|------|----------|\n| **Type 0** | ìœ ì§€ | ë³€ê²½í•˜ì§€ ì•ŠìŒ | âŒ |\n| **Type 1** | ë®ì–´ì“°ê¸° | ìµœì‹  ê°’ìœ¼ë¡œ êµì²´ | âŒ |\n| **Type 2** | íˆìŠ¤í† ë¦¬ | ìƒˆ í–‰ ì¶”ê°€ | âœ… |\n| **Type 3** | ì´ì „ê°’ ì»¬ëŸ¼ | í˜„ì¬ + ì´ì „ ê°’ | ì œí•œì  |\n\n### SCD Type 2 ìƒì„¸\n\n```mermaid\nflowchart TB\n    subgraph Before [\"ë³€ê²½ ì „\"]\n        R1[\"customer_key=1<br/>name='ê¹€ì² ìˆ˜'<br/>city='ì„œìš¸'<br/>is_current=true\"]\n    end\n    \n    subgraph After [\"ë³€ê²½ í›„\"]\n        R2[\"customer_key=1<br/>name='ê¹€ì² ìˆ˜'<br/>city='ì„œìš¸'<br/>is_current=false<br/>expiry_date='2024-03-01'\"]\n        R3[\"customer_key=2<br/>name='ê¹€ì² ìˆ˜'<br/>city='ë¶€ì‚°'<br/>is_current=true<br/>effective_date='2024-03-01'\"]\n    end\n    \n    Before --> After\n```\n\n### SCD Type 2 ìŠ¤í‚¤ë§ˆ\n\n```sql\nCREATE TABLE dim_customers (\n    -- Surrogate Key (ê° ë²„ì „ë§ˆë‹¤ ë‹¤ë¦„)\n    customer_key    BIGINT PRIMARY KEY,\n    \n    -- Natural Key (ë™ì¼)\n    customer_id     VARCHAR(50),\n    \n    -- Attributes\n    name            VARCHAR(200),\n    email           VARCHAR(200),\n    city            VARCHAR(100),\n    country         VARCHAR(50),\n    \n    -- SCD Type 2 Tracking\n    effective_date  DATE,\n    expiry_date     DATE,        -- NULL = í˜„ì¬ ë ˆì½”ë“œ\n    is_current      BOOLEAN\n);\n```\n\n### Delta Lake MERGEë¡œ SCD Type 2 êµ¬í˜„\n\n```python\nfrom delta.tables import DeltaTable\nfrom pyspark.sql.functions import current_date, lit, col\n\n# ê¸°ì¡´ ì°¨ì› í…Œì´ë¸”\ndim_customers = DeltaTable.forPath(spark, \"/delta/dim_customers\")\n\n# ìƒˆë¡œìš´/ë³€ê²½ëœ ë°ì´í„°\nstaging = spark.read.parquet(\"/staging/customers\")\n\n# Step 1: ë³€ê²½ëœ ë ˆì½”ë“œ ì°¾ê¸° (ê¸°ì¡´ current ë ˆì½”ë“œì™€ ë¹„êµ)\nchanges = dim_customers.toDF().alias(\"dim\") \\\n    .join(staging.alias(\"stg\"), \n          (col(\"dim.customer_id\") == col(\"stg.customer_id\")) & \n          col(\"dim.is_current\")) \\\n    .filter(\n        (col(\"dim.name\") != col(\"stg.name\")) | \n        (col(\"dim.city\") != col(\"stg.city\"))\n    ) \\\n    .select(\"dim.customer_key\")\n\n# Step 2: ê¸°ì¡´ ë ˆì½”ë“œ ë§Œë£Œ ì²˜ë¦¬\ndim_customers.alias(\"dim\").merge(\n    changes.alias(\"chg\"),\n    \"dim.customer_key = chg.customer_key\"\n).whenMatched().update(\n    set={\n        \"is_current\": lit(False),\n        \"expiry_date\": current_date()\n    }\n).execute()\n\n# Step 3: ìƒˆ ë²„ì „ ì‚½ì…\nnew_records = staging.withColumn(\"effective_date\", current_date()) \\\n    .withColumn(\"expiry_date\", lit(None)) \\\n    .withColumn(\"is_current\", lit(True)) \\\n    .withColumn(\"customer_key\", monotonically_increasing_id())\n\nnew_records.write.format(\"delta\") \\\n    .mode(\"append\") \\\n    .save(\"/delta/dim_customers\")\n```\n\n---\n\n## ì‹¤ì „ ì˜ˆì œ: ì´ì»¤ë¨¸ìŠ¤ ë°ì´í„° ëª¨ë¸\n\n### ì „ì²´ ìŠ¤í‚¤ë§ˆ\n\n```mermaid\nflowchart TB\n    subgraph Facts [\"Fact Tables\"]\n        F1[\"fact_orders<br/>(ì£¼ë¬¸)\"]\n        F2[\"fact_page_views<br/>(í˜ì´ì§€ë·°)\"]\n    end\n    \n    subgraph Dimensions [\"Dimension Tables\"]\n        D1[\"dim_customers<br/>(SCD2)\"]\n        D2[\"dim_products<br/>(SCD1)\"]\n        D3[\"dim_date\"]\n        D4[\"dim_promotions\"]\n    end\n    \n    F1 --> D1\n    F1 --> D2\n    F1 --> D3\n    F1 --> D4\n    \n    F2 --> D1\n    F2 --> D3\n```\n\n### ë¶„ì„ ì¿¼ë¦¬ ì˜ˆì‹œ\n\n```sql\n-- ì›”ë³„/ì„¸ê·¸ë¨¼íŠ¸ë³„ ë§¤ì¶œ\nSELECT \n    d.year,\n    d.month,\n    c.segment,\n    SUM(f.total_amount) AS revenue,\n    COUNT(DISTINCT c.customer_key) AS customers\nFROM fact_orders f\nJOIN dim_date d ON f.date_key = d.date_key\nJOIN dim_customers c ON f.customer_key = c.customer_key\n    AND c.is_current = TRUE  -- í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ ê¸°ì¤€\nWHERE d.year = 2024\nGROUP BY d.year, d.month, c.segment\nORDER BY d.year, d.month;\n\n-- ì£¼ë¬¸ ë‹¹ì‹œ ê³ ê° ì •ë³´ë¡œ ë¶„ì„ (íˆìŠ¤í† ë¦¬)\nSELECT \n    d.year,\n    d.month,\n    c.city,  -- ì£¼ë¬¸ ë‹¹ì‹œ ê±°ì£¼ ë„ì‹œ\n    SUM(f.total_amount) AS revenue\nFROM fact_orders f\nJOIN dim_date d ON f.date_key = d.date_key\nJOIN dim_customers c ON f.customer_key = c.customer_key\n    -- SCD2: ì£¼ë¬¸ ë‚ ì§œê°€ ìœ íš¨ ê¸°ê°„ ë‚´\n    AND d.full_date >= c.effective_date\n    AND (d.full_date < c.expiry_date OR c.expiry_date IS NULL)\nGROUP BY d.year, d.month, c.city;\n```\n\n---\n\n## Date Dimension\n\n### í•„ìˆ˜ íŒ¨í„´\n\n```sql\nCREATE TABLE dim_date (\n    date_key        INT PRIMARY KEY,    -- YYYYMMDD\n    full_date       DATE,\n    year            INT,\n    quarter         INT,\n    month           INT,\n    month_name      VARCHAR(20),\n    week            INT,\n    day_of_week     INT,\n    day_name        VARCHAR(20),\n    is_weekend      BOOLEAN,\n    is_holiday      BOOLEAN,\n    fiscal_year     INT,\n    fiscal_quarter  INT\n);\n```\n\n### ë¯¸ë¦¬ ì±„ìš°ê¸°\n\n```python\nimport pandas as pd\nfrom pyspark.sql import SparkSession\n\n# 10ë…„ì¹˜ ë‚ ì§œ ìƒì„±\ndates = pd.date_range('2020-01-01', '2030-12-31')\n\ndim_date = pd.DataFrame({\n    'date_key': dates.strftime('%Y%m%d').astype(int),\n    'full_date': dates,\n    'year': dates.year,\n    'quarter': dates.quarter,\n    'month': dates.month,\n    'month_name': dates.strftime('%B'),\n    'week': dates.isocalendar().week,\n    'day_of_week': dates.dayofweek,\n    'day_name': dates.strftime('%A'),\n    'is_weekend': dates.dayofweek >= 5,\n})\n\nspark.createDataFrame(dim_date).write.format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .save(\"/delta/dim_date\")\n```\n\n---\n\n## ì •ë¦¬\n\n```mermaid\nmindmap\n  root((ë°ì´í„°<br/>ëª¨ë¸ë§))\n    OLTP vs OLAP\n      ì •ê·œí™” vs ë¹„ì •ê·œí™”\n      ë¬´ê²°ì„± vs ì„±ëŠ¥\n    Kimball\n      Bottom-Up\n      Star Schema\n      ë¹ ë¥¸ êµ¬í˜„\n    Star Schema\n      Fact Table\n        Measures\n        Foreign Keys\n      Dimension Table\n        Attributes\n        Surrogate Key\n    SCD\n      Type 0: ìœ ì§€\n      Type 1: ë®ì–´ì“°ê¸°\n      Type 2: íˆìŠ¤í† ë¦¬\n      Type 3: ì´ì „ê°’ ì»¬ëŸ¼\n    Date Dimension\n      ë¯¸ë¦¬ ìƒì„±\n      ë¶„ì„ í¸ì˜\n```\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**12í¸: ë°ì´í„° í’ˆì§ˆ**ì—ì„œëŠ” ìš´ì˜ì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- dbtë¥¼ ì´ìš©í•œ ë³€í™˜ê³¼ í…ŒìŠ¤íŠ¸\n- Great Expectations\n- ë°ì´í„° ê³„ë³´ (Lineage)\n- ëª¨ë‹ˆí„°ë§ê³¼ ê´€ì¸¡ì„±\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- Ralph Kimball, \"The Data Warehouse Toolkit\"\n- dbt Labs, \"Building Slowly Changing Dimensions\"\n- Databricks, \"Data Modeling Best Practices\"",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Data Engineering",
      "Data Modeling"
    ],
    "readingTime": 6,
    "wordCount": 1191,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-10-lakehouse-architecture",
    "slug": "de-10-lakehouse-architecture",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-10-lakehouse-architecture",
    "title": "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #10: ë°ì´í„° ë ˆì´í¬ vs ì›¨ì–´í•˜ìš°ìŠ¤ - ë ˆì´í¬í•˜ìš°ìŠ¤ ì•„í‚¤í…ì²˜",
    "excerpt": "ë°ì´í„° ì €ì¥ì†Œ ì•„í‚¤í…ì²˜ì˜ ì¢…ë¥˜ì™€ ì„ íƒ ê¸°ì¤€ì„ ë°°ì›ë‹ˆë‹¤. Delta Lakeì˜ ACID, Time Travel, Schema Evolutionì„ ì‹¬ì¸µ ë¶„ì„í•©ë‹ˆë‹¤.",
    "content": "# ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #10: ë°ì´í„° ë ˆì´í¬ vs ì›¨ì–´í•˜ìš°ìŠ¤ - ë ˆì´í¬í•˜ìš°ìŠ¤ ì•„í‚¤í…ì²˜\n\n> **ëŒ€ìƒ ë…ì**: ì¶©ë¶„í•œ ê²½í—˜ì„ ê°€ì§„ ë°±ì—”ë“œ/í’€ìŠ¤íƒ ì—”ì§€ë‹ˆì–´ë¡œ, PostgreSQL ACIDì— ìµìˆ™í•˜ì§€ë§Œ ë°ì´í„° ë ˆì´í¬/ì›¨ì–´í•˜ìš°ìŠ¤ëŠ” ì²˜ìŒì¸ ë¶„\n\n## ì´ í¸ì—ì„œ ë‹¤ë£¨ëŠ” ê²ƒ\n\n\"S3ì— Parquet ì˜¬ë ¤ë‘ë©´ ë˜ëŠ” ê±° ì•„ë‹Œê°€ìš”?\" ë¼ëŠ” ì§ˆë¬¸ì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤. **ì™œ Delta Lake ê°™ì€ í…Œì´ë¸” í¬ë§·ì´ í•„ìš”í•œì§€**, ê·¸ë¦¬ê³  **ë ˆì´í¬í•˜ìš°ìŠ¤ê°€ ë¬´ì—‡ì¸ì§€** ë°°ì›ë‹ˆë‹¤.\n\n---\n\n## ë°ì´í„° ì €ì¥ì†Œì˜ ì§„í™”\n\n### ì„¸ëŒ€ë³„ ë³€í™”\n\n```mermaid\nflowchart LR\n    subgraph Gen1 [\"1ì„¸ëŒ€: ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤\"]\n        DW1[\"ì˜¨í”„ë ˆë¯¸ìŠ¤<br/>Oracle, Teradata\"]\n        DW2[\"êµ¬ì¡°í™”ëœ ë°ì´í„°\"]\n        DW3[\"SQL ë¶„ì„\"]\n    end\n    \n    subgraph Gen2 [\"2ì„¸ëŒ€: ë°ì´í„° ë ˆì´í¬\"]\n        DL1[\"í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€<br/>S3, GCS\"]\n        DL2[\"ëª¨ë“  í˜•íƒœì˜ ë°ì´í„°\"]\n        DL3[\"Spark ì²˜ë¦¬\"]\n    end\n    \n    subgraph Gen3 [\"3ì„¸ëŒ€: ë ˆì´í¬í•˜ìš°ìŠ¤\"]\n        LH1[\"ë ˆì´í¬ ìœ„ì—<br/>ì›¨ì–´í•˜ìš°ìŠ¤ ê¸°ëŠ¥\"]\n        LH2[\"Delta Lake, Iceberg\"]\n        LH3[\"ACID + ìœ ì—°ì„±\"]\n    end\n    \n    Gen1 -->|\"í™•ì¥ì„± í•œê³„\"| Gen2 -->|\"í’ˆì§ˆ ë¬¸ì œ\"| Gen3\n```\n\n---\n\n## ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ (Data Warehouse)\n\n### íŠ¹ì§•\n\n```mermaid\nflowchart TB\n    subgraph DW [\"Data Warehouse\"]\n        direction TB\n        Feature1[\"âœ… ìŠ¤í‚¤ë§ˆ ì •ì˜ (Schema-on-Write)\"]\n        Feature2[\"âœ… ACID íŠ¸ëœì­ì…˜\"]\n        Feature3[\"âœ… SQL ì§€ì›\"]\n        Feature4[\"âœ… ë¹ ë¥¸ ì¿¼ë¦¬\"]\n        \n        Limit1[\"âŒ êµ¬ì¡°í™”ëœ ë°ì´í„°ë§Œ\"]\n        Limit2[\"âŒ ë¹„ìš© (ì €ì¥+ì»´í“¨íŒ…)\"]\n        Limit3[\"âŒ ë²¤ë” ì¢…ì†\"]\n    end\n    \n    Examples[\"ì˜ˆì‹œ:<br/>â€¢ Snowflake<br/>â€¢ BigQuery<br/>â€¢ Redshift\"]\n```\n\n### PostgreSQLê³¼ì˜ ë¹„êµ\n\n| íŠ¹ì„± | PostgreSQL (OLTP) | BigQuery (DW) |\n|------|-------------------|---------------|\n| **ëª©ì ** | íŠ¸ëœì­ì…˜ ì²˜ë¦¬ | ë¶„ì„ ì¿¼ë¦¬ |\n| **ìŠ¤í† ë¦¬ì§€** | Row-based | Column-based |\n| **ìŠ¤ì¼€ì¼** | ìˆ˜ì§ í™•ì¥ | ë¬´í•œ ìˆ˜í‰ í™•ì¥ |\n| **ë¹„ìš©** | ì„œë²„ ë¹„ìš© | ì¿¼ë¦¬ë‹¹ ë¹„ìš© |\n| **ì¿¼ë¦¬ ì†ë„** | ë‹¨ê±´ ë¹ ë¦„ | ì§‘ê³„ ë¹ ë¦„ |\n\n---\n\n## ë°ì´í„° ë ˆì´í¬ (Data Lake)\n\n### íŠ¹ì§•\n\n```mermaid\nflowchart TB\n    subgraph DL [\"Data Lake\"]\n        direction TB\n        Feature1[\"âœ… ëª¨ë“  í˜•íƒœì˜ ë°ì´í„°\"]\n        Feature2[\"âœ… ì €ë ´í•œ ì €ì¥ ë¹„ìš©\"]\n        Feature3[\"âœ… ë¶„ë¦¬ëœ ì €ì¥/ì»´í“¨íŒ…\"]\n        Feature4[\"âœ… ìœ ì—°í•œ ì²˜ë¦¬ (Spark ë“±)\"]\n        \n        Limit1[\"âŒ ACID ì—†ìŒ\"]\n        Limit2[\"âŒ ìŠ¤í‚¤ë§ˆ ê´€ë¦¬ ì–´ë ¤ì›€\"]\n        Limit3[\"âŒ ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ\"]\n    end\n    \n    Examples[\"ì˜ˆì‹œ:<br/>â€¢ S3 + Parquet<br/>â€¢ GCS + Avro<br/>â€¢ ADLS + JSON\"]\n```\n\n### ë°ì´í„° ë ˆì´í¬ì˜ ë¬¸ì œì \n\n```mermaid\nflowchart TB\n    subgraph Problems [\"ë ˆì´í¬ì˜ ê³ ì§ˆì  ë¬¸ì œ\"]\n        P1[\"ë™ì‹œ ì“°ê¸° ì¶©ëŒ\"]\n        P2[\"ë¶€ë¶„ ì‹¤íŒ¨ â†’ ê¹¨ì§„ ë°ì´í„°\"]\n        P3[\"ìŠ¤í‚¤ë§ˆ ë³€ê²½ â†’ í˜¸í™˜ì„± ë¬¸ì œ\"]\n        P4[\"ì‚­ì œ/ìˆ˜ì • ì–´ë ¤ì›€\"]\n        P5[\"ì‘ì€ íŒŒì¼ ë¬¸ì œ\"]\n    end\n    \n    Result[\"ê²°êµ­... ë°ì´í„° ëŠª(Data Swamp)\"]\n    \n    Problems --> Result\n```\n\n---\n\n## ë ˆì´í¬í•˜ìš°ìŠ¤ (Lakehouse)\n\n### ë‘ ì„¸ê³„ì˜ í†µí•©\n\n```mermaid\nflowchart TB\n    subgraph Lakehouse [\"Lakehouse Architecture\"]\n        subgraph Top [\"ì›¨ì–´í•˜ìš°ìŠ¤ ê¸°ëŠ¥\"]\n            T1[\"ACID íŠ¸ëœì­ì…˜\"]\n            T2[\"ìŠ¤í‚¤ë§ˆ ê´€ë¦¬\"]\n            T3[\"Time Travel\"]\n            T4[\"SQL ì§€ì›\"]\n        end\n        \n        subgraph Middle [\"í…Œì´ë¸” í¬ë§·\"]\n            M1[\"Delta Lake\"]\n            M2[\"Apache Iceberg\"]\n            M3[\"Apache Hudi\"]\n        end\n        \n        subgraph Bottom [\"ì˜¤í”ˆ ìŠ¤í† ë¦¬ì§€\"]\n            B1[\"S3\"]\n            B2[\"GCS\"]\n            B3[\"ADLS\"]\n        end\n        \n        Top --> Middle --> Bottom\n    end\n```\n\n### í•µì‹¬ ê°€ì¹˜\n\n| íŠ¹ì„± | ë ˆì´í¬ | ì›¨ì–´í•˜ìš°ìŠ¤ | ë ˆì´í¬í•˜ìš°ìŠ¤ |\n|------|--------|-----------|-------------|\n| **ì €ì¥ ë¹„ìš©** | ì €ë ´ âœ… | ë¹„ìŒˆ | ì €ë ´ âœ… |\n| **ACID** | âŒ | âœ… | âœ… |\n| **ì˜¤í”ˆ í¬ë§·** | âœ… | âŒ (ë²¤ë”) | âœ… |\n| **ML ì§€ì›** | âœ… | ì œí•œì  | âœ… |\n| **SQL ë¶„ì„** | ì œí•œì  | âœ… | âœ… |\n\n---\n\n## Medallion Architecture (Bronze/Silver/Gold)\n\në ˆì´í¬í•˜ìš°ìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ **ê³„ì¸µí™”í•˜ì—¬ ê´€ë¦¬í•˜ëŠ” í‘œì¤€ íŒ¨í„´**ì…ë‹ˆë‹¤. Databricksê°€ ì œì•ˆí•˜ê³  í˜„ì¬ ì—…ê³„ í‘œì¤€ìœ¼ë¡œ ìë¦¬ì¡ì•˜ìŠµë‹ˆë‹¤.\n\n> **ì¶œì²˜**: [Databricks - Medallion Architecture](https://docs.databricks.com/en/lakehouse/medallion.html), Armbrust et al., \"Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores\" (VLDB 2020)\n\n### ì„¸ ë ˆì´ì–´ êµ¬ì¡°\n\n```mermaid\nflowchart LR\n    subgraph Bronze [\"ğŸ¥‰ Bronze Layer\"]\n        B1[\"ì›ë³¸ ê·¸ëŒ€ë¡œ ì €ì¥\"]\n        B2[\"ìŠ¤í‚¤ë§ˆ ë³€ê²½ ë³´í˜¸\"]\n        B3[\"ê°ì‚¬/ì¬ì²˜ë¦¬ ê°€ëŠ¥\"]\n    end\n    \n    subgraph Silver [\"ğŸ¥ˆ Silver Layer\"]\n        S1[\"ì •ì œ/ê²€ì¦\"]\n        S2[\"ì¡°ì¸/í†µí•©\"]\n        S3[\"ë¹„ì¦ˆë‹ˆìŠ¤ ì—”í‹°í‹°\"]\n    end\n    \n    subgraph Gold [\"ğŸ¥‡ Gold Layer\"]\n        G1[\"ì§‘ê³„/ìš”ì•½\"]\n        G2[\"ë¹„ì¦ˆë‹ˆìŠ¤ ë¦¬í¬íŠ¸\"]\n        G3[\"ML Features\"]\n    end\n    \n    Bronze -->|\"ì •ì œ\"| Silver -->|\"ì§‘ê³„\"| Gold\n```\n\n### ê° ë ˆì´ì–´ì˜ ì—­í• \n\n| Layer | ëª©ì  | ë°ì´í„° íŠ¹ì„± | ì†Œë¹„ì |\n|-------|------|------------|--------|\n| **Bronze** | ì›ë³¸ ë³´ì¡´ | Raw, ìŠ¤í‚¤ë§ˆ ìœ ì—° | ë°ì´í„° ì—”ì§€ë‹ˆì–´ |\n| **Silver** | ì •ì œ/í†µí•© | Cleaned, ì¡°ì¸ë¨ | ë°ì´í„° ë¶„ì„ê°€, DS |\n| **Gold** | ë¹„ì¦ˆë‹ˆìŠ¤ ì§‘ê³„ | Aggregated, ìµœì í™” | BI, ê²½ì˜ì§„ |\n\n### ì½”ë“œ ì˜ˆì‹œ\n\n```python\n# Bronze: ì›ë³¸ ê·¸ëŒ€ë¡œ ì €ì¥\nraw_events = spark.readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n    .option(\"subscribe\", \"user_events\") \\\n    .load()\n\nraw_events.writeStream \\\n    .format(\"delta\") \\\n    .option(\"checkpointLocation\", \"/checkpoints/bronze\") \\\n    .start(\"/delta/bronze/events\")\n\n# Silver: ì •ì œ ë° ìŠ¤í‚¤ë§ˆ ì ìš©\nbronze_df = spark.read.format(\"delta\").load(\"/delta/bronze/events\")\n\nsilver_df = bronze_df \\\n    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n    .select(\"data.*\") \\\n    .filter(col(\"user_id\").isNotNull()) \\\n    .dropDuplicates([\"event_id\"])\n\nsilver_df.write.format(\"delta\").mode(\"overwrite\") \\\n    .save(\"/delta/silver/events\")\n\n# Gold: ë¹„ì¦ˆë‹ˆìŠ¤ ì§‘ê³„\nsilver_df = spark.read.format(\"delta\").load(\"/delta/silver/events\")\n\ngold_df = silver_df \\\n    .groupBy(\"date\", \"event_type\") \\\n    .agg(\n        count(\"*\").alias(\"event_count\"),\n        countDistinct(\"user_id\").alias(\"unique_users\")\n    )\n\ngold_df.write.format(\"delta\").mode(\"overwrite\") \\\n    .save(\"/delta/gold/daily_metrics\")\n```\n\n### ì™œ ì´ íŒ¨í„´ì¸ê°€?\n\n| ë¬¸ì œ | Medallion í•´ê²°ì±… |\n|------|-----------------|\n| ì›ë³¸ ë°ì´í„° ìœ ì‹¤ | Bronzeì— ì›ë³¸ ë³´ì¡´ |\n| ìŠ¤í‚¤ë§ˆ ë³€ê²½ ëŒ€ì‘ | BronzeëŠ” ìŠ¤í‚¤ë§ˆ ìœ ì—°, Silverì—ì„œ ê²€ì¦ |\n| ì¬ì²˜ë¦¬ í•„ìš” | Bronze â†’ Silver â†’ Gold ìˆœì„œëŒ€ë¡œ ì¬ì‹¤í–‰ |\n| ë‹¤ì–‘í•œ ì†Œë¹„ì ë‹ˆì¦ˆ | ë ˆì´ì–´ë³„ ìµœì í™”ëœ ë°ì´í„° ì œê³µ |\n\n---\n\n## Delta Lake ì‹¬ì¸µ ë¶„ì„\n\n### ACID íŠ¸ëœì­ì…˜\n\n```mermaid\nflowchart TB\n    subgraph WithoutACID [\"ACID ì—†ì´ (ì¼ë°˜ Parquet)\"]\n        W1[\"writer 1: íŒŒì¼ A ì“°ê¸°\"]\n        W2[\"writer 2: íŒŒì¼ B ì“°ê¸°\"]\n        W3[\"ë™ì‹œì— ì‹¤í–‰\"]\n        W4[\"ì¶©ëŒ/ë®ì–´ì“°ê¸° ë°œìƒ ğŸ’¥\"]\n        \n        W1 --> W3\n        W2 --> W3\n        W3 --> W4\n    end\n    \n    subgraph WithACID [\"Delta Lake (ACID)\"]\n        D1[\"writer 1: íŠ¸ëœì­ì…˜ ì‹œì‘\"]\n        D2[\"writer 2: íŠ¸ëœì­ì…˜ ì‹œì‘\"]\n        D3[\"optimistic concurrency\"]\n        D4[\"í•˜ë‚˜ë§Œ ì»¤ë°‹ ì„±ê³µ<br/>ë‚˜ë¨¸ì§€ ì¬ì‹œë„\"]\n        \n        D1 --> D3\n        D2 --> D3\n        D3 --> D4\n    end\n```\n\n**Delta Lakeì˜ ë°©ë²•**: íŠ¸ëœì­ì…˜ ë¡œê·¸ (`_delta_log/`)\n\n```\ntable/\nâ”œâ”€â”€ _delta_log/\nâ”‚   â”œâ”€â”€ 00000000000000000000.json  # ì²« íŠ¸ëœì­ì…˜\nâ”‚   â”œâ”€â”€ 00000000000000000001.json  # ë‘ ë²ˆì§¸\nâ”‚   â””â”€â”€ 00000000000000000002.json  # ì„¸ ë²ˆì§¸\nâ”œâ”€â”€ part-00000.parquet\nâ”œâ”€â”€ part-00001.parquet\nâ””â”€â”€ part-00002.parquet\n```\n\n### Time Travel\n\n```mermaid\nflowchart LR\n    subgraph History [\"ë²„ì „ íˆìŠ¤í† ë¦¬\"]\n        V0[\"v0: ì´ˆê¸° ë°ì´í„°\"]\n        V1[\"v1: ì¶”ê°€\"]\n        V2[\"v2: ìˆ˜ì •\"]\n        V3[\"v3: ì‚­ì œ (í˜„ì¬)\"]\n        \n        V0 --> V1 --> V2 --> V3\n    end\n    \n    Query[\"ì–´ë–¤ ë²„ì „ì´ë“ <br/>ì¿¼ë¦¬ ê°€ëŠ¥!\"]\n```\n\n```python\n# íŠ¹ì • ë²„ì „ìœ¼ë¡œ ì½ê¸°\ndf = spark.read.format(\"delta\") \\\n    .option(\"versionAsOf\", 2) \\\n    .load(\"/delta/users\")\n\n# íŠ¹ì • ì‹œì ìœ¼ë¡œ ì½ê¸°\ndf = spark.read.format(\"delta\") \\\n    .option(\"timestampAsOf\", \"2024-01-01\") \\\n    .load(\"/delta/users\")\n\n# íˆìŠ¤í† ë¦¬ ì¡°íšŒ\nfrom delta.tables import DeltaTable\n\ndt = DeltaTable.forPath(spark, \"/delta/users\")\ndt.history().show()\n```\n\n### Schema Evolution\n\n```mermaid\nflowchart TB\n    subgraph Problem [\"ìŠ¤í‚¤ë§ˆ ë³€ê²½ ë¬¸ì œ\"]\n        P1[\"ê¸°ì¡´: id, name, email\"]\n        P2[\"ìƒˆë¡œìš´: id, name, email, phone\"]\n        P3[\"ê¸°ì¡´ íŒŒì¼ì€ phone ì—†ìŒ\"]\n        P4[\"ì–´ë–»ê²Œ í•¨ê»˜ ì½ì§€?\"]\n    end\n    \n    subgraph Solution [\"Delta Lake í•´ê²°ì±…\"]\n        S1[\"ìŠ¤í‚¤ë§ˆ ìë™ ë³‘í•©\"]\n        S2[\"ìƒˆ ì»¬ëŸ¼ NULL í—ˆìš©\"]\n        S3[\"í˜¸í™˜ì„± ê²€ì‚¬\"]\n    end\n```\n\n```python\n# ìë™ ìŠ¤í‚¤ë§ˆ ë³‘í•©\ndf_new.write.format(\"delta\") \\\n    .mode(\"append\") \\\n    .option(\"mergeSchema\", \"true\") \\\n    .save(\"/delta/users\")\n\n# ìŠ¤í‚¤ë§ˆ ë®ì–´ì“°ê¸° (ì£¼ì˜!)\ndf_new.write.format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .option(\"overwriteSchema\", \"true\") \\\n    .save(\"/delta/users\")\n```\n\n### MERGE (Upsert)\n\n```mermaid\nflowchart TB\n    subgraph Before [\"MERGE ì „\"]\n        Source[\"Source (ìƒˆ ë°ì´í„°)\"]\n        Target[\"Target (ê¸°ì¡´ í…Œì´ë¸”)\"]\n    end\n    \n    subgraph Logic [\"MERGE ë¡œì§\"]\n        Match[\"ON ì¡°ê±´ìœ¼ë¡œ ë§¤ì¹­\"]\n        WhenMatched[\"WHEN MATCHED â†’ UPDATE\"]\n        WhenNotMatched[\"WHEN NOT MATCHED â†’ INSERT\"]\n    end\n    \n    subgraph After [\"MERGE í›„\"]\n        Result[\"í†µí•©ëœ ê²°ê³¼\"]\n    end\n    \n    Before --> Logic --> After\n```\n\n```python\nfrom delta.tables import DeltaTable\n\n# íƒ€ê²Ÿ í…Œì´ë¸”\ntarget = DeltaTable.forPath(spark, \"/delta/users\")\n\n# ì†ŒìŠ¤ ë°ì´í„° (ì—…ë°ì´íŠ¸í•  ë°ì´í„°)\nsource = spark.read.parquet(\"/staging/users\")\n\n# MERGE ì‹¤í–‰\ntarget.alias(\"t\").merge(\n    source.alias(\"s\"),\n    \"t.user_id = s.user_id\"\n).whenMatchedUpdate(\n    set={\n        \"name\": \"s.name\",\n        \"email\": \"s.email\",\n        \"updated_at\": \"current_timestamp()\"\n    }\n).whenNotMatchedInsert(\n    values={\n        \"user_id\": \"s.user_id\",\n        \"name\": \"s.name\",\n        \"email\": \"s.email\",\n        \"created_at\": \"current_timestamp()\"\n    }\n).execute()\n```\n\n---\n\n## Delta Lake vs Apache Iceberg\n\n### ë¹„êµ\n\n```mermaid\nflowchart TB\n    subgraph Delta [\"Delta Lake\"]\n        D1[\"âœ… Databricks ìµœì í™”\"]\n        D2[\"âœ… Spark í†µí•© ìš°ìˆ˜\"]\n        D3[\"âœ… ì„±ìˆ™í•œ ìƒíƒœê³„\"]\n        D4[\"âš ï¸ Databricks ì™¸ ì§€ì› ì œí•œì \"]\n    end\n    \n    subgraph Iceberg [\"Apache Iceberg\"]\n        I1[\"âœ… ë²¤ë” ì¤‘ë¦½\"]\n        I2[\"âœ… ë‹¤ì–‘í•œ ì—”ì§„ ì§€ì›\"]\n        I3[\"âœ… Hidden Partitioning\"]\n        I4[\"âš ï¸ ìƒëŒ€ì ìœ¼ë¡œ ì‹ ìƒ\"]\n    end\n```\n\n| íŠ¹ì„± | Delta Lake | Apache Iceberg |\n|------|-----------|----------------|\n| **ê°œë°œì‚¬** | Databricks | Netflixâ†’Apache |\n| **Spark ì§€ì›** | ìµœê³  | ì¢‹ìŒ |\n| **Flink ì§€ì›** | ì œí•œì  | ì¢‹ìŒ |\n| **Trino ì§€ì›** | ì¢‹ìŒ | ì¢‹ìŒ |\n| **íŒŒí‹°ì…”ë‹** | ëª…ì‹œì  | Hidden (íˆ¬ëª…) |\n| **ì±„íƒìœ¨** | ë†’ìŒ | ì¦ê°€ ì¤‘ |\n\n### ì„ íƒ ê°€ì´ë“œ\n\n```mermaid\nflowchart TB\n    Q1{\"Databricks<br/>ì‚¬ìš©?\"}\n    Q2{\"Flink<br/>í•„ìš”?\"}\n    Q3{\"ë²¤ë” ì¤‘ë¦½<br/>ì¤‘ìš”?\"}\n    \n    Q1 -->|\"ì˜ˆ\"| Delta[\"Delta Lake\"]\n    Q1 -->|\"ì•„ë‹ˆì˜¤\"| Q2\n    Q2 -->|\"ì˜ˆ\"| Iceberg[\"Apache Iceberg\"]\n    Q2 -->|\"ì•„ë‹ˆì˜¤\"| Q3\n    Q3 -->|\"ì˜ˆ\"| Iceberg\n    Q3 -->|\"ì•„ë‹ˆì˜¤\"| Delta\n```\n\n---\n\n## ì•„í‚¤í…ì²˜ ê²°ì • ê°€ì´ë“œ\n\n### ì–¸ì œ ë¬´ì—‡ì„ ì„ íƒí•˜ë‚˜?\n\n```mermaid\nflowchart TB\n    subgraph Decision [\"ê²°ì • íŠ¸ë¦¬\"]\n        D1{\"ë°ì´í„° í¬ê¸°?\"}\n        D2{\"íŒ€ SQL ì—­ëŸ‰?\"}\n        D3{\"ML ì›Œí¬ë¡œë“œ?\"}\n        D4{\"ì˜ˆì‚°?\"}\n        \n        D1 -->|\"< 100GB\"| DW[\"Data Warehouse<br/>(BigQuery, Snowflake)\"]\n        D1 -->|\">= 100GB\"| D2\n        D2 -->|\"SQL ìœ„ì£¼\"| DW\n        D2 -->|\"Python/Spark í˜¼í•©\"| D3\n        D3 -->|\"ML ì¤‘ìš”\"| LH[\"Lakehouse<br/>(Delta Lake)\"]\n        D3 -->|\"ë¶„ì„ ìœ„ì£¼\"| D4\n        D4 -->|\"ë¹„ìš© ë¯¼ê°\"| LH\n        D4 -->|\"ê´€ë¦¬ í¸ì˜\"| DW\n    end\n```\n\n---\n\n## ì •ë¦¬\n\n```mermaid\nmindmap\n  root((ë°ì´í„°<br/>ì €ì¥ì†Œ))\n    Data Warehouse\n      êµ¬ì¡°í™”ëœ ë°ì´í„°\n      ACID\n      SQL ìµœì í™”\n      ë¹„ìš© ë†’ìŒ\n    Data Lake\n      ëª¨ë“  ë°ì´í„°\n      ì €ë ´\n      ACID ì—†ìŒ\n      í’ˆì§ˆ ë¬¸ì œ\n    Lakehouse\n      ë ˆì´í¬ + ACID\n      Delta Lake\n      Iceberg\n      ìµœì‹  íŠ¸ë Œë“œ\n    Medallion\n      Bronze: ì›ë³¸\n      Silver: ì •ì œ\n      Gold: ì§‘ê³„\n    Delta Lake\n      íŠ¸ëœì­ì…˜ ë¡œê·¸\n      Time Travel\n      Schema Evolution\n      MERGE\n```\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**11í¸: ë°ì´í„° ëª¨ë¸ë§**ì—ì„œëŠ” ë¶„ì„ìš© ëª¨ë¸ë§ì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- Star Schema vs Snowflake Schema\n- Fact Table vs Dimension Table\n- Slowly Changing Dimensions (SCD)\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Delta Lake Documentation](https://docs.delta.io/)\n- [Databricks Medallion Architecture](https://docs.databricks.com/en/lakehouse/medallion.html)\n- [Apache Iceberg Documentation](https://iceberg.apache.org/docs/latest/)\n- Armbrust et al., \"Delta Lake: High-Performance ACID Table Storage\" (VLDB 2020)\n- Databricks, \"The Data Lakehouse\" White Paper\n- Martin Kleppmann, \"Designing Data-Intensive Applications\" - Chapter 3",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Data Architecture",
      "Data Engineering",
      "Delta Lake",
      "Iceberg",
      "Transaction"
    ],
    "readingTime": 7,
    "wordCount": 1364,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-09-spark-structured-streaming",
    "slug": "de-09-spark-structured-streaming",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-09-spark-structured-streaming",
    "title": "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #9: Spark Structured Streaming - ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬",
    "excerpt": "Spark Structured Streamingìœ¼ë¡œ ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. Kafka ì—°ë™, Watermark, Window ì—°ì‚°, ì²´í¬í¬ì¸íŒ…ê¹Œì§€.",
    "content": "# ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #9: Spark Structured Streaming - ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬\n\n> **ëŒ€ìƒ ë…ì**: ì¶©ë¶„í•œ ê²½í—˜ì„ ê°€ì§„ ë°±ì—”ë“œ/í’€ìŠ¤íƒ ì—”ì§€ë‹ˆì–´ë¡œ, Sparkê³¼ Kafka ê¸°ë³¸ ê°œë…ì„ ìµíˆê³  ì‹¤ì‹œê°„ ì²˜ë¦¬ë¥¼ ë°°ìš°ë ¤ëŠ” ë¶„\n\n## ì´ í¸ì—ì„œ ë‹¤ë£¨ëŠ” ê²ƒ\n\në°°ì¹˜ ì²˜ë¦¬ì™€ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ë¥¼ **ê°™ì€ APIë¡œ** ë‹¤ë£¨ëŠ” Spark Structured Streamingì˜ í•µì‹¬ì„ ë°°ì›ë‹ˆë‹¤.\n\n---\n\n## ë°°ì¹˜ì™€ ìŠ¤íŠ¸ë¦¬ë°ì˜ í†µí•©\n\n### Structured Streamingì˜ ì² í•™\n\n```mermaid\nflowchart TB\n    subgraph Traditional [\"ì „í†µì  ì ‘ê·¼\"]\n        Batch[\"ë°°ì¹˜ ì½”ë“œ<br/>(Spark SQL)\"]\n        Stream[\"ìŠ¤íŠ¸ë¦¬ë° ì½”ë“œ<br/>(DStream)\"]\n        Two[\"ì„œë¡œ ë‹¤ë¥¸ API ğŸ˜“\"]\n    end\n    \n    subgraph Unified [\"Structured Streaming\"]\n        Single[\"ë™ì¼í•œ DataFrame API\"]\n        Batch2[\"ë°°ì¹˜ ì²˜ë¦¬\"]\n        Stream2[\"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬\"]\n        Single --> Batch2\n        Single --> Stream2\n    end\n```\n\n### ë¬´í•œ í…Œì´ë¸” ê°œë…\n\n```mermaid\nflowchart LR\n    subgraph Input [\"ì…ë ¥ ìŠ¤íŠ¸ë¦¼\"]\n        T1[\"t=1: row 1, 2\"]\n        T2[\"t=2: row 3\"]\n        T3[\"t=3: row 4, 5, 6\"]\n        \n        T1 --> T2 --> T3\n    end\n    \n    subgraph Table [\"ë¬´í•œ í…Œì´ë¸”\"]\n        Row1[\"row 1\"]\n        Row2[\"row 2\"]\n        Row3[\"row 3\"]\n        Row4[\"row 4\"]\n        Row5[\"row 5\"]\n        Row6[\"row 6\"]\n        Dots[\"...\"]\n        \n        Row1 --> Row2 --> Row3 --> Row4 --> Row5 --> Row6 --> Dots\n    end\n    \n    subgraph Output [\"ê²°ê³¼\"]\n        Q[\"ë™ì¼í•œ ì¿¼ë¦¬ ì ìš©\"]\n    end\n    \n    Input --> Table --> Output\n```\n\n**í•µì‹¬ ì•„ì´ë””ì–´**: ìŠ¤íŠ¸ë¦¼ì„ \"ê³„ì† ì¶”ê°€ë˜ëŠ” í…Œì´ë¸”\"ë¡œ ìƒê°\n\n---\n\n## Sourceì™€ Sink\n\n### ì§€ì›ë˜ëŠ” Source\n\n```mermaid\nflowchart TB\n    subgraph Sources [\"Input Sources\"]\n        Kafka[\"Kafka<br/>âœ… í”„ë¡œë•ì…˜\"]\n        File[\"File Source<br/>(JSON, Parquet, CSV)\"]\n        Socket[\"Socket<br/>(í…ŒìŠ¤íŠ¸ìš©)\"]\n        Rate[\"Rate Source<br/>(í…ŒìŠ¤íŠ¸ìš©)\"]\n    end\n```\n\n### ì§€ì›ë˜ëŠ” Sink\n\n```mermaid\nflowchart TB\n    subgraph Sinks [\"Output Sinks\"]\n        Kafka2[\"Kafka\"]\n        File2[\"File<br/>(Parquet, JSON)\"]\n        Console[\"Console<br/>(ë””ë²„ê¹…)\"]\n        Memory[\"Memory<br/>(í…ŒìŠ¤íŠ¸)\"]\n        ForeachBatch[\"foreachBatch<br/>(ì»¤ìŠ¤í…€ ë¡œì§)\"]\n    end\n```\n\n---\n\n## Kafka â†’ Spark Streaming ì—°ë™\n\n### ê¸°ë³¸ êµ¬ì¡°\n\n```mermaid\nflowchart LR\n    subgraph Kafka [\"Kafka\"]\n        Topic[\"Topic: events\"]\n    end\n    \n    subgraph Spark [\"Spark Streaming\"]\n        Read[\"readStream\"]\n        Transform[\"ë³€í™˜ ë¡œì§\"]\n        Write[\"writeStream\"]\n    end\n    \n    subgraph Output [\"ì¶œë ¥\"]\n        DeltaLake[\"Delta Lake\"]\n    end\n    \n    Kafka --> Read --> Transform --> Write --> Output\n```\n\n### ì½”ë“œ ì˜ˆì‹œ\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import from_json, col\nfrom pyspark.sql.types import StructType, StringType, TimestampType, DoubleType\n\nspark = SparkSession.builder \\\n    .appName(\"StreamingApp\") \\\n    .getOrCreate()\n\n# ìŠ¤í‚¤ë§ˆ ì •ì˜\nevent_schema = StructType() \\\n    .add(\"user_id\", StringType()) \\\n    .add(\"event_type\", StringType()) \\\n    .add(\"timestamp\", TimestampType()) \\\n    .add(\"amount\", DoubleType())\n\n# Kafkaì—ì„œ ì½ê¸°\ndf = spark.readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n    .option(\"subscribe\", \"user_events\") \\\n    .option(\"startingOffsets\", \"latest\") \\\n    .load()\n\n# value íŒŒì‹± (Kafka ë©”ì‹œì§€ëŠ” binary)\nparsed = df.select(\n    from_json(col(\"value\").cast(\"string\"), event_schema).alias(\"data\")\n).select(\"data.*\")\n\n# ë³€í™˜ ë¡œì§ (ë°°ì¹˜ì™€ ë™ì¼!)\nresult = parsed.filter(col(\"amount\") > 0)\n\n# ì¶œë ¥\nquery = result.writeStream \\\n    .format(\"delta\") \\\n    .option(\"checkpointLocation\", \"/checkpoints/events\") \\\n    .outputMode(\"append\") \\\n    .start(\"/delta/events\")\n\nquery.awaitTermination()\n```\n\n---\n\n## Output Modes\n\n### ì„¸ ê°€ì§€ ëª¨ë“œ\n\n```mermaid\nflowchart TB\n    subgraph Append [\"Append Mode\"]\n        A1[\"ìƒˆë¡œ ì¶”ê°€ëœ í–‰ë§Œ ì¶œë ¥\"]\n        A2[\"ì§‘ê³„ ì—†ëŠ” ì¿¼ë¦¬ì— ì í•©\"]\n        A3[\"ì˜ˆ: í•„í„°ë§, ë§µí•‘\"]\n    end\n    \n    subgraph Complete [\"Complete Mode\"]\n        C1[\"ì „ì²´ ê²°ê³¼ ë§¤ë²ˆ ì¶œë ¥\"]\n        C2[\"ì§‘ê³„ ì¿¼ë¦¬ì— ì í•©\"]\n        C3[\"ì˜ˆ: groupBy().count()\"]\n    end\n    \n    subgraph Update [\"Update Mode\"]\n        U1[\"ë³€ê²½ëœ í–‰ë§Œ ì¶œë ¥\"]\n        U2[\"ì§‘ê³„ ì¿¼ë¦¬ì— íš¨ìœ¨ì \"]\n        U3[\"ì˜ˆ: ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸\"]\n    end\n```\n\n### ì–¸ì œ ì–´ë–¤ ëª¨ë“œ?\n\n| ì¿¼ë¦¬ ìœ í˜• | Append | Complete | Update |\n|----------|--------|----------|--------|\n| **SELECT, WHERE** | âœ… | âŒ | âœ… |\n| **ì§‘ê³„ (groupBy)** | âŒ* | âœ… | âœ… |\n| **ì›Œí„°ë§ˆí¬ + ì§‘ê³„** | âœ… | âœ… | âœ… |\n\n*ì›Œí„°ë§ˆí¬ ì—†ëŠ” ì§‘ê³„ëŠ” Append ë¶ˆê°€\n\n---\n\n## Event Time vs Processing Time\n\n### ë‘ ì‹œê°„ì˜ ì°¨ì´\n\n```mermaid\nflowchart LR\n    subgraph EventTime [\"Event Time\"]\n        ET[\"ì´ë²¤íŠ¸ê°€ ì‹¤ì œë¡œ ë°œìƒí•œ ì‹œê°„<br/>(ë°ì´í„°ì— í¬í•¨ëœ timestamp)\"]\n    end\n    \n    subgraph ProcessingTime [\"Processing Time\"]\n        PT[\"Sparkì´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì‹œê°„<br/>(ì‹œìŠ¤í…œ ì‹œê³„)\"]\n    end\n    \n    subgraph Problem [\"ë¬¸ì œ ìƒí™©\"]\n        P1[\"Event: 10:00:00\"]\n        P2[\"ë„¤íŠ¸ì›Œí¬ ì§€ì—°\"]\n        P3[\"Processing: 10:05:00\"]\n        P1 --> P2 --> P3\n        \n        Q[\"ì–´ë–¤ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ìœˆë„ìš°?\"]\n    end\n```\n\n### Event Time ì²˜ë¦¬\n\n```python\n# timestamp ì»¬ëŸ¼ì„ Event Timeìœ¼ë¡œ ì‚¬ìš©\nparsed = df.select(\n    from_json(col(\"value\").cast(\"string\"), event_schema).alias(\"data\")\n).select(\"data.*\")\n\n# Event Time ê¸°ì¤€ ìœˆë„ìš° ì§‘ê³„\nresult = parsed \\\n    .groupBy(\n        window(col(\"timestamp\"), \"5 minutes\"),\n        col(\"event_type\")\n    ) \\\n    .count()\n```\n\n---\n\n## Watermarkì™€ Late Data\n\n### ì™œ Watermarkê°€ í•„ìš”í•œê°€?\n\n```mermaid\nflowchart TB\n    subgraph Problem [\"ë¬¸ì œ: ì§€ì—° ë°ì´í„°\"]\n        W1[\"10:00 ìœˆë„ìš° ì²˜ë¦¬ ì¤‘\"]\n        W2[\"10:05ì— ë„ì°©í•œ ë°ì´í„°\"]\n        W3[\"ê·¼ë° event_timeì€ 09:55!\"]\n        W4[\"ì–´ë–»ê²Œ ì²˜ë¦¬?\"]\n        \n        W1 --> W2 --> W3 --> W4\n    end\n    \n    subgraph Solution [\"í•´ê²°: Watermark\"]\n        S1[\"í—ˆìš© ì§€ì—° ì‹œê°„ ì„¤ì •<br/>(ì˜ˆ: 10ë¶„)\"]\n        S2[\"Watermark = max(event_time) - 10ë¶„\"]\n        S3[\"Watermark ì´ì „ ìœˆë„ìš°ëŠ” ë‹«í˜\"]\n        \n        S1 --> S2 --> S3\n    end\n```\n\n### Watermark ë™ì‘ ë°©ì‹\n\n```mermaid\nflowchart LR\n    subgraph Timeline [\"ì‹œê°„ íë¦„\"]\n        T1[\"Event: 10:05\"]\n        T2[\"Event: 10:10\"]\n        T3[\"Event: 10:08\"]\n        T4[\"Event: 10:15\"]\n        T5[\"Late: 09:55\"]\n    end\n    \n    subgraph Watermark [\"Watermark (ì§€ì—° 10ë¶„)\"]\n        W1[\"max=10:05<br/>WM=09:55\"]\n        W2[\"max=10:10<br/>WM=10:00\"]\n        W3[\"max=10:10<br/>WM=10:00\"]\n        W4[\"max=10:15<br/>WM=10:05\"]\n        W5[\"âŒ 09:55 < 10:05<br/>â†’ ë²„ë ¤ì§\"]\n    end\n    \n    T1 --> W1\n    T2 --> W2\n    T3 --> W3\n    T4 --> W4\n    T5 --> W5\n```\n\n### ì½”ë“œ ì˜ˆì‹œ\n\n```python\nfrom pyspark.sql.functions import window, col\n\n# Watermark ì„¤ì •: 10ë¶„ ì§€ì—° í—ˆìš©\nresult = parsed \\\n    .withWatermark(\"timestamp\", \"10 minutes\") \\\n    .groupBy(\n        window(col(\"timestamp\"), \"5 minutes\"),\n        col(\"page\")\n    ) \\\n    .agg(count(\"*\").alias(\"views\"))\n\n# Watermark ë•ë¶„ì— Append ëª¨ë“œ ê°€ëŠ¥\nquery = result.writeStream \\\n    .format(\"delta\") \\\n    .outputMode(\"append\") \\\n    .option(\"checkpointLocation\", \"/checkpoints/views\") \\\n    .start(\"/delta/page_views\")\n```\n\n---\n\n## Window ì—°ì‚°\n\n### Window ì¢…ë¥˜\n\n```mermaid\nflowchart TB\n    subgraph Tumbling [\"Tumbling Window\"]\n        T1[\"0-5ë¶„\"]\n        T2[\"5-10ë¶„\"]\n        T3[\"10-15ë¶„\"]\n        T1 --> T2 --> T3\n        TNote[\"ê²¹ì¹˜ì§€ ì•ŠìŒ\"]\n    end\n    \n    subgraph Sliding [\"Sliding Window\"]\n        S1[\"0-5ë¶„\"]\n        S2[\"2.5-7.5ë¶„\"]\n        S3[\"5-10ë¶„\"]\n        SNote[\"ê²¹ì¹¨ (slide < window)\"]\n    end\n    \n    subgraph Session [\"Session Window\"]\n        SE1[\"í™œë™ ê¸°ê°„ A\"]\n        Gap[\"ë¹„í™œë™ gap\"]\n        SE2[\"í™œë™ ê¸°ê°„ B\"]\n        SE1 --> Gap --> SE2\n        SENote[\"gap ê¸°ì¤€ ë¶„ë¦¬\"]\n    end\n```\n\n### Window í•¨ìˆ˜ ì‚¬ìš©\n\n```python\nfrom pyspark.sql.functions import window, sum, avg\n\n# Tumbling Window: 5ë¶„ ìœˆë„ìš°\ntumbling = parsed \\\n    .groupBy(window(\"timestamp\", \"5 minutes\")) \\\n    .agg(sum(\"amount\").alias(\"total\"))\n\n# Sliding Window: 10ë¶„ ìœˆë„ìš°, 5ë¶„ ìŠ¬ë¼ì´ë“œ\nsliding = parsed \\\n    .groupBy(window(\"timestamp\", \"10 minutes\", \"5 minutes\")) \\\n    .agg(avg(\"amount\").alias(\"avg_amount\"))\n\n# Session Window (Spark 3.2+)\nsession = parsed \\\n    .groupBy(\n        session_window(\"timestamp\", \"10 minutes\"),\n        col(\"user_id\")\n    ) \\\n    .agg(count(\"*\").alias(\"session_events\"))\n```\n\n---\n\n## ì²´í¬í¬ì¸íŒ…ê³¼ ì¥ì•  ë³µêµ¬\n\n### ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡°\n\n```mermaid\nflowchart TB\n    subgraph Checkpoint [\"ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬\"]\n        Offsets[\"offsets/<br/>Kafka offset ì •ë³´\"]\n        Commits[\"commits/<br/>ì²˜ë¦¬ ì™„ë£Œ ë°°ì¹˜\"]\n        State[\"state/<br/>ì§‘ê³„ ìƒíƒœ\"]\n        Metadata[\"metadata/<br/>ì¿¼ë¦¬ ë©”íƒ€ë°ì´í„°\"]\n    end\n    \n    subgraph Recovery [\"ì¥ì•  ë³µêµ¬\"]\n        R1[\"ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ\"]\n        R2[\"ë¯¸ì²˜ë¦¬ offsetë¶€í„° ì¬ì‹œì‘\"]\n        R3[\"ìƒíƒœ ë³µì›\"]\n        R1 --> R2 --> R3\n    end\n```\n\n### Exactly-Once ë³´ì¥\n\n```python\n# ì²´í¬í¬ì¸íŠ¸ í•„ìˆ˜ ì„¤ì •\nquery = result.writeStream \\\n    .format(\"delta\") \\\n    .outputMode(\"append\") \\\n    .option(\"checkpointLocation\", \"hdfs://path/checkpoints/my_query\") \\\n    .trigger(processingTime=\"1 minute\") \\\n    .start(\"/delta/output\")\n```\n\n**ì²´í¬í¬ì¸íŠ¸ê°€ ë³´ì¥í•˜ëŠ” ê²ƒ**:\n\n- Kafka offset ì¶”ì  â†’ ì¤‘ë³µ ì½ê¸° ë°©ì§€\n- ìƒíƒœ ì €ì¥ â†’ ì§‘ê³„ ê²°ê³¼ ìœ ì§€\n- Atomic ì»¤ë°‹ â†’ Exactly-Once\n\n---\n\n## ì‹¤ì „ ì˜ˆì œ: ì‹¤ì‹œê°„ í´ë¦­ìŠ¤íŠ¸ë¦¼ ë¶„ì„\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import (\n    from_json, col, window, count, sum, avg,\n    current_timestamp, expr\n)\nfrom pyspark.sql.types import StructType, StringType, TimestampType\n\nspark = SparkSession.builder \\\n    .appName(\"ClickstreamAnalysis\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .getOrCreate()\n\n# ìŠ¤í‚¤ë§ˆ\nclick_schema = StructType() \\\n    .add(\"user_id\", StringType()) \\\n    .add(\"page\", StringType()) \\\n    .add(\"action\", StringType()) \\\n    .add(\"timestamp\", TimestampType())\n\n# Kafkaì—ì„œ ì½ê¸°\nclicks = spark.readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n    .option(\"subscribe\", \"clickstream\") \\\n    .option(\"startingOffsets\", \"latest\") \\\n    .load() \\\n    .select(\n        from_json(col(\"value\").cast(\"string\"), click_schema).alias(\"click\")\n    ).select(\"click.*\")\n\n# 5ë¶„ ìœˆë„ìš°ë¡œ í˜ì´ì§€ë³„ í†µê³„\npage_stats = clicks \\\n    .withWatermark(\"timestamp\", \"10 minutes\") \\\n    .groupBy(\n        window(col(\"timestamp\"), \"5 minutes\"),\n        col(\"page\")\n    ) \\\n    .agg(\n        count(\"*\").alias(\"view_count\"),\n        count(\"user_id\").alias(\"unique_users\")\n    )\n\n# Delta Lakeì— ì €ì¥\nquery = page_stats.writeStream \\\n    .format(\"delta\") \\\n    .outputMode(\"append\") \\\n    .option(\"checkpointLocation\", \"/checkpoints/clickstream\") \\\n    .trigger(processingTime=\"1 minute\") \\\n    .start(\"/delta/page_stats\")\n\n# ì½˜ì†”ì—ë„ ì¶œë ¥ (ë””ë²„ê¹…ìš©)\ndebug_query = page_stats.writeStream \\\n    .format(\"console\") \\\n    .outputMode(\"update\") \\\n    .trigger(processingTime=\"30 seconds\") \\\n    .start()\n\nquery.awaitTermination()\n```\n\n---\n\n## ëª¨ë‹ˆí„°ë§\n\n### Streaming Query ìƒíƒœ í™•ì¸\n\n```python\n# ì¿¼ë¦¬ ì§„í–‰ ìƒí™©\nprint(query.status)\n# {'message': 'Processing new data', 'isActive': True, ...}\n\n# ìµœê·¼ ì§„í–‰ ìƒí™©\nfor progress in query.recentProgress:\n    print(f\"Batch {progress['batchId']}\")\n    print(f\"  Input rows: {progress['numInputRows']}\")\n    print(f\"  Processing time: {progress['batchDuration']} ms\")\n```\n\n### Spark UIì—ì„œ í™•ì¸\n\n```mermaid\nflowchart TB\n    subgraph SparkUI [\"Structured Streaming UI\"]\n        Tab[\"Streaming íƒ­\"]\n        Metrics[\"â€¢ Input Rate<br/>â€¢ Processing Rate<br/>â€¢ Batch Duration<br/>â€¢ State Rows\"]\n    end\n```\n\n---\n\n## ì •ë¦¬\n\n```mermaid\nmindmap\n  root((Spark<br/>Structured<br/>Streaming))\n    í•µì‹¬ ê°œë…\n      ë¬´í•œ í…Œì´ë¸”\n      ë™ì¼í•œ API\n      ë°°ì¹˜ & ìŠ¤íŠ¸ë¦¬ë° í†µí•©\n    Source/Sink\n      Kafka\n      File\n      Delta Lake\n    Output Mode\n      Append\n      Complete\n      Update\n    ì‹œê°„ ì²˜ë¦¬\n      Event Time\n      Processing Time\n      Watermark\n    Window\n      Tumbling\n      Sliding\n      Session\n    ì•ˆì •ì„±\n      Checkpoint\n      Exactly-Once\n      ì¥ì•  ë³µêµ¬\n```\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**10í¸: ë ˆì´í¬í•˜ìš°ìŠ¤ ì•„í‚¤í…ì²˜**ì—ì„œëŠ” ë°ì´í„° ì €ì¥ì†Œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- Data Lake vs Data Warehouse\n- Delta Lake ì‹¬ì¸µ ë¶„ì„\n- ACID, Time Travel, Schema Evolution\n- Apache Iceberg ë¹„êµ\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n- [Kafka Integration](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)\n- Databricks, \"Real-time Streaming with Spark 3.0\"",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Data Engineering",
      "Kafka",
      "Spark",
      "Streaming",
      "Window"
    ],
    "readingTime": 7,
    "wordCount": 1243,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-08-kafka-fundamentals",
    "slug": "de-08-kafka-fundamentals",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-08-kafka-fundamentals",
    "title": "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #8: Kafka í•µì‹¬ - ë©”ì‹œì§€ íë¥¼ ë„˜ì–´ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ",
    "excerpt": "Kafkaì˜ í•µì‹¬ ê°œë…ì„ ë°°ì›ë‹ˆë‹¤. Redis Streamsì™€ ë¹„êµí•˜ë©° Topic, Partition, Consumer Group, Exactly-Once Semanticsë¥¼ ì´í•´í•©ë‹ˆë‹¤.",
    "content": "# ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #8: Kafka í•µì‹¬ - ë©”ì‹œì§€ íë¥¼ ë„˜ì–´ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ\n\n> **ëŒ€ìƒ ë…ì**: ì¶©ë¶„í•œ ê²½í—˜ì„ ê°€ì§„ ë°±ì—”ë“œ/í’€ìŠ¤íƒ ì—”ì§€ë‹ˆì–´ë¡œ, Redis Streamsë‚˜ RabbitMQì— ìµìˆ™í•˜ì§€ë§Œ KafkaëŠ” ì²˜ìŒì¸ ë¶„\n\n## ì´ í¸ì—ì„œ ë‹¤ë£¨ëŠ” ê²ƒ\n\nRedis Streamsë¥¼ ì¨ë´¤ë‹¤ë©´ \"Kafkaê°€ ë­ê°€ ë‹¤ë¥´ì§€?\"ë¼ëŠ” ì˜ë¬¸ì´ ìˆì„ ê²ë‹ˆë‹¤. **ì™œ ëŒ€ê·œëª¨ ì‹œìŠ¤í…œì—ì„œ Kafkaë¥¼ ì„ íƒí•˜ëŠ”ì§€**, í•µì‹¬ ê°œë…ì„ ë°°ì›ë‹ˆë‹¤.\n\n---\n\n## Redis Streams vs Kafka\n\n### ì¹œìˆ™í•œ Redis Streamsì™€ ë¹„êµ\n\n```mermaid\nflowchart TB\n    subgraph Redis [\"Redis Streams\"]\n        RS_Stream[\"Stream: orders\"]\n        RS_Group1[\"Consumer Group A\"]\n        RS_Group2[\"Consumer Group B\"]\n        RS_C1[\"Consumer 1\"]\n        RS_C2[\"Consumer 2\"]\n        \n        RS_Stream --> RS_Group1 --> RS_C1\n        RS_Stream --> RS_Group2 --> RS_C2\n    end\n    \n    subgraph Kafka [\"Apache Kafka\"]\n        K_Topic[\"Topic: orders<br/>(Partitioned)\"]\n        K_Group1[\"Consumer Group A\"]\n        K_Group2[\"Consumer Group B\"]\n        K_C1[\"Consumer 1\"]\n        K_C2[\"Consumer 2\"]\n        \n        K_Topic --> K_Group1 --> K_C1\n        K_Topic --> K_Group2 --> K_C2\n    end\n    \n    Similar[\"ìœ ì‚¬í•œ ê°œë…!\"]\n```\n\n### ì£¼ìš” ì°¨ì´ì \n\n| íŠ¹ì„± | Redis Streams | Kafka |\n|------|--------------|-------|\n| **ì„¤ê³„ ëª©ì ** | ìºì‹œ + ê°€ë²¼ìš´ ìŠ¤íŠ¸ë¦¬ë° | ëŒ€ìš©ëŸ‰ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ì „ìš© |\n| **ë°ì´í„° ì €ì¥** | ë©”ëª¨ë¦¬ (ì œí•œì  ë³´ì¡´) | ë””ìŠ¤í¬ (ì¥ê¸° ë³´ì¡´ ê°€ëŠ¥) |\n| **ìŠ¤ì¼€ì¼ë§** | ìˆ˜ì§ í™•ì¥ ìœ„ì£¼ | ìˆ˜í‰ í™•ì¥ (Partition) |\n| **ì²˜ë¦¬ëŸ‰** | ìˆ˜ë§Œ TPS | **ìˆ˜ë°±ë§Œ TPS** |\n| **ë³µì œ** | Master-Replica | Multi-broker ë³µì œ |\n| **ìˆœì„œ ë³´ì¥** | Stream ë‚´ ë³´ì¥ | Partition ë‚´ ë³´ì¥ |\n| **Consumer ê´€ë¦¬** | ìì²´ ê´€ë¦¬ í•„ìš” | Coordinator ìë™ ê´€ë¦¬ |\n\n### ìŠ¤ì¼€ì¼ ë¹„êµ\n\n```mermaid\nflowchart TB\n    subgraph RedisScale [\"Redis Streams ìŠ¤ì¼€ì¼ë§\"]\n        RS1[\"ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤<br/>100K TPS\"]\n        RS2[\"Cluster Sharding<br/>ë³µì¡í•œ ê´€ë¦¬\"]\n    end\n    \n    subgraph KafkaScale [\"Kafka ìŠ¤ì¼€ì¼ë§\"]\n        K1[\"Partition ì¶”ê°€\"]\n        K2[\"Broker ì¶”ê°€\"]\n        K3[\"Consumer ì¶”ê°€\"]\n        \n        K1 --> K2 --> K3\n        Result[\"ì„ í˜• í™•ì¥ ê°€ëŠ¥<br/>ìˆ˜ë°±ë§Œ TPS\"]\n    end\n```\n\n---\n\n## Kafka í•µì‹¬ ê°œë…\n\n### ì „ì²´ êµ¬ì¡°\n\n```mermaid\nflowchart TB\n    subgraph Producers [\"Producers\"]\n        P1[\"Producer 1\"]\n        P2[\"Producer 2\"]\n    end\n    \n    subgraph Kafka [\"Kafka Cluster\"]\n        subgraph Broker1 [\"Broker 1\"]\n            T1P0[\"Topic A<br/>Partition 0\"]\n            T1P1[\"Topic A<br/>Partition 1\"]\n        end\n        \n        subgraph Broker2 [\"Broker 2\"]\n            T1P2[\"Topic A<br/>Partition 2\"]\n            T2P0[\"Topic B<br/>Partition 0\"]\n        end\n        \n        subgraph Broker3 [\"Broker 3\"]\n            T2P1[\"Topic B<br/>Partition 1\"]\n        end\n    end\n    \n    subgraph Consumers [\"Consumer Groups\"]\n        subgraph CG1 [\"Consumer Group 1\"]\n            C1[\"Consumer 1\"]\n            C2[\"Consumer 2\"]\n        end\n        \n        subgraph CG2 [\"Consumer Group 2\"]\n            C3[\"Consumer 3\"]\n        end\n    end\n    \n    Producers --> Kafka --> Consumers\n```\n\n### Topic\n\n```mermaid\nflowchart LR\n    subgraph Topic [\"Topic: user_events\"]\n        direction TB\n        Desc[\"â€¢ ì´ë²¤íŠ¸ì˜ ì¹´í…Œê³ ë¦¬/ì±„ë„<br/>â€¢ Nê°œì˜ Partitionìœ¼ë¡œ êµ¬ì„±<br/>â€¢ ì„¤ì •ëœ ê¸°ê°„ë§Œí¼ ë³´ì¡´\"]\n    end\n    \n    Examples[\"ì˜ˆì‹œ:<br/>â€¢ orders<br/>â€¢ user_signups<br/>â€¢ page_views\"]\n    \n    Topic --- Examples\n```\n\n### Partition\n\n```mermaid\nflowchart TB\n    subgraph Topic [\"Topic: orders\"]\n        subgraph P0 [\"Partition 0\"]\n            M0_0[\"offset 0\"] --> M0_1[\"offset 1\"] --> M0_2[\"offset 2\"]\n        end\n        \n        subgraph P1 [\"Partition 1\"]\n            M1_0[\"offset 0\"] --> M1_1[\"offset 1\"] --> M1_2[\"offset 2\"]\n        end\n        \n        subgraph P2 [\"Partition 2\"]\n            M2_0[\"offset 0\"] --> M2_1[\"offset 1\"]\n        end\n    end\n    \n    Features[\"â€¢ ìˆœì„œ ë³´ì¥ ë‹¨ìœ„<br/>â€¢ ë³‘ë ¬ ì²˜ë¦¬ ë‹¨ìœ„<br/>â€¢ íŒŒí‹°ì…˜ í‚¤ë¡œ ë¶„ë°°\"]\n```\n\n**í•µì‹¬ ì¸ì‚¬ì´íŠ¸**:\n\n- **ìˆœì„œ ë³´ì¥ì€ Partition ë‚´ì—ì„œë§Œ!**\n- Partition ìˆ˜ = ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜ì¤€\n- ê°™ì€ í‚¤ëŠ” ê°™ì€ Partitionìœ¼ë¡œ\n\n### Offset\n\n```mermaid\nflowchart LR\n    subgraph Partition [\"Partition 0\"]\n        O0[\"offset 0<br/>msg_a\"]\n        O1[\"offset 1<br/>msg_b\"]\n        O2[\"offset 2<br/>msg_c\"]\n        O3[\"offset 3<br/>msg_d\"]\n        O4[\"offset 4<br/>msg_e\"]\n        \n        O0 --> O1 --> O2 --> O3 --> O4\n    end\n    \n    subgraph Consumers [\"Consumer ìœ„ì¹˜\"]\n        C1[\"Group A<br/>offset: 3\"]\n        C2[\"Group B<br/>offset: 1\"]\n    end\n    \n    O3 -.->|\"ì½ëŠ” ì¤‘\"| C1\n    O1 -.->|\"ì½ëŠ” ì¤‘\"| C2\n```\n\n**Offsetì˜ ì—­í• **:\n\n- ê° Consumer Groupì´ ì–´ë””ê¹Œì§€ ì½ì—ˆëŠ”ì§€ ì¶”ì \n- ì¬ì‹œì‘ ì‹œ ì´ì–´ì„œ ì½ê¸° ê°€ëŠ¥\n- ê³¼ê±° ë°ì´í„° ë‹¤ì‹œ ì½ê¸° ê°€ëŠ¥ (rewind)\n\n### Consumer Group\n\n```mermaid\nflowchart TB\n    subgraph Topic [\"Topic: orders (3 partitions)\"]\n        P0[\"Partition 0\"]\n        P1[\"Partition 1\"]\n        P2[\"Partition 2\"]\n    end\n    \n    subgraph Group1 [\"Consumer Group A (3 consumers)\"]\n        C1[\"Consumer 1\"]\n        C2[\"Consumer 2\"]\n        C3[\"Consumer 3\"]\n    end\n    \n    subgraph Group2 [\"Consumer Group B (1 consumer)\"]\n        C4[\"Consumer 4\"]\n    end\n    \n    P0 --> C1\n    P1 --> C2\n    P2 --> C3\n    \n    P0 --> C4\n    P1 --> C4\n    P2 --> C4\n    \n    Note1[\"Group A: ê° Consumerê°€<br/>1ê°œ Partition ë‹´ë‹¹\"]\n    Note2[\"Group B: 1 Consumerê°€<br/>ëª¨ë“  Partition ë‹´ë‹¹\"]\n```\n\n**í•µì‹¬ ê·œì¹™**:\n\n- í•œ Partitionì€ Group ë‚´ **í•˜ë‚˜ì˜ Consumer**ë§Œ ì½ì„ ìˆ˜ ìˆìŒ\n- Consumer ìˆ˜ > Partition ìˆ˜ â†’ ì¼ë¶€ Consumer ìœ íœ´\n- Consumer ìˆ˜ < Partition ìˆ˜ â†’ ì¼ë¶€ Consumerê°€ ì—¬ëŸ¬ Partition ë‹´ë‹¹\n\n---\n\n## Producer: ë©”ì‹œì§€ ë³´ë‚´ê¸°\n\n### íŒŒí‹°ì…˜ ê²°ì • ì „ëµ\n\n```mermaid\nflowchart TB\n    Message[\"ë©”ì‹œì§€ ì „ì†¡\"]\n    \n    HasKey{\"í‚¤ê°€<br/>ìˆëŠ”ê°€?\"}\n    \n    Hash[\"hash(key) % partition_count<br/>â†’ ê°™ì€ í‚¤ = ê°™ì€ Partition\"]\n    RoundRobin[\"ë¼ìš´ë“œ ë¡œë¹ˆ<br/>â†’ ê³ ë¥´ê²Œ ë¶„ë°°\"]\n    Sticky[\"Sticky Partitioner<br/>â†’ ë°°ì¹˜ ìµœì í™”\"]\n    \n    Message --> HasKey\n    HasKey -->|\"ì˜ˆ\"| Hash\n    HasKey -->|\"ì•„ë‹ˆì˜¤ (2.4+)\"| Sticky\n    HasKey -->|\"ì•„ë‹ˆì˜¤ (êµ¬ë²„ì „)\"| RoundRobin\n```\n\n### Python Producer ì˜ˆì‹œ\n\n```python\nfrom confluent_kafka import Producer\n\ndef delivery_callback(err, msg):\n    if err:\n        print(f\"Delivery failed: {err}\")\n    else:\n        print(f\"Delivered to {msg.topic()} [{msg.partition()}] @ {msg.offset()}\")\n\n# Producer ì„¤ì •\nproducer = Producer({\n    'bootstrap.servers': 'localhost:9092',\n    'acks': 'all',  # ëª¨ë“  replica í™•ì¸\n    'enable.idempotence': True,  # ì¤‘ë³µ ë°©ì§€\n})\n\n# í‚¤ê°€ ìˆëŠ” ë©”ì‹œì§€ (ê°™ì€ user_id = ê°™ì€ Partition)\nproducer.produce(\n    topic='user_events',\n    key='user_123',\n    value='{\"event\": \"purchase\", \"amount\": 100}',\n    callback=delivery_callback\n)\n\n# í‚¤ê°€ ì—†ëŠ” ë©”ì‹œì§€ (ìë™ ë¶„ë°°)\nproducer.produce(\n    topic='logs',\n    value='{\"level\": \"info\", \"message\": \"hello\"}',\n    callback=delivery_callback\n)\n\nproducer.flush()\n```\n\n---\n\n## Consumer: ë©”ì‹œì§€ ì½ê¸°\n\n### Consumer ë¼ì´í”„ì‚¬ì´í´\n\n```mermaid\nflowchart TB\n    subgraph Lifecycle [\"Consumer ë¼ì´í”„ì‚¬ì´í´\"]\n        Subscribe[\"Subscribe<br/>Topic êµ¬ë…\"]\n        Poll[\"Poll<br/>ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°\"]\n        Process[\"Process<br/>ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§\"]\n        Commit[\"Commit<br/>Offset ì €ì¥\"]\n        \n        Subscribe --> Poll --> Process --> Commit --> Poll\n    end\n```\n\n### Python Consumer ì˜ˆì‹œ\n\n```python\nfrom confluent_kafka import Consumer, KafkaError\n\n# Consumer ì„¤ì •\nconsumer = Consumer({\n    'bootstrap.servers': 'localhost:9092',\n    'group.id': 'my-consumer-group',\n    'auto.offset.reset': 'earliest',  # ì²˜ìŒë¶€í„° ì½ê¸°\n    'enable.auto.commit': False,  # ìˆ˜ë™ ì»¤ë°‹\n})\n\nconsumer.subscribe(['user_events'])\n\ntry:\n    while True:\n        msg = consumer.poll(timeout=1.0)\n        \n        if msg is None:\n            continue\n        if msg.error():\n            if msg.error().code() == KafkaError._PARTITION_EOF:\n                continue\n            raise KafkaException(msg.error())\n        \n        # ë©”ì‹œì§€ ì²˜ë¦¬\n        key = msg.key().decode('utf-8') if msg.key() else None\n        value = msg.value().decode('utf-8')\n        \n        print(f\"Received: key={key}, value={value}\")\n        \n        # ì²˜ë¦¬ ì™„ë£Œ í›„ ì»¤ë°‹\n        consumer.commit(asynchronous=False)\n        \nfinally:\n    consumer.close()\n```\n\n---\n\n## Exactly-Once Semantics\n\n> âš ï¸ **ì£¼ì˜**: Kafkaì˜ Exactly-OnceëŠ” **\"Kafka ë‚´ë¶€\"**ì—ì„œì˜ ë³´ì¥ì…ë‹ˆë‹¤. ì™¸ë¶€ DB/APIë¡œì˜ End-to-End Exactly-OnceëŠ” **ì• í”Œë¦¬ì¼€ì´ì…˜ ë ˆë²¨ì—ì„œ ì¶”ê°€ ì²˜ë¦¬**ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n\n### ë©”ì‹œì§€ ì „ë‹¬ ë³´ì¥ ìˆ˜ì¤€\n\n```mermaid\nflowchart TB\n    subgraph Levels [\"ì „ë‹¬ ë³´ì¥ ìˆ˜ì¤€\"]\n        AtMost[\"At-Most-Once<br/>ìµœëŒ€ 1ë²ˆ\"]\n        AtLeast[\"At-Least-Once<br/>ìµœì†Œ 1ë²ˆ\"]\n        Exactly[\"Exactly-Once<br/>ì •í™•íˆ 1ë²ˆ\"]\n    end\n    \n    AtMost -->|\"ë©”ì‹œì§€ ìœ ì‹¤ ê°€ëŠ¥\"| L1[\"âŒ ë°ì´í„° ì†ì‹¤\"]\n    AtLeast -->|\"ì¤‘ë³µ ê°€ëŠ¥\"| L2[\"âš ï¸ ì¤‘ë³µ ì²˜ë¦¬\"]\n    Exactly -->|\"ì •í™•í•¨\"| L3[\"âœ… ì™„ë²½\"]\n    \n    Difficulty[\"êµ¬í˜„ ë‚œì´ë„: At-Most < At-Least < Exactly\"]\n```\n\n### Idempotent Producer\n\n```mermaid\nflowchart LR\n    subgraph Problem [\"ë¬¸ì œ ìƒí™©\"]\n        P1[\"Producer ì „ì†¡\"]\n        P2[\"Broker ì €ì¥\"]\n        P3[\"ACK ìœ ì‹¤\"]\n        P4[\"Producer ì¬ì „ì†¡\"]\n        P5[\"ì¤‘ë³µ ì €ì¥!\"]\n        \n        P1 --> P2 --> P3 --> P4 --> P5\n    end\n    \n    subgraph Solution [\"Idempotent Producer\"]\n        S1[\"Producer ì „ì†¡<br/>(PID + SeqNum)\"]\n        S2[\"Broker ì €ì¥<br/>(SeqNum ê¸°ë¡)\"]\n        S3[\"ACK ìœ ì‹¤\"]\n        S4[\"ì¬ì „ì†¡ ì‹œ<br/>ì¤‘ë³µ ê°ì§€\"]\n        S5[\"ë¬´ì‹œë¨ âœ…\"]\n        \n        S1 --> S2 --> S3 --> S4 --> S5\n    end\n```\n\n```python\n# Idempotent Producer ì„¤ì •\nproducer = Producer({\n    'bootstrap.servers': 'localhost:9092',\n    'enable.idempotence': True,  # í•µì‹¬ ì„¤ì •!\n    'acks': 'all',\n    'retries': 5,\n})\n```\n\n### Transactional Producer\n\n```mermaid\nflowchart TB\n    subgraph Transaction [\"íŠ¸ëœì­ì…˜\"]\n        Begin[\"begin_transaction()\"]\n        Send1[\"produce(topic_a)\"]\n        Send2[\"produce(topic_b)\"]\n        Commit[\"commit_transaction()\"]\n        \n        Begin --> Send1 --> Send2 --> Commit\n    end\n    \n    Result[\"ëª¨ë‘ ì„±ê³µ ë˜ëŠ” ëª¨ë‘ ì‹¤íŒ¨<br/>â†’ Atomic\"]\n```\n\n```python\nfrom confluent_kafka import Producer\n\nproducer = Producer({\n    'bootstrap.servers': 'localhost:9092',\n    'enable.idempotence': True,\n    'transactional.id': 'my-transactional-producer',\n})\n\n# íŠ¸ëœì­ì…˜ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ)\nproducer.init_transactions()\n\ntry:\n    producer.begin_transaction()\n    \n    producer.produce('orders', key='order_1', value='...')\n    producer.produce('payments', key='order_1', value='...')\n    \n    producer.commit_transaction()\nexcept Exception as e:\n    producer.abort_transaction()\n    raise\n```\n\n### Consumer ì¸¡ Exactly-Once\n\n```python\nconsumer = Consumer({\n    'bootstrap.servers': 'localhost:9092',\n    'group.id': 'exactly-once-group',\n    'isolation.level': 'read_committed',  # ì»¤ë°‹ëœ ë©”ì‹œì§€ë§Œ ì½ê¸°\n    'enable.auto.commit': False,\n})\n```\n\n---\n\n## KRaft: Zookeeper ì—†ëŠ” Kafka\n\n### ê¸°ì¡´ ì•„í‚¤í…ì²˜ì˜ ë¬¸ì œ\n\n```mermaid\nflowchart TB\n    subgraph Old [\"ê¸°ì¡´ (Zookeeper ê¸°ë°˜)\"]\n        ZK[\"Zookeeper Cluster\"]\n        B1[\"Broker 1\"]\n        B2[\"Broker 2\"]\n        B3[\"Broker 3\"]\n        \n        ZK <-->|\"ë©”íƒ€ë°ì´í„°\"| B1\n        ZK <-->|\"ë©”íƒ€ë°ì´í„°\"| B2\n        ZK <-->|\"ë©”íƒ€ë°ì´í„°\"| B3\n        \n        Problems[\"ë¬¸ì œì :<br/>â€¢ ë³„ë„ í´ëŸ¬ìŠ¤í„° ê´€ë¦¬<br/>â€¢ ë©”íƒ€ë°ì´í„° ë™ê¸°í™” ì§€ì—°<br/>â€¢ ìš´ì˜ ë³µì¡ë„\"]\n    end\n```\n\n### KRaft ì•„í‚¤í…ì²˜\n\n```mermaid\nflowchart TB\n    subgraph New [\"KRaft (Kafka 3.0+)\"]\n        subgraph Controllers [\"Controller ì—­í• \"]\n            C1[\"Controller 1\"]\n            C2[\"Controller 2\"]\n            C3[\"Controller 3\"]\n        end\n        \n        subgraph Brokers [\"Broker ì—­í• \"]\n            B1[\"Broker 1\"]\n            B2[\"Broker 2\"]\n            B3[\"Broker 3\"]\n        end\n        \n        Controllers <-->|\"Raft í•©ì˜\"| Controllers\n        Controllers -->|\"ë©”íƒ€ë°ì´í„°\"| Brokers\n        \n        Benefits[\"ì¥ì :<br/>â€¢ ë‹¨ì¼ ì‹œìŠ¤í…œ<br/>â€¢ ë¹ ë¥¸ ë©”íƒ€ë°ì´í„° ì „íŒŒ<br/>â€¢ ì‰¬ìš´ ìš´ì˜\"]\n    end\n```\n\n---\n\n## Schema Registry: ìŠ¤í‚¤ë§ˆ ë²„ì „ ê´€ë¦¬\n\ní”„ë¡œë•ì…˜ Kafkaì—ì„œ **ìŠ¤í‚¤ë§ˆ ì§„í™”(Schema Evolution)**ë¥¼ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ í•„ìˆ˜ ì»´í¬ë„ŒíŠ¸ì…ë‹ˆë‹¤.\n\n> **ì¶œì²˜**: [Confluent Schema Registry Documentation](https://docs.confluent.io/platform/current/schema-registry/), Kleppmann, \"Designing Data-Intensive Applications\" Chapter 4\n\n### ì™œ í•„ìš”í•œê°€?\n\n```mermaid\nflowchart TB\n    subgraph Problem [\"ìŠ¤í‚¤ë§ˆ ì—†ì´ ìš´ì˜\"]\n        P1[\"Producer: {name, age}\"]\n        P2[\"Consumer: {name, age, email} ê¸°ëŒ€\"]\n        P3[\"ğŸ’¥ íŒŒì‹± ì‹¤íŒ¨\"]\n        \n        P1 --> P3\n        P2 --> P3\n    end\n    \n    subgraph Solution [\"Schema Registry ì‚¬ìš©\"]\n        S1[\"ìŠ¤í‚¤ë§ˆ ì¤‘ì•™ ì €ì¥\"]\n        S2[\"ë²„ì „ ê´€ë¦¬\"]\n        S3[\"í˜¸í™˜ì„± ê²€ì¦\"]\n        S4[\"âœ… ì•ˆì „í•œ ì§„í™”\"]\n        \n        S1 --> S2 --> S3 --> S4\n    end\n```\n\n### ì§€ì› í¬ë§·\n\n| í¬ë§· | íŠ¹ì§• | ì‚¬ìš© ì‚¬ë¡€ |\n|------|------|----------|\n| **Avro** | ìŠ¤í‚¤ë§ˆ ì§„í™” ìš°ìˆ˜, ì••ì¶• íš¨ìœ¨ | ê°€ì¥ ë„ë¦¬ ì‚¬ìš© |\n| **Protobuf** | gRPC í˜¸í™˜, ê°•íƒ€ì… | ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ |\n| **JSON Schema** | ì½ê¸° ì‰¬ì›€ | ë””ë²„ê¹…, í˜¸í™˜ì„± |\n\n### í˜¸í™˜ì„± ëª¨ë“œ\n\n```mermaid\nflowchart TB\n    subgraph Modes [\"í˜¸í™˜ì„± ëª¨ë“œ\"]\n        BACKWARD[\"BACKWARD<br/>ìƒˆ ìŠ¤í‚¤ë§ˆê°€ ì´ì „ ë°ì´í„° ì½ê¸° ê°€ëŠ¥\"]\n        FORWARD[\"FORWARD<br/>ì´ì „ ìŠ¤í‚¤ë§ˆê°€ ìƒˆ ë°ì´í„° ì½ê¸° ê°€ëŠ¥\"]\n        FULL[\"FULL<br/>ì–‘ë°©í–¥ í˜¸í™˜\"]\n        NONE[\"NONE<br/>ê²€ì¦ ì—†ìŒ (ë¹„ê¶Œì¥)\"]\n    end\n    \n    Recommend[\"ê¶Œì¥: BACKWARD ë˜ëŠ” FULL\"]\n```\n\n| ëª¨ë“œ | í—ˆìš© ë³€ê²½ | ì˜ˆì‹œ |\n|------|----------|------|\n| **BACKWARD** | í•„ë“œ ì‚­ì œ, ê¸°ë³¸ê°’ ìˆëŠ” í•„ë“œ ì¶”ê°€ | ìƒˆ Consumerê°€ ì´ì „ ë°ì´í„° ì½ìŒ |\n| **FORWARD** | í•„ë“œ ì¶”ê°€, ê¸°ë³¸ê°’ ìˆëŠ” í•„ë“œ ì‚­ì œ | ì´ì „ Consumerê°€ ìƒˆ ë°ì´í„° ì½ìŒ |\n| **FULL** | ê¸°ë³¸ê°’ ìˆëŠ” í•„ë“œë§Œ ì¶”ê°€/ì‚­ì œ | ê°€ì¥ ì•ˆì „ |\n\n### Python ì‚¬ìš© ì˜ˆì‹œ\n\n```python\nfrom confluent_kafka import SerializingProducer\nfrom confluent_kafka.schema_registry import SchemaRegistryClient\nfrom confluent_kafka.schema_registry.avro import AvroSerializer\n\n# Schema Registry ì—°ê²°\nschema_registry = SchemaRegistryClient({\n    'url': 'http://schema-registry:8081'\n})\n\n# Avro ìŠ¤í‚¤ë§ˆ ì •ì˜\nuser_schema = \"\"\"\n{\n    \"type\": \"record\",\n    \"name\": \"User\",\n    \"fields\": [\n        {\"name\": \"name\", \"type\": \"string\"},\n        {\"name\": \"age\", \"type\": \"int\"},\n        {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null}\n    ]\n}\n\"\"\"\n\n# Serializer ìƒì„± (ìŠ¤í‚¤ë§ˆ ìë™ ë“±ë¡)\navro_serializer = AvroSerializer(\n    schema_registry,\n    user_schema,\n    to_dict=lambda user, ctx: user\n)\n\n# Producer ì„¤ì •\nproducer = SerializingProducer({\n    'bootstrap.servers': 'localhost:9092',\n    'value.serializer': avro_serializer\n})\n\n# ë©”ì‹œì§€ ì „ì†¡\nproducer.produce(\n    topic='users',\n    value={'name': 'Kim', 'age': 30, 'email': 'kim@example.com'}\n)\nproducer.flush()\n```\n\n---\n\n## ì‚¬ìš© ì‚¬ë¡€\n\n```mermaid\nflowchart TB\n    subgraph UseCases [\"Kafka ì‚¬ìš© ì‚¬ë¡€\"]\n        subgraph Logging [\"ë¡œê·¸ ìˆ˜ì§‘\"]\n            L1[\"App Logs\"]\n            L2[\"Kafka\"]\n            L3[\"Elasticsearch\"]\n            L1 --> L2 --> L3\n        end\n        \n        subgraph Events [\"ì´ë²¤íŠ¸ ì†Œì‹±\"]\n            E1[\"User Actions\"]\n            E2[\"Kafka<br/>(Event Store)\"]\n            E3[\"State ì¬êµ¬ì„±\"]\n            E1 --> E2 --> E3\n        end\n        \n        subgraph CDC [\"Change Data Capture\"]\n            D1[(DB)]\n            D2[\"Debezium\"]\n            D3[\"Kafka\"]\n            D4[\"Data Lake\"]\n            D1 --> D2 --> D3 --> D4\n        end\n        \n        subgraph Stream [\"ì‹¤ì‹œê°„ ë¶„ì„\"]\n            S1[\"Click Stream\"]\n            S2[\"Kafka\"]\n            S3[\"Flink/Spark\"]\n            S4[\"Dashboard\"]\n            S1 --> S2 --> S3 --> S4\n        end\n    end\n```\n\n---\n\n## ì •ë¦¬\n\n```mermaid\nmindmap\n  root((Kafka<br/>í•µì‹¬))\n    vs Redis Streams\n      ë” í° ìŠ¤ì¼€ì¼\n      ë” ê¸´ ë³´ì¡´\n      ë” ë‚˜ì€ ë³µì œ\n    êµ¬ì„± ìš”ì†Œ\n      Topic\n      Partition\n      Offset\n      Consumer Group\n    Producer\n      íŒŒí‹°ì…˜ ê²°ì •\n      í‚¤ ê¸°ë°˜ ë¶„ë°°\n      Idempotent\n    Consumer\n      Group ê´€ë¦¬\n      Offset ì»¤ë°‹\n      Rebalancing\n    Exactly-Once\n      Idempotent Producer\n      Transactional\n      read_committed\n    KRaft\n      Zookeeper ì œê±°\n      ë‹¨ìˆœí•œ ìš´ì˜\n      ë¹ ë¥¸ ë©”íƒ€ë°ì´í„°\n    Schema Registry\n      ìŠ¤í‚¤ë§ˆ ë²„ì „ ê´€ë¦¬\n      í˜¸í™˜ì„± ê²€ì¦\n      Avro/Protobuf\n```\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**9í¸: Spark Structured Streaming**ì—ì„œëŠ” ì‹¤ì‹œê°„ ì²˜ë¦¬ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- Kafka + Spark ì—°ë™\n- Watermarkì™€ Late Data\n- Window ì—°ì‚°\n- ì²´í¬í¬ì¸íŒ…\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)\n- [Confluent Schema Registry](https://docs.confluent.io/platform/current/schema-registry/)\n- [Confluent Developer](https://developer.confluent.io/)\n- \"Kafka: The Definitive Guide\" (O'Reilly)\n- Martin Kleppmann, \"Designing Data-Intensive Applications\" - Chapter 4\n- [KRaft Overview](https://kafka.apache.org/documentation/#kraft)",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Data Engineering",
      "Kafka",
      "Partitioning",
      "Streaming"
    ],
    "readingTime": 9,
    "wordCount": 1692,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-07-airflow-production",
    "slug": "de-07-airflow-production",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-07-airflow-production",
    "title": "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #7: Airflow ì‹¤ì „ - í”„ë¡œë•ì…˜ê¸‰ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•",
    "excerpt": "í”„ë¡œë•ì…˜ì—ì„œ Airflowë¥¼ ìš´ì˜í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤. DAG ëª¨ë“ˆí™”, ë™ì  Task ìƒì„±, í…ŒìŠ¤íŠ¸, ì—ëŸ¬ ì²˜ë¦¬, ëª¨ë‹ˆí„°ë§ê¹Œì§€.",
    "content": "# ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #7: Airflow ì‹¤ì „ - í”„ë¡œë•ì…˜ê¸‰ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\n\n> **ëŒ€ìƒ ë…ì**: ì¶©ë¶„í•œ ê²½í—˜ì„ ê°€ì§„ ë°±ì—”ë“œ/í’€ìŠ¤íƒ ì—”ì§€ë‹ˆì–´ë¡œ, Airflow ê¸°ë³¸ ê°œë…ì„ ìµíˆê³  í”„ë¡œë•ì…˜ ìš´ì˜ì— ê´€ì‹¬ ìˆëŠ” ë¶„\n\n## ì´ í¸ì—ì„œ ë‹¤ë£¨ëŠ” ê²ƒ\n\n6í¸ì—ì„œ Airflow ê°œë…ì„ ë°°ì› ë‹¤ë©´, ì´ì œ **ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œ ì–´ë–»ê²Œ ìš´ì˜í•˜ëŠ”ì§€** ì‹¤ì „ íŒ¨í„´ì„ ë°°ì›ë‹ˆë‹¤.\n\n---\n\n## DAG ëª¨ë“ˆí™” ì „ëµ\n\n### ì™œ ëª¨ë“ˆí™”ê°€ í•„ìš”í•œê°€?\n\n```mermaid\nflowchart TB\n    subgraph Bad [\"âŒ ëª¨ë“  ê²ƒì´ í•œ íŒŒì¼ì—\"]\n        B1[\"dag_everything.py<br/>â€¢ DB ì—°ê²°<br/>â€¢ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§<br/>â€¢ ì„¤ì •<br/>â€¢ í—¬í¼ í•¨ìˆ˜<br/>...<br/>2000ì¤„ ğŸ˜±\"]\n    end\n    \n    subgraph Good [\"âœ… ëª¨ë“ˆí™”ëœ êµ¬ì¡°\"]\n        G1[\"dags/daily_etl.py\"]\n        G2[\"plugins/operators/\"]\n        G3[\"plugins/hooks/\"]\n        G4[\"config/\"]\n        G5[\"utils/\"]\n    end\n    \n    Bad -->|\"ë¦¬íŒ©í† ë§\"| Good\n```\n\n### ê¶Œì¥ ë””ë ‰í† ë¦¬ êµ¬ì¡°\n\n```\nairflow/\nâ”œâ”€â”€ dags/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ daily_etl.py\nâ”‚   â”œâ”€â”€ hourly_metrics.py\nâ”‚   â””â”€â”€ config/\nâ”‚       â”œâ”€â”€ __init__.py\nâ”‚       â”œâ”€â”€ daily_etl_config.py\nâ”‚       â””â”€â”€ tables.py\nâ”‚\nâ”œâ”€â”€ plugins/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ operators/\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â””â”€â”€ custom_operators.py\nâ”‚   â””â”€â”€ hooks/\nâ”‚       â”œâ”€â”€ __init__.py\nâ”‚       â””â”€â”€ custom_hooks.py\nâ”‚\nâ”œâ”€â”€ tests/\nâ”‚   â”œâ”€â”€ dags/\nâ”‚   â”‚   â””â”€â”€ test_daily_etl.py\nâ”‚   â””â”€â”€ plugins/\nâ”‚       â””â”€â”€ test_operators.py\nâ”‚\nâ””â”€â”€ requirements.txt\n```\n\n### ê³µí†µ ì„¤ì • ì¶”ì¶œ\n\n```python\n# config/daily_etl_config.py\nfrom dataclasses import dataclass\nfrom datetime import timedelta\n\n@dataclass\nclass ETLConfig:\n    source_conn_id: str = \"production_db\"\n    target_conn_id: str = \"warehouse_db\"\n    retries: int = 3\n    retry_delay: timedelta = timedelta(minutes=5)\n    \n    @property\n    def default_args(self):\n        return {\n            \"owner\": \"data-team\",\n            \"retries\": self.retries,\n            \"retry_delay\": self.retry_delay,\n        }\n\n# í…Œì´ë¸”ë³„ ì„¤ì •\nTABLES = {\n    \"users\": {\"schedule\": \"@daily\", \"partition_key\": \"created_at\"},\n    \"orders\": {\"schedule\": \"@hourly\", \"partition_key\": \"order_date\"},\n    \"events\": {\"schedule\": \"*/15 * * * *\", \"partition_key\": \"event_time\"},\n}\n```\n\n---\n\n## DAG Factory íŒ¨í„´\n\n### ë™ì  DAG ìƒì„±\n\n```mermaid\nflowchart TB\n    subgraph Factory [\"DAG Factory\"]\n        Config[\"ì„¤ì • íŒŒì¼<br/>(tables.py)\"]\n        Template[\"DAG í…œí”Œë¦¿<br/>(create_etl_dag)\"]\n    end\n    \n    subgraph Output [\"ìƒì„±ëœ DAGë“¤\"]\n        D1[\"etl_users\"]\n        D2[\"etl_orders\"]\n        D3[\"etl_events\"]\n    end\n    \n    Config --> Template --> Output\n```\n\n```python\n# dags/etl_factory.py\nfrom airflow.decorators import dag, task\nfrom datetime import datetime\nfrom config.daily_etl_config import ETLConfig, TABLES\n\ndef create_etl_dag(table_name: str, table_config: dict):\n    \"\"\"í…Œì´ë¸”ë³„ ETL DAGë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±\"\"\"\n    \n    config = ETLConfig()\n    \n    @dag(\n        dag_id=f\"etl_{table_name}\",\n        schedule=table_config[\"schedule\"],\n        start_date=datetime(2024, 1, 1),\n        catchup=False,\n        default_args=config.default_args,\n        tags=[\"etl\", \"generated\"]\n    )\n    def etl_pipeline():\n        \n        @task\n        def extract(**context):\n            from airflow.providers.postgres.hooks.postgres import PostgresHook\n            \n            hook = PostgresHook(postgres_conn_id=config.source_conn_id)\n            date = context[\"data_interval_start\"].strftime(\"%Y-%m-%d\")\n            partition_key = table_config[\"partition_key\"]\n            \n            sql = f\"\"\"\n                SELECT * FROM {table_name}\n                WHERE DATE({partition_key}) = '{date}'\n            \"\"\"\n            return hook.get_records(sql)\n        \n        @task\n        def transform(raw_data):\n            # ë³€í™˜ ë¡œì§\n            return raw_data\n        \n        @task\n        def load(data, **context):\n            from airflow.providers.postgres.hooks.postgres import PostgresHook\n            \n            hook = PostgresHook(postgres_conn_id=config.target_conn_id)\n            # ë¡œë“œ ë¡œì§\n            print(f\"Loaded {len(data)} records to warehouse.{table_name}\")\n        \n        raw = extract()\n        transformed = transform(raw)\n        load(transformed)\n    \n    return etl_pipeline()\n\n# ëª¨ë“  í…Œì´ë¸”ì— ëŒ€í•´ DAG ìƒì„±\nfor table_name, table_config in TABLES.items():\n    globals()[f\"etl_{table_name}\"] = create_etl_dag(table_name, table_config)\n```\n\n---\n\n## Dynamic Task Mapping (Airflow 2.3+)\n\n### ëŸ°íƒ€ì„ì— Task ê°œìˆ˜ ê²°ì •\n\n```mermaid\nflowchart LR\n    subgraph Before [\"ì •ì  ë°©ì‹\"]\n        B1[\"process_user_1\"]\n        B2[\"process_user_2\"]\n        B3[\"process_user_3\"]\n        Note1[\"ë¯¸ë¦¬ ì •í•´ì§„ ìˆ˜\"]\n    end\n    \n    subgraph After [\"Dynamic Mapping\"]\n        List[\"get_users()<br/>â†’ [u1, u2, ... uN]\"]\n        Expand[\"process.expand(user=users)\"]\n        Tasks[\"Nê°œì˜ Task ìƒì„±\"]\n        \n        List --> Expand --> Tasks\n        Note2[\"ëŸ°íƒ€ì„ì— ê²°ì •\"]\n    end\n```\n\n```python\nfrom airflow.decorators import dag, task\nfrom datetime import datetime\n\n@dag(\n    dag_id=\"dynamic_processing\",\n    schedule=\"@daily\",\n    start_date=datetime(2024, 1, 1),\n    catchup=False\n)\ndef dynamic_processing():\n    \n    @task\n    def get_partitions(**context):\n        \"\"\"ì²˜ë¦¬í•  íŒŒí‹°ì…˜ ëª©ë¡ ë™ì  ë°˜í™˜\"\"\"\n        date = context[\"data_interval_start\"]\n        # ì˜ˆ: ë‚ ì§œì— ë”°ë¼ ë‹¤ë¥¸ ê°œìˆ˜\n        return [f\"partition_{i}\" for i in range(10)]  # 10ê°œ íŒŒí‹°ì…˜\n    \n    @task\n    def process_partition(partition: str):\n        \"\"\"ê° íŒŒí‹°ì…˜ ë³‘ë ¬ ì²˜ë¦¬\"\"\"\n        print(f\"Processing {partition}\")\n        return {\"partition\": partition, \"count\": 1000}\n    \n    @task\n    def aggregate(results: list):\n        \"\"\"ëª¨ë“  ê²°ê³¼ ì§‘ê³„\"\"\"\n        total = sum(r[\"count\"] for r in results)\n        print(f\"Total: {total} records from {len(results)} partitions\")\n    \n    # Dynamic Task Mapping\n    partitions = get_partitions()\n    results = process_partition.expand(partition=partitions)  # Nê°œ Task ìƒì„±\n    aggregate(results)\n\ndynamic_processing()\n```\n\n---\n\n## í…ŒìŠ¤íŠ¸ ì „ëµ\n\n### í…ŒìŠ¤íŠ¸ í”¼ë¼ë¯¸ë“œ\n\n```mermaid\nflowchart TB\n    subgraph Pyramid [\"í…ŒìŠ¤íŠ¸ í”¼ë¼ë¯¸ë“œ\"]\n        E2E[\"E2E í…ŒìŠ¤íŠ¸<br/>(ì‹¤ì œ í™˜ê²½)\"]\n        Integration[\"í†µí•© í…ŒìŠ¤íŠ¸<br/>(DAG ìœ íš¨ì„±)\"]\n        Unit[\"ë‹¨ìœ„ í…ŒìŠ¤íŠ¸<br/>(ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§)\"]\n    end\n    \n    Unit -->|\"ê°€ì¥ ë§ì´\"| Integration -->|\"ì ë‹¹íˆ\"| E2E\n```\n\n### DAG ìœ íš¨ì„± í…ŒìŠ¤íŠ¸\n\n```python\n# tests/dags/test_dag_validity.py\nimport pytest\nfrom airflow.models import DagBag\n\nclass TestDAGValidity:\n    \"\"\"ëª¨ë“  DAGì˜ ê¸°ë³¸ ìœ íš¨ì„± ê²€ì‚¬\"\"\"\n    \n    @pytest.fixture\n    def dagbag(self):\n        return DagBag(include_examples=False)\n    \n    def test_no_import_errors(self, dagbag):\n        \"\"\"DAG import ì˜¤ë¥˜ ì—†ìŒ\"\"\"\n        assert dagbag.import_errors == {}, f\"Import errors: {dagbag.import_errors}\"\n    \n    def test_all_dags_have_tags(self, dagbag):\n        \"\"\"ëª¨ë“  DAGì— íƒœê·¸ ìˆìŒ\"\"\"\n        for dag_id, dag in dagbag.dags.items():\n            assert dag.tags, f\"DAG {dag_id} has no tags\"\n    \n    def test_no_cycles(self, dagbag):\n        \"\"\"ìˆœí™˜ ì˜ì¡´ì„± ì—†ìŒ\"\"\"\n        for dag_id, dag in dagbag.dags.items():\n            # Airflowê°€ ìë™ìœ¼ë¡œ ê²€ì‚¬í•˜ì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ\n            assert not dag.test_cycle(), f\"DAG {dag_id} has a cycle\"\n    \n    def test_default_args(self, dagbag):\n        \"\"\"í•„ìˆ˜ default_args ì¡´ì¬\"\"\"\n        required_keys = [\"owner\", \"retries\"]\n        for dag_id, dag in dagbag.dags.items():\n            for key in required_keys:\n                assert key in dag.default_args, f\"DAG {dag_id} missing {key}\"\n```\n\n### ê°œë³„ Task í…ŒìŠ¤íŠ¸\n\n```python\n# tests/dags/test_daily_etl.py\nimport pytest\nfrom unittest.mock import patch, MagicMock\nfrom dags.daily_etl import calculate_metrics\n\nclass TestDailyETL:\n    \n    def test_calculate_metrics(self):\n        \"\"\"ë©”íŠ¸ë¦­ ê³„ì‚° ë¡œì§ í…ŒìŠ¤íŠ¸\"\"\"\n        events = [\n            {\"user_id\": 1, \"event\": \"click\"},\n            {\"user_id\": 1, \"event\": \"view\"},\n            {\"user_id\": 2, \"event\": \"click\"},\n        ]\n        \n        result = calculate_metrics.function(events)\n        \n        assert result[\"total_events\"] == 3\n        assert result[\"unique_users\"] == 2\n        assert result[\"events_per_user\"] == 1.5\n    \n    @patch(\"dags.daily_etl.PostgresHook\")\n    def test_extract_users(self, mock_hook):\n        \"\"\"ì¶”ì¶œ Task í…ŒìŠ¤íŠ¸ (Mock ì‚¬ìš©)\"\"\"\n        mock_hook.return_value.get_pandas_df.return_value = pd.DataFrame({\n            \"user_id\": [1, 2],\n            \"event_type\": [\"click\", \"view\"]\n        })\n        \n        # Task ì‹¤í–‰\n        result = extract_users.function(data_interval_start=datetime(2024, 1, 1))\n        \n        assert len(result) == 2\n        mock_hook.assert_called_once()\n```\n\n---\n\n## ì—ëŸ¬ ì²˜ë¦¬ì™€ ì•Œë¦¼\n\n### ì½œë°± í•¨ìˆ˜\n\n```mermaid\nflowchart TB\n    subgraph Callbacks [\"ì½œë°± ì¢…ë¥˜\"]\n        C1[\"on_success_callback<br/>ì„±ê³µ ì‹œ\"]\n        C2[\"on_failure_callback<br/>ì‹¤íŒ¨ ì‹œ\"]\n        C3[\"on_retry_callback<br/>ì¬ì‹œë„ ì‹œ\"]\n        C4[\"sla_miss_callback<br/>SLA ì´ˆê³¼ ì‹œ\"]\n    end\n    \n    subgraph Actions [\"ê°€ëŠ¥í•œ ì•¡ì…˜\"]\n        A1[\"Slack ì•Œë¦¼\"]\n        A2[\"PagerDuty í˜¸ì¶œ\"]\n        A3[\"ì´ë©”ì¼ ì „ì†¡\"]\n        A4[\"ë©”íŠ¸ë¦­ ê¸°ë¡\"]\n    end\n    \n    Callbacks --> Actions\n```\n\n```python\nfrom airflow.decorators import dag, task\nfrom datetime import datetime, timedelta\n\ndef send_slack_alert(context):\n    \"\"\"ì‹¤íŒ¨ ì‹œ Slack ì•Œë¦¼\"\"\"\n    from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook\n    \n    task_instance = context[\"task_instance\"]\n    dag_id = context[\"dag\"].dag_id\n    task_id = task_instance.task_id\n    execution_date = context[\"execution_date\"]\n    log_url = task_instance.log_url\n    \n    message = f\"\"\"\n    ğŸš¨ *Task Failed*\n    â€¢ DAG: `{dag_id}`\n    â€¢ Task: `{task_id}`\n    â€¢ Execution: {execution_date}\n    â€¢ <{log_url}|View Logs>\n    \"\"\"\n    \n    hook = SlackWebhookHook(slack_webhook_conn_id=\"slack_webhook\")\n    hook.send(text=message)\n\ndef send_success_notification(context):\n    \"\"\"ì„±ê³µ ì‹œ ì•Œë¦¼ (ì„ íƒì )\"\"\"\n    # ì¤‘ìš”í•œ DAGë§Œ ì„±ê³µ ì•Œë¦¼\n    pass\n\n@dag(\n    dag_id=\"monitored_pipeline\",\n    schedule=\"@daily\",\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    default_args={\n        \"retries\": 3,\n        \"retry_delay\": timedelta(minutes=5),\n        \"on_failure_callback\": send_slack_alert,\n    },\n    on_success_callback=send_success_notification\n)\ndef monitored_pipeline():\n    \n    @task(\n        retries=5,  # Task ê°œë³„ ì„¤ì •ë„ ê°€ëŠ¥\n        retry_delay=timedelta(minutes=2)\n    )\n    def critical_task():\n        # ì¤‘ìš” ë¡œì§\n        pass\n    \n    critical_task()\n\nmonitored_pipeline()\n```\n\n### SLA (Service Level Agreement)\n\n```python\nfrom airflow.decorators import dag, task\nfrom datetime import datetime, timedelta\n\n@dag(\n    dag_id=\"sla_monitored\",\n    schedule=\"@hourly\",\n    start_date=datetime(2024, 1, 1),\n    sla_miss_callback=send_sla_alert\n)\ndef sla_monitored():\n    \n    @task(\n        sla=timedelta(minutes=30)  # 30ë¶„ ë‚´ ì™„ë£Œë˜ì–´ì•¼ í•¨\n    )\n    def time_sensitive_task():\n        # SLAë¥¼ ì´ˆê³¼í•˜ë©´ sla_miss_callback í˜¸ì¶œ\n        pass\n```\n\n---\n\n## ëª¨ë‹ˆí„°ë§ê³¼ ê´€ì¸¡ì„±\n\n### Airflow ë©”íŠ¸ë¦­\n\n```mermaid\nflowchart TB\n    subgraph Metrics [\"ì£¼ìš” ëª¨ë‹ˆí„°ë§ ì§€í‘œ\"]\n        subgraph DAG [\"DAG ë ˆë²¨\"]\n            D1[\"DAG Run ì„±ê³µë¥ \"]\n            D2[\"í‰ê·  ì‹¤í–‰ ì‹œê°„\"]\n            D3[\"ì§€ì—°(Lag)\"]\n        end\n        \n        subgraph Task [\"Task ë ˆë²¨\"]\n            T1[\"Task ì„±ê³µ/ì‹¤íŒ¨ìœ¨\"]\n            T2[\"Task Duration\"]\n            T3[\"Queue ëŒ€ê¸° ì‹œê°„\"]\n        end\n        \n        subgraph System [\"ì‹œìŠ¤í…œ\"]\n            S1[\"Scheduler Heartbeat\"]\n            S2[\"Worker ìƒíƒœ\"]\n            S3[\"DB Connection Pool\"]\n        end\n    end\n```\n\n### StatsD + Grafana ì—°ë™\n\n```python\n# airflow.cfg\n[metrics]\nstatsd_on = True\nstatsd_host = statsd-exporter\nstatsd_port = 9125\nstatsd_prefix = airflow\n```\n\n### ìœ ìš©í•œ ëŒ€ì‹œë³´ë“œ ì¿¼ë¦¬\n\n```python\n# ì‹¤íŒ¨í•œ DAG Run ì¡°íšŒ\nfrom airflow.models import DagRun\n\nfailed_runs = DagRun.find(\n    state=\"failed\",\n    execution_start_date=datetime.now() - timedelta(days=1)\n)\n\nfor run in failed_runs:\n    print(f\"{run.dag_id}: {run.execution_date}\")\n```\n\n---\n\n## ë©±ë“±ì„± ë³´ì¥\n\n### ì™œ ë©±ë“±ì„±ì´ ì¤‘ìš”í•œê°€?\n\n```mermaid\nflowchart TB\n    subgraph Problem [\"ë©±ë“±í•˜ì§€ ì•Šì€ ê²½ìš°\"]\n        Run1[\"ì²« ì‹¤í–‰: 100ê±´ ì‚½ì…\"]\n        Run2[\"ì¬ì‹¤í–‰: 100ê±´ ì¶”ê°€ ì‚½ì…\"]\n        Result1[\"ê²°ê³¼: 200ê±´ (ì¤‘ë³µ!)\"]\n    end\n    \n    subgraph Solution [\"ë©±ë“±í•œ ê²½ìš°\"]\n        Run3[\"ì²« ì‹¤í–‰: 100ê±´ ì‚½ì…\"]\n        Run4[\"ì¬ì‹¤í–‰: 100ê±´ ë®ì–´ì“°ê¸°\"]\n        Result2[\"ê²°ê³¼: 100ê±´ (ì •í™•!)\"]\n    end\n```\n\n### ë©±ë“±ì„± í™•ë³´ íŒ¨í„´\n\n```python\n@task\ndef load_idempotent(data, **context):\n    \"\"\"ë©±ë“±í•œ ë¡œë“œ\"\"\"\n    from airflow.providers.postgres.hooks.postgres import PostgresHook\n    \n    hook = PostgresHook(postgres_conn_id=\"warehouse\")\n    date = context[\"data_interval_start\"].strftime(\"%Y-%m-%d\")\n    table = \"daily_metrics\"\n    \n    # íŒ¨í„´ 1: DELETE + INSERT\n    hook.run(f\"DELETE FROM {table} WHERE date = '{date}'\")\n    hook.insert_rows(table, data)\n    \n    # íŒ¨í„´ 2: UPSERT (PostgreSQL)\n    hook.run(f\"\"\"\n        INSERT INTO {table} (date, value)\n        VALUES ('{date}', {data['value']})\n        ON CONFLICT (date) DO UPDATE SET\n            value = EXCLUDED.value,\n            updated_at = NOW()\n    \"\"\")\n    \n    # íŒ¨í„´ 3: Partition êµì²´ (S3/GCS)\n    # s3://bucket/table/date=2024-01-01/ ì „ì²´ êµì²´\n```\n\n---\n\n## ì •ë¦¬\n\n```mermaid\nmindmap\n  root((Airflow<br/>ì‹¤ì „))\n    ëª¨ë“ˆí™”\n      ë””ë ‰í† ë¦¬ êµ¬ì¡°\n      ì„¤ì • ë¶„ë¦¬\n      ì¬ì‚¬ìš© ê°€ëŠ¥\n    DAG Factory\n      ë™ì  ìƒì„±\n      ì„¤ì • ê¸°ë°˜\n      ìœ ì§€ë³´ìˆ˜ ìš©ì´\n    Dynamic Mapping\n      ëŸ°íƒ€ì„ ê²°ì •\n      expand ì‚¬ìš©\n      ë³‘ë ¬ ì²˜ë¦¬\n    í…ŒìŠ¤íŠ¸\n      DAG ìœ íš¨ì„±\n      Task ë‹¨ìœ„\n      Mock í™œìš©\n    ì—ëŸ¬ ì²˜ë¦¬\n      on_failure_callback\n      SLA ì„¤ì •\n      Slack ì—°ë™\n    ëª¨ë‹ˆí„°ë§\n      ë©”íŠ¸ë¦­ ìˆ˜ì§‘\n      Grafana\n      ë¡œê·¸ ì§‘ê³„\n    ë©±ë“±ì„±\n      DELETE + INSERT\n      UPSERT\n      íŒŒí‹°ì…˜ êµì²´\n```\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**8í¸: Kafka í•µì‹¬**ì—ì„œëŠ” ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°ì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- Redis Streamsì™€ì˜ ë¹„êµ\n- Topic, Partition, Consumer Group\n- Exactly-Once Semantics\n- KRaft ëª¨ë“œ\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)\n- [Testing Airflow DAGs](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html#testing-a-dag)\n- [Dynamic Task Mapping](https://airflow.apache.org/docs/apache-airflow/stable/concepts/dynamic-task-mapping.html)\n- Astronomer, \"Airflow in Production\"",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Airflow",
      "Data Engineering",
      "Observability",
      "Orchestration",
      "Testing"
    ],
    "readingTime": 7,
    "wordCount": 1297,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-06-airflow-core-concepts",
    "slug": "de-06-airflow-core-concepts",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-06-airflow-core-concepts",
    "title": "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #6: Airflow í•µì‹¬ ê°œë… - DAG, Operator, Task",
    "excerpt": "ì™œ cronìœ¼ë¡œëŠ” ë¶€ì¡±í• ê¹Œìš”? Airflowì˜ í•µì‹¬ ê°œë…ì¸ DAG, Operator, Taskë¥¼ ì´í•´í•˜ê³  TaskFlow APIë¡œ í˜„ëŒ€ì ì¸ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‘ì„±í•˜ëŠ” ë²•ì„ ë°°ì›ë‹ˆë‹¤.",
    "content": "# ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #6: Airflow í•µì‹¬ ê°œë… - DAG, Operator, Task\n\n> **ëŒ€ìƒ ë…ì**: ì¶©ë¶„í•œ ê²½í—˜ì„ ê°€ì§„ ë°±ì—”ë“œ/í’€ìŠ¤íƒ ì—”ì§€ë‹ˆì–´ë¡œ, CI/CD íŒŒì´í”„ë¼ì¸ì´ë‚˜ cron jobì— ìµìˆ™í•˜ì§€ë§Œ AirflowëŠ” ì²˜ìŒì¸ ë¶„\n\n## ì´ í¸ì—ì„œ ë‹¤ë£¨ëŠ” ê²ƒ\n\nGitHub Actionsë‚˜ cronìœ¼ë¡œ ë°°ì¹˜ ì‘ì—…ì„ ëŒë ¤ë³¸ ê²½í—˜ì´ ìˆë‹¤ë©´, **ì™œ ë°ì´í„° íŒ€ì€ Airflowë¥¼ ì“°ëŠ”ì§€** ê¶ê¸ˆí–ˆì„ ê²ë‹ˆë‹¤. ê·¸ ì´ìœ ì™€ í•µì‹¬ ê°œë…ì„ ë°°ì›ë‹ˆë‹¤.\n\n---\n\n## ì™œ cron jobìœ¼ë¡œëŠ” ë¶€ì¡±í•œê°€?\n\n### cronì˜ í•œê³„\n\n```mermaid\nflowchart TB\n    subgraph Cron [\"cron ë°©ì‹ì˜ ë¬¸ì œ\"]\n        C1[\"0 1 * * * extract.sh\"]\n        C2[\"0 2 * * * transform.sh\"]\n        C3[\"0 3 * * * load.sh\"]\n        \n        Problem1[\"â“ extractê°€ ëŠ¦ì–´ì§€ë©´?\"]\n        Problem2[\"â“ ì¤‘ê°„ì— ì‹¤íŒ¨í•˜ë©´?\"]\n        Problem3[\"â“ ì–´ì œ ë°ì´í„°ë¥¼ ì¬ì²˜ë¦¬í•˜ë ¤ë©´?\"]\n        Problem4[\"â“ ì‹¤í–‰ ìƒíƒœë¥¼ ì–´ë–»ê²Œ í™•ì¸?\"]\n    end\n```\n\n| ë¬¸ì œ | cron | Airflow |\n|------|------|---------|\n| **ì˜ì¡´ì„± ê´€ë¦¬** | ì‹œê°„ìœ¼ë¡œë§Œ (ë¶ˆí™•ì‹¤) | ëª…ì‹œì  ì˜ì¡´ì„± âœ… |\n| **ì‹¤íŒ¨ ì²˜ë¦¬** | ìˆ˜ë™ í™•ì¸/ì¬ì‹¤í–‰ | ìë™ ì¬ì‹œë„ âœ… |\n| **ë°±í•„** | ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ë™ ìˆ˜ì • | ë‚ ì§œ ì§€ì • ì¬ì‹¤í–‰ âœ… |\n| **ëª¨ë‹ˆí„°ë§** | ë¡œê·¸ íŒŒì¼ ë’¤ì§€ê¸° | ì›¹ UI âœ… |\n| **ì•Œë¦¼** | ì§ì ‘ êµ¬í˜„ | Slack/Email ì—°ë™ âœ… |\n\n### ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤\n\n```mermaid\nflowchart LR\n    subgraph Reality [\"í˜„ì‹¤ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì¼\"]\n        A[\"Extract<br/>(01:00 ì˜ˆì •)\"]\n        B[\"Transform<br/>(02:00 ì˜ˆì •)\"]\n        C[\"Load<br/>(03:00 ì˜ˆì •)\"]\n        \n        A -->|\"01:30ì— ëë‚¨\"| Delay\n        Delay[\"âš ï¸ Transformì´<br/>ë¶ˆì™„ì „í•œ ë°ì´í„°ë¡œ ì‹œì‘\"]\n        Delay --> Bad[\"âŒ ì˜ëª»ëœ ê²°ê³¼\"]\n    end\n```\n\n**Airflowì˜ í•´ê²°ì±…**: Task ê°„ **ì˜ì¡´ì„±**ì„ ì •ì˜í•˜ì—¬ ì´ì „ Taskê°€ ì™„ë£Œë˜ì–´ì•¼ ë‹¤ìŒì´ ì‹œì‘\n\n---\n\n## Airflow ì•„í‚¤í…ì²˜\n\n### êµ¬ì„± ìš”ì†Œ\n\n```mermaid\nflowchart TB\n    subgraph Airflow [\"Airflow ì‹œìŠ¤í…œ\"]\n        Web[\"Webserver<br/>ğŸ“Š UI ì œê³µ\"]\n        Sched[\"Scheduler<br/>â° DAG íŒŒì‹±/ìŠ¤ì¼€ì¤„ë§\"]\n        Worker[\"Worker(s)<br/>âš™ï¸ Task ì‹¤í–‰\"]\n        DB[(Metadata DB<br/>ğŸ“ ìƒíƒœ ì €ì¥)]\n        \n        Web <--> DB\n        Sched <--> DB\n        Worker <--> DB\n        Sched -->|\"Task í• ë‹¹\"| Worker\n    end\n    \n    subgraph DAGs [\"DAG íŒŒì¼\"]\n        D1[\"dag1.py\"]\n        D2[\"dag2.py\"]\n        D3[\"dag3.py\"]\n    end\n    \n    DAGs -->|\"íŒŒì‹±\"| Sched\n```\n\n### Executor ì¢…ë¥˜\n\n| Executor | íŠ¹ì§• | ì í•©í•œ í™˜ê²½ |\n|----------|------|------------|\n| **LocalExecutor** | ë‹¨ì¼ ë¨¸ì‹ , ë©€í‹° í”„ë¡œì„¸ìŠ¤ | ê°œë°œ, ì†Œê·œëª¨ |\n| **CeleryExecutor** | ë¶„ì‚° ì›Œì»¤ (Redis/RabbitMQ) | ì¤‘ê·œëª¨ í”„ë¡œë•ì…˜ |\n| **KubernetesExecutor** | ê° Taskë¥¼ Podë¡œ | ëŒ€ê·œëª¨, í´ë¼ìš°ë“œ |\n\n---\n\n## DAG (Directed Acyclic Graph)\n\n### DAGë€?\n\n```mermaid\nflowchart LR\n    subgraph DAG [\"DAG: Directed Acyclic Graph\"]\n        A[\"Task A\"]\n        B[\"Task B\"]\n        C[\"Task C\"]\n        D[\"Task D\"]\n        E[\"Task E\"]\n        \n        A --> B\n        A --> C\n        B --> D\n        C --> D\n        D --> E\n    end\n    \n    subgraph Rules [\"ê·œì¹™\"]\n        R1[\"âœ… Directed: ë°©í–¥ì´ ìˆìŒ\"]\n        R2[\"âœ… Acyclic: ìˆœí™˜ ì—†ìŒ\"]\n        R3[\"âŒ A â†’ B â†’ A (ë¶ˆê°€)\"]\n    end\n```\n\n**ì™œ ê·¸ë˜í”„ì¸ê°€?**\n\n- ìˆœì°¨ ì‹¤í–‰ë§Œ ìˆëŠ” ê²Œ ì•„ë‹˜\n- ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥ (Bì™€ C ë™ì‹œ ì‹¤í–‰)\n- ì˜ì¡´ì„± ëª…í™•íˆ í‘œí˜„\n\n### DAG ì •ì˜ ì˜ˆì‹œ\n\n```python\nfrom airflow import DAG\nfrom datetime import datetime\n\n# DAG ì •ì˜\ndag = DAG(\n    dag_id=\"my_etl_pipeline\",\n    start_date=datetime(2024, 1, 1),\n    schedule=\"@daily\",  # ë§¤ì¼ ì‹¤í–‰\n    catchup=False,\n    tags=[\"etl\", \"production\"]\n)\n```\n\n---\n\n## Operatorì™€ Task\n\n### Operator: ë¬´ì—‡ì„ í•  ê²ƒì¸ê°€?\n\n```mermaid\nflowchart TB\n    subgraph Operators [\"ì£¼ìš” Operator ì¢…ë¥˜\"]\n        subgraph Basic [\"ê¸°ë³¸\"]\n            O1[\"BashOperator<br/>ì‰˜ ëª…ë ¹ ì‹¤í–‰\"]\n            O2[\"PythonOperator<br/>Python í•¨ìˆ˜ ì‹¤í–‰\"]\n            O3[\"EmptyOperator<br/>ì•„ë¬´ê²ƒë„ ì•ˆ í•¨\"]\n        end\n        \n        subgraph Transfer [\"ë°ì´í„° ì „ì†¡\"]\n            O4[\"S3ToRedshiftOperator\"]\n            O5[\"GCSToGCSOperator\"]\n        end\n        \n        subgraph External [\"ì™¸ë¶€ ì‹œìŠ¤í…œ\"]\n            O6[\"SparkSubmitOperator<br/>Spark ì‘ì—… ì œì¶œ\"]\n            O7[\"DockerOperator<br/>ì»¨í…Œì´ë„ˆ ì‹¤í–‰\"]\n            O8[\"PostgresOperator<br/>SQL ì‹¤í–‰\"]\n        end\n        \n        subgraph Sensors [\"ì„¼ì„œ (ëŒ€ê¸°)\"]\n            O9[\"FileSensor<br/>íŒŒì¼ ì¡´ì¬ ëŒ€ê¸°\"]\n            O10[\"HttpSensor<br/>API ì‘ë‹µ ëŒ€ê¸°\"]\n        end\n    end\n```\n\n### Task: Operatorì˜ ì¸ìŠ¤í„´ìŠ¤\n\n```mermaid\nflowchart LR\n    subgraph Definition [\"ì •ì˜\"]\n        Operator[\"PythonOperator<br/>(í´ë˜ìŠ¤)\"]\n    end\n    \n    subgraph Instance [\"ì¸ìŠ¤í„´ìŠ¤\"]\n        Task1[\"extract_task<br/>(Task)\"]\n        Task2[\"transform_task<br/>(Task)\"]\n        Task3[\"load_task<br/>(Task)\"]\n    end\n    \n    Operator --> Task1\n    Operator --> Task2\n    Operator --> Task3\n```\n\n```python\nfrom airflow.operators.python import PythonOperator\n\ndef extract_data():\n    # ë°ì´í„° ì¶”ì¶œ ë¡œì§\n    return {\"records\": 1000}\n\ndef transform_data(**context):\n    # ì´ì „ Task ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°\n    data = context[\"ti\"].xcom_pull(task_ids=\"extract\")\n    # ë³€í™˜ ë¡œì§\n    return {\"processed\": data[\"records\"]}\n\n# Task ì •ì˜\nextract_task = PythonOperator(\n    task_id=\"extract\",\n    python_callable=extract_data,\n    dag=dag\n)\n\ntransform_task = PythonOperator(\n    task_id=\"transform\",\n    python_callable=transform_data,\n    dag=dag\n)\n\n# ì˜ì¡´ì„± ì •ì˜\nextract_task >> transform_task\n```\n\n---\n\n## TaskFlow API (Airflow 2.0+)\n\n### ì „í†µì  ë°©ì‹ vs TaskFlow\n\n```mermaid\nflowchart TB\n    subgraph Traditional [\"ì „í†µì  ë°©ì‹\"]\n        T1[\"Operator ì •ì˜\"]\n        T2[\"XComìœ¼ë¡œ ë°ì´í„° ì „ë‹¬\"]\n        T3[\"ì˜ì¡´ì„± ë³„ë„ ì •ì˜\"]\n        \n        T1 --> T2 --> T3\n        Note1[\"ì¥í™©í•œ ì½”ë“œ ğŸ˜“\"]\n    end\n    \n    subgraph TaskFlow [\"TaskFlow API\"]\n        TF1[\"@task ë°ì½”ë ˆì´í„°\"]\n        TF2[\"returnìœ¼ë¡œ ì „ë‹¬\"]\n        TF3[\"í•¨ìˆ˜ í˜¸ì¶œë¡œ ì˜ì¡´ì„±\"]\n        \n        TF1 --> TF2 --> TF3\n        Note2[\"ê¹”ë”í•œ ì½”ë“œ âœ¨\"]\n    end\n```\n\n### TaskFlow ì˜ˆì‹œ\n\n```python\nfrom airflow.decorators import dag, task\nfrom datetime import datetime\n\n@dag(\n    dag_id=\"taskflow_etl\",\n    start_date=datetime(2024, 1, 1),\n    schedule=\"@daily\",\n    catchup=False\n)\ndef my_etl_pipeline():\n    \"\"\"TaskFlow APIë¥¼ ì‚¬ìš©í•œ ETL íŒŒì´í”„ë¼ì¸\"\"\"\n    \n    @task\n    def extract():\n        \"\"\"ë°ì´í„° ì¶”ì¶œ\"\"\"\n        return {\"data\": [1, 2, 3, 4, 5]}\n    \n    @task\n    def transform(raw_data: dict):\n        \"\"\"ë°ì´í„° ë³€í™˜\"\"\"\n        return {\n            \"data\": [x * 2 for x in raw_data[\"data\"]],\n            \"count\": len(raw_data[\"data\"])\n        }\n    \n    @task\n    def load(processed_data: dict):\n        \"\"\"ë°ì´í„° ì ì¬\"\"\"\n        print(f\"Loaded {processed_data['count']} records\")\n    \n    # ì˜ì¡´ì„±ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì •ì˜ë¨\n    raw = extract()\n    processed = transform(raw)\n    load(processed)\n\n# DAG ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\nmy_etl_pipeline()\n```\n\n### XCom ìë™ ì²˜ë¦¬\n\n```mermaid\nflowchart LR\n    subgraph Traditional [\"ì „í†µì  XCom\"]\n        E1[\"extract\"]\n        X1[\"xcom_push()\"]\n        X2[\"xcom_pull()\"]\n        T1[\"transform\"]\n        \n        E1 --> X1 --> X2 --> T1\n        Note1[\"ëª…ì‹œì  push/pull í•„ìš”\"]\n    end\n    \n    subgraph TaskFlow [\"TaskFlow XCom\"]\n        E2[\"@task<br/>return data\"]\n        T2[\"@task<br/>def fn(data):\"]\n        \n        E2 -->|\"ìë™!\"| T2\n        Note2[\"return/íŒŒë¼ë¯¸í„°ë¡œ ìë™ ì „ë‹¬\"]\n    end\n```\n\n---\n\n## ìŠ¤ì¼€ì¤„ë§ê³¼ Data Interval\n\n### schedule í‘œí˜„ì‹\n\n| í‘œí˜„ì‹ | ì˜ë¯¸ | cron í‘œí˜„ |\n|--------|------|----------|\n| `@once` | í•œ ë²ˆë§Œ | - |\n| `@hourly` | ë§¤ì‹œ | `0 * * * *` |\n| `@daily` | ë§¤ì¼ | `0 0 * * *` |\n| `@weekly` | ë§¤ì£¼ | `0 0 * * 0` |\n| `@monthly` | ë§¤ì›” | `0 0 1 * *` |\n| `0 6 * * *` | ë§¤ì¼ 6ì‹œ | - |\n| `None` | ìˆ˜ë™ íŠ¸ë¦¬ê±°ë§Œ | - |\n\n### Data Interval ê°œë… (ì¤‘ìš”!)\n\n```mermaid\nflowchart TB\n    subgraph Timeline [\"ì‹œê°„ì„ \"]\n        T1[\"2024-01-01<br/>00:00\"]\n        T2[\"2024-01-02<br/>00:00\"]\n        T3[\"2024-01-03<br/>00:00\"]\n    end\n    \n    subgraph DAGRun [\"DAG ì‹¤í–‰\"]\n        D1[\"DAG Run 1<br/>data_interval: 01-01 ~ 01-02\"]\n        D2[\"DAG Run 2<br/>data_interval: 01-02 ~ 01-03\"]\n    end\n    \n    T2 -->|\"ì‹¤í–‰ ì‹œì \"| D1\n    T3 -->|\"ì‹¤í–‰ ì‹œì \"| D2\n    \n    Note[\"âš ï¸ 1ì›” 2ì¼ì— 1ì›” 1ì¼ ë°ì´í„°ë¥¼ ì²˜ë¦¬!\"]\n```\n\n```python\n@task\ndef process_data(**context):\n    # ì²˜ë¦¬í•  ë°ì´í„°ì˜ ë‚ ì§œ ë²”ìœ„\n    data_interval_start = context[\"data_interval_start\"]\n    data_interval_end = context[\"data_interval_end\"]\n    \n    # ì˜ˆ: 2024-01-01 00:00 ~ 2024-01-02 00:00\n    print(f\"Processing data from {data_interval_start} to {data_interval_end}\")\n```\n\n### Catchupê³¼ Backfill\n\n```mermaid\nflowchart TB\n    subgraph Catchup [\"catchup=True\"]\n        C1[\"DAG ìƒì„±: 2024-01-05\"]\n        C2[\"start_date: 2024-01-01\"]\n        C3[\"ëˆ„ë½ëœ 4ì¼ì¹˜ ìë™ ì‹¤í–‰\"]\n        \n        C1 --> C2 --> C3\n    end\n    \n    subgraph NoCatchup [\"catchup=False\"]\n        N1[\"DAG ìƒì„±: 2024-01-05\"]\n        N2[\"start_date: 2024-01-01\"]\n        N3[\"ì˜¤ëŠ˜(01-05)ë¶€í„°ë§Œ ì‹¤í–‰\"]\n        \n        N1 --> N2 --> N3\n    end\n```\n\n```bash\n# ìˆ˜ë™ Backfill\nairflow dags backfill \\\n    --start-date 2024-01-01 \\\n    --end-date 2024-01-10 \\\n    my_etl_pipeline\n```\n\n---\n\n## Task ì˜ì¡´ì„± íŒ¨í„´\n\n### ê¸°ë³¸ íŒ¨í„´\n\n```mermaid\nflowchart LR\n    subgraph Sequential [\"ìˆœì°¨\"]\n        S1[\"A\"] --> S2[\"B\"] --> S3[\"C\"]\n    end\n    \n    subgraph Parallel [\"ë³‘ë ¬\"]\n        P1[\"A\"] --> P2[\"B\"]\n        P1 --> P3[\"C\"]\n        P2 --> P4[\"D\"]\n        P3 --> P4\n    end\n    \n    subgraph FanOut [\"Fan-out\"]\n        F1[\"A\"] --> F2[\"B1\"]\n        F1 --> F3[\"B2\"]\n        F1 --> F4[\"B3\"]\n    end\n```\n\n### ì½”ë“œì—ì„œ ì˜ì¡´ì„± ì •ì˜\n\n```python\n# ë°©ë²• 1: >> ì—°ì‚°ì\ntask_a >> task_b >> task_c\n\n# ë°©ë²• 2: << ì—°ì‚°ì (ì—­ë°©í–¥)\ntask_c << task_b << task_a\n\n# ë°©ë²• 3: ë¦¬ìŠ¤íŠ¸ë¡œ ë³‘ë ¬\ntask_a >> [task_b, task_c] >> task_d\n\n# ë°©ë²• 4: set_downstream/set_upstream\ntask_a.set_downstream(task_b)\ntask_b.set_upstream(task_a)\n```\n\n### TaskFlowì—ì„œëŠ” ë” ìì—°ìŠ¤ëŸ½ê²Œ\n\n```python\n@dag(...)\ndef pipeline():\n    @task\n    def start(): pass\n    \n    @task\n    def process_a(data): pass\n    \n    @task\n    def process_b(data): pass\n    \n    @task\n    def end(a, b): pass\n    \n    data = start()\n    result_a = process_a(data)\n    result_b = process_b(data)\n    end(result_a, result_b)  # ìë™ìœ¼ë¡œ ì˜ì¡´ì„± ìƒì„±\n```\n\n---\n\n## ì‹¤ì „ ì˜ˆì œ: ë°ì´í„° íŒŒì´í”„ë¼ì¸\n\n```python\nfrom airflow.decorators import dag, task\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    \"owner\": \"data-team\",\n    \"retries\": 3,\n    \"retry_delay\": timedelta(minutes=5),\n}\n\n@dag(\n    dag_id=\"daily_user_analytics\",\n    start_date=datetime(2024, 1, 1),\n    schedule=\"@daily\",\n    catchup=False,\n    default_args=default_args,\n    tags=[\"analytics\", \"production\"]\n)\ndef daily_user_analytics():\n    \"\"\"ì¼ì¼ ì‚¬ìš©ì ë¶„ì„ íŒŒì´í”„ë¼ì¸\"\"\"\n    \n    @task\n    def extract_users(**context):\n        \"\"\"PostgreSQLì—ì„œ ì‚¬ìš©ì ë°ì´í„° ì¶”ì¶œ\"\"\"\n        from airflow.providers.postgres.hooks.postgres import PostgresHook\n        \n        date = context[\"data_interval_start\"].strftime(\"%Y-%m-%d\")\n        hook = PostgresHook(postgres_conn_id=\"production_db\")\n        \n        sql = f\"\"\"\n            SELECT user_id, event_type, created_at\n            FROM user_events\n            WHERE DATE(created_at) = '{date}'\n        \"\"\"\n        \n        df = hook.get_pandas_df(sql)\n        return df.to_dict(\"records\")\n    \n    @task\n    def calculate_metrics(events: list):\n        \"\"\"ì‚¬ìš©ì ë©”íŠ¸ë¦­ ê³„ì‚°\"\"\"\n        from collections import Counter\n        \n        user_events = Counter(e[\"user_id\"] for e in events)\n        \n        return {\n            \"total_events\": len(events),\n            \"unique_users\": len(user_events),\n            \"events_per_user\": len(events) / len(user_events) if user_events else 0\n        }\n    \n    @task\n    def save_to_warehouse(metrics: dict, **context):\n        \"\"\"ê²°ê³¼ë¥¼ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì— ì €ì¥\"\"\"\n        from airflow.providers.postgres.hooks.postgres import PostgresHook\n        \n        date = context[\"data_interval_start\"].strftime(\"%Y-%m-%d\")\n        hook = PostgresHook(postgres_conn_id=\"analytics_db\")\n        \n        hook.run(f\"\"\"\n            INSERT INTO daily_metrics (date, total_events, unique_users, events_per_user)\n            VALUES ('{date}', {metrics['total_events']}, {metrics['unique_users']}, {metrics['events_per_user']})\n            ON CONFLICT (date) DO UPDATE SET\n                total_events = EXCLUDED.total_events,\n                unique_users = EXCLUDED.unique_users,\n                events_per_user = EXCLUDED.events_per_user\n        \"\"\")\n    \n    @task\n    def notify_slack(metrics: dict):\n        \"\"\"Slack ì•Œë¦¼ ì „ì†¡\"\"\"\n        from airflow.providers.slack.hooks.slack import SlackHook\n        \n        hook = SlackHook(slack_conn_id=\"slack\")\n        hook.send(\n            channel=\"#data-alerts\",\n            text=f\"ğŸ“Š Daily Metrics: {metrics['unique_users']} users, {metrics['total_events']} events\"\n        )\n    \n    # ì˜ì¡´ì„± ì •ì˜\n    events = extract_users()\n    metrics = calculate_metrics(events)\n    save_to_warehouse(metrics)\n    notify_slack(metrics)\n\ndaily_user_analytics()\n```\n\n---\n\n## ì •ë¦¬\n\n```mermaid\nmindmap\n  root((Airflow<br/>í•µì‹¬ ê°œë…))\n    ì™œ Airflow?\n      ì˜ì¡´ì„± ê´€ë¦¬\n      ì‹¤íŒ¨ ì²˜ë¦¬\n      ë°±í•„\n      ëª¨ë‹ˆí„°ë§\n    ì•„í‚¤í…ì²˜\n      Webserver\n      Scheduler\n      Worker\n      Metadata DB\n    DAG\n      ë°©í–¥ì„± ê·¸ë˜í”„\n      ìˆœí™˜ ì—†ìŒ\n      Taskë“¤ì˜ ëª¨ìŒ\n    Operator/Task\n      Operator: ë¬´ì—‡ì„\n      Task: ì¸ìŠ¤í„´ìŠ¤\n      ì˜ì¡´ì„± ì •ì˜\n    TaskFlow API\n      @task ë°ì½”ë ˆì´í„°\n      ìë™ XCom\n      ê¹”ë”í•œ ì½”ë“œ\n    ìŠ¤ì¼€ì¤„ë§\n      Data Interval\n      Catchup\n      Backfill\n```\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**7í¸: Airflow ì‹¤ì „**ì—ì„œëŠ” í”„ë¡œë•ì…˜ ìš´ì˜ì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- DAG ëª¨ë“ˆí™” ì „ëµ\n- ë™ì  Task ìƒì„±\n- í…ŒìŠ¤íŠ¸ ë°©ë²•\n- ì—ëŸ¬ ì²˜ë¦¬ì™€ ì•Œë¦¼\n- ëª¨ë‹ˆí„°ë§\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Apache Airflow Documentation](https://airflow.apache.org/docs/)\n- [TaskFlow API Tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html)\n- Astronomer, \"Airflow Best Practices\"\n- \"Data Pipelines with Apache Airflow\" (Manning)",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Airflow",
      "Data Engineering",
      "Data Pipeline",
      "Orchestration"
    ],
    "readingTime": 8,
    "wordCount": 1413,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-05-pyspark-patterns",
    "slug": "de-05-pyspark-patterns",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-05-pyspark-patterns",
    "title": "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #5: PySpark ì‹¤ì „ - ë°ì´í„° ì²˜ë¦¬ íŒ¨í„´ê³¼ ìµœì í™”",
    "excerpt": "ì‹¤ë¬´ì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” PySpark íŒ¨í„´ì„ ë°°ì›ë‹ˆë‹¤. DataFrame ì—°ì‚°, UDF ìµœì í™”, ì¡°ì¸ ì „ëµ, ìºì‹±, ê·¸ë¦¬ê³  í”¼í•´ì•¼ í•  ì•ˆí‹°íŒ¨í„´ê¹Œì§€.",
    "content": "# ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #5: PySpark ì‹¤ì „ - ë°ì´í„° ì²˜ë¦¬ íŒ¨í„´ê³¼ ìµœì í™”\n\n> **ëŒ€ìƒ ë…ì**: ì¶©ë¶„í•œ ê²½í—˜ì„ ê°€ì§„ ë°±ì—”ë“œ/í’€ìŠ¤íƒ ì—”ì§€ë‹ˆì–´ë¡œ, Spark ê°œë…ì„ ìµíˆê³  ì‹¤ì „ ì½”ë“œë¥¼ ì‘ì„±í•˜ë ¤ëŠ” ë¶„\n\n## ì´ í¸ì—ì„œ ë‹¤ë£¨ëŠ” ê²ƒ\n\n4í¸ì—ì„œ Spark ë‚´ë¶€ë¥¼ ì´í•´í–ˆë‹¤ë©´, ì´ì œ **ì‹¤ì œë¡œ ì½”ë“œë¥¼ ì–´ë–»ê²Œ ì‘ì„±í•´ì•¼ í•˜ëŠ”ì§€** íŒ¨í„´ê³¼ ìµœì í™” ê¸°ë²•ì„ ë°°ì›ë‹ˆë‹¤.\n\n---\n\n## ìì£¼ ì‚¬ìš©í•˜ëŠ” DataFrame ì—°ì‚°\n\n### ê¸°ë³¸ ì—°ì‚° ë§µ\n\n```mermaid\nflowchart TB\n    subgraph Selection [\"ì„ íƒ/í•„í„°\"]\n        S1[\"select()\"]\n        S2[\"filter() / where()\"]\n        S3[\"drop()\"]\n    end\n    \n    subgraph Transform [\"ë³€í™˜\"]\n        T1[\"withColumn()\"]\n        T2[\"withColumnRenamed()\"]\n        T3[\"cast()\"]\n    end\n    \n    subgraph Aggregate [\"ì§‘ê³„\"]\n        A1[\"groupBy()\"]\n        A2[\"agg()\"]\n        A3[\"pivot()\"]\n    end\n    \n    subgraph Join [\"ì¡°ì¸\"]\n        J1[\"join()\"]\n        J2[\"crossJoin()\"]\n        J3[\"union()\"]\n    end\n    \n    subgraph Window [\"ìœˆë„ìš°\"]\n        W1[\"over()\"]\n        W2[\"partitionBy()\"]\n        W3[\"orderBy()\"]\n    end\n    \n    Selection --> Transform --> Aggregate --> Output[\"ê²°ê³¼\"]\n    Join --> Aggregate\n    Window --> Aggregate\n```\n\n### ì„ íƒê³¼ í•„í„°ë§\n\n```python\nfrom pyspark.sql import functions as F\n\n# ì»¬ëŸ¼ ì„ íƒ - í•„ìš”í•œ ê²ƒë§Œ!\ndf.select(\"user_id\", \"name\", \"email\")\n\n# ì—¬ëŸ¬ ë°©ì‹ì˜ ì»¬ëŸ¼ ì°¸ì¡°\ndf.select(\n    F.col(\"user_id\"),\n    df.name,\n    df[\"email\"]\n)\n\n# í•„í„°ë§\ndf.filter(F.col(\"age\") > 20)\ndf.filter((F.col(\"age\") > 20) & (F.col(\"city\") == \"Seoul\"))\n\n# SQL í‘œí˜„ì‹ë„ ê°€ëŠ¥\ndf.filter(\"age > 20 AND city = 'Seoul'\")\n```\n\n### ì»¬ëŸ¼ ë³€í™˜\n\n```python\n# ìƒˆ ì»¬ëŸ¼ ì¶”ê°€\ndf.withColumn(\"age_group\", \n    F.when(F.col(\"age\") < 20, \"teen\")\n     .when(F.col(\"age\") < 30, \"20s\")\n     .when(F.col(\"age\") < 40, \"30s\")\n     .otherwise(\"40+\")\n)\n\n# íƒ€ì… ë³€í™˜\ndf.withColumn(\"amount\", F.col(\"amount\").cast(\"double\"))\n\n# ë¬¸ìì—´ ì²˜ë¦¬\ndf.withColumn(\"email_domain\", \n    F.split(F.col(\"email\"), \"@\").getItem(1)\n)\n\n# ë‚ ì§œ ì²˜ë¦¬\ndf.withColumn(\"year\", F.year(\"created_at\"))\ndf.withColumn(\"month\", F.month(\"created_at\"))\ndf.withColumn(\"date_str\", F.date_format(\"created_at\", \"yyyy-MM-dd\"))\n```\n\n### ì§‘ê³„ ì—°ì‚°\n\n```python\n# ê¸°ë³¸ ì§‘ê³„\ndf.groupBy(\"city\").agg(\n    F.count(\"*\").alias(\"user_count\"),\n    F.avg(\"age\").alias(\"avg_age\"),\n    F.sum(\"purchase_amount\").alias(\"total_purchase\"),\n    F.max(\"last_login\").alias(\"last_activity\")\n)\n\n# ì—¬ëŸ¬ ê·¸ë£¹ ê¸°ì¤€\ndf.groupBy(\"city\", \"gender\").count()\n\n# Pivot (í–‰â†’ì—´)\ndf.groupBy(\"year\").pivot(\"quarter\", [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]).sum(\"revenue\")\n```\n\n### ìœˆë„ìš° í•¨ìˆ˜\n\n```mermaid\nflowchart TB\n    subgraph WindowConcept [\"ìœˆë„ìš° í•¨ìˆ˜ ê°œë…\"]\n        Data[\"ë°ì´í„°\"]\n        Partition[\"íŒŒí‹°ì…˜ë³„ ê·¸ë£¹í•‘<br/>(groupByì™€ ìœ ì‚¬)\"]\n        Order[\"ì •ë ¬\"]\n        Frame[\"ìœˆë„ìš° í”„ë ˆì„\"]\n        Calculate[\"ê³„ì‚° (rank, sum, etc.)\"]\n        \n        Data --> Partition --> Order --> Frame --> Calculate\n    end\n    \n    Note[\"âœ… groupByì™€ ë‹¬ë¦¬<br/>ì›ë³¸ í–‰ ìœ ì§€\"]\n```\n\n```python\nfrom pyspark.sql.window import Window\n\n# ìœˆë„ìš° ì •ì˜\nwindow_spec = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n\n# ìˆœìœ„ (íŒŒí‹°ì…˜ ë‚´ ìˆœì„œ)\ndf.withColumn(\"row_num\", F.row_number().over(window_spec))\ndf.withColumn(\"rank\", F.rank().over(window_spec))\n\n# ì´ì „/ë‹¤ìŒ ê°’\ndf.withColumn(\"prev_value\", F.lag(\"value\", 1).over(window_spec))\ndf.withColumn(\"next_value\", F.lead(\"value\", 1).over(window_spec))\n\n# ëˆ„ì  í•©ê³„\ndf.withColumn(\"cumsum\", F.sum(\"amount\").over(window_spec))\n\n# íŒŒí‹°ì…˜ ì „ì²´ ê¸°ì¤€ (ì •ë ¬ ì—†ì´)\nunbounded = Window.partitionBy(\"user_id\")\ndf.withColumn(\"user_total\", F.sum(\"amount\").over(unbounded))\n```\n\n---\n\n## UDF vs Built-in Functions\n\n### ì™œ Built-inì„ ì¨ì•¼ í•˜ëŠ”ê°€?\n\n```mermaid\nflowchart TB\n    subgraph UDF [\"Python UDF\"]\n        U1[\"Python í•¨ìˆ˜ ì •ì˜\"]\n        U2[\"ì§ë ¬í™” (pickle)\"]\n        U3[\"JVM â†’ Python ì „ì†¡\"]\n        U4[\"Pythonì—ì„œ ì‹¤í–‰\"]\n        U5[\"ê²°ê³¼ ì§ë ¬í™”\"]\n        U6[\"Python â†’ JVM\"]\n        \n        U1 --> U2 --> U3 --> U4 --> U5 --> U6\n    end\n    \n    subgraph BuiltIn [\"Built-in Function\"]\n        B1[\"API í˜¸ì¶œ\"]\n        B2[\"JVMì—ì„œ ì§ì ‘ ì‹¤í–‰\"]\n        B3[\"Catalyst ìµœì í™”\"]\n        \n        B1 --> B2 --> B3\n    end\n    \n    UDF -->|\"ğŸ¢ 20~100x ëŠë¦¼\"| Slow[\"ì„±ëŠ¥ ì €í•˜\"]\n    BuiltIn -->|\"ğŸš€ ë¹ ë¦„\"| Fast[\"ìµœì  ì„±ëŠ¥\"]\n```\n\n### ë¹„êµ ì˜ˆì‹œ\n\n```python\n# âŒ ë‚˜ìœ ì˜ˆ: UDF ì‚¬ìš©\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\n@udf(returnType=StringType())\ndef extract_domain(email):\n    if email:\n        return email.split(\"@\")[-1]\n    return None\n\ndf.withColumn(\"domain\", extract_domain(F.col(\"email\")))\n\n# âœ… ì¢‹ì€ ì˜ˆ: Built-in í•¨ìˆ˜ ì‚¬ìš©\ndf.withColumn(\"domain\", \n    F.split(F.col(\"email\"), \"@\").getItem(1)\n)\n```\n\n### ì •ë§ UDFê°€ í•„ìš”í•œ ê²½ìš°: Pandas UDF\n\n```mermaid\nflowchart LR\n    subgraph Types [\"UDF ìœ í˜•ë³„ ì„±ëŠ¥\"]\n        T1[\"Python UDF<br/>ğŸ¢ ëŠë¦¼\"]\n        T2[\"Pandas UDF<br/>âš¡ ë¹ ë¦„\"]\n        T3[\"Built-in<br/>ğŸš€ ê°€ì¥ ë¹ ë¦„\"]\n    end\n    \n    T1 -->|\"ë²¡í„°í™”\"| T2 -->|\"ê°€ëŠ¥í•˜ë©´\"| T3\n```\n\n```python\nimport pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n\n# Pandas UDF - Series â†’ Series (ë²¡í„°í™”)\n@pandas_udf(\"double\")\ndef calculate_zscore(values: pd.Series) -> pd.Series:\n    return (values - values.mean()) / values.std()\n\ndf.withColumn(\"zscore\", calculate_zscore(F.col(\"value\")))\n\n# Pandas UDF - GroupBy Aggregate\n@pandas_udf(\"double\")\ndef median_value(v: pd.Series) -> float:\n    return v.median()\n\ndf.groupBy(\"category\").agg(median_value(F.col(\"price\")))\n```\n\n---\n\n## ì¡°ì¸ ìµœì í™”\n\n### ì¡°ì¸ ì¢…ë¥˜ì™€ ì„ íƒ\n\n```mermaid\nflowchart TB\n    subgraph JoinTypes [\"ì¡°ì¸ ì¢…ë¥˜\"]\n        direction TB\n        Broadcast[\"Broadcast Join<br/>ì‘ì€ í…Œì´ë¸”ì„ ì „ì²´ ë³µì‚¬\"]\n        SortMerge[\"Sort-Merge Join<br/>ì •ë ¬ í›„ ë³‘í•©\"]\n        Shuffle[\"Shuffle Hash Join<br/>í•´ì‹œ ê¸°ë°˜ ì¬ë°°ì¹˜\"]\n    end\n    \n    Decision{\"ì‘ì€ í…Œì´ë¸”ì´<br/>10MB ì´í•˜?\"}\n    \n    Decision -->|\"ì˜ˆ\"| Broadcast\n    Decision -->|\"ì•„ë‹ˆì˜¤\"| SortMerge\n```\n\n### Broadcast Join (í•„ìˆ˜!)\n\n```mermaid\nflowchart TB\n    subgraph NoBroadcast [\"ì¼ë°˜ ì¡°ì¸\"]\n        L1[\"Large Table<br/>100GB\"]\n        S1[\"Small Table<br/>10MB\"]\n        Shuffle1[\"Shuffle ğŸ”€\"]\n        \n        L1 --> Shuffle1\n        S1 --> Shuffle1\n    end\n    \n    subgraph WithBroadcast [\"Broadcast ì¡°ì¸\"]\n        L2[\"Large Table<br/>100GB\"]\n        S2[\"Small Table<br/>10MB\"]\n        \n        S2 -->|\"ê° Executorë¡œ ë³µì‚¬\"| E1[\"Executor 1\"]\n        S2 --> E2[\"Executor 2\"]\n        S2 --> E3[\"Executor 3\"]\n        \n        L2 -->|\"Shuffle ì—†ìŒ\"| E1 & E2 & E3\n    end\n    \n    NoBroadcast -->|\"âŒ ëŠë¦¼\"| Slow\n    WithBroadcast -->|\"âœ… ë¹ ë¦„\"| Fast\n```\n\n```python\nfrom pyspark.sql.functions import broadcast\n\n# ì‘ì€ í…Œì´ë¸”ì— broadcast íŒíŠ¸\nresult = large_df.join(\n    broadcast(small_df), \n    \"join_key\"\n)\n\n# ë˜ëŠ” ì„¤ì •ìœ¼ë¡œ ìë™ ì ìš©\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10 * 1024 * 1024)  # 10MB\n```\n\n### ì¡°ì¸ ìˆœì„œ ìµœì í™”\n\n```python\n# âŒ ë‚˜ìœ ì˜ˆ: í•„í„° í›„ì¡°ì¸ì´ ì•„ë‹˜\nresult = df1.join(df2, \"key\").filter(df1.status == \"active\")\n\n# âœ… ì¢‹ì€ ì˜ˆ: ì¡°ì¸ ì „ì— í•„í„°\ndf1_filtered = df1.filter(df1.status == \"active\")\nresult = df1_filtered.join(df2, \"key\")\n```\n\n---\n\n## ìºì‹±ê³¼ ì²´í¬í¬ì¸íŒ…\n\n### ì–¸ì œ ìºì‹œí•˜ëŠ”ê°€?\n\n```mermaid\nflowchart TB\n    Q1{\"ê°™ì€ DataFrameì„<br/>ì—¬ëŸ¬ ë²ˆ ì‚¬ìš©?\"}\n    Q2{\"ê³„ì‚° ë¹„ìš©ì´<br/>ë¹„ì‹¼ê°€?\"}\n    Q3{\"ë©”ëª¨ë¦¬ì—<br/>ë“¤ì–´ê°€ëŠ”ê°€?\"}\n    \n    Q1 -->|\"ì˜ˆ\"| Q2\n    Q1 -->|\"ì•„ë‹ˆì˜¤\"| NoCache[\"ìºì‹œ ë¶ˆí•„ìš”\"]\n    Q2 -->|\"ì˜ˆ\"| Q3\n    Q2 -->|\"ì•„ë‹ˆì˜¤\"| NoCache\n    Q3 -->|\"ì˜ˆ\"| Cache[\"âœ… cache() ì‚¬ìš©\"]\n    Q3 -->|\"ì•„ë‹ˆì˜¤\"| Persist[\"persist(DISK) ì‚¬ìš©\"]\n```\n\n```python\n# ê¸°ë³¸ ìºì‹œ (ë©”ëª¨ë¦¬)\nexpensive_df = df.groupBy(\"category\").agg(...)\nexpensive_df.cache()\n\n# ì²« ë²ˆì§¸ Actionì—ì„œ ìºì‹œë¨\nexpensive_df.count()  \n\n# ì´í›„ ì¬ì‚¬ìš© ì‹œ ìºì‹œì—ì„œ ì½ìŒ\nexpensive_df.filter(...).show()\nexpensive_df.select(...).write.parquet(...)\n\n# ìºì‹œ í•´ì œ\nexpensive_df.unpersist()\n```\n\n### cache() vs persist()\n\n```python\nfrom pyspark import StorageLevel\n\n# cache() = persist(MEMORY_AND_DISK)\ndf.cache()\n\n# ëª…ì‹œì  ìŠ¤í† ë¦¬ì§€ ë ˆë²¨\ndf.persist(StorageLevel.MEMORY_ONLY)\ndf.persist(StorageLevel.MEMORY_AND_DISK)\ndf.persist(StorageLevel.DISK_ONLY)\ndf.persist(StorageLevel.MEMORY_AND_DISK_SER)  # ì§ë ¬í™”í•˜ì—¬ ì €ì¥\n```\n\n### ì²´í¬í¬ì¸íŒ…\n\n```python\n# checkpointëŠ” ê³„ë³´(lineage)ë¥¼ ëŠìŒ\nspark.sparkContext.setCheckpointDir(\"hdfs://path/checkpoints\")\n\n# ë³µì¡í•œ ë³€í™˜ í›„\nresult = complex_transformations(df)\nresult.checkpoint()\n\n# ì´í›„ ì¥ì•  ì‹œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µêµ¬\n```\n\n---\n\n## ì•ˆí‹°íŒ¨í„´ í”¼í•˜ê¸°\n\n### âŒ collect() ë‚¨ìš©\n\n```mermaid\nflowchart LR\n    subgraph Cluster [\"í´ëŸ¬ìŠ¤í„°\"]\n        E1[\"1TB\"]\n        E2[\"1TB\"]\n        E3[\"1TB\"]\n    end\n    \n    subgraph Driver [\"Driver\"]\n        D[\"ë©”ëª¨ë¦¬: 4GB\"]\n    end\n    \n    Cluster -->|\"collect()\"| Driver\n    Driver -->|\"ğŸ’¥ OOM\"| Crash[\"OutOfMemory!\"]\n```\n\n```python\n# âŒ ë‚˜ìœ ì˜ˆ\nall_data = df.collect()  # ì „ì²´ë¥¼ Driverë¡œ!\nfor row in all_data:\n    process(row)\n\n# âœ… ì¢‹ì€ ì˜ˆ: ì§‘ê³„ í›„ collect\nsummary = df.groupBy(\"category\").count().collect()\n\n# âœ… ì¢‹ì€ ì˜ˆ: limit ì‚¬ìš©\nsample = df.limit(1000).collect()\n\n# âœ… ì¢‹ì€ ì˜ˆ: Iterator ì‚¬ìš©\nfor row in df.toLocalIterator():\n    process(row)  # í•œ ë²ˆì— í•˜ë‚˜ì”©\n```\n\n### âŒ ì‘ì€ íŒŒì¼ ë¬¸ì œ\n\n```mermaid\nflowchart TB\n    subgraph Problem [\"ë¬¸ì œ ìƒí™©\"]\n        P1[\"10,000ê°œ íŒŒì¼\"]\n        P2[\"ê° 1MB\"]\n        P3[\"ì´ 10GB\"]\n        \n        P1 --> Overhead[\"âŒ íŒŒì¼ ì˜¤í”ˆ ì˜¤ë²„í—¤ë“œ<br/>âŒ ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ ë¹„ìš©\"]\n    end\n    \n    subgraph Solution [\"í•´ê²°ì±…\"]\n        S1[\"100ê°œ íŒŒì¼\"]\n        S2[\"ê° 100MB\"]\n        S3[\"ì´ 10GB\"]\n        \n        S1 --> Efficient[\"âœ… I/O íš¨ìœ¨ì \"]\n    end\n```\n\n```python\n# âŒ ë‚˜ìœ ì˜ˆ: íŒŒí‹°ì…˜ë§ˆë‹¤ íŒŒì¼ ìƒì„±\ndf.write.parquet(\"output/\")  # íŒŒí‹°ì…˜ ìˆ˜ë§Œí¼ íŒŒì¼\n\n# âœ… ì¢‹ì€ ì˜ˆ: coalesceë¡œ íŒŒì¼ ìˆ˜ ì¡°ì ˆ\ndf.coalesce(10).write.parquet(\"output/\")  # 10ê°œ íŒŒì¼\n\n# âœ… ì¢‹ì€ ì˜ˆ: ì ì • í¬ê¸°ë¡œ ë¶„í• \ndf.repartition(100).write.parquet(\"output/\")  # 100ê°œ íŒŒì¼\n```\n\n### âŒ ë¶ˆí•„ìš”í•œ Shuffle\n\n```python\n# âŒ ë‚˜ìœ ì˜ˆ: groupBy ë‘ ë²ˆ\nresult = df.groupBy(\"a\").count() \\\n           .groupBy(\"a\").agg(F.sum(\"count\"))\n\n# âœ… ì¢‹ì€ ì˜ˆ: í•œ ë²ˆì— ì²˜ë¦¬\nresult = df.groupBy(\"a\").agg(F.count(\"*\").alias(\"count\"))\n```\n\n---\n\n## ì‹¤ì „ ì˜ˆì œ: ë¡œê·¸ ë¶„ì„ íŒŒì´í”„ë¼ì¸\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nspark = SparkSession.builder \\\n    .appName(\"LogAnalysis\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .getOrCreate()\n\n# 1. ë°ì´í„° ë¡œë“œ (í•„ìš”í•œ ì»¬ëŸ¼ë§Œ)\nlogs = spark.read.json(\"logs/*.json\").select(\n    \"timestamp\", \"user_id\", \"event_type\", \"page\", \"duration\"\n)\n\n# 2. ë°ì´í„° ì •ì œ\ncleaned = logs \\\n    .filter(F.col(\"user_id\").isNotNull()) \\\n    .withColumn(\"event_date\", F.to_date(\"timestamp\")) \\\n    .withColumn(\"event_hour\", F.hour(\"timestamp\"))\n\n# 3. ì—¬ëŸ¬ ë²ˆ ì‚¬ìš©í•  ê²ƒì´ë¯€ë¡œ ìºì‹œ\ncleaned.cache()\n\n# 4. ì¼ë³„ ì§‘ê³„\ndaily_stats = cleaned.groupBy(\"event_date\").agg(\n    F.countDistinct(\"user_id\").alias(\"dau\"),\n    F.count(\"*\").alias(\"total_events\"),\n    F.avg(\"duration\").alias(\"avg_duration\")\n)\n\n# 5. ì‹œê°„ëŒ€ë³„ íŒ¨í„´\nhourly_pattern = cleaned.groupBy(\"event_hour\").agg(\n    F.count(\"*\").alias(\"events\")\n).orderBy(\"event_hour\")\n\n# 6. ìœ ì €ë³„ ì„¸ì…˜ ë¶„ì„ (ìœˆë„ìš° í•¨ìˆ˜)\nuser_window = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n\nsessions = cleaned \\\n    .withColumn(\"prev_timestamp\", F.lag(\"timestamp\").over(user_window)) \\\n    .withColumn(\"time_gap\", \n        F.unix_timestamp(\"timestamp\") - F.unix_timestamp(\"prev_timestamp\")) \\\n    .withColumn(\"new_session\", \n        F.when(F.col(\"time_gap\") > 1800, 1).otherwise(0)) \\\n    .withColumn(\"session_id\", \n        F.sum(\"new_session\").over(user_window))\n\n# 7. ì €ì¥\ndaily_stats.write.mode(\"overwrite\").parquet(\"output/daily_stats\")\nhourly_pattern.write.mode(\"overwrite\").parquet(\"output/hourly_pattern\")\n\n# 8. ìºì‹œ í•´ì œ\ncleaned.unpersist()\n```\n\n---\n\n## ì •ë¦¬\n\n```mermaid\nmindmap\n  root((PySpark<br/>ì‹¤ì „))\n    DataFrame ì—°ì‚°\n      select, filter\n      withColumn\n      groupBy, agg\n      Window í•¨ìˆ˜\n    UDF ìµœì í™”\n      Built-in ìš°ì„ \n      Pandas UDF\n      Python UDF í”¼í•˜ê¸°\n    ì¡°ì¸\n      Broadcast Join\n      ì¡°ì¸ ì „ í•„í„°\n      ì‘ì€ í…Œì´ë¸” íŒíŠ¸\n    ìºì‹±\n      ì—¬ëŸ¬ ë²ˆ ì‚¬ìš© ì‹œ\n      cache vs persist\n      unpersist ìŠì§€ ì•Šê¸°\n    ì•ˆí‹°íŒ¨í„´\n      collect ë‚¨ìš©\n      ì‘ì€ íŒŒì¼ ë¬¸ì œ\n      ë¶ˆí•„ìš”í•œ Shuffle\n```\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**6í¸: Airflow í•µì‹¬ ê°œë…**ì—ì„œëŠ” ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- ì™œ cronìœ¼ë¡œëŠ” ë¶€ì¡±í•œê°€?\n- DAG, Operator, Task ì´í•´\n- TaskFlow API (Airflow 2.0+)\n- ìŠ¤ì¼€ì¤„ë§ê³¼ Backfill\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)\n- [Spark SQL Built-in Functions](https://spark.apache.org/docs/latest/api/sql/)\n- Databricks, \"PySpark Best Practices\"\n- [Window Functions Guide](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-window.html)",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Data Engineering",
      "Performance",
      "Spark",
      "UDF"
    ],
    "readingTime": 7,
    "wordCount": 1232,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-04-spark-internals",
    "slug": "de-04-spark-internals",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-04-spark-internals",
    "title": "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #4: Spark ë‚´ë¶€ ë™ì‘ ì›ë¦¬ - Job, Stage, Task",
    "excerpt": "Sparkì˜ ì‹¤í–‰ ëª¨ë¸ì„ ì´í•´í•©ë‹ˆë‹¤. Job, Stage, Task ê³„ì¸µ, Shuffleì˜ ë¹„ìš©, íŒŒí‹°ì…”ë‹ ì „ëµ, ê·¸ë¦¬ê³  Spark UIë¥¼ ì½ëŠ” ë²•ê¹Œì§€.",
    "content": "# ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #4: Spark ë‚´ë¶€ ë™ì‘ ì›ë¦¬ - Job, Stage, Task\n\n> **ëŒ€ìƒ ë…ì**: ì¶©ë¶„í•œ ê²½í—˜ì„ ê°€ì§„ ë°±ì—”ë“œ/í’€ìŠ¤íƒ ì—”ì§€ë‹ˆì–´ë¡œ, Sparkì˜ ê¸°ë³¸ ê°œë…ì„ ì•Œê³  ì„±ëŠ¥ íŠœë‹ì— ê´€ì‹¬ ìˆëŠ” ë¶„\n\n## ì´ í¸ì—ì„œ ë‹¤ë£¨ëŠ” ê²ƒ\n\nSpark ì½”ë“œë¥¼ ì‘ì„±í•  ë•Œ **ì™œ ì–´ë–¤ ì½”ë“œëŠ” ëŠë¦¬ê³  ì–´ë–¤ ì½”ë“œëŠ” ë¹ ë¥¸ì§€** ì´í•´í•˜ë ¤ë©´, ë‚´ë¶€ ì‹¤í–‰ ëª¨ë¸ì„ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤.\n\n---\n\n## ì‹¤í–‰ ê³„ì¸µ êµ¬ì¡°: Application â†’ Job â†’ Stage â†’ Task\n\n### ì „ì²´ êµ¬ì¡°\n\n```mermaid\nflowchart TB\n    subgraph App [\"Application (ì•± ì „ì²´)\"]\n        subgraph Job1 [\"Job 1 (Action 1)\"]\n            subgraph Stage1 [\"Stage 1\"]\n                T1[\"Task 1\"]\n                T2[\"Task 2\"]\n                T3[\"Task 3\"]\n            end\n            subgraph Stage2 [\"Stage 2\"]\n                T4[\"Task 4\"]\n                T5[\"Task 5\"]\n            end\n        end\n        \n        subgraph Job2 [\"Job 2 (Action 2)\"]\n            subgraph Stage3 [\"Stage 3\"]\n                T6[\"Task 6\"]\n                T7[\"Task 7\"]\n            end\n        end\n    end\n    \n    Stage1 -->|\"Shuffle\"| Stage2\n```\n\n### ê° ê³„ì¸µì˜ ì—­í• \n\n| ê³„ì¸µ | ë¬´ì—‡ | ì–¸ì œ ìƒì„± | ë³‘ë ¬ì„± |\n|------|------|---------|--------|\n| **Application** | ì „ì²´ Spark í”„ë¡œê·¸ë¨ | spark-submit ì‹œ | 1ê°œ |\n| **Job** | í•˜ë‚˜ì˜ Action ì‹¤í–‰ ë‹¨ìœ„ | count(), save() í˜¸ì¶œ | ìˆœì°¨ |\n| **Stage** | Shuffle ê¸°ì¤€ ë¶„ë¦¬ | ìë™ ë¶„ë¦¬ | ìˆœì°¨ |\n| **Task** | íŒŒí‹°ì…˜ë‹¹ ì‹¤í–‰ ë‹¨ìœ„ | íŒŒí‹°ì…˜ ìˆ˜ë§Œí¼ | **ë³‘ë ¬** |\n\n### ì½”ë“œ ì˜ˆì‹œì™€ ì‹¤í–‰ íë¦„\n\n```python\n# ì´ ì½”ë“œê°€ ì–´ë–»ê²Œ ì‹¤í–‰ë ê¹Œ?\ndf = spark.read.parquet(\"data/\")  # Transformation\nfiltered = df.filter(df.age > 20)  # Transformation\ngrouped = filtered.groupBy(\"city\").count()  # Transformation\ngrouped.show()  # Action â†’ Job ìƒì„±!\n```\n\n```mermaid\nflowchart LR\n    subgraph Job [\"Job (show í˜¸ì¶œ)\"]\n        subgraph Stage1 [\"Stage 1: Wide ì „ê¹Œì§€\"]\n            Read[\"read.parquet\"]\n            Filter[\"filter(age > 20)\"]\n            Read --> Filter\n        end\n        \n        Shuffle[\"âš¡ Shuffle<br/>(city ê¸°ì¤€ ì¬ë°°ì¹˜)\"]\n        \n        subgraph Stage2 [\"Stage 2: ì§‘ê³„\"]\n            GroupBy[\"groupBy + count\"]\n            Show[\"show()\"]\n            GroupBy --> Show\n        end\n        \n        Stage1 --> Shuffle --> Stage2\n    end\n```\n\n---\n\n## Narrow vs Wide Transformations\n\n### ì´ê²ƒì´ ì„±ëŠ¥ì˜ í•µì‹¬\n\n```mermaid\nflowchart TB\n    subgraph Narrow [\"Narrow Transformations\"]\n        direction TB\n        N1[\"map\"]\n        N2[\"filter\"]\n        N3[\"flatMap\"]\n        N4[\"select\"]\n        \n        NP1[\"Partition 1\"] --> NP1R[\"ê²°ê³¼ 1\"]\n        NP2[\"Partition 2\"] --> NP2R[\"ê²°ê³¼ 2\"]\n        NP3[\"Partition 3\"] --> NP3R[\"ê²°ê³¼ 3\"]\n        \n        Desc1[\"âœ… íŒŒí‹°ì…˜ ë…ë¦½ ì²˜ë¦¬<br/>âœ… ë„¤íŠ¸ì›Œí¬ í†µì‹  ì—†ìŒ<br/>âœ… ë§¤ìš° ë¹ ë¦„\"]\n    end\n    \n    subgraph Wide [\"Wide Transformations\"]\n        direction TB\n        W1[\"groupBy\"]\n        W2[\"join\"]\n        W3[\"orderBy\"]\n        W4[\"repartition\"]\n        \n        WP1[\"Partition 1\"] --> WS[\"Shuffle<br/>ğŸ”€\"] --> WR1[\"ê²°ê³¼ 1\"]\n        WP2[\"Partition 2\"] --> WS --> WR2[\"ê²°ê³¼ 2\"]\n        WP3[\"Partition 3\"] --> WS --> WR3[\"ê²°ê³¼ 3\"]\n        \n        Desc2[\"âš ï¸ ë°ì´í„° ì¬ë°°ì¹˜<br/>âš ï¸ ë„¤íŠ¸ì›Œí¬ I/O ë°œìƒ<br/>âš ï¸ ëŠë¦¼\"]\n    end\n```\n\n### Shuffleì´ ë¹„ì‹¼ ì´ìœ \n\n```mermaid\nflowchart TB\n    subgraph Before [\"Shuffle ì „\"]\n        P1[\"Executor 1<br/>í‚¤: A, B, C\"]\n        P2[\"Executor 2<br/>í‚¤: A, D, E\"]\n        P3[\"Executor 3<br/>í‚¤: B, C, F\"]\n    end\n    \n    subgraph Network [\"ë„¤íŠ¸ì›Œí¬ ì „ì†¡\"]\n        direction TB\n        N1[\"A ë°ì´í„° â†’ Reducer 1ë¡œ\"]\n        N2[\"B ë°ì´í„° â†’ Reducer 2ë¡œ\"]\n        N3[\"C ë°ì´í„° â†’ Reducer 3ìœ¼ë¡œ\"]\n        N4[\"...\"]\n    end\n    \n    subgraph After [\"Shuffle í›„\"]\n        R1[\"Reducer 1<br/>í‚¤ Aë§Œ\"]\n        R2[\"Reducer 2<br/>í‚¤ Bë§Œ\"]\n        R3[\"Reducer 3<br/>í‚¤ Cë§Œ\"]\n    end\n    \n    Before --> Network --> After\n    \n    Cost[\"ğŸ’¸ ë¹„ìš© ë°œìƒ<br/>â€¢ ë””ìŠ¤í¬ ì“°ê¸°<br/>â€¢ ë„¤íŠ¸ì›Œí¬ ì „ì†¡<br/>â€¢ ë””ìŠ¤í¬ ì½ê¸°<br/>â€¢ ì •ë ¬\"]\n```\n\n**Shuffleì´ ë°œìƒí•˜ë©´**:\n\n1. ê° Executorê°€ ê²°ê³¼ë¥¼ **ë””ìŠ¤í¬ì— ì €ì¥**\n2. í‚¤ ê¸°ì¤€ìœ¼ë¡œ **ë„¤íŠ¸ì›Œí¬ë¡œ ì „ì†¡**\n3. ë°›ëŠ” ìª½ì—ì„œ **ë””ìŠ¤í¬ì— ì €ì¥**\n4. í‚¤ ê¸°ì¤€ **ì •ë ¬**\n5. ë©”ëª¨ë¦¬ë¡œ **ì½ì–´ì„œ ì²˜ë¦¬**\n\n---\n\n## íŒŒí‹°ì…”ë‹ ì „ëµ\n\n### íŒŒí‹°ì…˜ì´ë€?\n\n```mermaid\nflowchart TB\n    subgraph Data [\"ì›ë³¸ ë°ì´í„°\"]\n        BigData[\"1TB ë°ì´í„°\"]\n    end\n    \n    subgraph Partitions [\"íŒŒí‹°ì…˜ ë¶„í• \"]\n        P1[\"Partition 1<br/>100GB\"]\n        P2[\"Partition 2<br/>100GB\"]\n        P3[\"Partition 3<br/>100GB\"]\n        PN[\"...<br/>100GB\"]\n    end\n    \n    subgraph Tasks [\"ë³‘ë ¬ ì²˜ë¦¬\"]\n        T1[\"Task 1<br/>â†’ Core 1\"]\n        T2[\"Task 2<br/>â†’ Core 2\"]\n        T3[\"Task 3<br/>â†’ Core 3\"]\n        TN[\"Task N<br/>â†’ Core N\"]\n    end\n    \n    BigData --> Partitions --> Tasks\n```\n\n### íŒŒí‹°ì…˜ ìˆ˜ì™€ ë³‘ë ¬ì„±\n\n```mermaid\nflowchart TB\n    subgraph TooFew [\"íŒŒí‹°ì…˜ì´ ë„ˆë¬´ ì ìŒ\"]\n        F1[\"4 íŒŒí‹°ì…˜\"]\n        F2[\"100 ì½”ì–´ í´ëŸ¬ìŠ¤í„°\"]\n        F3[\"âŒ 96 ì½”ì–´ ë†€ê³  ìˆìŒ\"]\n    end\n    \n    subgraph TooMany [\"íŒŒí‹°ì…˜ì´ ë„ˆë¬´ ë§ìŒ\"]\n        M1[\"10000 íŒŒí‹°ì…˜\"]\n        M2[\"100 ì½”ì–´ í´ëŸ¬ìŠ¤í„°\"]\n        M3[\"âŒ ìŠ¤ì¼€ì¤„ë§ ì˜¤ë²„í—¤ë“œ\"]\n    end\n    \n    subgraph JustRight [\"ì ì ˆí•œ íŒŒí‹°ì…˜\"]\n        R1[\"200~400 íŒŒí‹°ì…˜\"]\n        R2[\"100 ì½”ì–´ í´ëŸ¬ìŠ¤í„°\"]\n        R3[\"âœ… ì½”ì–´ë‹¹ 2~4 Task\"]\n    end\n```\n\n**ê²½í—˜ì¹™**:\n\n- íŒŒí‹°ì…˜ ìˆ˜ = ì½”ì–´ ìˆ˜ Ã— 2~4\n- íŒŒí‹°ì…˜ë‹¹ í¬ê¸° = 100MB ~ 1GB\n\n### ë°ì´í„° ìŠ¤í(Skew) ë¬¸ì œ\n\n```mermaid\nflowchart TB\n    subgraph Skewed [\"ìŠ¤í ë°œìƒ\"]\n        S1[\"Partition 1<br/>10MB\"]\n        S2[\"Partition 2<br/>10MB\"]\n        S3[\"Partition 3<br/>10GB !!\"]\n        \n        ST1[\"Task 1<br/>1ì´ˆ\"]\n        ST2[\"Task 2<br/>1ì´ˆ\"]\n        ST3[\"Task 3<br/>100ì´ˆ ğŸ˜±\"]\n        \n        S1 --> ST1\n        S2 --> ST2\n        S3 --> ST3\n    end\n    \n    Result[\"ì „ì²´ ì‹œê°„ = 100ì´ˆ<br/>(ê°€ì¥ ëŠë¦° Task ê¸°ì¤€)\"]\n    \n    Skewed --> Result\n```\n\n**í•´ê²°ì±…**:\n\n1. **Salting**: í•« í‚¤ì— ëœë¤ ì ‘ë‘ì‚¬ ì¶”ê°€\n2. **Broadcast Join**: ì‘ì€ í…Œì´ë¸”ì€ ì „ì²´ ë³µì‚¬\n3. **Adaptive Query Execution (AQE)**: Spark 3.0+ ìë™ ìµœì í™”\n\n---\n\n## ë©”ëª¨ë¦¬ ê´€ë¦¬\n\n### Executor ë©”ëª¨ë¦¬ êµ¬ì¡°\n\n```mermaid\nflowchart TB\n    subgraph Executor [\"Executor ë©”ëª¨ë¦¬\"]\n        subgraph Reserved [\"Reserved (300MB)\"]\n            R[\"ì‹œìŠ¤í…œìš©\"]\n        end\n        \n        subgraph Unified [\"Unified Memory (60%)\"]\n            Storage[\"Storage<br/>(ìºì‹œ)\"]\n            Execution[\"Execution<br/>(Shuffle, ì •ë ¬)\"]\n            Storage <-->|\"ë™ì  ê³µìœ \"| Execution\n        end\n        \n        subgraph User [\"User Memory (40%)\"]\n            UDF[\"UDF ê°ì²´\"]\n            Meta[\"ë©”íƒ€ë°ì´í„°\"]\n        end\n    end\n```\n\n### ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ: Spill to Disk\n\n```mermaid\nflowchart LR\n    subgraph Normal [\"ì •ìƒ ìƒíƒœ\"]\n        Mem1[\"ë©”ëª¨ë¦¬ ì‚¬ìš©<br/>3GB\"]\n        Limit1[\"í• ë‹¹ëŸ‰<br/>4GB\"]\n    end\n    \n    subgraph Spill [\"Spill ë°œìƒ\"]\n        Mem2[\"ë©”ëª¨ë¦¬ ì‚¬ìš©<br/>4GB+\"]\n        Disk[\"ë””ìŠ¤í¬ë¡œ<br/>ë‚´ë³´ë‚´ê¸° ğŸ’¾\"]\n        Slow[\"ğŸ¢ ëŠë ¤ì§\"]\n        \n        Mem2 --> Disk --> Slow\n    end\n    \n    Normal -->|\"ë°ì´í„° ì¦ê°€\"| Spill\n```\n\n**Spill ê°ì§€ ë°©ë²•**: Spark UIì—ì„œ \"Spill (Memory)\" / \"Spill (Disk)\" í™•ì¸\n\n---\n\n## Spark UI ì½ëŠ” ë²•\n\n### í•µì‹¬ ì§€í‘œë“¤\n\n```mermaid\nflowchart TB\n    subgraph SparkUI [\"Spark UI\"]\n        subgraph JobsTab [\"Jobs íƒ­\"]\n            J1[\"Job ì„±ê³µ/ì‹¤íŒ¨\"]\n            J2[\"ì „ì²´ ì†Œìš” ì‹œê°„\"]\n        end\n        \n        subgraph StagesTab [\"Stages íƒ­ â­\"]\n            S1[\"Stageë³„ ì‹œê°„\"]\n            S2[\"Shuffle Read/Write\"]\n            S3[\"Task ë¶„í¬\"]\n        end\n        \n        subgraph SQLTab [\"SQL íƒ­\"]\n            SQL1[\"ì‹¤í–‰ ê³„íš\"]\n            SQL2[\"ë¬¼ë¦¬ ê³„íš\"]\n        end\n    end\n```\n\n### Stages íƒ­ í•´ì„\n\n```mermaid\nflowchart TB\n    subgraph StageMetrics [\"Stage ì§€í‘œ\"]\n        subgraph Good [\"âœ… ì •ìƒ\"]\n            G1[\"Task Duration ê· ì¼\"]\n            G2[\"Shuffle Write ì ìŒ\"]\n            G3[\"Spill ì—†ìŒ\"]\n        end\n        \n        subgraph Bad [\"âš ï¸ ë¬¸ì œ\"]\n            B1[\"Task Duration í¸ì°¨ í¼<br/>â†’ ë°ì´í„° ìŠ¤í\"]\n            B2[\"Shuffle í¬ê¸° ê±°ëŒ€<br/>â†’ ì¡°ì¸/ê·¸ë£¹ ìµœì í™” í•„ìš”\"]\n            B3[\"Spill ë°œìƒ<br/>â†’ ë©”ëª¨ë¦¬ ë¶€ì¡±\"]\n        end\n    end\n```\n\n### ì‹¤ì „ ë””ë²„ê¹… í”Œë¡œìš°\n\n```mermaid\nflowchart TB\n    Start[\"Jobì´ ëŠë¦¼\"]\n    \n    Q1{\"Shuffleì´<br/>í°ê°€?\"}\n    Q2{\"Task Duration<br/>í¸ì°¨ê°€ í°ê°€?\"}\n    Q3{\"Spillì´<br/>ë°œìƒí•˜ëŠ”ê°€?\"}\n    Q4{\"GC ì‹œê°„ì´<br/>ê¸´ê°€?\"}\n    \n    A1[\"ì¡°ì¸/ê·¸ë£¹ ìµœì í™”<br/>Broadcast Join ê³ ë ¤\"]\n    A2[\"ë°ì´í„° ìŠ¤í í•´ê²°<br/>Salting, AQE\"]\n    A3[\"ë©”ëª¨ë¦¬ ì¦ê°€<br/>íŒŒí‹°ì…˜ ìˆ˜ ì¡°ì •\"]\n    A4[\"Executor ë©”ëª¨ë¦¬ ì¦ê°€<br/>GC íŠœë‹\"]\n    \n    Start --> Q1\n    Q1 -->|\"ì˜ˆ\"| A1\n    Q1 -->|\"ì•„ë‹ˆì˜¤\"| Q2\n    Q2 -->|\"ì˜ˆ\"| A2\n    Q2 -->|\"ì•„ë‹ˆì˜¤\"| Q3\n    Q3 -->|\"ì˜ˆ\"| A3\n    Q3 -->|\"ì•„ë‹ˆì˜¤\"| Q4\n    Q4 -->|\"ì˜ˆ\"| A4\n    Q4 -->|\"ì•„ë‹ˆì˜¤\"| Other[\"ë‹¤ë¥¸ ì›ì¸ ì¡°ì‚¬\"]\n```\n\n---\n\n## ì‹¤ì „ ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸\n\n### ì½”ë“œ ë ˆë²¨\n\n| í•­ëª© | ì¢‹ì€ ì˜ˆ | ë‚˜ìœ ì˜ˆ |\n|------|--------|--------|\n| **ì¡°ì¸** | Broadcast Join (ì‘ì€ í…Œì´ë¸”) | ì–‘ìª½ ë‹¤ í° Shuffle Join |\n| **í•„í„°** | ì¡°ì¸ ì „ì— filter | ì¡°ì¸ í›„ì— filter |\n| **ì»¬ëŸ¼ ì„ íƒ** | í•„ìš”í•œ ì»¬ëŸ¼ë§Œ select | SELECT * |\n| **UDF** | Built-in í•¨ìˆ˜ ì‚¬ìš© | Python UDF ë‚¨ìš© |\n| **collect** | ì§‘ê³„ í›„ collect | í° ë°ì´í„° collect |\n\n### ì„¤ì • ë ˆë²¨\n\n```python\n# ê¶Œì¥ ì„¤ì •\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")  # AQE\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")  # ê¸°ë³¸ 200\n```\n\n---\n\n## ì •ë¦¬\n\n```mermaid\nmindmap\n  root((Spark<br/>ë‚´ë¶€ ë™ì‘))\n    ê³„ì¸µ êµ¬ì¡°\n      Application\n      Job (Actionë§ˆë‹¤)\n      Stage (Shuffleë§ˆë‹¤)\n      Task (íŒŒí‹°ì…˜ë§ˆë‹¤)\n    Transformation\n      Narrow\n        map, filter\n        ë¹ ë¦„\n      Wide\n        groupBy, join\n        Shuffle ë°œìƒ\n        ëŠë¦¼\n    Shuffle\n      ë„¤íŠ¸ì›Œí¬ ì „ì†¡\n      ë””ìŠ¤í¬ I/O\n      ì„±ëŠ¥ ë³‘ëª©\n    íŒŒí‹°ì…”ë‹\n      ì ì • ìˆ˜: ì½”ì–´ x 2~4\n      ìŠ¤í ì£¼ì˜\n      AQE í™œìš©\n    ë©”ëª¨ë¦¬\n      Storage + Execution\n      Spill ë°œìƒ ì‹œ ëŠë ¤ì§\n    Spark UI\n      Stage íƒ­ í™•ì¸\n      Shuffle í¬ê¸°\n      Task ë¶„í¬\n```\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**5í¸: PySpark ì‹¤ì „**ì—ì„œëŠ” ì‹¤ë¬´ íŒ¨í„´ì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- ìì£¼ ì“°ëŠ” DataFrame ì—°ì‚°\n- UDF vs Built-in Functions\n- ì¡°ì¸ ìµœì í™” ê¸°ë²•\n- ìºì‹±ê³¼ ì²´í¬í¬ì¸íŒ…\n- ì•ˆí‹°íŒ¨í„´ í”¼í•˜ê¸°\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Spark Web UI](https://spark.apache.org/docs/latest/web-ui.html)\n- [Tuning Spark](https://spark.apache.org/docs/latest/tuning.html)\n- [Adaptive Query Execution](https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution)\n- Jacek Laskowski, \"The Internals of Apache Spark\"",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Data Engineering",
      "Partitioning",
      "Performance",
      "Spark"
    ],
    "readingTime": 6,
    "wordCount": 1124,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-03-spark-core-concepts",
    "slug": "de-03-spark-core-concepts",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-03-spark-core-concepts",
    "title": "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #3: Spark í•µì‹¬ ê°œë… - RDDì—ì„œ DataFrameê¹Œì§€",
    "excerpt": "ë¶„ì‚° ì²˜ë¦¬ì˜ í•µì‹¬ ê°œë…ê³¼ Sparkì˜ ì¶”ìƒí™” ê³„ì¸µì„ ì´í•´í•©ë‹ˆë‹¤. Goroutine, ThreadPoolExecutorì™€ ë¹„êµí•˜ë©° Sparkê°€ í•´ê²°í•˜ëŠ” ë¬¸ì œë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.",
    "content": "# ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #3: Spark í•µì‹¬ ê°œë… - RDDì—ì„œ DataFrameê¹Œì§€\n\n> **ëŒ€ìƒ ë…ì**: ì¶©ë¶„í•œ ê²½í—˜ì„ ê°€ì§„ ë°±ì—”ë“œ/í’€ìŠ¤íƒ ì—”ì§€ë‹ˆì–´ë¡œ, Goì˜ Goroutineì´ë‚˜ Pythonì˜ ThreadPoolExecutorì— ìµìˆ™í•˜ì§€ë§Œ SparkëŠ” ì²˜ìŒì¸ ë¶„\n\n## ì´ í¸ì—ì„œ ë‹¤ë£¨ëŠ” ê²ƒ\n\n\"Sparkê°€ ë¹ ë¥´ë‹¤\"ëŠ” ë§ì€ ë§ì´ ë“¤ì–´ë´¤ì„ ê²ë‹ˆë‹¤. í•˜ì§€ë§Œ **ì™œ ë¹ ë¥¸ì§€**, **ê¸°ì¡´ ë³‘ë ¬ ì²˜ë¦¬ì™€ ë¬´ì—‡ì´ ë‹¤ë¥¸ì§€** ì´í•´í•˜ëŠ” ê²ƒì´ ë¨¼ì €ì…ë‹ˆë‹¤.\n\n---\n\n## ì™œ ë¶„ì‚° ì²˜ë¦¬ê°€ í•„ìš”í•œê°€?\n\n### ë‹¨ì¼ ì„œë²„ì˜ í•œê³„\n\në°±ì—”ë“œ ê°œë°œì—ì„œ ì„±ëŠ¥ì„ ë†’ì´ë ¤ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?\n\n```mermaid\nflowchart TB\n    subgraph SingleServer [\"ë‹¨ì¼ ì„œë²„ ìµœì í™”\"]\n        direction TB\n        S1[\"1. ì•Œê³ ë¦¬ì¦˜ ê°œì„ \"]\n        S2[\"2. ì¸ë±ìŠ¤ ì¶”ê°€\"]\n        S3[\"3. ìºì‹±\"]\n        S4[\"4. ë³‘ë ¬ ì²˜ë¦¬<br/>(Goroutine, ThreadPool)\"]\n        S5[\"5. ìˆ˜ì§ í™•ì¥<br/>(ë” ì¢‹ì€ ì„œë²„)\"]\n    end\n    \n    Limit[\"ê·¸ë˜ë„ ì•ˆ ë˜ë©´?\"]\n    \n    SingleServer --> Limit\n    Limit --> Distribute[\"ìˆ˜í‰ í™•ì¥<br/>(ì—¬ëŸ¬ ì„œë²„ë¡œ ë¶„ì‚°)\"]\n```\n\nì–¸ì  ê°€ëŠ” **ë‹¨ì¼ ì„œë²„ë¡œëŠ” ë¶ˆê°€ëŠ¥í•œ ìˆœê°„**ì´ ì˜µë‹ˆë‹¤:\n\n| ìƒí™© | ì˜ˆì‹œ |\n|------|------|\n| **ë°ì´í„°ê°€ ë©”ëª¨ë¦¬ì— ì•ˆ ë“¤ì–´ê°** | 1TB ë°ì´í„°ë¥¼ 32GB ì„œë²„ì—ì„œ ì²˜ë¦¬ |\n| **ì²˜ë¦¬ ì‹œê°„ì´ ë„ˆë¬´ ê¹€** | ë‹¨ì¼ ì½”ì–´ë¡œ 10ì–µ ê±´ ì²˜ë¦¬ì— 10ì‹œê°„ |\n| **ë””ìŠ¤í¬ I/O ë³‘ëª©** | ì´ˆë‹¹ ì½ê¸° í•œê³„ ë„ë‹¬ |\n\n### ë¶„ì‚° ì²˜ë¦¬ì˜ í•µì‹¬ ì•„ì´ë””ì–´\n\n```mermaid\nflowchart LR\n    subgraph Before [\"ë‹¨ì¼ ì„œë²„\"]\n        Server1[\"ì„œë²„ 1ëŒ€<br/>ğŸ–¥ï¸\"]\n        Data1[\"1TB ë°ì´í„°\"]\n        Time1[\"â±ï¸ 10ì‹œê°„\"]\n        Data1 --> Server1 --> Time1\n    end\n    \n    subgraph After [\"ë¶„ì‚° ì²˜ë¦¬\"]\n        Data2[\"1TB ë°ì´í„°\"]\n        \n        subgraph Cluster [\"10ëŒ€ í´ëŸ¬ìŠ¤í„°\"]\n            C1[\"ğŸ–¥ï¸ 100GB\"]\n            C2[\"ğŸ–¥ï¸ 100GB\"]\n            C3[\"ğŸ–¥ï¸ 100GB\"]\n            CN[\"ğŸ–¥ï¸ ...\"]\n        end\n        \n        Time2[\"â±ï¸ 1ì‹œê°„\"]\n        \n        Data2 --> Cluster --> Time2\n    end\n    \n    Before -.->|\"10ë°° ë¹ ë¥´ê²Œ\"| After\n```\n\n**í•µì‹¬**: ë°ì´í„°ì™€ ì—°ì‚°ì„ ì—¬ëŸ¬ ì„œë²„ì— **ë‚˜ëˆ ì„œ** ë™ì‹œì— ì²˜ë¦¬\n\n---\n\n## Goroutine/ThreadPoolExecutorì™€ Sparkì˜ ì°¨ì´\n\n### ê¸°ì¡´ ë³‘ë ¬ ì²˜ë¦¬: ë‹¨ì¼ ì„œë²„ ë‚´\n\nGoì™€ Pythonì—ì„œì˜ ë³‘ë ¬ ì²˜ë¦¬ëŠ” **ë‹¨ì¼ ì„œë²„ì˜ CPU ì½”ì–´ë¥¼ í™œìš©**í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph Go [\"Go: Goroutines\"]\n        GoRuntime[\"Go Runtime<br/>(ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤)\"]\n        G1[\"goroutine\"]\n        G2[\"goroutine\"]\n        G3[\"goroutine\"]\n        G4[\"goroutine\"]\n        \n        GoRuntime --> G1 & G2 & G3 & G4\n        \n        subgraph GoCPU [\"CPU ì½”ì–´\"]\n            GC1[\"Core 1\"]\n            GC2[\"Core 2\"]\n            GC3[\"Core 3\"]\n            GC4[\"Core 4\"]\n        end\n        \n        G1 -.-> GC1\n        G2 -.-> GC2\n        G3 -.-> GC3\n        G4 -.-> GC4\n    end\n    \n    subgraph Python [\"Python: ThreadPoolExecutor\"]\n        Pool[\"ThreadPoolExecutor\"]\n        T1[\"Thread 1\"]\n        T2[\"Thread 2\"]\n        T3[\"Thread 3\"]\n        T4[\"Thread 4\"]\n        \n        Pool --> T1 & T2 & T3 & T4\n        \n        GIL[\"âš ï¸ GIL ì œì•½\"]\n    end\n```\n\n**í•œê³„**:\n\n- **ë©”ëª¨ë¦¬ í•œê³„**: ì„œë²„ RAM í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ëŠ” ë°ì´í„° ì²˜ë¦¬ ë¶ˆê°€\n- **CPU í•œê³„**: ì½”ì–´ ìˆ˜ ì´ìƒì˜ ë³‘ë ¬ì„± ë¶ˆê°€\n- **GIL (Python)**: CPU-bound ì‘ì—… ì‹œ ì§„ì •í•œ ë³‘ë ¬ì„± ì–´ë ¤ì›€\n\n### Spark: ì—¬ëŸ¬ ì„œë²„ì— ë¶„ì‚°\n\n```mermaid\nflowchart TB\n    subgraph Driver [\"Driver (ë§ˆìŠ¤í„°)\"]\n        App[\"Spark Application\"]\n    end\n    \n    subgraph Cluster [\"í´ëŸ¬ìŠ¤í„° (ì›Œì»¤ë“¤)\"]\n        subgraph Worker1 [\"Worker 1 (ì„œë²„ A)\"]\n            E1[\"Executor\"]\n            E1T1[\"Task\"]\n            E1T2[\"Task\"]\n            E1 --> E1T1 & E1T2\n        end\n        \n        subgraph Worker2 [\"Worker 2 (ì„œë²„ B)\"]\n            E2[\"Executor\"]\n            E2T1[\"Task\"]\n            E2T2[\"Task\"]\n            E2 --> E2T1 & E2T2\n        end\n        \n        subgraph Worker3 [\"Worker 3 (ì„œë²„ C)\"]\n            E3[\"Executor\"]\n            E3T1[\"Task\"]\n            E3T2[\"Task\"]\n            E3 --> E3T1 & E3T2\n        end\n    end\n    \n    App -->|\"ì‘ì—… ë¶„ë°°\"| Worker1 & Worker2 & Worker3\n```\n\n**Sparkì˜ í•´ê²°ì±…**:\n\n- **ë©”ëª¨ë¦¬ ë¶„ì‚°**: ê° ì„œë²„ê°€ ë°ì´í„° ì¼ë¶€ë§Œ ì²˜ë¦¬\n- **CPU ë¶„ì‚°**: ì´ CPU = ì„œë²„ ìˆ˜ Ã— ì„œë²„ë‹¹ ì½”ì–´\n- **ì¥ì•  ë³µêµ¬**: í•œ ì„œë²„ê°€ ì£½ì–´ë„ ë‹¤ë¥¸ ì„œë²„ê°€ ì¬ì²˜ë¦¬\n\n### ë¹„êµ ì •ë¦¬\n\n| íŠ¹ì„± | Goroutine / ThreadPool | Spark |\n|------|----------------------|-------|\n| **ë²”ìœ„** | ë‹¨ì¼ ì„œë²„ | ì—¬ëŸ¬ ì„œë²„ í´ëŸ¬ìŠ¤í„° |\n| **ìŠ¤ì¼€ì¼ë§** | ìˆ˜ì§ (ë” ì¢‹ì€ ì„œë²„) | ìˆ˜í‰ (ì„œë²„ ì¶”ê°€) |\n| **ë©”ëª¨ë¦¬** | ì„œë²„ RAM í•œê³„ | í´ëŸ¬ìŠ¤í„° í•©ì‚° RAM |\n| **ì¥ì•  ì²˜ë¦¬** | í”„ë¡œì„¸ìŠ¤ ì¬ì‹œì‘ | ë‹¤ë¥¸ ë…¸ë“œê°€ ì¬ì²˜ë¦¬ |\n| **ë°ì´í„° ê³µìœ ** | ë©”ëª¨ë¦¬ ì§ì ‘ ê³µìœ  | ë„¤íŠ¸ì›Œí¬ í†µì‹  |\n| **ì í•©í•œ ë°ì´í„°** | GB ì´í•˜ | TB ~ PB |\n\n---\n\n## MapReduce íŒ¨ëŸ¬ë‹¤ì„\n\nSparkë¥¼ ì´í•´í•˜ë ¤ë©´ ë¨¼ì € **MapReduce**ë¥¼ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤.\n\n### í´ë˜ì‹ ì˜ˆì œ: Word Count\n\n\"Hello World Hello\" ë¼ëŠ” í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ë³„ ê°œìˆ˜ë¥¼ ì„¸ëŠ” ë¬¸ì œì…ë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph Input [\"ì…ë ¥\"]\n        I1[\"Hello World Hello\"]\n    end\n    \n    subgraph Map [\"Map (ë³€í™˜)\"]\n        M1[\"(Hello, 1)\"]\n        M2[\"(World, 1)\"]\n        M3[\"(Hello, 1)\"]\n    end\n    \n    subgraph Shuffle [\"Shuffle (ì¬ë°°ì¹˜)\"]\n        S1[\"Hello â†’ [(Hello, 1), (Hello, 1)]\"]\n        S2[\"World â†’ [(World, 1)]\"]\n    end\n    \n    subgraph Reduce [\"Reduce (ì§‘ê³„)\"]\n        R1[\"(Hello, 2)\"]\n        R2[\"(World, 1)\"]\n    end\n    \n    Input --> Map --> Shuffle --> Reduce\n```\n\n### ë¶„ì‚° í™˜ê²½ì—ì„œì˜ MapReduce\n\n```mermaid\nflowchart TB\n    subgraph Data [\"ë¶„ì‚°ëœ ë°ì´í„°\"]\n        D1[\"Partition 1<br/>'Hello World'\"]\n        D2[\"Partition 2<br/>'Hello Spark'\"]\n        D3[\"Partition 3<br/>'World Spark'\"]\n    end\n    \n    subgraph MapPhase [\"Map Phase (ë³‘ë ¬)\"]\n        M1[\"Worker 1<br/>(Hello,1) (World,1)\"]\n        M2[\"Worker 2<br/>(Hello,1) (Spark,1)\"]\n        M3[\"Worker 3<br/>(World,1) (Spark,1)\"]\n    end\n    \n    subgraph ShufflePhase [\"Shuffle Phase\"]\n        direction LR\n        SH[\"í‚¤ ê¸°ì¤€ìœ¼ë¡œ ì¬ë°°ì¹˜<br/>(ë„¤íŠ¸ì›Œí¬ ì „ì†¡ ë°œìƒ)\"]\n    end\n    \n    subgraph ReducePhase [\"Reduce Phase (ë³‘ë ¬)\"]\n        R1[\"Reducer 1<br/>Hello â†’ 2\"]\n        R2[\"Reducer 2<br/>World â†’ 2\"]\n        R3[\"Reducer 3<br/>Spark â†’ 2\"]\n    end\n    \n    D1 --> M1\n    D2 --> M2\n    D3 --> M3\n    \n    M1 & M2 & M3 --> ShufflePhase --> R1 & R2 & R3\n```\n\n**í•µì‹¬ ì¸ì‚¬ì´íŠ¸**:\n\n- **Map**: ê° ì„œë²„ê°€ ìê¸° íŒŒí‹°ì…˜ë§Œ ì²˜ë¦¬ (ë³‘ë ¬, ë¹ ë¦„)\n- **Shuffle**: í‚¤ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ì¬ë°°ì¹˜ (ë„¤íŠ¸ì›Œí¬ í†µì‹ , ëŠë¦¼ âš ï¸)\n- **Reduce**: ê°™ì€ í‚¤ë¼ë¦¬ ëª¨ì—¬ì„œ ì§‘ê³„ (ë³‘ë ¬)\n\n---\n\n## RDD (Resilient Distributed Dataset)\n\nSparkì˜ í•µì‹¬ ì¶”ìƒí™”ì…ë‹ˆë‹¤.\n\n### RDDë€?\n\n```mermaid\nflowchart TB\n    subgraph RDD [\"RDD: Resilient Distributed Dataset\"]\n        R[\"ë¶ˆë³€(Immutable)\"]\n        D[\"ë¶„ì‚°(Distributed)\"]\n        F[\"ì¥ì•  ë³µêµ¬(Fault-tolerant)\"]\n    end\n    \n    subgraph Partitions [\"íŒŒí‹°ì…˜ìœ¼ë¡œ ë¶„ì‚°\"]\n        P1[\"Partition 1<br/>ì„œë²„ A\"]\n        P2[\"Partition 2<br/>ì„œë²„ B\"]\n        P3[\"Partition 3<br/>ì„œë²„ C\"]\n    end\n    \n    RDD --> Partitions\n```\n\n**í•µì‹¬ íŠ¹ì„±**:\n\n| íŠ¹ì„± | ì˜ë¯¸ | ì™œ ì¤‘ìš”í•œê°€? |\n|------|------|------------|\n| **Resilient** | ì¥ì•  ë³µêµ¬ ê°€ëŠ¥ | ë…¸ë“œê°€ ì£½ì–´ë„ ë°ì´í„° ë³µêµ¬ |\n| **Distributed** | í´ëŸ¬ìŠ¤í„°ì— ë¶„ì‚° | ì—¬ëŸ¬ ì„œë²„ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬ |\n| **Immutable** | ë³€ê²½ ë¶ˆê°€ | ì—°ì‚° ê²°ê³¼ëŠ” ìƒˆ RDD ìƒì„± |\n\n### Transformations vs Actions\n\nRDD ì—°ì‚°ì€ ë‘ ì¢…ë¥˜ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph Transformations [\"Transformations (ë³€í™˜)\"]\n        T1[\"map\"]\n        T2[\"filter\"]\n        T3[\"flatMap\"]\n        T4[\"groupBy\"]\n        T5[\"join\"]\n        \n        Lazy[\"â¸ï¸ Lazy: ë°”ë¡œ ì‹¤í–‰ ì•ˆ í•¨\"]\n    end\n    \n    subgraph Actions [\"Actions (ì‹¤í–‰)\"]\n        A1[\"count\"]\n        A2[\"collect\"]\n        A3[\"save\"]\n        A4[\"reduce\"]\n        \n        Execute[\"â–¶ï¸ ì‹¤í–‰: ì´ë•Œ ê³„ì‚° ì‹œì‘\"]\n    end\n    \n    RDD1[\"RDD\"] --> Transformations --> RDD2[\"ìƒˆ RDD\"]\n    RDD2 --> Actions --> Result[\"ê²°ê³¼\"]\n```\n\n### Lazy Evaluationì˜ í˜\n\n```mermaid\nflowchart TB\n    subgraph Eager [\"Eager Evaluation (ì¼ë°˜ì ì¸ ë°©ì‹)\"]\n        EE1[\"data = load()\"] -->|\"ì‹¤í–‰\"| EE2[\"filtered = filter()\"]\n        EE2 -->|\"ì‹¤í–‰\"| EE3[\"mapped = map()\"]\n        EE3 -->|\"ì‹¤í–‰\"| EE4[\"result = count()\"]\n    end\n    \n    subgraph Lazy [\"Lazy Evaluation (Spark)\"]\n        LE1[\"rdd = load()\"] -->|\"ê¸°ë¡ë§Œ\"| LE2[\"filtered = filter()\"]\n        LE2 -->|\"ê¸°ë¡ë§Œ\"| LE3[\"mapped = map()\"]\n        LE3 -->|\"ê¸°ë¡ë§Œ\"| LE4[\"count()\"]\n        LE4 -->|\"ìµœì í™” í›„ ì‹¤í–‰!\"| Result[\"ê²°ê³¼\"]\n    end\n    \n    Lazy -->|\"ì¥ì \"| Optimize[\"âœ… ì‹¤í–‰ ê³„íš ìµœì í™”<br/>âœ… ë¶ˆí•„ìš”í•œ ì—°ì‚° ì œê±°<br/>âœ… íŒŒì´í”„ë¼ì´ë‹\"]\n```\n\n**ì‹¤ì œ ì˜ˆì‹œ**:\n\n```python\n# SparkëŠ” ì´ ì‹œì ì— ì•„ë¬´ê²ƒë„ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ\nrdd = spark.read.text(\"huge_file.txt\")  # ê¸°ë¡ë§Œ\nfiltered = rdd.filter(lambda x: \"error\" in x)  # ê¸°ë¡ë§Œ\nmapped = filtered.map(lambda x: (x, 1))  # ê¸°ë¡ë§Œ\n\n# ì´ ì‹œì ì— ìµœì í™”ëœ ê³„íšìœ¼ë¡œ í•œ ë²ˆì— ì‹¤í–‰\ncount = mapped.count()  # ì‹¤í–‰!\n```\n\n---\n\n## DataFrame: RDDì˜ ì§„í™”\n\n### RDDì˜ í•œê³„\n\n```mermaid\nflowchart TB\n    subgraph Problem [\"RDDì˜ ë¬¸ì œ\"]\n        P1[\"íƒ€ì… ì •ë³´ ì—†ìŒ<br/>(Python ê°ì²´)\"]\n        P2[\"ìµœì í™” ì–´ë ¤ì›€<br/>(ë¸”ë™ë°•ìŠ¤)\"]\n        P3[\"ì§ë ¬í™” ì˜¤ë²„í—¤ë“œ<br/>(Python â†” JVM)\"]\n    end\n    \n    Problem --> Solution[\"DataFrame ë“±ì¥\"]\n```\n\n### DataFrameì´ë€?\n\n```mermaid\nflowchart TB\n    subgraph DataFrame [\"DataFrame\"]\n        direction TB\n        Schema[\"ìŠ¤í‚¤ë§ˆ (ì»¬ëŸ¼ëª…, íƒ€ì…)\"]\n        Rows[\"Row ë°ì´í„°\"]\n        Catalyst[\"Catalyst Optimizer\"]\n    end\n    \n    subgraph Analogy [\"ìµìˆ™í•œ ë¹„ìœ \"]\n        SQL[\"SQL í…Œì´ë¸”\"]\n        Pandas[\"Pandas DataFrame\"]\n        Excel[\"ì—‘ì…€ ì‹œíŠ¸\"]\n    end\n    \n    DataFrame --> Analogy\n```\n\n**DataFrame vs RDD**:\n\n| íŠ¹ì„± | RDD | DataFrame |\n|------|-----|-----------|\n| **ìŠ¤í‚¤ë§ˆ** | ì—†ìŒ (Python ê°ì²´) | ìˆìŒ (ì»¬ëŸ¼ëª…, íƒ€ì…) |\n| **ìµœì í™”** | ìˆ˜ë™ (ê°œë°œìê°€) | ìë™ (Catalyst) |\n| **API** | map, filter (í•¨ìˆ˜í˜•) | select, where (SQLí˜•) |\n| **ì„±ëŠ¥** | ëŠë¦¼ (ì§ë ¬í™”) | ë¹ ë¦„ (ìµœì í™”) |\n| **ì–¸ì–´** | ì–¸ì–´ë³„ ì°¨ì´ í¼ | ì–¸ì–´ë³„ ì°¨ì´ ì ìŒ |\n\n### ì™œ DataFrameì´ ë” ë¹ ë¥¸ê°€?\n\n```mermaid\nflowchart TB\n    subgraph RDDPath [\"RDD ê²½ë¡œ\"]\n        R1[\"Python í•¨ìˆ˜\"] --> R2[\"ì§ë ¬í™”<br/>(pickle)\"]\n        R2 --> R3[\"JVM ì „ì†¡\"]\n        R3 --> R4[\"ì—­ì§ë ¬í™”\"]\n        R4 --> R5[\"ì‹¤í–‰\"]\n    end\n    \n    subgraph DFPath [\"DataFrame ê²½ë¡œ\"]\n        D1[\"DataFrame API\"] --> D2[\"Catalyst<br/>ìµœì í™”\"]\n        D2 --> D3[\"JVM ì½”ë“œ<br/>ì§ì ‘ ì‹¤í–‰\"]\n    end\n    \n    RDDPath -->|\"ğŸ¢\"| Slow[\"ëŠë¦¼\"]\n    DFPath -->|\"ğŸš€\"| Fast[\"ë¹ ë¦„\"]\n```\n\n---\n\n## Spark Connect (4.0+)\n\nSpark 4.0ì˜ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.\n\n### ê¸°ì¡´ ë°©ì‹ vs Spark Connect\n\n```mermaid\nflowchart TB\n    subgraph Before [\"ê¸°ì¡´ ë°©ì‹\"]\n        Client1[\"Python Driver\"] -->|\"ê°™ì€ ì„œë²„\"| Cluster1[\"Spark Cluster\"]\n    end\n    \n    subgraph After [\"Spark Connect\"]\n        Client2[\"Thin Client<br/>(ì–´ë””ì„œë“ )\"] -->|\"gRPC\"| Server[\"Spark Connect<br/>Server\"]\n        Server --> Cluster2[\"Spark Cluster\"]\n    end\n    \n    After -->|\"ì¥ì \"| Benefits[\"âœ… í´ë¼ì´ì–¸íŠ¸ ê°€ë²¼ì›€<br/>âœ… ì›ê²© ì—°ê²° ê°€ëŠ¥<br/>âœ… ë‹¤ì–‘í•œ ì–¸ì–´ ì§€ì›\"]\n```\n\n```python\n# Spark Connect ì‚¬ìš© ì˜ˆ\nfrom pyspark.sql import SparkSession\n\n# ì›ê²© í´ëŸ¬ìŠ¤í„°ì— ì—°ê²°\nspark = SparkSession.builder \\\n    .remote(\"sc://spark-server:15002\") \\\n    .getOrCreate()\n\n# ë‚˜ë¨¸ì§€ëŠ” ë™ì¼í•˜ê²Œ ì‚¬ìš©\ndf = spark.range(1000000)\nresult = df.groupBy((df.id % 10).alias(\"group\")).count()\nresult.show()\n```\n\n---\n\n## ì‹¤ì „ ì½”ë“œ: Word Count ë¹„êµ\n\n### Python (ThreadPoolExecutor)\n\n```python\nfrom concurrent.futures import ThreadPoolExecutor\nfrom collections import Counter\n\ndef count_words_in_chunk(text_chunk):\n    words = text_chunk.lower().split()\n    return Counter(words)\n\ndef word_count_threaded(text, num_workers=4):\n    # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• \n    chunks = [text[i::num_workers] for i in range(num_workers)]\n    \n    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n        results = list(executor.map(count_words_in_chunk, chunks))\n    \n    # ê²°ê³¼ í•©ì¹˜ê¸°\n    total = Counter()\n    for result in results:\n        total.update(result)\n    \n    return total\n\n# í•œê³„: ë©”ëª¨ë¦¬ì— ì „ì²´ í…ìŠ¤íŠ¸ê°€ ì˜¬ë¼ì™€ì•¼ í•¨\n```\n\n### Go (Goroutines)\n\n```go\nfunc wordCount(texts []string) map[string]int {\n    results := make(chan map[string]int, len(texts))\n    \n    // ê° ì²­í¬ë¥¼ goroutineìœ¼ë¡œ ì²˜ë¦¬\n    for _, text := range texts {\n        go func(t string) {\n            counts := make(map[string]int)\n            for _, word := range strings.Fields(strings.ToLower(t)) {\n                counts[word]++\n            }\n            results <- counts\n        }(text)\n    }\n    \n    // ê²°ê³¼ í•©ì¹˜ê¸°\n    total := make(map[string]int)\n    for i := 0; i < len(texts); i++ {\n        for word, count := range <-results {\n            total[word] += count\n        }\n    }\n    \n    return total\n}\n\n// í•œê³„: ë‹¨ì¼ ì„œë²„ ë©”ëª¨ë¦¬ í•œê³„\n```\n\n### PySpark (ë¶„ì‚° ì²˜ë¦¬)\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode, split, lower, col\n\nspark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n\n# TB ë‹¨ìœ„ íŒŒì¼ë„ ì²˜ë¦¬ ê°€ëŠ¥\ndf = spark.read.text(\"hdfs://path/to/huge_files/*.txt\")\n\nword_counts = df \\\n    .select(explode(split(lower(col(\"value\")), \"\\\\s+\")).alias(\"word\")) \\\n    .groupBy(\"word\") \\\n    .count() \\\n    .orderBy(col(\"count\").desc())\n\nword_counts.show(20)\n\n# ì¥ì : ìë™ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° ì „ì²´ì— ë¶„ì‚° ì²˜ë¦¬\n```\n\n---\n\n## ì •ë¦¬\n\n```mermaid\nmindmap\n  root((Spark<br/>í•µì‹¬ ê°œë…))\n    ì™œ ë¶„ì‚° ì²˜ë¦¬?\n      ë‹¨ì¼ ì„œë²„ í•œê³„\n      ë°ì´í„° > ë©”ëª¨ë¦¬\n      ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•\n    vs ê¸°ì¡´ ë³‘ë ¬ì²˜ë¦¬\n      Goroutine: ë‹¨ì¼ ì„œë²„\n      Spark: ì—¬ëŸ¬ ì„œë²„\n      ìŠ¤ì¼€ì¼ ì°¨ì´\n    MapReduce\n      Map: ë³€í™˜\n      Shuffle: ì¬ë°°ì¹˜\n      Reduce: ì§‘ê³„\n    RDD\n      Immutable\n      Distributed\n      Fault-tolerant\n    Lazy Evaluation\n      ê¸°ë¡ë§Œ í•˜ë‹¤ê°€\n      Actionì—ì„œ ì‹¤í–‰\n      ìµœì í™” ê°€ëŠ¥\n    DataFrame\n      ìŠ¤í‚¤ë§ˆ ìˆìŒ\n      Catalyst ìµœì í™”\n      ë¹ ë¦„\n```\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**4í¸: Spark ë‚´ë¶€ ë™ì‘ ì›ë¦¬**ì—ì„œëŠ” ë” ê¹Šì´ ë“¤ì–´ê°‘ë‹ˆë‹¤:\n\n- Job â†’ Stage â†’ Task ê³„ì¸µ\n- Shuffleì´ ëŠë¦° ì´ìœ \n- íŒŒí‹°ì…”ë‹ ì „ëµ\n- ë©”ëª¨ë¦¬ ê´€ë¦¬ì™€ Spill\n- Spark UI ì½ëŠ” ë²•\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)\n- [Spark: The Definitive Guide](https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/) (O'Reilly)\n- [Spark Connect Overview](https://spark.apache.org/docs/latest/spark-connect-overview.html)\n- [Learning Spark, 2nd Edition](https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/)",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Data Engineering",
      "Distributed Systems",
      "Spark"
    ],
    "readingTime": 8,
    "wordCount": 1559,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-02-data-architecture-101",
    "slug": "de-02-data-architecture-101",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-02-data-architecture-101",
    "title": "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #2: ë°ì´í„° ì•„í‚¤í…ì²˜ 101 - ì „ì²´ ê·¸ë¦¼ ì´í•´í•˜ê¸°",
    "excerpt": "ë°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ ì „ì²´ êµ¬ì¡°ë¥¼ ì¡°ë§í•©ë‹ˆë‹¤. ETL vs ELT, ë°°ì¹˜ vs ìŠ¤íŠ¸ë¦¬ë°, Lambda vs Kappa ì•„í‚¤í…ì²˜, ê·¸ë¦¬ê³  Modern Data Stackê¹Œì§€.",
    "content": "# ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #2: ë°ì´í„° ì•„í‚¤í…ì²˜ 101 - ì „ì²´ ê·¸ë¦¼ ì´í•´í•˜ê¸°\n\n> **ëŒ€ìƒ ë…ì**: ì¶©ë¶„í•œ ê²½í—˜ì„ ê°€ì§„ ë°±ì—”ë“œ/í’€ìŠ¤íƒ ì—”ì§€ë‹ˆì–´ë¡œ, ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜ë¥¼ ì²˜ìŒ ì ‘í•˜ëŠ” ë¶„\n\n## ì´ í¸ì—ì„œ ë‹¤ë£¨ëŠ” ê²ƒ\n\n1í¸ì—ì„œ **ì™œ** ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ì´ í•„ìš”í•œì§€ ì•Œì•˜ë‹¤ë©´, ì´ì œ **ì–´ë–»ê²Œ** êµ¬ì„±ë˜ëŠ”ì§€ ì „ì²´ ê·¸ë¦¼ì„ ë´…ë‹ˆë‹¤.\n\n---\n\n## ë°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ 5ë‹¨ê³„\n\në°ì´í„°ëŠ” ìƒì„±ë¶€í„° ì†Œë¹„ê¹Œì§€ 5ë‹¨ê³„ë¥¼ ê±°ì¹©ë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph Stage1 [\"1ï¸âƒ£ Sources\"]\n        S1[(PostgreSQL)]\n        S2[(MongoDB)]\n        S3[REST APIs]\n        S4[ë¡œê·¸ íŒŒì¼]\n        S5[IoT ì„¼ì„œ]\n    end\n    \n    subgraph Stage2 [\"2ï¸âƒ£ Ingestion\"]\n        I1[Kafka]\n        I2[Debezium]\n        I3[Airbyte]\n        I4[Fivetran]\n    end\n    \n    subgraph Stage3 [\"3ï¸âƒ£ Processing\"]\n        P1[Spark]\n        P2[Flink]\n        P3[dbt]\n    end\n    \n    subgraph Stage4 [\"4ï¸âƒ£ Storage\"]\n        ST1[(Data Lake<br/>S3, GCS)]\n        ST2[(Data Warehouse<br/>BigQuery)]\n        ST3[(Lakehouse<br/>Delta Lake)]\n    end\n    \n    subgraph Stage5 [\"5ï¸âƒ£ Serving\"]\n        SV1[Metabase]\n        SV2[Looker]\n        SV3[ML Models]\n        SV4[Reverse ETL]\n    end\n    \n    Stage1 --> Stage2 --> Stage3 --> Stage4 --> Stage5\n```\n\n### ê° ë‹¨ê³„ë³„ ì—­í• \n\n| ë‹¨ê³„ | ì—­í•  | ë°±ì—”ë“œ ê°œë°œìì—ê²Œ ìµìˆ™í•œ ë¹„ìœ  |\n|------|------|---------------------------|\n| **Sources** | ë°ì´í„° ì›ì²œ | ìš´ì˜ DB, ì™¸ë¶€ API |\n| **Ingestion** | ë°ì´í„° ìˆ˜ì§‘/ì „ì†¡ | ë©”ì‹œì§€ í, ì›¹í›… |\n| **Processing** | ë³€í™˜/ì •ì œ/ì§‘ê³„ | ë°°ì¹˜ ì‘ì—…, Worker |\n| **Storage** | ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ | ìºì‹œ, Read Replica |\n| **Serving** | ìµœì¢… ì‚¬ìš©ìì—ê²Œ ì œê³µ | API Gateway, CDN |\n\n---\n\n## ETL vs ELT: íŒ¨ëŸ¬ë‹¤ì„ì˜ ë³€í™”\n\në°ì´í„°ë¥¼ ì˜®ê¸°ëŠ” ë°©ì‹ì—ëŠ” ë‘ ê°€ì§€ íŒ¨ëŸ¬ë‹¤ì„ì´ ìˆìŠµë‹ˆë‹¤.\n\n### ETL (Extract â†’ Transform â†’ Load)\n\n**ì „í†µì  ë°©ì‹**: ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ, ë³€í™˜í•œ í›„, ì €ì¥í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph Source [\"ì›ì²œ ì‹œìŠ¤í…œ\"]\n        DB[(ìš´ì˜ DB)]\n    end\n    \n    subgraph ETL [\"ETL ì„œë²„\"]\n        E[\"Extract<br/>ì¶”ì¶œ\"]\n        T[\"Transform<br/>ë³€í™˜\"]\n    end\n    \n    subgraph DW [\"Data Warehouse\"]\n        L[(ì €ì¥)]\n    end\n    \n    DB -->|\"ì›ë³¸ ë°ì´í„°\"| E\n    E -->|\"ì›ë³¸\"| T\n    T -->|\"ë³€í™˜ëœ ë°ì´í„°\"| L\n    \n    style ETL fill:#ffcdd2\n```\n\n**íŠ¹ì§•**:\n\n- ë³€í™˜ ë¡œì§ì´ **ETL ì„œë²„**ì— ìˆìŒ\n- Informatica, Talend ê°™ì€ ì „ìš© ë„êµ¬ ì‚¬ìš©\n- ì €ì¥ ì „ì— ë°ì´í„° í’ˆì§ˆ ë³´ì¥\n\n### ELT (Extract â†’ Load â†’ Transform)\n\n**í˜„ëŒ€ì  ë°©ì‹**: ë°ì´í„°ë¥¼ ë¨¼ì € ì €ì¥í•˜ê³ , ì €ì¥ì†Œ ë‚´ì—ì„œ ë³€í™˜í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph Source [\"ì›ì²œ ì‹œìŠ¤í…œ\"]\n        DB[(ìš´ì˜ DB)]\n    end\n    \n    subgraph Ingestion [\"ìˆ˜ì§‘ ë„êµ¬\"]\n        E[\"Extract\"]\n        L1[\"Load\"]\n    end\n    \n    subgraph DW [\"Data Warehouse\"]\n        Raw[(Raw Layer)]\n        T[\"Transform<br/>(dbt, SQL)\"]\n        Final[(Mart Layer)]\n    end\n    \n    DB -->|\"ì›ë³¸ ë°ì´í„°\"| E\n    E --> L1\n    L1 -->|\"ì›ë³¸ ê·¸ëŒ€ë¡œ\"| Raw\n    Raw --> T --> Final\n    \n    style DW fill:#c8e6c9\n```\n\n**íŠ¹ì§•**:\n\n- ë³€í™˜ ë¡œì§ì´ **ì›¨ì–´í•˜ìš°ìŠ¤ ë‚´ë¶€**ì— ìˆìŒ (SQL ê¸°ë°˜)\n- dbt, BigQuery, Snowflake í™œìš©\n- ì›ë³¸ ë°ì´í„° ë³´ì¡´ (ì–¸ì œë“  ì¬ë³€í™˜ ê°€ëŠ¥)\n\n### ì–¸ì œ ë¬´ì—‡ì„ ì„ íƒí• ê¹Œ?\n\n```mermaid\nflowchart TB\n    Q1{\"ì›ë³¸ ë°ì´í„°ë¥¼<br/>ë³´ì¡´í•´ì•¼ í•˜ë‚˜ìš”?\"}\n    Q2{\"ë³€í™˜ ë¡œì§ì´<br/>ë³µì¡í•œê°€ìš”?\"}\n    Q3{\"ì›¨ì–´í•˜ìš°ìŠ¤ê°€<br/>ê°•ë ¥í•œê°€ìš”?\"}\n    \n    Q1 -->|\"ì˜ˆ\"| ELT[\"âœ… ELT ê¶Œì¥\"]\n    Q1 -->|\"ì•„ë‹ˆì˜¤\"| Q2\n    Q2 -->|\"ë§¤ìš° ë³µì¡\"| ETL[\"âœ… ETL ê¶Œì¥\"]\n    Q2 -->|\"SQLë¡œ ê°€ëŠ¥\"| Q3\n    Q3 -->|\"BigQuery/Snowflake\"| ELT\n    Q3 -->|\"ì œí•œì \"| ETL\n```\n\n> **í˜„ëŒ€ íŠ¸ë Œë“œ**: í´ë¼ìš°ë“œ ì›¨ì–´í•˜ìš°ìŠ¤(BigQuery, Snowflake)ì˜ ì»´í“¨íŒ… íŒŒì›Œê°€ ê°•ë ¥í•´ì§€ë©´ì„œ **ELTê°€ í‘œì¤€**ì´ ë˜ì–´ê°€ê³  ìˆìŠµë‹ˆë‹¤.\n\n---\n\n## ë°°ì¹˜ vs ìŠ¤íŠ¸ë¦¬ë°\n\në°ì´í„° ì²˜ë¦¬ ì£¼ê¸°ì— ë”°ë¼ ë‘ ê°€ì§€ ë°©ì‹ì´ ìˆìŠµë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph Batch [\"ë°°ì¹˜ ì²˜ë¦¬ (Batch)\"]\n        B1[\"ğŸ“Š ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼\"]\n        B2[\"â° ì •í•´ì§„ ì‹œê°„ì—\"]\n        B3[\"ğŸ”„ í•œ ë²ˆì— ì²˜ë¦¬\"]\n        B1 --> B2 --> B3\n    end\n    \n    subgraph Stream [\"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (Streaming)\"]\n        S1[\"ğŸ“¨ ë°ì´í„°ê°€ ë°œìƒí•  ë•Œ\"]\n        S2[\"âš¡ ì‹¤ì‹œê°„ìœ¼ë¡œ\"]\n        S3[\"ğŸ” ì§€ì†ì ìœ¼ë¡œ ì²˜ë¦¬\"]\n        S1 --> S2 --> S3\n    end\n    \n    Batch -->|\"ì˜ˆì‹œ\"| BE[\"ì¼ì¼ ë§¤ì¶œ ë¦¬í¬íŠ¸<br/>ì£¼ê°„ ì½”í˜¸íŠ¸ ë¶„ì„\"]\n    Stream -->|\"ì˜ˆì‹œ\"| SE[\"ì‹¤ì‹œê°„ ì´ìƒ íƒì§€<br/>ë¼ì´ë¸Œ ëŒ€ì‹œë³´ë“œ\"]\n```\n\n### ë¹„êµ\n\n| íŠ¹ì„± | ë°°ì¹˜ | ìŠ¤íŠ¸ë¦¬ë° |\n|------|------|---------|\n| **ì²˜ë¦¬ ì£¼ê¸°** | ë¶„/ì‹œê°„/ì¼ ë‹¨ìœ„ | ë°€ë¦¬ì´ˆ~ì´ˆ ë‹¨ìœ„ |\n| **ì§€ì—° ì‹œê°„** | ë†’ìŒ (ë¶„~ì‹œê°„) | ë‚®ìŒ (ì´ˆ ì´ë‚´) |\n| **êµ¬í˜„ ë³µì¡ë„** | ìƒëŒ€ì ìœ¼ë¡œ ë‹¨ìˆœ | ë³µì¡ (ìƒíƒœ ê´€ë¦¬, ìˆœì„œ ë³´ì¥) |\n| **ë¦¬ì†ŒìŠ¤ ì‚¬ìš©** | í”¼í¬ íƒ€ì„ì— ì§‘ì¤‘ | ì§€ì†ì ìœ¼ë¡œ ì‚¬ìš© |\n| **ë„êµ¬** | Spark, Airflow | Kafka, Flink, Spark Streaming |\n| **ì í•©í•œ ê²½ìš°** | ë¦¬í¬íŒ…, ML í•™ìŠµ | ì‹¤ì‹œê°„ ì•Œë¦¼, ì¶”ì²œ |\n\n### ì‹¤ì œë¡œëŠ” ë‘˜ ë‹¤ í•„ìš”\n\n```mermaid\nflowchart LR\n    Source[(ì´ë²¤íŠ¸ ë°œìƒ)] --> Kafka\n    \n    subgraph Processing\n        Kafka --> Stream[\"ì‹¤ì‹œê°„ ì²˜ë¦¬<br/>(Flink)\"] --> RT[\"ì‹¤ì‹œê°„ ì•Œë¦¼\"]\n        Kafka --> Lake[(Data Lake)]\n        Lake --> Batch[\"ë°°ì¹˜ ì²˜ë¦¬<br/>(Spark)\"] --> Report[\"ì¼ì¼ ë¦¬í¬íŠ¸\"]\n    end\n```\n\nëŒ€ë¶€ë¶„ì˜ ì‹œìŠ¤í…œì€ **ë°°ì¹˜ì™€ ìŠ¤íŠ¸ë¦¬ë°ì„ í•¨ê»˜ ì‚¬ìš©**í•©ë‹ˆë‹¤. ì´ë¥¼ ì–´ë–»ê²Œ ì¡°í•©í• ì§€ê°€ ë°”ë¡œ ë‹¤ìŒì— ë‹¤ë£° ì•„í‚¤í…ì²˜ íŒ¨í„´ì…ë‹ˆë‹¤.\n\n---\n\n## Lambda Architecture vs Kappa Architecture\n\n### Lambda Architecture (ëŒë‹¤ ì•„í‚¤í…ì²˜)\n\n**ë°°ì¹˜ + ìŠ¤íŠ¸ë¦¬ë° ì´ì¤‘ íŒŒì´í”„ë¼ì¸**ì„ ìš´ì˜í•˜ëŠ” íŒ¨í„´ì…ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    Source[\"ë°ì´í„° ì†ŒìŠ¤\"] --> Kafka[\"ë©”ì‹œì§€ í<br/>(Kafka)\"]\n    \n    subgraph Lambda [\"Lambda Architecture\"]\n        Kafka --> Speed[\"âš¡ Speed Layer<br/>(ì‹¤ì‹œê°„ ì²˜ë¦¬)\"]\n        Kafka --> Batch[\"ğŸ“Š Batch Layer<br/>(ë°°ì¹˜ ì²˜ë¦¬)\"]\n        \n        Speed --> Serving[\"Serving Layer\"]\n        Batch --> Serving\n    end\n    \n    Serving --> Query[\"ì¿¼ë¦¬\"]\n    \n    subgraph Legend [\"ì—­í• \"]\n        L1[\"Speed: ìµœì‹  ë°ì´í„° (ê·¼ì‚¬ì¹˜)\"]\n        L2[\"Batch: ì „ì²´ ë°ì´í„° (ì •í™•í•œ ê°’)\"]\n        L3[\"Serving: ë‘ ê²°ê³¼ë¥¼ í•©ì³ì„œ ì œê³µ\"]\n    end\n```\n\n**ì¥ì **:\n\n- ì‹¤ì‹œê°„ì„± + ì •í™•ì„± ëª¨ë‘ í™•ë³´\n- ë°°ì¹˜ê°€ ì •í™•í•œ ê²°ê³¼ë¡œ ë³´ì •\n\n**ë‹¨ì **:\n\n- **ë™ì¼í•œ ë¡œì§ì„ ë‘ ë²ˆ êµ¬í˜„** (ë°°ì¹˜ ì½”ë“œ + ìŠ¤íŠ¸ë¦¬ë° ì½”ë“œ)\n- ë³µì¡í•œ ìš´ì˜\n- ê²°ê³¼ í•©ì¹˜ëŠ” ë¡œì§ í•„ìš”\n\n### Kappa Architecture (ì¹´íŒŒ ì•„í‚¤í…ì²˜)\n\n**ìŠ¤íŠ¸ë¦¬ë° ë‹¨ì¼ íŒŒì´í”„ë¼ì¸**ìœ¼ë¡œ ëª¨ë“  ê²ƒì„ ì²˜ë¦¬í•˜ëŠ” íŒ¨í„´ì…ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    Source[\"ë°ì´í„° ì†ŒìŠ¤\"] --> Kafka[\"ë©”ì‹œì§€ í<br/>(Kafka)\"]\n    \n    subgraph Kappa [\"Kappa Architecture\"]\n        Kafka --> Stream[\"âš¡ Stream Processing<br/>(ëª¨ë“  ì²˜ë¦¬)\"]\n        Stream --> Serving[\"Serving Layer\"]\n    end\n    \n    Serving --> Query[\"ì¿¼ë¦¬\"]\n    \n    subgraph Reprocess [\"ì¬ì²˜ë¦¬ê°€ í•„ìš”í•˜ë©´?\"]\n        R1[\"Kafkaì—ì„œ ê³¼ê±° ë°ì´í„° ë‹¤ì‹œ ì½ìŒ\"]\n        R2[\"(Kafkaì˜ log retention í™œìš©)\"]\n    end\n```\n\n**ì¥ì **:\n\n- ë‹¨ì¼ ì½”ë“œë² ì´ìŠ¤\n- ë‹¨ìˆœí•œ ì•„í‚¤í…ì²˜\n- ìœ ì§€ë³´ìˆ˜ ìš©ì´\n\n**ë‹¨ì **:\n\n- ëª¨ë“  ê²ƒì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ\n- ê³¼ê±° ë°ì´í„° ì¬ì²˜ë¦¬ ì‹œ ì‹œê°„ ì†Œìš”\n\n### í˜„ëŒ€ì  ê´€ì ì—ì„œì˜ ì„ íƒ\n\n```mermaid\nflowchart TB\n    Q1{\"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ë§Œìœ¼ë¡œ<br/>ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•˜ë‚˜ìš”?\"}\n    Q2{\"ë³µì¡í•œ ML/í†µê³„ ë¶„ì„ì´<br/>í•„ìš”í•œê°€ìš”?\"}\n    Q3{\"íŒ€ì˜ ìŠ¤íŠ¸ë¦¬ë°<br/>ê²½í—˜ì´ ì¶©ë¶„í•œê°€ìš”?\"}\n    \n    Q1 -->|\"ì˜ˆ\"| Kappa[\"âœ… Kappa\"]\n    Q1 -->|\"ì•„ë‹ˆì˜¤\"| Q2\n    Q2 -->|\"ì˜ˆ\"| Lambda[\"âœ… Lambda\"]\n    Q2 -->|\"ì•„ë‹ˆì˜¤\"| Q3\n    Q3 -->|\"ì˜ˆ\"| Kappa\n    Q3 -->|\"ì•„ë‹ˆì˜¤\"| Hybrid[\"ğŸ”„ ë°°ì¹˜ ìš°ì„ ,<br/>ì ì§„ì  ìŠ¤íŠ¸ë¦¬ë° ë„ì…\"]\n```\n\n> **í˜„ì‹¤ì  ì¡°ì–¸**: ì²˜ìŒë¶€í„° Lambdaë¥¼ êµ¬ì¶•í•˜ë ¤ í•˜ì§€ ë§ˆì„¸ìš”. **ë°°ì¹˜ë¡œ ì‹œì‘**í•˜ê³ , ì •ë§ ì‹¤ì‹œê°„ì´ í•„ìš”í•œ ë¶€ë¶„ë§Œ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í™•ì¥í•˜ì„¸ìš”.\n\n---\n\n## Modern Data Stack\n\nìµœê·¼ ëª‡ ë…„ê°„ ë°ì´í„° ë„êµ¬ ìƒíƒœê³„ê°€ ê¸‰ê²©íˆ ë°œì „í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ **Modern Data Stack**ì´ë¼ ë¶€ë¦…ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph Sources [\"ë°ì´í„° ì†ŒìŠ¤\"]\n        S1[(PostgreSQL)]\n        S2[SaaS APIs]\n        S3[ì´ë²¤íŠ¸ ë¡œê·¸]\n    end\n    \n    subgraph Ingestion [\"ìˆ˜ì§‘ (Ingestion)\"]\n        direction TB\n        I1[\"Fivetran\"]\n        I2[\"Airbyte\"]\n        I3[\"Stitch\"]\n    end\n    \n    subgraph Storage [\"ì €ì¥ & ì²˜ë¦¬\"]\n        direction TB\n        ST1[\"Snowflake\"]\n        ST2[\"BigQuery\"]\n        ST3[\"Databricks\"]\n    end\n    \n    subgraph Transform [\"ë³€í™˜\"]\n        T1[\"dbt\"]\n    end\n    \n    subgraph BI [\"ë¶„ì„ & ì‹œê°í™”\"]\n        direction TB\n        B1[\"Looker\"]\n        B2[\"Metabase\"]\n        B3[\"Superset\"]\n    end\n    \n    subgraph Orchestration [\"ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜\"]\n        O1[\"Airflow\"]\n        O2[\"Dagster\"]\n        O3[\"Prefect\"]\n    end\n    \n    subgraph Quality [\"í’ˆì§ˆ & ê´€ì¸¡ì„±\"]\n        Q1[\"Great Expectations\"]\n        Q2[\"Monte Carlo\"]\n        Q3[\"dbt tests\"]\n    end\n    \n    Sources --> Ingestion --> Storage\n    Storage --> Transform --> Storage\n    Storage --> BI\n    \n    Orchestration -.->|\"ìŠ¤ì¼€ì¤„ë§\"| Ingestion\n    Orchestration -.->|\"ìŠ¤ì¼€ì¤„ë§\"| Transform\n    Quality -.->|\"ê²€ì¦\"| Storage\n```\n\n### í•µì‹¬ ë„êµ¬ë“¤\n\n| ì¹´í…Œê³ ë¦¬ | ë„êµ¬ | ì„¤ëª… |\n|---------|------|------|\n| **ìˆ˜ì§‘** | Airbyte, Fivetran | SaaS/DBì—ì„œ ë°ì´í„° ì¶”ì¶œ |\n| **ì €ì¥ì†Œ** | Snowflake, BigQuery | í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ ì›¨ì–´í•˜ìš°ìŠ¤ |\n| **ë³€í™˜** | dbt | SQL ê¸°ë°˜ ë³€í™˜, í…ŒìŠ¤íŠ¸, ë¬¸ì„œí™” |\n| **ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜** | Airflow, Dagster | ì›Œí¬í”Œë¡œìš° ìŠ¤ì¼€ì¤„ë§ |\n| **ì‹œê°í™”** | Metabase, Looker | ëŒ€ì‹œë³´ë“œ, ë¦¬í¬íŒ… |\n| **í’ˆì§ˆ** | Great Expectations | ë°ì´í„° í…ŒìŠ¤íŠ¸ ìë™í™” |\n\n### ì™œ \"Modern\"ì¸ê°€?\n\n1. **í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ**: ì§ì ‘ ì¸í”„ë¼ ê´€ë¦¬ X, ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ê³¼ê¸ˆ\n2. **ë¶„ë¦¬ëœ ì»´í“¨íŒ…/ìŠ¤í† ë¦¬ì§€**: ì €ì¥ì€ S3, ì²˜ë¦¬ëŠ” í•„ìš”í•  ë•Œë§Œ\n3. **SQL ì¤‘ì‹¬**: ë³µì¡í•œ ì½”ë“œ ëŒ€ì‹  SQLë¡œ ë³€í™˜\n4. **Git ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°**: dbtëŠ” ì½”ë“œì²˜ëŸ¼ ë²„ì „ ê´€ë¦¬\n5. **API ìš°ì„ **: ëª¨ë“  ë„êµ¬ê°€ APIë¡œ ì—°ê²°\n\n---\n\n## ë°±ì—”ë“œ ê°œë°œìë¡œì„œ ê¸°ì–µí•  ê²ƒ\n\n### 1. ì „ì²´ íŒŒì´í”„ë¼ì¸ì—ì„œ ë‚´ ì‹œìŠ¤í…œì˜ ìœ„ì¹˜\n\n```mermaid\nflowchart LR\n    subgraph YourSystem [\"ë‹¹ì‹ ì´ ë§Œë“  ì‹œìŠ¤í…œ\"]\n        API[\"API ì„œë²„\"]\n        DB[(ìš´ì˜ DB)]\n        Events[\"ì´ë²¤íŠ¸ ë°œí–‰\"]\n    end\n    \n    subgraph DataTeam [\"ë°ì´í„° íŒ€ì´ ê°€ì ¸ê°€ëŠ” ê²ƒ\"]\n        CDC[\"DB ë³€ê²½ ìº¡ì²˜<br/>(Debezium)\"]\n        Kafka[\"ì´ë²¤íŠ¸ ìˆ˜ì§‘<br/>(Kafka)\"]\n        API2[\"API í˜¸ì¶œ\"]\n    end\n    \n    DB -->|\"CDC\"| CDC\n    Events --> Kafka\n    API -->|\"Pull\"| API2\n    \n    DataTeam --> Pipeline[\"ë°ì´í„° íŒŒì´í”„ë¼ì¸\"]\n```\n\n### 2. ë°ì´í„° ì¶”ì¶œì„ ê³ ë ¤í•œ ì„¤ê³„\n\n**ì¢‹ì€ ì„¤ê³„**:\n\n- ì´ë²¤íŠ¸ì— **íƒ€ì„ìŠ¤íƒ¬í”„** í¬í•¨\n- ë ˆì½”ë“œì— **created_at, updated_at** ì»¬ëŸ¼\n- **Soft delete** ì ìš© (deleted_at)\n- ë³€ê²½ ì´ë ¥ ì¶”ì  ê°€ëŠ¥í•œ ì„¤ê³„\n\n**í”¼í•´ì•¼ í•  ì„¤ê³„**:\n\n- Hard deleteë¡œ ë°ì´í„° ì¦ë°œ\n- ìƒíƒœ ë³€ê²½ ì‹œ ë®ì–´ì“°ê¸°\n- íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ëŠ” ë ˆì½”ë“œ\n\n### 3. í˜‘ì—… í¬ì¸íŠ¸\n\n| ìƒí™© | ë°±ì—”ë“œê°€ í•  ì¼ |\n|------|--------------|\n| CDC ë„ì… ì‹œ | ìš´ì˜ DB ë¶€í•˜ ëª¨ë‹ˆí„°ë§ |\n| ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ ë³€ê²½ | ë°ì´í„° íŒ€ê³¼ ì‚¬ì „ í˜‘ì˜ |\n| API ë³€ê²½ | ì—­í˜¸í™˜ì„± ìœ ì§€ ë˜ëŠ” ë²„ì €ë‹ |\n| ì¥ì•  ë°œìƒ | ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì˜í–¥ë„ í™•ì¸ |\n\n---\n\n## ì •ë¦¬\n\n```mermaid\nmindmap\n  root((ë°ì´í„°<br/>ì•„í‚¤í…ì²˜))\n    íŒŒì´í”„ë¼ì¸ 5ë‹¨ê³„\n      Sources\n      Ingestion\n      Processing\n      Storage\n      Serving\n    ì²˜ë¦¬ íŒ¨ëŸ¬ë‹¤ì„\n      ETL\n        ë³€í™˜ í›„ ì ì¬\n        ë ˆê±°ì‹œ ë„êµ¬\n      ELT\n        ì ì¬ í›„ ë³€í™˜\n        í˜„ëŒ€ì  ë°©ì‹\n    ì²˜ë¦¬ ì£¼ê¸°\n      Batch\n        ì •í•´ì§„ ì‹œê°„\n        ëŒ€ëŸ‰ ì²˜ë¦¬\n      Streaming\n        ì‹¤ì‹œê°„\n        ì§€ì†ì  ì²˜ë¦¬\n    ì•„í‚¤í…ì²˜ íŒ¨í„´\n      Lambda\n        ë°°ì¹˜ + ìŠ¤íŠ¸ë¦¬ë°\n        ì´ì¤‘ ìœ ì§€ë³´ìˆ˜\n      Kappa\n        ìŠ¤íŠ¸ë¦¬ë° ë‹¨ì¼\n        ë‹¨ìˆœí•¨\n    Modern Data Stack\n      í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ\n      SQL ì¤‘ì‹¬\n      API ì—°ê²°\n```\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**3í¸: Spark í•µì‹¬ ê°œë…**ì—ì„œëŠ” ë¶„ì‚° ì²˜ë¦¬ì˜ í•µì‹¬ì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- ì™œ ë¶„ì‚° ì²˜ë¦¬ê°€ í•„ìš”í•œê°€?\n- RDD, DataFrameì˜ ê°œë…\n- Lazy Evaluationì˜ ì˜ë¯¸\n- Goroutine/ThreadPoolExecutorì™€ Sparkì˜ ì°¨ì´\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- Databricks, \"The Data Lakehouse\"\n- Maxime Beauchemin, \"Functional Data Engineering\" (Medium)\n- Martin Kleppmann, \"Designing Data-Intensive Applications\" - Chapter 10, 11\n- [Kappa Architecture](https://www.oreilly.com/radar/questioning-the-lambda-architecture/)",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Data Architecture",
      "Data Engineering",
      "Data Pipeline"
    ],
    "readingTime": 7,
    "wordCount": 1341,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "de-01-why-data-engineering",
    "slug": "de-01-why-data-engineering",
    "path": "data-engineering",
    "fullPath": "data-engineering/de-01-why-data-engineering",
    "title": "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #1: ë°±ì—”ë“œ ê°œë°œìê°€ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ì„ ë°°ì›Œì•¼ í•˜ëŠ” ì´ìœ ",
    "excerpt": "ì™œ ë°±ì—”ë“œ ê°œë°œìê°€ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ì„ ì•Œì•„ì•¼ í• ê¹Œìš”? OLTPì˜ í•œê³„ì—ì„œ ì‹œì‘í•˜ëŠ” ë°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ ì„¸ê³„ë¡œ ì•ˆë‚´í•©ë‹ˆë‹¤.",
    "content": "# ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #1: ë°±ì—”ë“œ ê°œë°œìê°€ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ì„ ë°°ì›Œì•¼ í•˜ëŠ” ì´ìœ \n\n> **ëŒ€ìƒ ë…ì**: 6ë…„ ì´ìƒì˜ ê²½í—˜ì„ ê°€ì§„ ë°±ì—”ë“œ/í’€ìŠ¤íƒ ì—”ì§€ë‹ˆì–´ë¡œ, Spark, Airflow ë“± ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ê¸°ìˆ ì— ì²˜ìŒ ì ‘ê·¼í•˜ëŠ” ë¶„\n\n## ì‹œë¦¬ì¦ˆ ì†Œê°œ\n\n| # | ì£¼ì œ | ìµìˆ™í•œ ê°œë…ê³¼ì˜ ì—°ê²° |\n|---|------|---------------------|\n| **1** | ì™œ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ì¸ê°€ | ìŠ¬ë¡œìš° ì¿¼ë¦¬, DB ë¶€í•˜ |\n| 2 | ë°ì´í„° ì•„í‚¤í…ì²˜ 101 | ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ |\n| 3 | Spark í•µì‹¬ ê°œë… | Goroutine, ThreadPoolExecutor |\n| 4 | Spark ë‚´ë¶€ ë™ì‘ | Task Queue, Worker Pool |\n| 5 | PySpark ì‹¤ì „ | ORM, Query Optimization |\n| 6 | Airflow í•µì‹¬ ê°œë… | CI/CD Pipeline, cron |\n| 7 | Airflow ì‹¤ì „ | GitHub Actions, ArgoCD |\n| 8 | Kafka í•µì‹¬ | Redis Streams |\n| 9 | Spark Streaming | Event-Driven Architecture |\n| 10 | ë ˆì´í¬í•˜ìš°ìŠ¤ | PostgreSQL ACID |\n| 11 | ë°ì´í„° ëª¨ë¸ë§ | ERD, ì •ê·œí™” |\n| 12 | ë°ì´í„° í’ˆì§ˆ | í…ŒìŠ¤íŠ¸ ìë™í™”, ëª¨ë‹ˆí„°ë§ |\n\n---\n\n## ìƒˆë²½ 3ì‹œì˜ ìŠ¬ë¡œìš° ì¿¼ë¦¬\n\nì–´ëŠ ë‚  ìƒˆë²½, ë‹¹ì‹ ì—ê²Œ ì•Œë¦¼ì´ ì˜µë‹ˆë‹¤.\n\n> \"í”„ë¡œë•ì…˜ DB CPU 100%, ì‘ë‹µ ì‹œê°„ 30ì´ˆ ì´ˆê³¼\"\n\nì›ì¸ì„ ì°¾ì•„ë³´ë‹ˆ, ë§ˆì¼€íŒ…íŒ€ì´ ìš”ì²­í•œ ëŒ€ì‹œë³´ë“œ ì¿¼ë¦¬ì˜€ìŠµë‹ˆë‹¤:\n\n```sql\nSELECT \n    DATE_TRUNC('month', created_at) AS month,\n    COUNT(*) AS user_count,\n    AVG(lifetime_value) AS avg_ltv,\n    SUM(purchase_amount) AS total_revenue\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE created_at >= '2024-01-01'\nGROUP BY DATE_TRUNC('month', created_at)\nORDER BY month;\n```\n\nì´ ì¿¼ë¦¬ í•˜ë‚˜ê°€ í”„ë¡œë•ì…˜ ì„œë¹„ìŠ¤ë¥¼ ë§ˆë¹„ì‹œì¼°ìŠµë‹ˆë‹¤. **ì™œì¼ê¹Œìš”?**\n\n---\n\n## OLTPì˜ í•œê³„: ì„¤ê³„ ëª©ì ì´ ë‹¤ë¥´ë‹¤\n\nìš°ë¦¬ê°€ ìµìˆ™í•œ PostgreSQL, MySQL ê°™ì€ ë°ì´í„°ë² ì´ìŠ¤ëŠ” **OLTP(Online Transaction Processing)**ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph OLTP [\"OLTP (íŠ¸ëœì­ì…˜ ì²˜ë¦¬)\"]\n        direction TB\n        A1[\"âœ… ë¹ ë¥¸ ë‹¨ê±´ ì¡°íšŒ<br/>SELECT * FROM users WHERE id = 123\"]\n        A2[\"âœ… ë¹ ë¥¸ ì‚½ì…/ìˆ˜ì •<br/>INSERT, UPDATE\"]\n        A3[\"âœ… ACID ë³´ì¥\"]\n    end\n    \n    subgraph OLAP [\"OLAP (ë¶„ì„ ì²˜ë¦¬)\"]\n        direction TB\n        B1[\"âœ… ëŒ€ëŸ‰ ë°ì´í„° ì§‘ê³„<br/>GROUP BY, SUM, AVG\"]\n        B2[\"âœ… ë³µì¡í•œ ì¡°ì¸\"]\n        B3[\"âœ… ê¸´ ì¿¼ë¦¬ í—ˆìš©\"]\n    end\n    \n    OLTP -.->|\"ì´ê±¸ë¡œ ë¶„ì„í•˜ë©´?\"| Problem[\"âŒ ìŠ¬ë¡œìš° ì¿¼ë¦¬<br/>âŒ ë½ ê²½ìŸ<br/>âŒ ì„œë¹„ìŠ¤ ì¥ì• \"]\n```\n\n### OLTP vs OLAP: ê·¼ë³¸ì  ì°¨ì´\n\n| íŠ¹ì„± | OLTP | OLAP |\n|------|------|------|\n| **ëª©ì ** | íŠ¸ëœì­ì…˜ ì²˜ë¦¬ | ë¶„ì„/ë¦¬í¬íŒ… |\n| **ì¿¼ë¦¬ íŒ¨í„´** | ë‹¨ê±´ ì¡°íšŒ/ìˆ˜ì • | ëŒ€ëŸ‰ ì§‘ê³„ |\n| **ë°ì´í„° ì €ì¥** | Row-based | Column-based |\n| **ì¸ë±ìŠ¤** | B-Tree (íŠ¹ì • í–‰ ì°¾ê¸°) | ì»¬ëŸ¼ ìŠ¤ìº”ì— ìµœì í™” |\n| **ë™ì‹œì„±** | ë†’ì€ ë™ì‹œì„±, ì§§ì€ íŠ¸ëœì­ì…˜ | ë‚®ì€ ë™ì‹œì„±, ê¸´ ì¿¼ë¦¬ |\n| **ì˜ˆì‹œ DB** | PostgreSQL, MySQL | BigQuery, Snowflake |\n\n### Row-based vs Column-based ì €ì¥\n\n```mermaid\nflowchart TB\n    subgraph RowBased [\"Row-Based Storage (PostgreSQL)\"]\n        direction TB\n        R1[\"Row 1: id=1, name='Kim', age=30, city='Seoul'\"]\n        R2[\"Row 2: id=2, name='Lee', age=25, city='Busan'\"]\n        R3[\"Row 3: id=3, name='Park', age=35, city='Seoul'\"]\n    end\n    \n    subgraph ColBased [\"Column-Based Storage (BigQuery)\"]\n        direction TB\n        C1[\"id: [1, 2, 3]\"]\n        C2[\"name: ['Kim', 'Lee', 'Park']\"]\n        C3[\"age: [30, 25, 35]\"]\n        C4[\"city: ['Seoul', 'Busan', 'Seoul']\"]\n    end\n    \n    Query[\"SELECT AVG(age) FROM users\"]\n    \n    RowBased -->|\"ì „ì²´ í–‰ ìŠ¤ìº” í•„ìš”\"| Slow[\"ğŸ¢ ëŠë¦¼\"]\n    ColBased -->|\"age ì»¬ëŸ¼ë§Œ ì½ìŒ\"| Fast[\"ğŸš€ ë¹ ë¦„\"]\n```\n\n**í•µì‹¬ ì¸ì‚¬ì´íŠ¸**: OLTP ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë¶„ì„ ì¿¼ë¦¬ë¥¼ ëŒë¦¬ëŠ” ê²ƒì€, ìŠ¤í¬ì¸ ì¹´ë¡œ ì´ì‚¬ì§ì„ ë‚˜ë¥´ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ê°€ëŠ¥ì€ í•˜ì§€ë§Œ, ì í•©í•œ ë„êµ¬ê°€ ì•„ë‹™ë‹ˆë‹¤.\n\n---\n\n## ë°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ ë“±ì¥\n\nê·¸ë ‡ë‹¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”? **ë°ì´í„°ë¥¼ ë¶„ì„ì— ì í•©í•œ í˜•íƒœë¡œ ë³µì‚¬í•´ ë‘ëŠ” ê²ƒ**ì…ë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph Source [\"ìš´ì˜ ì‹œìŠ¤í…œ\"]\n        DB1[(PostgreSQL)]\n        DB2[(MongoDB)]\n        API[REST APIs]\n    end\n    \n    subgraph Pipeline [\"ë°ì´í„° íŒŒì´í”„ë¼ì¸\"]\n        E[Extract<br/>ì¶”ì¶œ]\n        T[Transform<br/>ë³€í™˜]\n        L[Load<br/>ì ì¬]\n    end\n    \n    subgraph Dest [\"ë¶„ì„ ì‹œìŠ¤í…œ\"]\n        DW[(Data Warehouse<br/>BigQuery, Snowflake)]\n        BI[ëŒ€ì‹œë³´ë“œ<br/>Metabase, Looker]\n    end\n    \n    DB1 --> E\n    DB2 --> E\n    API --> E\n    E --> T --> L --> DW --> BI\n    \n    style Pipeline fill:#e1f5fe\n```\n\n### ì™œ ê·¸ëƒ¥ \"ë³µì‚¬\"ê°€ ì•„ë‹Œê°€?\n\në‹¨ìˆœíˆ `pg_dump`ë¡œ ë³µì‚¬í•˜ë©´ ì•ˆ ë ê¹Œìš”? ì‹¤ì œë¡œëŠ” ì´ëŸ° ë¬¸ì œë“¤ì´ ìˆìŠµë‹ˆë‹¤:\n\n1. **ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜**: ìš´ì˜ DB ìŠ¤í‚¤ë§ˆê°€ ë¶„ì„ì— ì í•©í•˜ì§€ ì•ŠìŒ\n2. **ë°ì´í„° ì •ì œ**: NULL ì²˜ë¦¬, íƒ€ì… ë³€í™˜, ì¤‘ë³µ ì œê±° í•„ìš”\n3. **ì¦ë¶„ ì²˜ë¦¬**: ë§¤ë²ˆ ì „ì²´ë¥¼ ë³µì‚¬í•˜ë©´ ë¹„íš¨ìœ¨ì \n4. **ì˜ì¡´ì„± ê´€ë¦¬**: í…Œì´ë¸” A â†’ B â†’ C ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•´ì•¼ í•¨\n5. **ì‹¤íŒ¨ ë³µêµ¬**: ì¤‘ê°„ì— ì‹¤íŒ¨í•˜ë©´ ì–´ë””ì„œë¶€í„° ë‹¤ì‹œ?\n6. **ëª¨ë‹ˆí„°ë§**: íŒŒì´í”„ë¼ì¸ì´ ì œëŒ€ë¡œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸\n\nì´ ëª¨ë“  ê²ƒì„ í•´ê²°í•˜ëŠ” ê²ƒì´ ë°”ë¡œ **ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§**ì…ë‹ˆë‹¤.\n\n---\n\n## ë°±ì—”ë“œ ê°œë°œìì—ê²Œ ìµìˆ™í•œ ê°œë…ê³¼ì˜ ì—°ê²°\n\në°ì´í„° ì—”ì§€ë‹ˆì–´ë§ì˜ ê°œë…ë“¤ì€ ë°±ì—”ë“œ ê°œë°œì—ì„œ ì´ë¯¸ ì ‘í•´ë³¸ ê²ƒë“¤ì˜ í™•ì¥ì…ë‹ˆë‹¤.\n\n### ë³‘ë ¬ ì²˜ë¦¬: Goroutine / ThreadPoolExecutor â†’ Spark\n\në°±ì—”ë“œì—ì„œ ì„±ëŠ¥ì„ ìœ„í•´ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í•˜ëŠ” ê²ƒì²˜ëŸ¼, ë°ì´í„° ì²˜ë¦¬ë„ ë³‘ë ¬í™”í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph Go [\"Go: Goroutine\"]\n        G1[\"goroutine 1\"]\n        G2[\"goroutine 2\"]\n        G3[\"goroutine 3\"]\n        G4[\"goroutine N\"]\n    end\n    \n    subgraph Python [\"Python: ThreadPoolExecutor\"]\n        P1[\"Thread 1\"]\n        P2[\"Thread 2\"]\n        P3[\"Thread 3\"]\n        P4[\"Thread N\"]\n    end\n    \n    subgraph Spark [\"Spark: Distributed Processing\"]\n        S1[\"Executor 1<br/>(ë‹¤ë¥¸ ì„œë²„)\"]\n        S2[\"Executor 2<br/>(ë‹¤ë¥¸ ì„œë²„)\"]\n        S3[\"Executor 3<br/>(ë‹¤ë¥¸ ì„œë²„)\"]\n        S4[\"Executor N<br/>(ë‹¤ë¥¸ ì„œë²„)\"]\n    end\n    \n    SingleServer[\"ë‹¨ì¼ ì„œë²„ ë‚´ ë³‘ë ¬í™”\"]\n    MultiServer[\"ì—¬ëŸ¬ ì„œë²„ë¡œ ë¶„ì‚°\"]\n    \n    Go --> SingleServer\n    Python --> SingleServer\n    SingleServer -.->|\"ë°ì´í„°ê°€ ë„ˆë¬´ í¬ë©´?\"| MultiServer\n    MultiServer --> Spark\n```\n\n**í•µì‹¬ ì°¨ì´**:\n\n- Goroutine/ThreadPoolExecutor: **ë‹¨ì¼ ì„œë²„** ë‚´ì—ì„œ CPU ì½”ì–´ë¥¼ í™œìš©\n- Spark: **ì—¬ëŸ¬ ì„œë²„**ì— ê±¸ì³ ë°ì´í„°ì™€ ì—°ì‚°ì„ ë¶„ì‚°\n\n### ìŠ¤ì¼€ì¤„ë§: cron / GitHub Actions â†’ Airflow\n\nì •ê¸°ì ì¸ ì‘ì—… ì‹¤í–‰ì„ ê´€ë¦¬í•˜ëŠ” ê²ƒë„ ë¹„ìŠ·í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph Cron [\"cron / GitHub Actions\"]\n        C1[\"Task A (00:00)\"]\n        C2[\"Task B (01:00)\"]\n        C3[\"Task C (02:00)\"]\n        C1 -.->|\"ì˜ì¡´ì„± ê´€ë¦¬?\"| Problem1[\"âŒ ìˆ˜ë™ ê´€ë¦¬\"]\n        C2 -.->|\"ì‹¤íŒ¨ ì‹œ?\"| Problem2[\"âŒ ìˆ˜ë™ ì¬ì‹¤í–‰\"]\n    end\n    \n    subgraph Airflow [\"Apache Airflow\"]\n        direction LR\n        A1[\"Task A\"] --> A2[\"Task B\"] --> A3[\"Task C\"]\n        A1 --> A4[\"Task D\"] --> A3\n    end\n    \n    Airflow -->|\"ì˜ì¡´ì„±\"| Dep[\"âœ… DAGë¡œ ìë™ ê´€ë¦¬\"]\n    Airflow -->|\"ì‹¤íŒ¨ ì‹œ\"| Retry[\"âœ… ìë™ ì¬ì‹œë„\"]\n    Airflow -->|\"ëª¨ë‹ˆí„°ë§\"| UI[\"âœ… ì›¹ UI ì œê³µ\"]\n```\n\n### ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¬ë°: Redis Streams â†’ Kafka\n\nRedis Streamsë¥¼ ì‚¬ìš©í•´ ë´¤ë‹¤ë©´, Kafkaì˜ ê°œë…ì´ ìµìˆ™í•  ê²ƒì…ë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph Redis [\"Redis Streams\"]\n        RS[\"Stream: orders\"]\n        RC1[\"Consumer Group A\"]\n        RC2[\"Consumer Group B\"]\n        RS --> RC1\n        RS --> RC2\n    end\n    \n    subgraph Kafka [\"Apache Kafka\"]\n        KT[\"Topic: orders<br/>(Partitioned)\"]\n        KC1[\"Consumer Group A\"]\n        KC2[\"Consumer Group B\"]\n        KT --> KC1\n        KT --> KC2\n    end\n    \n    Redis -.->|\"ë°ì´í„° ê·œëª¨ê°€ ì»¤ì§€ë©´\"| Kafka\n```\n\n| íŠ¹ì„± | Redis Streams | Kafka |\n|------|---------------|-------|\n| **ì„¤ê³„ ëª©ì ** | ìºì‹œ + ê°€ë²¼ìš´ ìŠ¤íŠ¸ë¦¬ë° | ëŒ€ìš©ëŸ‰ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° |\n| **ë°ì´í„° ë³´ì¡´** | ë©”ëª¨ë¦¬ ê¸°ë°˜ (ì œí•œì ) | ë””ìŠ¤í¬ ê¸°ë°˜ (ë¬´ì œí•œ) |\n| **í™•ì¥ì„±** | ìˆ˜ì§ í™•ì¥ ìœ„ì£¼ | ìˆ˜í‰ í™•ì¥ (íŒŒí‹°ì…”ë‹) |\n| **ì²˜ë¦¬ëŸ‰** | ìˆ˜ë§Œ TPS | ìˆ˜ë°±ë§Œ TPS |\n| **ë³µì œ** | Master-Replica | Multi-broker ë³µì œ |\n\n---\n\n## ë°ì´í„° ì—”ì§€ë‹ˆì–´ vs ë°±ì—”ë“œ ê°œë°œì\n\n```mermaid\nflowchart TB\n    subgraph Backend [\"ë°±ì—”ë“œ ê°œë°œì ì˜ì—­\"]\n        BE1[\"API ì„œë²„\"]\n        BE2[\"ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§\"]\n        BE3[\"ìš´ì˜ DB\"]\n        BE4[\"ìºì‹œ\"]\n    end\n    \n    subgraph DE [\"ë°ì´í„° ì—”ì§€ë‹ˆì–´ ì˜ì—­\"]\n        DE1[\"ë°ì´í„° íŒŒì´í”„ë¼ì¸\"]\n        DE2[\"ë¶„ì„ í”Œë«í¼\"]\n        DE3[\"ML ì¸í”„ë¼\"]\n        DE4[\"ë°ì´í„° í’ˆì§ˆ\"]\n    end\n    \n    subgraph Overlap [\"ê²¹ì¹˜ëŠ” ì˜ì—­\"]\n        O1[\"ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°\"]\n        O2[\"ë¡œê·¸ ìˆ˜ì§‘\"]\n        O3[\"ë©”íŠ¸ë¦­/ëª¨ë‹ˆí„°ë§\"]\n    end\n    \n    Backend --- Overlap --- DE\n    \n    style Overlap fill:#fff3e0\n```\n\n### ì™œ ë°±ì—”ë“œ ê°œë°œìë„ ì•Œì•„ì•¼ í•˜ëŠ”ê°€?\n\n1. **í˜‘ì—…**: ë°ì´í„° íŒ€ê³¼ íš¨ê³¼ì ìœ¼ë¡œ ì†Œí†µí•˜ë ¤ë©´ ê·¸ë“¤ì˜ ì–¸ì–´ë¥¼ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤\n2. **ì„¤ê³„**: ë°ì´í„° ì¶”ì¶œì´ ìš©ì´í•œ APIì™€ ì´ë²¤íŠ¸ ì„¤ê³„ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n3. **ë¬¸ì œ í•´ê²°**: \"ì™œ ëŒ€ì‹œë³´ë“œ ìˆ«ìê°€ ë‹¤ë¥´ì£ ?\" ê°™ì€ ì§ˆë¬¸ì— í•¨ê»˜ ë‹µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n4. **ì»¤ë¦¬ì–´**: Full-stack Data Engineerì˜ ê°€ì¹˜ê°€ ë†’ì•„ì§€ê³  ìˆìŠµë‹ˆë‹¤\n\n---\n\n## ì´ ì‹œë¦¬ì¦ˆì—ì„œ ë°°ìš¸ ê²ƒë“¤\n\n```mermaid\nflowchart TB\n    subgraph Part1 [\"Part 1: ê°œë…\"]\n        P1[\"#1 ì™œ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§?\"]\n        P2[\"#2 ë°ì´í„° ì•„í‚¤í…ì²˜ ì „ì²´ ê·¸ë¦¼\"]\n    end\n    \n    subgraph Part2 [\"Part 2: Spark\"]\n        P3[\"#3 Spark í•µì‹¬ ê°œë…\"]\n        P4[\"#4 ë‚´ë¶€ ë™ì‘ ì›ë¦¬\"]\n        P5[\"#5 PySpark ì‹¤ì „\"]\n    end\n    \n    subgraph Part3 [\"Part 3: Airflow\"]\n        P6[\"#6 Airflow í•µì‹¬ ê°œë…\"]\n        P7[\"#7 í”„ë¡œë•ì…˜ íŒŒì´í”„ë¼ì¸\"]\n    end\n    \n    subgraph Part4 [\"Part 4: ìŠ¤íŠ¸ë¦¬ë°\"]\n        P8[\"#8 Kafka í•µì‹¬\"]\n        P9[\"#9 Spark Streaming\"]\n    end\n    \n    subgraph Part5 [\"Part 5: ì €ì¥ì†Œ\"]\n        P10[\"#10 ë ˆì´í¬í•˜ìš°ìŠ¤\"]\n        P11[\"#11 ë°ì´í„° ëª¨ë¸ë§\"]\n    end\n    \n    subgraph Part6 [\"Part 6: ìš´ì˜\"]\n        P12[\"#12 ë°ì´í„° í’ˆì§ˆ\"]\n    end\n    \n    Part1 --> Part2 --> Part3 --> Part4 --> Part5 --> Part6\n```\n\nê° í¸ì—ì„œëŠ”:\n\n- **\"ì™œ?\"**ì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤ - ê¸°ìˆ ì´ í•´ê²°í•˜ëŠ” ë¬¸ì œ\n- **ì‹œê°í™”**ë¡œ êµ¬ì¡°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤ - Mermaid ë‹¤ì´ì–´ê·¸ë¨\n- **ìµìˆ™í•œ ê°œë…ê³¼ ì—°ê²°**í•©ë‹ˆë‹¤ - Go, Python ê²½í—˜ í™œìš©\n- **ì‹¤ì „ ì˜ˆì œ**ë¡œ ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤ - ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ ì½”ë“œ\n\n---\n\n## ì •ë¦¬\n\n| ê°œë… | ì„¤ëª… |\n|------|------|\n| **OLTP** | íŠ¸ëœì­ì…˜ ì²˜ë¦¬ì— ìµœì í™” (PostgreSQL, MySQL) |\n| **OLAP** | ë¶„ì„ ì²˜ë¦¬ì— ìµœì í™” (BigQuery, Snowflake) |\n| **ë°ì´í„° íŒŒì´í”„ë¼ì¸** | ë°ì´í„°ë¥¼ ì¶”ì¶œ â†’ ë³€í™˜ â†’ ì ì¬í•˜ëŠ” ìë™í™”ëœ íë¦„ |\n| **ETL** | Extract, Transform, Load |\n| **Row vs Column Storage** | í–‰ ê¸°ë°˜(ë‹¨ê±´ ì¡°íšŒ) vs ì—´ ê¸°ë°˜(ì§‘ê³„ ë¶„ì„) |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**2í¸: ë°ì´í„° ì•„í‚¤í…ì²˜ 101**ì—ì„œëŠ” ë°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ ì „ì²´ êµ¬ì¡°ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤:\n\n- ETL vs ELT íŒ¨ëŸ¬ë‹¤ì„\n- ë°°ì¹˜ vs ìŠ¤íŠ¸ë¦¬ë°\n- Lambda vs Kappa ì•„í‚¤í…ì²˜\n- Modern Data Stack ì†Œê°œ\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- Martin Kleppmann, \"Designing Data-Intensive Applications\" (O'Reilly)\n- Maxime Beauchemin, \"The Rise of the Data Engineer\" (Airbnb Engineering Blog)\n- [OLTP vs OLAP: What's the Difference?](https://www.ibm.com/topics/oltp)",
    "docType": "original",
    "category": "Data Engineering",
    "tags": [
      "Backend",
      "Data Engineering",
      "Data Pipeline",
      "OLAP",
      "OLTP"
    ],
    "readingTime": 7,
    "wordCount": 1316,
    "isFeatured": false,
    "isPublic": true,
    "series": "data-engineering",
    "date": "2026-01-02"
  },
  {
    "id": "interface-design-principles",
    "slug": "interface-design-principles",
    "path": "backend/go",
    "fullPath": "backend/go/interface-design-principles",
    "title": "Go ì¸í„°í˜ì´ìŠ¤ ì„¤ê³„ ì›ì¹™ - Accept Interfaces, Return Structs",
    "excerpt": "'Accept interfaces, return structs' ì›ì¹™ê³¼ í¬ì¸í„° vs ê°’ ìˆ˜ì‹ ì ì„ íƒ ê¸°ì¤€ì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# Go ì¸í„°í˜ì´ìŠ¤ ì„¤ê³„ ì›ì¹™\n\n## ê°œìš”\n\nGoì˜ **\"Accept interfaces, return structs\"** ì›ì¹™ì€ ìœ ì—°í•˜ê³  í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•œ ì½”ë“œë¥¼ ì‘ì„±í•˜ëŠ” í•µì‹¬ ê°€ì´ë“œë¼ì¸ì…ë‹ˆë‹¤. ì´ ê¸€ì—ì„œëŠ” ì´ ì›ì¹™ì˜ ì² í•™ê³¼ ì‹¤ì „ ì ìš©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n\n## Accept Interfaces, Return Structs\n\n### ì›ì¹™ ì„¤ëª…\n\n```go\n// âœ… ì¢‹ì€ ì˜ˆ: ì¸í„°í˜ì´ìŠ¤ë¥¼ ë°›ê³ , êµ¬ì²´ íƒ€ì…ì„ ë°˜í™˜\nfunc NewUserService(repo UserRepository) *UserService {\n    return &UserService{repo: repo}\n}\n\n// âŒ ë‚˜ìœ ì˜ˆ: êµ¬ì²´ íƒ€ì…ì„ ë°›ìŒ\nfunc NewUserService(repo *PostgresUserRepo) *UserService {\n    return &UserService{repo: repo}\n}\n```\n\n### ì™œ ì´ë ‡ê²Œ í•´ì•¼ í• ê¹Œ?\n\n| ê´€ì  | ì¸í„°í˜ì´ìŠ¤ ìˆ˜ìš© | êµ¬ì²´ íƒ€ì… ë°˜í™˜ |\n|-----|---------------|--------------|\n| **ìœ ì—°ì„±** | ì–´ë–¤ êµ¬í˜„ì²´ë“  ì£¼ì… ê°€ëŠ¥ | í˜¸ì¶œìê°€ êµ¬ì²´ ë©”ì„œë“œ ì ‘ê·¼ ê°€ëŠ¥ |\n| **í…ŒìŠ¤íŠ¸** | Mock ì‰½ê²Œ ì£¼ì… | íƒ€ì… ë‹¨ì–¸ ì—†ì´ ì‚¬ìš© |\n| **ê²°í•©ë„** | ë‚®ìŒ (êµ¬í˜„ì— ë…ë¦½ì ) | API ëª…í™•ì„± |\n\n### ì‹¤ì „ ì˜ˆì‹œ\n\n```go\n// ì¸í„°í˜ì´ìŠ¤ ì •ì˜ (ì†Œë¹„ì ì¸¡ì—ì„œ ì •ì˜)\ntype UserRepository interface {\n    FindByID(ctx context.Context, id string) (*User, error)\n    Save(ctx context.Context, user *User) error\n}\n\n// êµ¬ì²´ êµ¬í˜„\ntype PostgresUserRepo struct {\n    db *sql.DB\n}\n\nfunc NewPostgresUserRepo(db *sql.DB) *PostgresUserRepo {\n    return &PostgresUserRepo{db: db}\n}\n\nfunc (r *PostgresUserRepo) FindByID(ctx context.Context, id string) (*User, error) {\n    // êµ¬í˜„\n}\n\nfunc (r *PostgresUserRepo) Save(ctx context.Context, user *User) error {\n    // êµ¬í˜„\n}\n\n// ì¶”ê°€ ë©”ì„œë“œ (ì¸í„°í˜ì´ìŠ¤ì— ì—†ìŒ)\nfunc (r *PostgresUserRepo) BulkInsert(ctx context.Context, users []*User) error {\n    // PostgreSQL ì „ìš© ìµœì í™”\n}\n\n// ì„œë¹„ìŠ¤ - ì¸í„°í˜ì´ìŠ¤ë¥¼ ë°›ìŒ\ntype UserService struct {\n    repo UserRepository\n}\n\nfunc NewUserService(repo UserRepository) *UserService {\n    return &UserService{repo: repo}\n}\n```\n\n## ì¸í„°í˜ì´ìŠ¤ ì •ì˜ ìœ„ì¹˜\n\n### ì†Œë¹„ì ì¸¡ì—ì„œ ì •ì˜ (ê¶Œì¥)\n\n[Go Wiki](https://go.dev/wiki/CodeReviewComments#interfaces)ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê¶Œì¥í•©ë‹ˆë‹¤:\n\n> \"Go interfaces generally belong in the package that **uses** values of the interface type, not the package that implements those values.\"\n\n```go\n// âŒ êµ¬í˜„ìê°€ ì¸í„°í˜ì´ìŠ¤ ì •ì˜ (Java ìŠ¤íƒ€ì¼)\n// repository/interfaces.go\ntype UserRepository interface { ... }\n\n// repository/postgres.go\ntype PostgresUserRepo struct { ... }\n\n// âœ… ì†Œë¹„ìê°€ ì¸í„°í˜ì´ìŠ¤ ì •ì˜ (Go ìŠ¤íƒ€ì¼)\n// service/user.go\ntype UserRepository interface {\n    FindByID(ctx context.Context, id string) (*User, error)\n}\n\ntype UserService struct {\n    repo UserRepository\n}\n```\n\n> [!IMPORTANT]\n> Goì˜ ì•”ë¬µì  ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„ ë•ë¶„ì—, ì†Œë¹„ìê°€ í•„ìš”í•œ ë©”ì„œë“œë§Œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n### ì‘ì€ ì¸í„°í˜ì´ìŠ¤ ì„ í˜¸\n\nRob Pikeì˜ [Go Proverbs](https://go-proverbs.github.io/)ì—ì„œ:\n\n> \"The bigger the interface, the weaker the abstraction.\"\n\n```go\n// âŒ ë„ˆë¬´ í° ì¸í„°í˜ì´ìŠ¤\ntype UserRepository interface {\n    FindByID(id string) (*User, error)\n    FindByEmail(email string) (*User, error)\n    FindAll() ([]*User, error)\n    Save(user *User) error\n    Delete(id string) error\n    UpdateProfile(id string, profile *Profile) error\n    // ... 10ê°œ ë”\n}\n\n// âœ… ì‘ì€ ì¸í„°í˜ì´ìŠ¤ (ì¸í„°í˜ì´ìŠ¤ ë¶„ë¦¬ ì›ì¹™)\ntype UserFinder interface {\n    FindByID(id string) (*User, error)\n}\n\ntype UserSaver interface {\n    Save(user *User) error\n}\n\n// í•„ìš”í•˜ë©´ ì¡°í•©\ntype UserRepository interface {\n    UserFinder\n    UserSaver\n}\n```\n\n## í¬ì¸í„° vs ê°’ ìˆ˜ì‹ ì\n\n### ê¸°ë³¸ ê°€ì´ë“œë¼ì¸\n\n| ìƒí™© | ì„ íƒ | ì´ìœ  |\n|-----|-----|-----|\n| ìƒíƒœ ë³€ê²½ í•„ìš” | `*T` í¬ì¸í„° | ì›ë³¸ ìˆ˜ì • ê°€ëŠ¥ |\n| í° êµ¬ì¡°ì²´ | `*T` í¬ì¸í„° | ë³µì‚¬ ë¹„ìš© ì ˆê° |\n| ì‘ì€ ë¶ˆë³€ ê°’ | `T` ê°’ | ì•ˆì „í•˜ê³  ê°„ë‹¨ |\n| ì¼ê´€ì„± ìœ ì§€ | í•˜ë‚˜ë¡œ í†µì¼ | í˜¼ë€ ë°©ì§€ |\n\n### ì˜ˆì‹œ\n\n```go\n// ê°’ ìˆ˜ì‹ ì - ì‘ê³  ë¶ˆë³€\ntype Point struct {\n    X, Y int\n}\n\nfunc (p Point) Distance() float64 {\n    return math.Sqrt(float64(p.X*p.X + p.Y*p.Y))\n}\n\n// í¬ì¸í„° ìˆ˜ì‹ ì - ìƒíƒœ ë³€ê²½\ntype Counter struct {\n    value int\n}\n\nfunc (c *Counter) Increment() {\n    c.value++\n}\n\nfunc (c *Counter) Value() int {\n    return c.value\n}\n```\n\n### ì¸í„°í˜ì´ìŠ¤ì™€ í¬ì¸í„°/ê°’\n\n```go\ntype Stringer interface {\n    String() string\n}\n\ntype MyType struct {\n    Name string\n}\n\n// ê°’ ìˆ˜ì‹ ìë¡œ ì •ì˜\nfunc (m MyType) String() string {\n    return m.Name\n}\n\nvar s Stringer\n\ns = MyType{Name: \"hello\"}  // âœ… ê°’ í• ë‹¹ ê°€ëŠ¥\ns = &MyType{Name: \"world\"} // âœ… í¬ì¸í„°ë„ í• ë‹¹ ê°€ëŠ¥\n```\n\n```go\n// í¬ì¸í„° ìˆ˜ì‹ ìë¡œ ì •ì˜\nfunc (m *MyType) String() string {\n    return m.Name\n}\n\ns = MyType{Name: \"hello\"}  // âŒ ì»´íŒŒì¼ ì—ëŸ¬!\ns = &MyType{Name: \"world\"} // âœ… í¬ì¸í„°ë§Œ í• ë‹¹ ê°€ëŠ¥\n```\n\n> [!WARNING]\n> í¬ì¸í„° ìˆ˜ì‹ ìë¡œ ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•˜ë©´, ê°’ì€ í•´ë‹¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ ë§Œì¡±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n### ì¼ê´€ì„± ê·œì¹™\n\n```go\n// âŒ í˜¼í•© ì‚¬ìš© - í˜¼ë€ìŠ¤ëŸ¬ì›€\nfunc (u User) Name() string { ... }\nfunc (u *User) SetName(name string) { ... }\nfunc (u User) Age() int { ... }\nfunc (u *User) SetAge(age int) { ... }\n\n// âœ… í†µì¼ - ëª…í™•í•¨\nfunc (u *User) Name() string { ... }\nfunc (u *User) SetName(name string) { ... }\nfunc (u *User) Age() int { ... }\nfunc (u *User) SetAge(age int) { ... }\n```\n\n## ì¸í„°í˜ì´ìŠ¤ ë°˜í™˜ì´ ì í•©í•œ ê²½ìš°\n\nì¼ë°˜ì ìœ¼ë¡œ êµ¬ì²´ íƒ€ì…ì„ ë°˜í™˜í•˜ì§€ë§Œ, ì˜ˆì™¸ ìƒí™©ë„ ìˆìŠµë‹ˆë‹¤.\n\n### 1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¸í„°í˜ì´ìŠ¤\n\n```go\n// io.Reader, io.Writer ë“± í‘œì¤€ ì¸í„°í˜ì´ìŠ¤\nfunc NewReader(data []byte) io.Reader {\n    return bytes.NewReader(data)\n}\n```\n\n### 2. íŒ©í† ë¦¬ íŒ¨í„´\n\n```go\ntype Database interface {\n    Query(query string) ([]Row, error)\n    Close() error\n}\n\n// ì„¤ì •ì— ë”°ë¼ ë‹¤ë¥¸ êµ¬í˜„ ë°˜í™˜\nfunc NewDatabase(config Config) (Database, error) {\n    switch config.Driver {\n    case \"postgres\":\n        return newPostgresDB(config)\n    case \"mysql\":\n        return newMySQLDB(config)\n    default:\n        return nil, errors.New(\"unknown driver\")\n    }\n}\n```\n\n### 3. ë‚´ë¶€ êµ¬í˜„ ìˆ¨ê¸°ê¸°\n\n```go\n// unexported êµ¬í˜„\ntype client struct {\n    httpClient *http.Client\n}\n\n// exported ì¸í„°í˜ì´ìŠ¤\ntype Client interface {\n    Get(url string) (*Response, error)\n}\n\nfunc NewClient() Client {\n    return &client{httpClient: &http.Client{}}\n}\n```\n\n## í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì¸í„°í˜ì´ìŠ¤\n\n### Mock êµ¬í˜„\n\n```go\n// í…ŒìŠ¤íŠ¸ìš© Mock\ntype MockUserRepository struct {\n    FindByIDFunc func(ctx context.Context, id string) (*User, error)\n    SaveFunc     func(ctx context.Context, user *User) error\n}\n\nfunc (m *MockUserRepository) FindByID(ctx context.Context, id string) (*User, error) {\n    return m.FindByIDFunc(ctx, id)\n}\n\nfunc (m *MockUserRepository) Save(ctx context.Context, user *User) error {\n    return m.SaveFunc(ctx, user)\n}\n\n// í…ŒìŠ¤íŠ¸\nfunc TestUserService_GetUser(t *testing.T) {\n    mockRepo := &MockUserRepository{\n        FindByIDFunc: func(ctx context.Context, id string) (*User, error) {\n            return &User{ID: id, Name: \"Test User\"}, nil\n        },\n    }\n    \n    service := NewUserService(mockRepo)\n    \n    user, err := service.GetUser(context.Background(), \"123\")\n    assert.NoError(t, err)\n    assert.Equal(t, \"Test User\", user.Name)\n}\n```\n\n### gomock ì‚¬ìš© (ê¶Œì¥)\n\n[go.uber.org/mock](https://github.com/uber-go/mock)ì„ í™œìš©í•œ ì½”ë“œ ìƒì„± ë°©ì‹:\n\n```bash\n# mockgen ì„¤ì¹˜\ngo install go.uber.org/mock/mockgen@latest\n\n# ì¸í„°í˜ì´ìŠ¤ì—ì„œ Mock ìƒì„±\nmockgen -source=repository.go -destination=mocks/repository_mock.go -package=mocks\n```\n\n```go\n// ìƒì„±ëœ mocks/repository_mock.go ì‚¬ìš©\nimport (\n    \"testing\"\n    \"go.uber.org/mock/gomock\"\n    \"myapp/mocks\"\n)\n\nfunc TestUserService_GetUser(t *testing.T) {\n    ctrl := gomock.NewController(t)\n    defer ctrl.Finish()\n    \n    mockRepo := mocks.NewMockUserRepository(ctrl)\n    \n    // ê¸°ëŒ€ ë™ì‘ ì„¤ì •\n    mockRepo.EXPECT().\n        FindByID(gomock.Any(), \"123\").\n        Return(&User{ID: \"123\", Name: \"Test User\"}, nil)\n    \n    service := NewUserService(mockRepo)\n    user, err := service.GetUser(context.Background(), \"123\")\n    \n    assert.NoError(t, err)\n    assert.Equal(t, \"Test User\", user.Name)\n}\n```\n\n> [!TIP]\n> gomockì€ ì»´íŒŒì¼ íƒ€ì„ì— íƒ€ì… ì•ˆì „ì„±ì„ ë³´ì¥í•˜ë©°, IDE ìë™ì™„ì„±ë„ ì§€ì›ë©ë‹ˆë‹¤.\n\n## ì²´í¬ë¦¬ìŠ¤íŠ¸\n\n### ì¸í„°í˜ì´ìŠ¤ ì„¤ê³„\n\n- [ ] ì†Œë¹„ì ì¸¡ì—ì„œ ì¸í„°í˜ì´ìŠ¤ ì •ì˜\n- [ ] ê°€ëŠ¥í•œ ì‘ì€ ì¸í„°í˜ì´ìŠ¤ (1-3 ë©”ì„œë“œ)\n- [ ] í•¨ìˆ˜ íŒŒë¼ë¯¸í„°ë¡œ ì¸í„°í˜ì´ìŠ¤ ìˆ˜ìš©\n- [ ] êµ¬ì²´ íƒ€ì… ë°˜í™˜ (íŠ¹ë³„í•œ ì´ìœ  ì—†ìœ¼ë©´)\n\n### ìˆ˜ì‹ ì ì„ íƒ\n\n- [ ] ìƒíƒœ ë³€ê²½ í•„ìš”í•˜ë©´ í¬ì¸í„° ìˆ˜ì‹ ì\n- [ ] struct í¬ê¸° > 64 bytesë©´ í¬ì¸í„° ìˆ˜ì‹ ì\n- [ ] ë™ì¼ íƒ€ì…ì—ì„œ ìˆ˜ì‹ ì í†µì¼\n- [ ] ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„ ì‹œ í¬ì¸í„°/ê°’ ì£¼ì˜\n\n## ì°¸ê³  ìë£Œ\n\n- [Go Wiki: CodeReviewComments](https://go.dev/wiki/CodeReviewComments#interfaces)\n- [Effective Go: Interfaces](https://go.dev/doc/effective_go#interfaces)\n- [Go Proverbs: The bigger the interface, the weaker the abstraction](https://go-proverbs.github.io/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Design Patterns",
      "Go",
      "Interface"
    ],
    "readingTime": 6,
    "wordCount": 1120,
    "isFeatured": false,
    "isPublic": true,
    "date": "2026-01-02"
  },
  {
    "id": "error-handling-strategy",
    "slug": "error-handling-strategy",
    "path": "backend/go",
    "fullPath": "backend/go/error-handling-strategy",
    "title": "Go ì—ëŸ¬ í•¸ë“¤ë§ ì „ëµ ì™„ë²½ ê°€ì´ë“œ",
    "excerpt": "errors.Is, errors.As, ì»¤ìŠ¤í…€ ì—ëŸ¬ íƒ€ì…, ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ë¥¼ í™œìš©í•œ Goì˜ íš¨ê³¼ì ì¸ ì—ëŸ¬ í•¸ë“¤ë§ ì „ëµì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# Go ì—ëŸ¬ í•¸ë“¤ë§ ì „ëµ ì™„ë²½ ê°€ì´ë“œ\n\n## ê°œìš”\n\nGoì˜ ì—ëŸ¬ ì²˜ë¦¬ëŠ” ëª…ì‹œì ì´ê³  ê°’ ê¸°ë°˜ì…ë‹ˆë‹¤. Go 1.13ë¶€í„° ë„ì…ëœ ì—ëŸ¬ ë˜í•‘(wrapping)ê³¼ `errors.Is`, `errors.As`ë¥¼ í™œìš©í•˜ë©´ ì²´ê³„ì ì¸ ì—ëŸ¬ í•¸ë“¤ë§ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\n## ì—ëŸ¬ ë˜í•‘ (Error Wrapping)\n\n### fmt.Errorfì™€ %w\n\n```go\nimport (\n    \"errors\"\n    \"fmt\"\n)\n\nfunc readConfig(path string) error {\n    data, err := os.ReadFile(path)\n    if err != nil {\n        // %wë¡œ ì›ë³¸ ì—ëŸ¬ ë˜í•‘ (ì²´ì¸ ìœ ì§€)\n        return fmt.Errorf(\"config íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ [%s]: %w\", path, err)\n    }\n    return nil\n}\n```\n\n> [!IMPORTANT]\n> `%v` ëŒ€ì‹  `%w`ë¥¼ ì‚¬ìš©í•´ì•¼ ì›ë³¸ ì—ëŸ¬ ì²´ì¸ì´ ìœ ì§€ë©ë‹ˆë‹¤. `%v`ëŠ” ë¬¸ìì—´ë¡œë§Œ ë³€í™˜ë©ë‹ˆë‹¤.\n\n### ì—ëŸ¬ ì²´ì¸ ì–¸ë˜í•‘\n\n```go\nerr := readConfig(\"config.yaml\")\nif err != nil {\n    // ê°€ì¥ ë°”ê¹¥ ì—ëŸ¬ ë©”ì‹œì§€\n    fmt.Println(err)\n    // â†’ config íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ [config.yaml]: open config.yaml: no such file or directory\n    \n    // ì›ë³¸ ì—ëŸ¬ ì¶”ì¶œ\n    unwrapped := errors.Unwrap(err)\n    fmt.Println(unwrapped)\n    // â†’ open config.yaml: no such file or directory\n}\n```\n\n## errors.Is - ì—ëŸ¬ ë™ë“±ì„± ê²€ì‚¬\n\n`errors.Is`ëŠ” ì—ëŸ¬ ì²´ì¸ ì „ì²´ë¥¼ ìˆœíšŒí•˜ë©° **íŠ¹ì • ì—ëŸ¬ì™€ ë™ë“±í•œì§€** ê²€ì‚¬í•©ë‹ˆë‹¤.\n\n### ì„¼í‹°ë„¬ ì—ëŸ¬ ë¹„êµ\n\n```go\nimport (\n    \"errors\"\n    \"io\"\n    \"os\"\n)\n\nvar ErrNotFound = errors.New(\"ë¦¬ì†ŒìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\")\n\nfunc findUser(id string) (*User, error) {\n    user, err := db.Find(id)\n    if err != nil {\n        if errors.Is(err, sql.ErrNoRows) {\n            return nil, ErrNotFound  // ë„ë©”ì¸ ì—ëŸ¬ë¡œ ë³€í™˜\n        }\n        return nil, fmt.Errorf(\"DB ì¡°íšŒ ì‹¤íŒ¨: %w\", err)\n    }\n    return user, nil\n}\n\n// í˜¸ì¶œì¸¡\nuser, err := findUser(\"123\")\nif errors.Is(err, ErrNotFound) {\n    // 404 ì‘ë‹µ\n}\n```\n\n### ì²´ì¸ ë‚´ ì—ëŸ¬ ê²€ì‚¬\n\n```go\nfunc processFile(path string) error {\n    data, err := os.ReadFile(path)\n    if err != nil {\n        return fmt.Errorf(\"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: %w\", err)\n    }\n    // ...\n}\n\nerr := processFile(\"data.txt\")\nif errors.Is(err, os.ErrNotExist) {\n    fmt.Println(\"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤\")\n}\nif errors.Is(err, os.ErrPermission) {\n    fmt.Println(\"ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤\")\n}\n```\n\n### Is ë©”ì„œë“œ ì»¤ìŠ¤í„°ë§ˆì´ì§•\n\n```go\ntype TemporaryError struct {\n    Msg string\n}\n\nfunc (e *TemporaryError) Error() string { return e.Msg }\n\n// Is ë©”ì„œë“œ êµ¬í˜„ìœ¼ë¡œ ì»¤ìŠ¤í…€ ë¹„êµ ë¡œì§\nfunc (e *TemporaryError) Is(target error) bool {\n    _, ok := target.(*TemporaryError)\n    return ok\n}\n\n// ì‚¬ìš©\nvar errTemp = &TemporaryError{Msg: \"ì¼ì‹œì  ì˜¤ë¥˜\"}\nerr := fmt.Errorf(\"ì‘ì—… ì‹¤íŒ¨: %w\", errTemp)\n\nif errors.Is(err, &TemporaryError{}) {\n    // ì¬ì‹œë„ ë¡œì§\n}\n```\n\n## errors.As - ì—ëŸ¬ íƒ€ì… ì¶”ì¶œ\n\n`errors.As`ëŠ” ì—ëŸ¬ ì²´ì¸ì—ì„œ **íŠ¹ì • íƒ€ì…ì˜ ì—ëŸ¬ë¥¼ ì¶”ì¶œ**í•©ë‹ˆë‹¤.\n\n### ê¸°ë³¸ ì‚¬ìš©ë²•\n\n```go\ntype ValidationError struct {\n    Field   string\n    Message string\n}\n\nfunc (e *ValidationError) Error() string {\n    return fmt.Sprintf(\"ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨ [%s]: %s\", e.Field, e.Message)\n}\n\nfunc validateUser(u User) error {\n    if u.Email == \"\" {\n        return &ValidationError{Field: \"email\", Message: \"í•„ìˆ˜ ì…ë ¥ê°’ì…ë‹ˆë‹¤\"}\n    }\n    return nil\n}\n\n// ì—ëŸ¬ íƒ€ì… ì¶”ì¶œ\nerr := validateUser(User{})\nvar validErr *ValidationError\nif errors.As(err, &validErr) {\n    fmt.Printf(\"í•„ë“œ: %s, ë©”ì‹œì§€: %s\\n\", validErr.Field, validErr.Message)\n}\n```\n\n### ë˜í•‘ëœ ì—ëŸ¬ì—ì„œ ì¶”ì¶œ\n\n```go\nfunc createUser(u User) error {\n    if err := validateUser(u); err != nil {\n        return fmt.Errorf(\"ì‚¬ìš©ì ìƒì„± ì‹¤íŒ¨: %w\", err)\n    }\n    // ...\n}\n\nerr := createUser(User{})\nvar validErr *ValidationError\nif errors.As(err, &validErr) {\n    // ë˜í•‘ë˜ì–´ ìˆì–´ë„ ì¶”ì¶œ ê°€ëŠ¥!\n    log.Printf(\"ê²€ì¦ ì‹¤íŒ¨ í•„ë“œ: %s\", validErr.Field)\n}\n```\n\n## ì»¤ìŠ¤í…€ ì—ëŸ¬ íƒ€ì…\n\n### ë„ë©”ì¸ ì—ëŸ¬ ì •ì˜\n\n```go\npackage domain\n\ntype ErrorCode string\n\nconst (\n    ErrCodeNotFound     ErrorCode = \"NOT_FOUND\"\n    ErrCodeUnauthorized ErrorCode = \"UNAUTHORIZED\"\n    ErrCodeConflict     ErrorCode = \"CONFLICT\"\n)\n\ntype DomainError struct {\n    Code    ErrorCode\n    Message string\n    Cause   error  // ì›ì¸ ì—ëŸ¬\n}\n\nfunc (e *DomainError) Error() string {\n    if e.Cause != nil {\n        return fmt.Sprintf(\"[%s] %s: %v\", e.Code, e.Message, e.Cause)\n    }\n    return fmt.Sprintf(\"[%s] %s\", e.Code, e.Message)\n}\n\n// Unwrapìœ¼ë¡œ ì²´ì¸ ì§€ì›\nfunc (e *DomainError) Unwrap() error {\n    return e.Cause\n}\n\n// ìƒì„±ì í•¨ìˆ˜ë“¤\nfunc NewNotFoundError(resource string, cause error) *DomainError {\n    return &DomainError{\n        Code:    ErrCodeNotFound,\n        Message: fmt.Sprintf(\"%së¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\", resource),\n        Cause:   cause,\n    }\n}\n\nfunc NewConflictError(msg string) *DomainError {\n    return &DomainError{\n        Code:    ErrCodeConflict,\n        Message: msg,\n    }\n}\n```\n\n### HTTP ìƒíƒœ ì½”ë“œ ë§¤í•‘\n\n```go\nfunc (e *DomainError) HTTPStatus() int {\n    switch e.Code {\n    case ErrCodeNotFound:\n        return http.StatusNotFound\n    case ErrCodeUnauthorized:\n        return http.StatusUnauthorized\n    case ErrCodeConflict:\n        return http.StatusConflict\n    default:\n        return http.StatusInternalServerError\n    }\n}\n\n// í•¸ë“¤ëŸ¬ì—ì„œ ì‚¬ìš©\nfunc userHandler(w http.ResponseWriter, r *http.Request) {\n    user, err := userService.Find(r.Context(), userID)\n    if err != nil {\n        var domainErr *DomainError\n        if errors.As(err, &domainErr) {\n            http.Error(w, domainErr.Message, domainErr.HTTPStatus())\n            return\n        }\n        http.Error(w, \"ì„œë²„ ì˜¤ë¥˜\", http.StatusInternalServerError)\n        return\n    }\n    // ...\n}\n```\n\n## ë‹¤ì¤‘ ì—ëŸ¬ ë˜í•‘ (Go 1.20+)\n\n### errors.Join\n\n```go\nfunc validateForm(data FormData) error {\n    var errs []error\n    \n    if data.Name == \"\" {\n        errs = append(errs, errors.New(\"ì´ë¦„ì€ í•„ìˆ˜ì…ë‹ˆë‹¤\"))\n    }\n    if data.Email == \"\" {\n        errs = append(errs, errors.New(\"ì´ë©”ì¼ì€ í•„ìˆ˜ì…ë‹ˆë‹¤\"))\n    }\n    if data.Age < 0 {\n        errs = append(errs, errors.New(\"ë‚˜ì´ëŠ” 0 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤\"))\n    }\n    \n    return errors.Join(errs...)  // nilì´ë©´ nil ë°˜í™˜\n}\n\nerr := validateForm(FormData{})\n// ì¶œë ¥: ì´ë¦„ì€ í•„ìˆ˜ì…ë‹ˆë‹¤\n//       ì´ë©”ì¼ì€ í•„ìˆ˜ì…ë‹ˆë‹¤\n```\n\n### ë‹¤ì¤‘ ì—ëŸ¬ ê²€ì‚¬\n\n```go\nvar (\n    ErrNameRequired  = errors.New(\"ì´ë¦„ì€ í•„ìˆ˜ì…ë‹ˆë‹¤\")\n    ErrEmailRequired = errors.New(\"ì´ë©”ì¼ì€ í•„ìˆ˜ì…ë‹ˆë‹¤\")\n)\n\nerr := errors.Join(ErrNameRequired, ErrEmailRequired)\n\n// ê°ê° ê²€ì‚¬ ê°€ëŠ¥\nerrors.Is(err, ErrNameRequired)  // true\nerrors.Is(err, ErrEmailRequired) // true\n```\n\n## ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤\n\ní‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•©ë‹ˆë‹¤.\n\n### pkg/errors (ë ˆê±°ì‹œ)\n\n```go\nimport \"github.com/pkg/errors\"\n\nfunc readFile(path string) error {\n    data, err := os.ReadFile(path)\n    if err != nil {\n        return errors.Wrap(err, \"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨\")  // ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ í¬í•¨\n    }\n    return nil\n}\n\n// ìŠ¤íƒ ì¶œë ¥\nerr := readFile(\"config.yaml\")\nfmt.Printf(\"%+v\\n\", err)  // ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ í¬í•¨ ì¶œë ¥\n```\n\n### cockroachdb/errors (ê¶Œì¥)\n\n```go\nimport \"github.com/cockroachdb/errors\"\n\nfunc processData() error {\n    if err := readConfig(); err != nil {\n        return errors.Wrap(err, \"ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨\")\n    }\n    return nil\n}\n\n// errors.Is, errors.As í˜¸í™˜\n// ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ìë™ í¬í•¨\n```\n\n### êµ¬ì¡°í™”ëœ ë¡œê¹…ê³¼ í•¨ê»˜\n\n```go\nimport (\n    \"github.com/cockroachdb/errors\"\n    \"log/slog\"\n)\n\nfunc handleRequest() {\n    if err := processData(); err != nil {\n        slog.Error(\"ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨\",\n            \"error\", err,\n            \"stack\", fmt.Sprintf(\"%+v\", err),\n        )\n    }\n}\n```\n\n## ì—ëŸ¬ í•¸ë“¤ë§ íŒ¨í„´\n\n### íŒ¨í„´ 1: ê³„ì¸µë³„ ë˜í•‘\n\n```go\n// Repository ê³„ì¸µ\nfunc (r *UserRepo) FindByID(id string) (*User, error) {\n    user, err := r.db.Get(id)\n    if err != nil {\n        return nil, fmt.Errorf(\"UserRepo.FindByID: %w\", err)\n    }\n    return user, nil\n}\n\n// Service ê³„ì¸µ\nfunc (s *UserService) GetUser(id string) (*User, error) {\n    user, err := s.repo.FindByID(id)\n    if err != nil {\n        if errors.Is(err, sql.ErrNoRows) {\n            return nil, domain.NewNotFoundError(\"user\", err)\n        }\n        return nil, fmt.Errorf(\"UserService.GetUser: %w\", err)\n    }\n    return user, nil\n}\n\n// Handler ê³„ì¸µ\nfunc (h *UserHandler) Get(w http.ResponseWriter, r *http.Request) {\n    user, err := h.service.GetUser(userID)\n    if err != nil {\n        var domainErr *domain.DomainError\n        if errors.As(err, &domainErr) {\n            respondError(w, domainErr)\n            return\n        }\n        slog.Error(\"ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬\", \"error\", err)\n        http.Error(w, \"Internal Server Error\", 500)\n        return\n    }\n    respondJSON(w, user)\n}\n```\n\n### íŒ¨í„´ 2: ì—ëŸ¬ ë¡œê¹… ìœ„ì¹˜\n\n```go\n// âŒ ëª¨ë“  ê³³ì—ì„œ ë¡œê¹… (ì¤‘ë³µ)\nfunc foo() error {\n    err := bar()\n    if err != nil {\n        log.Error(\"bar ì‹¤íŒ¨\", err)  // ì¤‘ë³µ!\n        return err\n    }\n}\n\n// âœ… ìµœìƒìœ„ì—ì„œë§Œ ë¡œê¹…\nfunc handler() {\n    err := foo()\n    if err != nil {\n        log.Error(\"ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨\", err)  // í•œ ê³³ì—ì„œë§Œ\n        // ì‘ë‹µ ì²˜ë¦¬\n    }\n}\n```\n\n### íŒ¨í„´ 3: ì¬ì‹œë„ ê°€ëŠ¥ ì—ëŸ¬\n\n```go\ntype RetryableError struct {\n    Err       error\n    RetryAfter time.Duration\n}\n\nfunc (e *RetryableError) Error() string {\n    return fmt.Sprintf(\"ì¬ì‹œë„ ê°€ëŠ¥: %v (after %v)\", e.Err, e.RetryAfter)\n}\n\nfunc (e *RetryableError) Unwrap() error { return e.Err }\n\n// ì‚¬ìš©\nfunc callExternalAPI() error {\n    resp, err := http.Get(url)\n    if err != nil {\n        return &RetryableError{Err: err, RetryAfter: 5 * time.Second}\n    }\n    if resp.StatusCode == 429 {\n        return &RetryableError{\n            Err:        errors.New(\"rate limited\"),\n            RetryAfter: parseRetryAfter(resp.Header),\n        }\n    }\n    return nil\n}\n\n// í˜¸ì¶œì¸¡\nfor i := 0; i < maxRetries; i++ {\n    err := callExternalAPI()\n    var retryErr *RetryableError\n    if errors.As(err, &retryErr) {\n        time.Sleep(retryErr.RetryAfter)\n        continue\n    }\n    if err != nil {\n        return err  // ì¬ì‹œë„ ë¶ˆê°€ëŠ¥í•œ ì—ëŸ¬\n    }\n    return nil  // ì„±ê³µ\n}\n```\n\n## ì£¼ì˜ì‚¬í•­\n\n1. âš ï¸ **`%w` vs `%v`** - ì²´ì¸ ìœ ì§€ê°€ í•„ìš”í•˜ë©´ ë°˜ë“œì‹œ `%w`\n2. âš ï¸ **errors.AsëŠ” í¬ì¸í„°ì˜ í¬ì¸í„°** - `var err *CustomError; errors.As(e, &err)`\n3. âš ï¸ **ì„¼í‹°ë„¬ ì—ëŸ¬ëŠ” íŒ¨í‚¤ì§€ ë ˆë²¨** - ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸\n4. âš ï¸ **ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ëŠ” ì„±ëŠ¥ ë¹„ìš©** - í”„ë¡œë•ì…˜ì—ì„œ ì‹ ì¤‘íˆ ì‚¬ìš©\n\n## ì°¸ê³  ìë£Œ\n\n- [Go Blog: Working with Errors in Go 1.13](https://go.dev/blog/go1.13-errors)\n- [errors íŒ¨í‚¤ì§€ ë¬¸ì„œ](https://pkg.go.dev/errors)\n- [cockroachdb/errors](https://github.com/cockroachdb/errors)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Error Handling",
      "Go"
    ],
    "readingTime": 7,
    "wordCount": 1273,
    "isFeatured": false,
    "isPublic": true,
    "date": "2026-01-02"
  },
  {
    "id": "docker-multistage-build-optimization",
    "slug": "docker-multistage-build-optimization",
    "path": "backend/devops",
    "fullPath": "backend/devops/docker-multistage-build-optimization",
    "title": "Docker ë©€í‹°ìŠ¤í…Œì´ì§€ ë¹Œë“œ ìµœì í™” ê°€ì´ë“œ",
    "excerpt": "ë©€í‹°ìŠ¤í…Œì´ì§€ ë¹Œë“œë¡œ Docker ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì¤„ì´ê³ , ë¹Œë“œ ì†ë„ì™€ ë³´ì•ˆì„ ê°œì„ í•˜ëŠ” ì‹¤ì „ ì „ëµì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# Docker ë©€í‹°ìŠ¤í…Œì´ì§€ ë¹Œë“œ ìµœì í™” ê°€ì´ë“œ\n\n## ê°œìš”\n\n**ë©€í‹°ìŠ¤í…Œì´ì§€ ë¹Œë“œ**ëŠ” ë‹¨ì¼ Dockerfileì—ì„œ ì—¬ëŸ¬ `FROM` ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹Œë“œ í™˜ê²½ê³¼ ëŸ°íƒ€ì„ í™˜ê²½ì„ ë¶„ë¦¬í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ **ì´ë¯¸ì§€ í¬ê¸° ê°ì†Œ**, **ë³´ì•ˆ ê°•í™”**, **ë¹Œë“œ ì†ë„ í–¥ìƒ**ì„ ë™ì‹œì— ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## ê¸°ë³¸ êµ¬ì¡°\n\n### ì‹±ê¸€ ìŠ¤í…Œì´ì§€ vs ë©€í‹° ìŠ¤í…Œì´ì§€\n\n```dockerfile\n# âŒ ì‹±ê¸€ ìŠ¤í…Œì´ì§€ - ë¹Œë“œ ë„êµ¬ê°€ í¬í•¨ë¨\nFROM golang:1.23\nWORKDIR /app\nCOPY . .\nRUN go build -o main .\nCMD [\"./main\"]\n# ê²°ê³¼: ~1GB ì´ë¯¸ì§€\n```\n\n```dockerfile\n# âœ… ë©€í‹° ìŠ¤í…Œì´ì§€ - ëŸ°íƒ€ì„ë§Œ í¬í•¨\nFROM golang:1.23 AS builder\nWORKDIR /app\nCOPY . .\nRUN CGO_ENABLED=0 go build -o main .\n\nFROM scratch\nCOPY --from=builder /app/main /main\nCMD [\"/main\"]\n# ê²°ê³¼: ~10MB ì´ë¯¸ì§€\n```\n\n> [!IMPORTANT]\n> `COPY --from=builder`ë¡œ ë¹Œë“œ ì•„í‹°íŒ©íŠ¸ë§Œ ë³µì‚¬í•˜ì—¬ ë¹Œë“œ ë„êµ¬, ì†ŒìŠ¤ ì½”ë“œ, ì¤‘ê°„ íŒŒì¼ì„ ëª¨ë‘ ì œì™¸í•©ë‹ˆë‹¤.\n\n## ì–¸ì–´ë³„ ìµœì í™” ì˜ˆì‹œ\n\n### Go\n\n```dockerfile\n# ë¹Œë“œ ìŠ¤í…Œì´ì§€\nFROM golang:1.23-alpine AS builder\nWORKDIR /app\n\n# ì˜ì¡´ì„± ë¨¼ì € (ìºì‹œ í™œìš©)\nCOPY go.mod go.sum ./\nRUN go mod download\n\n# ì†ŒìŠ¤ ë³µì‚¬ ë° ë¹Œë“œ\nCOPY . .\nRUN CGO_ENABLED=0 GOOS=linux go build -ldflags=\"-s -w\" -o /app/server ./cmd/server\n\n# ëŸ°íƒ€ì„ ìŠ¤í…Œì´ì§€\nFROM scratch\nCOPY --from=builder /app/server /server\nCOPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/\nEXPOSE 8080\nENTRYPOINT [\"/server\"]\n```\n\n### Node.js\n\n```dockerfile\n# ì˜ì¡´ì„± ì„¤ì¹˜ ìŠ¤í…Œì´ì§€\nFROM node:22-alpine AS deps\nWORKDIR /app\nCOPY package.json pnpm-lock.yaml ./\nRUN corepack enable && pnpm install --frozen-lockfile\n\n# ë¹Œë“œ ìŠ¤í…Œì´ì§€\nFROM node:22-alpine AS builder\nWORKDIR /app\nCOPY --from=deps /app/node_modules ./node_modules\nCOPY . .\nRUN npm run build\n\n# ëŸ°íƒ€ì„ ìŠ¤í…Œì´ì§€ (í”„ë¡œë•ì…˜ ì˜ì¡´ì„±ë§Œ)\nFROM node:22-alpine AS runner\nWORKDIR /app\nENV NODE_ENV=production\n\nCOPY --from=deps /app/node_modules ./node_modules\nCOPY --from=builder /app/dist ./dist\nCOPY --from=builder /app/package.json ./\n\nUSER node\nEXPOSE 3000\nCMD [\"node\", \"dist/main.js\"]\n```\n\n### Python\n\n```dockerfile\n# ë¹Œë“œ ìŠ¤í…Œì´ì§€\nFROM python:3.12-slim AS builder\nWORKDIR /app\n\nRUN pip install --no-cache-dir poetry\nCOPY pyproject.toml poetry.lock ./\nRUN poetry export -f requirements.txt -o requirements.txt --without-hashes\n\n# ëŸ°íƒ€ì„ ìŠ¤í…Œì´ì§€\nFROM python:3.12-slim\nWORKDIR /app\n\nCOPY --from=builder /app/requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\nUSER nobody\nCMD [\"python\", \"-m\", \"app.main\"]\n```\n\n## ë² ì´ìŠ¤ ì´ë¯¸ì§€ ì„ íƒ\n\n| ì´ë¯¸ì§€ | í¬ê¸° | ìš©ë„ |\n|-------|-----|-----|\n| `scratch` | 0MB | ì •ì  ë°”ì´ë„ˆë¦¬ (Go, Rust) |\n| `distroless` | ~2MB | ë³´ì•ˆ ì¤‘ì‹œ, ì…¸ ì—†ìŒ |\n| `alpine` | ~5MB | ë²”ìš© ê²½ëŸ‰ ë¦¬ëˆ…ìŠ¤ |\n| `slim` | ~80MB | í˜¸í™˜ì„± í•„ìš” ì‹œ |\n\n### Distroless ì˜ˆì‹œ\n\n```dockerfile\nFROM golang:1.23 AS builder\nWORKDIR /app\nCOPY . .\nRUN CGO_ENABLED=0 go build -o /server .\n\n# Google Distroless - ì…¸ ì—†ìŒ, ë³´ì•ˆ ê°•í™”\nFROM gcr.io/distroless/static-debian12\nCOPY --from=builder /server /server\nUSER nonroot:nonroot\nENTRYPOINT [\"/server\"]\n```\n\n## ìºì‹œ ìµœì í™”\n\n### ë ˆì´ì–´ ìˆœì„œ ìµœì í™”\n\n```dockerfile\n# âŒ ë‚˜ìœ ìˆœì„œ - ì†ŒìŠ¤ ë³€ê²½ ì‹œ ì˜ì¡´ì„± ì¬ì„¤ì¹˜\nCOPY . .\nRUN npm install\nRUN npm run build\n\n# âœ… ì¢‹ì€ ìˆœì„œ - ì˜ì¡´ì„± ìºì‹œ í™œìš©\nCOPY package.json package-lock.json ./\nRUN npm ci\nCOPY . .\nRUN npm run build\n```\n\n### BuildKit ìºì‹œ ë§ˆìš´íŠ¸\n\n```dockerfile\n# syntax=docker/dockerfile:1\n\nFROM golang:1.23 AS builder\nWORKDIR /app\nCOPY . .\n\n# ëª¨ë“ˆ ìºì‹œë¥¼ ë³¼ë¥¨ìœ¼ë¡œ ë§ˆìš´íŠ¸ (ë¹Œë“œ ê°„ ìœ ì§€)\nRUN --mount=type=cache,target=/go/pkg/mod \\\n    --mount=type=cache,target=/root/.cache/go-build \\\n    go build -o /server .\n```\n\n```dockerfile\n# Node.js ì˜ˆì‹œ\nFROM node:22 AS builder\nWORKDIR /app\nCOPY package*.json ./\n\nRUN --mount=type=cache,target=/root/.npm \\\n    npm ci\n```\n\n## ë¹Œë“œ ì¸ì í™œìš©\n\n### íƒ€ê²Ÿ ìŠ¤í…Œì´ì§€ ì§€ì •\n\n```dockerfile\nFROM node:22-alpine AS base\nWORKDIR /app\nCOPY package*.json ./\n\nFROM base AS development\nRUN npm install\nCMD [\"npm\", \"run\", \"dev\"]\n\nFROM base AS production\nRUN npm ci --only=production\nCOPY . .\nRUN npm run build\nCMD [\"npm\", \"start\"]\n```\n\n```bash\n# ê°œë°œ ì´ë¯¸ì§€ ë¹Œë“œ\ndocker build --target development -t myapp:dev .\n\n# í”„ë¡œë•ì…˜ ì´ë¯¸ì§€ ë¹Œë“œ\ndocker build --target production -t myapp:prod .\n```\n\n### ARGë¡œ ì¡°ê±´ë¶€ ë¹Œë“œ\n\n```dockerfile\nARG BUILD_ENV=production\n\nFROM node:22-alpine AS builder\nWORKDIR /app\nCOPY . .\n\nRUN if [ \"$BUILD_ENV\" = \"development\" ]; then \\\n      npm install; \\\n    else \\\n      npm ci --only=production; \\\n    fi\n```\n\n## BuildKit ê³ ê¸‰ ê¸°ëŠ¥\n\n### ë³‘ë ¬ ë¹Œë“œ\n\n```dockerfile\n# syntax=docker/dockerfile:1\n\nFROM alpine AS stage1\nRUN sleep 10 && echo \"stage1\" > /out1\n\nFROM alpine AS stage2\nRUN sleep 10 && echo \"stage2\" > /out2\n\n# BuildKitì´ stage1, stage2ë¥¼ ë³‘ë ¬ ì‹¤í–‰\nFROM alpine\nCOPY --from=stage1 /out1 /\nCOPY --from=stage2 /out2 /\n```\n\n### Secret ë§ˆìš´íŠ¸ (ë¯¼ê° ì •ë³´)\n\n```dockerfile\n# syntax=docker/dockerfile:1\n\nFROM node:22-alpine\nWORKDIR /app\n\n# .npmrcë¥¼ ì´ë¯¸ì§€ì— í¬í•¨í•˜ì§€ ì•ŠìŒ\nRUN --mount=type=secret,id=npmrc,target=/root/.npmrc \\\n    npm ci\n```\n\n```bash\ndocker build --secret id=npmrc,src=.npmrc -t myapp .\n```\n\n## ëª¨ë²” ì‚¬ë¡€ ì²´í¬ë¦¬ìŠ¤íŠ¸\n\n### ì´ë¯¸ì§€ í¬ê¸° ìµœì í™”\n\n- [ ] ëŸ°íƒ€ì„ì— ë¶ˆí•„ìš”í•œ ë¹Œë“œ ë„êµ¬ ì œì™¸\n- [ ] `scratch` ë˜ëŠ” `distroless` ì‚¬ìš© ê³ ë ¤\n- [ ] `.dockerignore` ì„¤ì •ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ íŒŒì¼ ì œì™¸\n- [ ] `--no-cache-dir` (pip), `--frozen-lockfile` (pnpm) ì‚¬ìš©\n\n### ë¹Œë“œ ì†ë„ ìµœì í™”\n\n- [ ] ë³€ê²½ ë¹ˆë„ ë‚®ì€ ë ˆì´ì–´ë¥¼ ìƒë‹¨ì— ë°°ì¹˜\n- [ ] BuildKit ìºì‹œ ë§ˆìš´íŠ¸ í™œìš©\n- [ ] ì˜ì¡´ì„± íŒŒì¼ ë¨¼ì € ë³µì‚¬ í›„ ì„¤ì¹˜\n\n### ë³´ì•ˆ\n\n- [ ] ë² ì´ìŠ¤ ì´ë¯¸ì§€ ë²„ì „ ê³ ì • (`:latest` ê¸ˆì§€)\n- [ ] `USER` ëª…ë ¹ì–´ë¡œ non-root ì‹¤í–‰\n- [ ] ë¶ˆí•„ìš”í•œ í¬íŠ¸ ë…¸ì¶œ ê¸ˆì§€\n- [ ] Secret ë§ˆìš´íŠ¸ë¡œ ë¯¼ê° ì •ë³´ ê´€ë¦¬\n\n## ì°¸ê³  ìë£Œ\n\n- [Docker Multi-stage builds](https://docs.docker.com/build/building/multi-stage/)\n- [Docker BuildKit](https://docs.docker.com/build/buildkit/)\n- [Google Distroless](https://github.com/GoogleContainerTools/distroless)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "Docker",
      "Performance"
    ],
    "readingTime": 5,
    "wordCount": 826,
    "isFeatured": false,
    "isPublic": true,
    "date": "2026-01-02"
  },
  {
    "id": "langchain-langgraph-practical-guide",
    "slug": "langchain-langgraph-practical-guide",
    "path": "ai-ml",
    "fullPath": "ai-ml/langchain-langgraph-practical-guide",
    "title": "LangChain & LangGraph ì‹¤ì „ ê°€ì´ë“œ (2025)",
    "excerpt": "LangChainê³¼ LangGraphì˜ ìµœì‹  APIë¥¼ í™œìš©í•œ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ë° ì—ì´ì „íŠ¸ ê°œë°œ ì‹¤ì „ ê°€ì´ë“œì…ë‹ˆë‹¤.",
    "content": "# LangChain & LangGraph ì‹¤ì „ ê°€ì´ë“œ (2025)\n\n## ê°œìš”\n\n**LangChain**ì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì´ê³ , **LangGraph**ëŠ” ìƒíƒœ ê¸°ë°˜ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì¶•í•˜ëŠ” ëŸ°íƒ€ì„ì…ë‹ˆë‹¤. 2025ë…„ LangGraph 1.0 ë¦´ë¦¬ìŠ¤ì™€ í•¨ê»˜ ë‘ í”„ë ˆì„ì›Œí¬ì˜ ì—­í• ì´ ëª…í™•í•´ì¡ŒìŠµë‹ˆë‹¤.\n\n| í”„ë ˆì„ì›Œí¬ | ì—­í•  | ì£¼ìš” ì‚¬ìš©ì²˜ |\n|-----------|-----|-----------|\n| **LangChain** | LLM ì¸í„°í˜ì´ìŠ¤, í”„ë¡¬í”„íŠ¸, ì²´ì¸ | RAG, ë‹¨ìˆœ ì²´ì¸, ë„êµ¬ í˜¸ì¶œ |\n| **LangGraph** | ìƒíƒœ ê´€ë¦¬, ê·¸ë˜í”„ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° | ë³µì¡í•œ ì—ì´ì „íŠ¸, ë©€í‹° ì—ì´ì „íŠ¸ |\n\n## ì„¤ì¹˜\n\n```bash\n# LangChain í•µì‹¬ + OpenAI í†µí•©\npip install langchain langchain-openai\n\n# LangGraph (ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°)\npip install langgraph\n\n# ì „ì²´ ìŠ¤íƒ\npip install langchain langchain-openai langgraph langchain-community\n```\n\n---\n\n## Part 1: LangChain ê¸°ì´ˆ\n\n### ChatOpenAI ì„¤ì •\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# ê¸°ë³¸ ì„¤ì •\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    temperature=0,\n    # openai_api_key=\"...\"  # ë˜ëŠ” OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜\n)\n\n# ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”\nllm_streaming = ChatOpenAI(model=\"gpt-4o\", streaming=True)\n```\n\n### LCEL (LangChain Expression Language)\n\nLCELì€ ì²´ì¸ì„ ì„ ì–¸ì ìœ¼ë¡œ êµ¬ì„±í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"ë‹¹ì‹ ì€ {role} ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\"),\n    (\"human\", \"{question}\")\n])\n\n# íŒŒì´í”„(|)ë¡œ ì²´ì¸ ì—°ê²°\nchain = prompt | llm | StrOutputParser()\n\n# ì‹¤í–‰\nresult = chain.invoke({\n    \"role\": \"Python\",\n    \"question\": \"asyncioì˜ ì¥ì ì€?\"\n})\nprint(result)\n```\n\n### ë„êµ¬ ë°”ì¸ë”© (Tool Calling)\n\n```python\nfrom langchain_core.tools import tool\n\n@tool\ndef get_weather(city: str) -> str:\n    \"\"\"ë„ì‹œì˜ í˜„ì¬ ë‚ ì”¨ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n    # ì‹¤ì œ API í˜¸ì¶œ ë¡œì§\n    return f\"{city}ì˜ ë‚ ì”¨: ë§‘ìŒ, 22Â°C\"\n\n@tool\ndef calculate(expression: str) -> str:\n    \"\"\"ìˆ˜í•™ í‘œí˜„ì‹ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\"\"\"\n    return str(eval(expression))\n\n# LLMì— ë„êµ¬ ë°”ì¸ë”©\nllm_with_tools = llm.bind_tools([get_weather, calculate])\n\n# ì‹¤í–‰\nresponse = llm_with_tools.invoke(\"ì„œìš¸ ë‚ ì”¨ ì–´ë•Œ?\")\nprint(response.tool_calls)\n# [{'name': 'get_weather', 'args': {'city': 'ì„œìš¸'}, 'id': '...'}]\n```\n\n### RAG ì²´ì¸\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.runnables import RunnablePassthrough\n\n# ë²¡í„° ìŠ¤í† ì–´ ì„¤ì • (ì˜ˆì‹œ)\nembeddings = OpenAIEmbeddings()\nvectorstore = FAISS.from_texts(\n    [\"LangChainì€ LLM í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\", \"LangGraphëŠ” ì—ì´ì „íŠ¸ ëŸ°íƒ€ì„ì…ë‹ˆë‹¤.\"],\n    embeddings\n)\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n\n# RAG í”„ë¡¬í”„íŠ¸\nrag_prompt = ChatPromptTemplate.from_template(\"\"\"\në‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n\nì»¨í…ìŠ¤íŠ¸:\n{context}\n\nì§ˆë¬¸: {question}\n\"\"\")\n\n# RAG ì²´ì¸\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | rag_prompt\n    | llm\n    | StrOutputParser()\n)\n\nresult = rag_chain.invoke(\"LangGraphê°€ ë­ì•¼?\")\n```\n\n---\n\n## Part 2: LangGraph ì—ì´ì „íŠ¸\n\n### StateGraph ê¸°ë³¸ êµ¬ì¡°\n\nLangGraphì˜ í•µì‹¬ì€ **ìƒíƒœ(State)**ì™€ **ë…¸ë“œ(Node)**ì…ë‹ˆë‹¤.\n\n```python\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\n\n# 1. ìƒíƒœ ìŠ¤í‚¤ë§ˆ ì •ì˜\nclass AgentState(TypedDict):\n    messages: Annotated[list, add_messages]  # ë©”ì‹œì§€ ë¦¬ë“€ì„œ\n    user_info: str\n\n# 2. ê·¸ë˜í”„ ë¹Œë” ìƒì„±\ngraph_builder = StateGraph(AgentState)\n```\n\n> [!NOTE]\n> `Annotated[list, add_messages]`ëŠ” ë¦¬ë“€ì„œ í•¨ìˆ˜ë¡œ, ìƒˆ ë©”ì‹œì§€ê°€ ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ë©ë‹ˆë‹¤.\n\n### ë…¸ë“œ ì •ì˜ ë‹¨ìœ„: í•¨ìˆ˜ vs ì—ì´ì „íŠ¸\n\në…¸ë“œë¥¼ ì–´ë–¤ ë‹¨ìœ„ë¡œ ì •ì˜í• ì§€ëŠ” í”„ë¡œì íŠ¸ì˜ ì„±ê²©ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤.\n\n#### í•¨ìˆ˜ ë‹¨ìœ„ (Fine-grained) - íŒŒì´í”„ë¼ì¸ ìŠ¤íƒ€ì¼\n\n[LangGraph ê³µì‹ README](https://github.com/langchain-ai/langgraph)ì˜ ê¸°ë³¸ ì˜ˆì‹œ:\n\n```python\nfrom langgraph.graph import START, StateGraph\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    text: str\n\ndef node_a(state: State) -> dict:\n    return {\"text\": state[\"text\"] + \"a\"}\n\ndef node_b(state: State) -> dict:\n    return {\"text\": state[\"text\"] + \"b\"}\n\ngraph = StateGraph(State)\ngraph.add_node(\"node_a\", node_a)\ngraph.add_node(\"node_b\", node_b)\ngraph.add_edge(START, \"node_a\")\ngraph.add_edge(\"node_a\", \"node_b\")\n\nprint(graph.compile().invoke({\"text\": \"\"}))\n# {'text': 'ab'}\n```\n\n**ì í•©í•œ ìƒí™©**: ETL íŒŒì´í”„ë¼ì¸, ë°ì´í„° ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°, DAG ìŠ¤íƒ€ì¼ ì‘ì—…\n\n#### ì—ì´ì „íŠ¸ ë‹¨ìœ„ (Coarse-grained) - ì—­í•  ê¸°ë°˜ ìŠ¤íƒ€ì¼\n\n```python\ndef researcher(state): ...  # ë¦¬ì„œì¹˜ ë‹´ë‹¹\ndef writer(state): ...      # ê¸€ì“°ê¸° ë‹´ë‹¹\ndef reviewer(state): ...    # ê²€í†  ë‹´ë‹¹\n```\n\n**ì í•©í•œ ìƒí™©**: ë©€í‹° ì—ì´ì „íŠ¸ í˜‘ì—…, ì—­í•  ë¶„ë¦¬ê°€ ëª…í™•í•œ ê²½ìš°\n\n#### ì„ íƒ ê°€ì´ë“œ\n\n| ì ‘ê·¼ë²• | ë…¸ë“œ ë‹¨ìœ„ | ì¥ì  | ì²´í¬í¬ì¸íŠ¸ |\n|-------|---------|-----|----------|\n| **í•¨ìˆ˜ ë‹¨ìœ„** | fetch, transform, validate ë“± | ì¬ì‚¬ìš©ì„± â†‘, í…ŒìŠ¤íŠ¸ ìš©ì´ | ì„¸ë°€í•œ ë³µêµ¬ ê°€ëŠ¥ |\n| **ì—ì´ì „íŠ¸ ë‹¨ìœ„** | researcher, writer ë“± | ì§ê´€ì , ì—­í•  ë¶„ë¦¬ | í° ë‹¨ìœ„ë¡œ ë³µêµ¬ |\n\n> [!IMPORTANT]\n> ì²´í¬í¬ì¸íŠ¸ëŠ” **ë…¸ë“œ ë‹¨ìœ„**ë¡œ ì €ì¥ë©ë‹ˆë‹¤. ì‹¤íŒ¨ ì‹œ í•´ë‹¹ ë…¸ë“œë¶€í„° ì¬ì‹¤í–‰ë˜ë¯€ë¡œ, ë…¸ë“œ í¬ê¸°ë¥¼ \"ìƒíƒœ ë³€ê²½ì˜ ì›ìì„±\"ìœ¼ë¡œ ê²°ì •í•˜ì„¸ìš”.\n\n### ë…¸ë“œì™€ ì—£ì§€ ì •ì˜\n\n```python\nfrom langchain_core.messages import HumanMessage, AIMessage\n\n# ë…¸ë“œ í•¨ìˆ˜ë“¤\ndef fetch_user_info(state: AgentState) -> dict:\n    \"\"\"ì‚¬ìš©ì ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë…¸ë“œ\"\"\"\n    return {\"user_info\": \"VIP ê³ ê°, ê°€ì…ì¼: 2023-01-15\"}\n\ndef chatbot(state: AgentState) -> dict:\n    \"\"\"LLM ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ\"\"\"\n    system_prompt = f\"ì‚¬ìš©ì ì •ë³´: {state['user_info']}\"\n    response = llm.invoke([\n        {\"role\": \"system\", \"content\": system_prompt},\n        *state[\"messages\"]\n    ])\n    return {\"messages\": [response]}\n\n# ë…¸ë“œ ì¶”ê°€\ngraph_builder.add_node(\"fetch_user\", fetch_user_info)\ngraph_builder.add_node(\"chatbot\", chatbot)\n\n# ì—£ì§€ ì—°ê²° (íë¦„ ì •ì˜)\ngraph_builder.add_edge(START, \"fetch_user\")\ngraph_builder.add_edge(\"fetch_user\", \"chatbot\")\ngraph_builder.add_edge(\"chatbot\", END)\n\n# ì»´íŒŒì¼\ngraph = graph_builder.compile()\n```\n\n### ì¡°ê±´ë¶€ ë¼ìš°íŒ…\n\n```python\nfrom langgraph.graph import END\n\ndef should_continue(state: AgentState) -> str:\n    \"\"\"ë‹¤ìŒ ë…¸ë“œë¥¼ ê²°ì •í•˜ëŠ” ì¡°ê±´ í•¨ìˆ˜\"\"\"\n    last_message = state[\"messages\"][-1]\n    \n    # ë„êµ¬ í˜¸ì¶œì´ ìˆìœ¼ë©´ ë„êµ¬ ì‹¤í–‰\n    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n        return \"tools\"\n    # ì—†ìœ¼ë©´ ì¢…ë£Œ\n    return END\n\n# ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€\ngraph_builder.add_conditional_edges(\n    \"chatbot\",\n    should_continue,\n    {\n        \"tools\": \"tool_executor\",\n        END: END\n    }\n)\n```\n\n### ì²´í¬í¬ì¸íŠ¸ (ë©”ëª¨ë¦¬ ì˜ì†ì„±)\n\n```python\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# ì²´í¬í¬ì¸í„° ì„¤ì •\ncheckpointer = InMemorySaver()\ngraph = graph_builder.compile(checkpointer=checkpointer)\n\n# thread_idë¡œ ëŒ€í™” ì„¸ì…˜ ê´€ë¦¬\nconfig = {\"configurable\": {\"thread_id\": \"user-123\"}}\n\n# ì²« ë²ˆì§¸ ë©”ì‹œì§€\nresult1 = graph.invoke(\n    {\"messages\": [HumanMessage(content=\"ì•ˆë…•í•˜ì„¸ìš”\")]},\n    config=config\n)\n\n# ê°™ì€ thread_idë¡œ ì´ì–´ì„œ ëŒ€í™”\nresult2 = graph.invoke(\n    {\"messages\": [HumanMessage(content=\"ì´ì „ì— ë­ë¼ê³  í–ˆì£ ?\")]},\n    config=config\n)\n# â†’ ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ë¨!\n```\n\n### ReAct ì—ì´ì „íŠ¸ (Pre-built)\n\nLangGraphëŠ” ì¼ë°˜ì ì¸ ì—ì´ì „íŠ¸ íŒ¨í„´ì„ ìœ„í•œ í”„ë¦¬ë¹ŒíŠ¸ í•¨ìˆ˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n\n```python\nfrom langgraph.prebuilt import create_react_agent\n\n# ë„êµ¬ ì •ì˜\ntools = [get_weather, calculate]\n\n# ReAct ì—ì´ì „íŠ¸ ìƒì„±\nagent = create_react_agent(llm, tools=tools)\n\n# ì‹¤í–‰\nresult = agent.invoke({\n    \"messages\": [HumanMessage(content=\"ì„œìš¸ ë‚ ì”¨ ì•Œë ¤ì£¼ê³ , 15+27 ê³„ì‚°í•´ì¤˜\")]\n})\n\nfor msg in result[\"messages\"]:\n    print(f\"{msg.type}: {msg.content}\")\n```\n\n### ìŠ¤íŠ¸ë¦¬ë°\n\n```python\n# ë…¸ë“œë³„ ìŠ¤íŠ¸ë¦¬ë°\nfor event in graph.stream(\n    {\"messages\": [HumanMessage(content=\"ì•ˆë…•\")]},\n    config=config\n):\n    for node_name, output in event.items():\n        print(f\"[{node_name}] {output}\")\n\n# í† í°ë³„ ìŠ¤íŠ¸ë¦¬ë° (LLM ì¶œë ¥)\nasync for event in graph.astream_events(\n    {\"messages\": [HumanMessage(content=\"ê¸´ ì´ì•¼ê¸° í•´ì¤˜\")]},\n    config=config,\n    version=\"v2\"\n):\n    if event[\"event\"] == \"on_chat_model_stream\":\n        print(event[\"data\"][\"chunk\"].content, end=\"\", flush=True)\n```\n\n---\n\n## Part 3: ì‹¤ì „ íŒ¨í„´\n\n### íŒ¨í„´ 1: Human-in-the-Loop\n\n```python\nfrom langgraph.types import interrupt\n\ndef review_step(state: AgentState) -> dict:\n    \"\"\"ì‚¬ëŒ ìŠ¹ì¸ì´ í•„ìš”í•œ ë…¸ë“œ\"\"\"\n    # interrupt()ë¡œ ì‹¤í–‰ ì¤‘ë‹¨, ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°\n    user_approval = interrupt(\"ì´ ì‘ì—…ì„ ì§„í–‰í• ê¹Œìš”? (yes/no)\")\n    \n    if user_approval != \"yes\":\n        return {\"messages\": [AIMessage(content=\"ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.\")]}\n    \n    return {\"messages\": [AIMessage(content=\"ì‘ì—…ì„ ì§„í–‰í•©ë‹ˆë‹¤.\")]}\n```\n\n### íŒ¨í„´ 2: ë©€í‹° ì—ì´ì „íŠ¸\n\n```python\nfrom langgraph.graph import StateGraph\n\nclass MultiAgentState(TypedDict):\n    messages: Annotated[list, add_messages]\n    current_agent: str\n\ndef researcher(state: MultiAgentState) -> dict:\n    \"\"\"ë¦¬ì„œì¹˜ ë‹´ë‹¹ ì—ì´ì „íŠ¸\"\"\"\n    # ë¦¬ì„œì¹˜ ë¡œì§...\n    return {\"messages\": [...], \"current_agent\": \"writer\"}\n\ndef writer(state: MultiAgentState) -> dict:\n    \"\"\"ê¸€ì“°ê¸° ë‹´ë‹¹ ì—ì´ì „íŠ¸\"\"\"\n    # ê¸€ì“°ê¸° ë¡œì§...\n    return {\"messages\": [...], \"current_agent\": \"reviewer\"}\n\ndef router(state: MultiAgentState) -> str:\n    return state[\"current_agent\"]\n\n# ê·¸ë˜í”„ êµ¬ì„±\nworkflow = StateGraph(MultiAgentState)\nworkflow.add_node(\"researcher\", researcher)\nworkflow.add_node(\"writer\", writer)\nworkflow.add_node(\"reviewer\", reviewer)\n\nworkflow.add_edge(START, \"researcher\")\nworkflow.add_conditional_edges(\"researcher\", router)\nworkflow.add_conditional_edges(\"writer\", router)\n```\n\n### íŒ¨í„´ 3: ì—ëŸ¬ í•¸ë“¤ë§\n\n```python\nfrom langgraph.errors import NodeInterrupt\n\ndef safe_tool_executor(state: AgentState) -> dict:\n    try:\n        # ë„êµ¬ ì‹¤í–‰\n        result = execute_tools(state[\"messages\"][-1].tool_calls)\n        return {\"messages\": result}\n    except Exception as e:\n        # ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ìƒíƒœì— ì¶”ê°€\n        error_msg = AIMessage(content=f\"ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}\")\n        return {\"messages\": [error_msg]}\n```\n\n---\n\n## LangChain vs LangGraph ì„ íƒ ê°€ì´ë“œ\n\n| ìš”êµ¬ì‚¬í•­ | ì„ íƒ |\n|---------|-----|\n| ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸ â†’ LLM â†’ ì¶œë ¥ | LangChain LCEL |\n| RAG íŒŒì´í”„ë¼ì¸ | LangChain LCEL |\n| ë‹¨ì¼ ë„êµ¬ í˜¸ì¶œ | LangChain `bind_tools` |\n| ë³µì¡í•œ ì—ì´ì „íŠ¸ ë£¨í”„ | **LangGraph** |\n| ë©€í‹° ì—ì´ì „íŠ¸ í˜‘ì—… | **LangGraph** |\n| ëŒ€í™” ìƒíƒœ ì˜ì†ì„± í•„ìš” | **LangGraph** checkpointer |\n| Human-in-the-loop | **LangGraph** interrupt |\n\n## ì°¸ê³  ìë£Œ\n\n- [LangChain Docs](https://python.langchain.com/docs/)\n- [LangGraph Docs](https://langchain-ai.github.io/langgraph/)\n- [LangGraph 1.0 Release](https://blog.langchain.dev/langgraph-v1/)\n- [LCEL Conceptual Guide](https://python.langchain.com/docs/concepts/lcel/)",
    "docType": "original",
    "category": "AI/ML",
    "tags": [
      "AI/ML",
      "Python"
    ],
    "readingTime": 6,
    "wordCount": 1135,
    "isFeatured": false,
    "isPublic": true,
    "date": "2026-01-02"
  },
  {
    "id": "enterprise-go-9-makefile",
    "slug": "enterprise-go-9-makefile",
    "path": "backend/go",
    "fullPath": "backend/go/enterprise-go-9-makefile",
    "title": "Enterprise Go ì‹œë¦¬ì¦ˆ #9: Makefileë¡œ ê°œë°œ ì›Œí¬í”Œë¡œìš° ìë™í™”",
    "excerpt": "ë³µì¡í•œ Go ëª…ë ¹ì–´ë“¤ì„ Makefileë¡œ ì¶”ìƒí™”í•˜ì—¬ íŒ€ ì „ì²´ì˜ ê°œë°œ ê²½í—˜ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. FaÃ§ade íŒ¨í„´ì˜ CLI ë²„ì „ì…ë‹ˆë‹¤.",
    "content": "# Enterprise Go ì‹œë¦¬ì¦ˆ #9: Makefileë¡œ ê°œë°œ ì›Œí¬í”Œë¡œìš° ìë™í™”\n\n> **í•µì‹¬ ì•„ì´ë””ì–´**: Makefileì€ **CLIì˜ FaÃ§ade íŒ¨í„´**ì…ë‹ˆë‹¤.\n> ë³µì¡í•œ ëª…ë ¹ì–´ ì¡°í•©ì„ ë‹¨ìˆœí•œ ì¸í„°í˜ì´ìŠ¤ ë’¤ì— ìˆ¨ê¹ë‹ˆë‹¤.\n\n## ì™œ Makefileì¸ê°€?\n\n### ë¬¸ì œ ìƒí™©\n\n```bash\n# ìƒˆ íŒ€ì›ì´ í”„ë¡œì íŠ¸ì— í•©ë¥˜í–ˆì„ ë•Œ\n$ go build -o bin/user-api ./apps/user-api/cmd\n$ go build -o bin/order-api ./apps/order-api/cmd\n$ go generate ./...\n$ mockgen -source=... -destination=...\n$ wire ./apps/user-api/cmd\n$ go test -race -cover ./...\n```\n\n### FaÃ§ade ì ìš© í›„\n\n```bash\nmake build      # ì „ì²´ ë¹Œë“œ\nmake test       # í…ŒìŠ¤íŠ¸\nmake generate   # ì½”ë“œ ìƒì„± (mock, wire)\n```\n\n**FaÃ§ade íŒ¨í„´**ì²˜ëŸ¼, ë³µì¡í•œ í•˜ìœ„ ì‹œìŠ¤í…œì„ ë‹¨ìˆœí•œ ì¸í„°í˜ì´ìŠ¤ë¡œ ê°ìŒ‰ë‹ˆë‹¤.\n\n---\n\n## ê¸°ë³¸ êµ¬ì¡°\n\n```makefile\n# ë³€ìˆ˜ ì •ì˜\nGO := go\nAPPS := user-api order-api worker\nBIN_DIR := bin\n\n# ê¸°ë³¸ íƒ€ê²Ÿ\n.PHONY: all build test lint clean\n\nall: build\n\nbuild: $(APPS)\n\n$(APPS):\n $(GO) build -o $(BIN_DIR)/$@ ./apps/$@/cmd\n\ntest:\n $(GO) test -race -cover ./...\n\nlint:\n golangci-lint run ./...\n\nclean:\n rm -rf $(BIN_DIR)\n```\n\n---\n\n## ëª¨ë…¸ë ˆí¬ìš© íƒ€ê²Ÿ\n\n### ì„œë¹„ìŠ¤ë³„ ë¹Œë“œ\n\n```makefile\n# íŠ¹ì • ì„œë¹„ìŠ¤ë§Œ ë¹Œë“œ\n.PHONY: build-user build-order\n\nbuild-user:\n $(GO) build -o $(BIN_DIR)/user-api ./apps/user-api/cmd\n\nbuild-order:\n $(GO) build -o $(BIN_DIR)/order-api ./apps/order-api/cmd\n\n# ë³€ê²½ëœ ì„œë¹„ìŠ¤ë§Œ ë¹Œë“œ (CIìš©)\nbuild-changed:\n @for app in $(shell git diff --name-only HEAD~1 | grep \"apps/\" | cut -d/ -f2 | sort -u); do \\\n  echo \"Building $$app...\"; \\\n  $(GO) build -o $(BIN_DIR)/$$app ./apps/$$app/cmd; \\\n done\n```\n\n---\n\n## ì½”ë“œ ìƒì„± ìë™í™”\n\n### Mock, Wire í†µí•©\n\n```makefile\n.PHONY: generate mock wire\n\ngenerate: mock wire\n\nmock:\n @echo \"Generating mocks...\"\n $(GO) generate ./...\n\nwire:\n @echo \"Running wire...\"\n @for app in $(APPS); do \\\n  wire ./apps/$$app/cmd; \\\n done\n```\n\n---\n\n## ê°œë°œ í™˜ê²½\n\n### ë¡œì»¬ ì‹¤í–‰\n\n```makefile\n.PHONY: run dev\n\n# ë‹¨ì¼ ì„œë¹„ìŠ¤ ì‹¤í–‰\nrun:\n $(GO) run ./apps/user-api/cmd serve\n\n# í•« ë¦¬ë¡œë“œ (air ì‚¬ìš©)\ndev:\n air -c .air.toml\n```\n\n### Docker\n\n```makefile\n.PHONY: docker-build docker-up docker-down\n\ndocker-build:\n docker build -t myapp/user-api -f apps/user-api/Dockerfile .\n\ndocker-up:\n docker-compose up -d\n\ndocker-down:\n docker-compose down\n```\n\n---\n\n## CI/CD ì—°ë™\n\n### GitHub Actions ì˜ˆì‹œ\n\n```yaml\n# .github/workflows/ci.yml\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-go@v5\n        with:\n          go-version: '1.21'\n      \n      - name: Lint\n        run: make lint\n      \n      - name: Test\n        run: make test\n      \n      - name: Build\n        run: make build\n```\n\nMakefileì„ ì‚¬ìš©í•˜ë©´ **ë¡œì»¬ê³¼ CI í™˜ê²½ì´ ë™ì¼**í•©ë‹ˆë‹¤.\n\n---\n\n## ìœ ìš©í•œ íŒ¨í„´\n\n### Help íƒ€ê²Ÿ\n\n```makefile\n.PHONY: help\n\nhelp:\n @echo \"ì‚¬ìš© ê°€ëŠ¥í•œ íƒ€ê²Ÿ:\"\n @echo \"  build    - ì „ì²´ ë¹Œë“œ\"\n @echo \"  test     - í…ŒìŠ¤íŠ¸ ì‹¤í–‰\"\n @echo \"  lint     - ë¦°íŠ¸ ê²€ì‚¬\"\n @echo \"  generate - ì½”ë“œ ìƒì„± (mock, wire)\"\n @echo \"  clean    - ë¹Œë“œ ê²°ê³¼ë¬¼ ì‚­ì œ\"\n```\n\n### ì˜ì¡´ì„± ì„¤ì¹˜\n\n```makefile\n.PHONY: tools\n\ntools:\n $(GO) install github.com/golangci/golangci-lint/cmd/golangci-lint@latest\n $(GO) install go.uber.org/mock/mockgen@latest\n $(GO) install github.com/google/wire/cmd/wire@latest\n```\n\n---\n\n## ì •ë¦¬: FaÃ§adeë¡œì„œì˜ Makefile\n\n```mermaid\ngraph TB\n    subgraph \"ê°œë°œì ì¸í„°í˜ì´ìŠ¤\"\n        MAKE[\"make build<br/>make test<br/>make generate\"]\n    end\n    \n    subgraph \"ìˆ¨ê²¨ì§„ ë³µì¡ì„±\"\n        BUILD[\"go build -o ... ./apps/.../cmd\"]\n        TEST[\"go test -race -cover ./...\"]\n        MOCK[\"go generate + mockgen\"]\n        WIRE[\"wire ./apps/.../cmd\"]\n    end\n    \n    MAKE --> BUILD\n    MAKE --> TEST\n    MAKE --> MOCK\n    MAKE --> WIRE\n```\n\n| ì¥ì  | ì„¤ëª… |\n|------|------|\n| **ì˜¨ë³´ë”© ë‹¨ìˆœí™”** | ìƒˆ íŒ€ì›ë„ `make build`ë¡œ ì‹œì‘ |\n| **ì¼ê´€ì„±** | ë¡œì»¬ = CI í™˜ê²½ |\n| **ë¬¸ì„œí™”** | Makefile ìì²´ê°€ ì‹¤í–‰ ê°€ëŠ¥í•œ ë¬¸ì„œ |\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [GNU Make Manual](https://www.gnu.org/software/make/manual/)\n- [Makefile Tutorial](https://makefiletutorial.com/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Architecture",
      "Backend",
      "Enterprise",
      "Go",
      "Tooling"
    ],
    "readingTime": 3,
    "wordCount": 518,
    "isFeatured": false,
    "isPublic": true,
    "series": "enterprise-go",
    "date": "2026-01-01"
  },
  {
    "id": "enterprise-go-8-observability",
    "slug": "enterprise-go-8-observability",
    "path": "backend/go",
    "fullPath": "backend/go/enterprise-go-8-observability",
    "title": "Enterprise Go ì‹œë¦¬ì¦ˆ #8: Observabilityì™€ Debugging",
    "excerpt": "Micrometer, Winstonì— ìµìˆ™í•œ ê°œë°œìë¥¼ ìœ„í•œ Go Observability ê°€ì´ë“œ. Grafana ëŒ€ì‹œë³´ë“œì™€ Alert ì—°ë™ê¹Œì§€ ë‹¤ë£¹ë‹ˆë‹¤.",
    "content": "# Enterprise Go ì‹œë¦¬ì¦ˆ #8: Observabilityì™€ Debugging\n\n> **ë‹¤ë¥¸ ìƒíƒœê³„ ê²½í—˜ìë¥¼ ìœ„í•œ ë§¤í•‘**\n>\n> - Java: Micrometer, Logback, Zipkin\n> - Node.js: Winston, prom-client, Jaeger\n\n## í•µì‹¬ ì§ˆë¬¸\n\ní”„ë¡œë•ì…˜ì—ì„œ:\n\n- ì¥ì•  ë°œìƒ ì‹œ ì›ì¸ì„ ì–´ë–»ê²Œ íŒŒì•…í•˜ë‚˜?\n- ì–´ë–¤ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•´ì•¼ í•˜ë‚˜?\n- Grafana ëŒ€ì‹œë³´ë“œì™€ AlertëŠ” ì–´ë–»ê²Œ êµ¬ì„±í•˜ë‚˜?\n\n---\n\n## Observability 3ìš”ì†Œ\n\n```mermaid\ngraph TB\n    subgraph \"Observability\"\n        LOGS[\"Logs<br/>ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚¬ë‚˜?\"]\n        METRICS[\"Metrics<br/>ì–¼ë§ˆë‚˜ ì¼ì–´ë‚¬ë‚˜?\"]\n        TRACES[\"Traces<br/>ì–´ë””ì„œ ì¼ì–´ë‚¬ë‚˜?\"]\n    end\n    \n    LOGS --> DEBUG[ë””ë²„ê¹…, ê°ì‚¬]\n    METRICS --> MONITOR[ëª¨ë‹ˆí„°ë§, ì•Œë¦¼]\n    TRACES --> ANALYSIS[ë³‘ëª© ë¶„ì„, ì˜ì¡´ì„± íŒŒì•…]\n```\n\n---\n\n## Logging (uber/zap)\n\n### êµ¬ì¡°í™”ëœ ë¡œê¹…\n\n| íŒ¨í„´ | Java | Go |\n|------|------|-----|\n| êµ¬ì¡°í™” ë¡œê¹… | Logback + JSON | uber/zap |\n| ì»¨í…ìŠ¤íŠ¸ ì „íŒŒ | MDC | Context + zap.With |\n\n### ë¡œê·¸ ë ˆë²¨ ê°€ì´ë“œ\n\n| ë ˆë²¨ | ì‚¬ìš© ì‹œì  | í”„ë¡œë•ì…˜ |\n|------|----------|---------|\n| DEBUG | ê°œë°œ/ë””ë²„ê¹… | OFF |\n| INFO | ì •ìƒ íë¦„ ê¸°ë¡ | ON |\n| WARN | ë³µêµ¬ ê°€ëŠ¥í•œ ë¬¸ì œ | ON |\n| ERROR | ì‹¤íŒ¨, ì¡°ì¹˜ í•„ìš” | ON + Alert |\n\n### Request ID ì „íŒŒ\n\n```mermaid\nsequenceDiagram\n    participant Middleware\n    participant Handler\n    participant UseCase\n    participant Logger\n    \n    Middleware->>Middleware: requestID ìƒì„±\n    Middleware->>Handler: ctxì— requestID ì €ì¥\n    Handler->>UseCase: ctx ì „ë‹¬\n    UseCase->>Logger: logger.With(ctx)\n    Logger->>Logger: requestID ìë™ í¬í•¨\n```\n\n### ë¡œê±° ì´ˆê¸°í™” íŒ¨í„´\n\n| íŒ¨í„´ | ë°©ì‹ | ì¥ë‹¨ì  |\n|------|------|--------|\n| **ReplaceGlobals** | `zap.ReplaceGlobals(logger)` | ê°„í¸, í…ŒìŠ¤íŠ¸ ì–´ë ¤ì›€ |\n| **DI ì£¼ì…** | Wireë¡œ ì£¼ì… | í…ŒìŠ¤íŠ¸ ìš©ì´, ëª…ì‹œì  |\n\n**ReplaceGlobals**: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ì— ì í•©\n\n```go\nlogger, _ := zap.NewProduction()\nzap.ReplaceGlobals(logger)\n// ì´í›„ ì–´ë””ì„œë“  zap.L().Info(...)\n```\n\n**DI (ê¶Œì¥)**: ì •ì‹ í”„ë¡œì íŠ¸ì—ì„œëŠ” Wireë¡œ ì£¼ì…\n\n```go\n// wire.go\nfunc InitializeServer(logger *zap.Logger) *Server { ... }\n```\n\ní…ŒìŠ¤íŠ¸ ì‹œ Mock ë¡œê±° ì£¼ì… ê°€ëŠ¥, ì˜ì¡´ì„± ëª…ì‹œì \n\n---\n\n## Metrics\n\n### ìˆ˜ì§‘í•´ì•¼ í•  í•µì‹¬ ë©”íŠ¸ë¦­\n\n#### RED Method (Request-driven)\n\n| ë©”íŠ¸ë¦­ | ì„¤ëª… | Prometheus íƒ€ì… |\n|--------|------|----------------|\n| **R**ate | ì´ˆë‹¹ ìš”ì²­ ìˆ˜ | Counter |\n| **E**rrors | ì—ëŸ¬ìœ¨ | Counter |\n| **D**uration | ì‘ë‹µ ì‹œê°„ ë¶„í¬ | Histogram |\n\n#### USE Method (Resource-driven)\n\n| ë©”íŠ¸ë¦­ | ì„¤ëª… | ì˜ˆì‹œ |\n|--------|------|------|\n| **U**tilization | ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  | CPU, Memory |\n| **S**aturation | í¬í™”ë„, ëŒ€ê¸°ì—´ | Goroutine ìˆ˜ |\n| **E**rrors | ë¦¬ì†ŒìŠ¤ ì—ëŸ¬ | Connection ì‹¤íŒ¨ |\n\n### Grafana ëŒ€ì‹œë³´ë“œ êµ¬ì„±\n\n```mermaid\ngraph TB\n    subgraph \"ëŒ€ì‹œë³´ë“œ ë ˆì´ì•„ì›ƒ\"\n        ROW1[\"Row 1: Overview\"]\n        ROW2[\"Row 2: HTTP\"]\n        ROW3[\"Row 3: Database\"]\n        ROW4[\"Row 4: External APIs\"]\n    end\n    \n    ROW1 --> P1[\"ìš”ì²­ë¥ <br/>Panel\"]\n    ROW1 --> P2[\"ì—ëŸ¬ìœ¨<br/>Panel\"]\n    ROW1 --> P3[\"P99 ë ˆì´í„´ì‹œ<br/>Panel\"]\n    \n    ROW2 --> P4[\"ìƒíƒœì½”ë“œë³„<br/>ìš”ì²­ ë¶„í¬\"]\n    ROW2 --> P5[\"ì—”ë“œí¬ì¸íŠ¸ë³„<br/>ë ˆì´í„´ì‹œ\"]\n```\n\n### Alert ì„¤ì • ì˜ˆì‹œ\n\n| Alert ì´ë¦„ | ì¡°ê±´ | ì‹¬ê°ë„ | ì±„ë„ |\n|-----------|------|--------|------|\n| HighErrorRate | ì—ëŸ¬ìœ¨ > 5% (5ë¶„) | Critical | Slack + PagerDuty |\n| HighLatency | P99 > 1s (5ë¶„) | Warning | Slack |\n| PodRestart | ì¬ì‹œì‘ > 3íšŒ (1ì‹œê°„) | Critical | Slack |\n\n---\n\n## Tracing\n\n### ë¶„ì‚° ì¶”ì ì´ í•„ìš”í•œ ìƒí™©\n\n```mermaid\nsequenceDiagram\n    participant Gateway\n    participant UserService\n    participant OrderService\n    participant PaymentService\n    \n    Note over Gateway,PaymentService: ê°™ì€ Trace ID ê³µìœ \n    \n    Gateway->>UserService: traceID: abc123\n    Gateway->>OrderService: traceID: abc123\n    OrderService->>PaymentService: traceID: abc123\n```\n\n### OpenTelemetry ì—°ë™\n\n| Java | Go |\n|------|-----|\n| Spring Cloud Sleuth | OpenTelemetry SDK |\n| Zipkin/Jaeger Exporter | OTLP Exporter |\n| @NewSpan | tracer.Start(ctx, name) |\n\n---\n\n## pprof: ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§\n\n### í”„ë¡œíŒŒì¼ ì¢…ë¥˜\n\n| í”„ë¡œíŒŒì¼ | ë¶„ì„ ëŒ€ìƒ | ì‚¬ìš© ì‹œì  |\n|---------|----------|----------|\n| CPU | í•¨ìˆ˜ë³„ CPU ì‚¬ìš© | ë†’ì€ CPU ì‚¬ìš©ë¥  |\n| Heap | ë©”ëª¨ë¦¬ í• ë‹¹ | ë©”ëª¨ë¦¬ ì¦ê°€ |\n| Goroutine | í™œì„± goroutine | ëˆ„ìˆ˜ ì˜ì‹¬ |\n| Block | ë¸”ë¡œí‚¹ ì§€ì  | ë™ê¸°í™” ë¬¸ì œ |\n\n### í”„ë¡œë•ì…˜ í™œì„±í™”\n\n```mermaid\ngraph LR\n    APP[Application] -->|\":6060\"| PPROF[pprof ì„œë²„]\n    PPROF -->|ì¸ì¦ í•„ìš”| ANALYSIS[ë¶„ì„ ë„êµ¬]\n```\n\n**ë³´ì•ˆ ì£¼ì˜**: pprof ì—”ë“œí¬ì¸íŠ¸ëŠ” ë‚´ë¶€ë§/VPNì—ì„œë§Œ ì ‘ê·¼\n\n---\n\n## í†µí•© ì•„í‚¤í…ì²˜\n\n```mermaid\ngraph TB\n    APP[Go Application]\n    \n    APP -->|zap| LOKI[Loki]\n    APP -->|Prometheus Client| PROM[Prometheus]\n    APP -->|OTLP| TEMPO[Tempo]\n    \n    LOKI --> GRAFANA[Grafana]\n    PROM --> GRAFANA\n    TEMPO --> GRAFANA\n    \n    GRAFANA --> ALERT[Alert Manager]\n    ALERT --> SLACK[Slack]\n    ALERT --> PAGER[PagerDuty]\n```\n\n---\n\n## SPoF ë°©ì§€\n\n### ë‹¨ì¼ ì¥ì•  ì§€ì  ì‹ë³„\n\n```mermaid\ngraph TD\n    subgraph \"í™•ì¸ ì‚¬í•­\"\n        Q1[\"ì™¸ë¶€ ì˜ì¡´ì„±ë³„<br/>Circuit Breaker?\"]\n        Q2[\"DB ì—°ê²° ì‹¤íŒ¨ ì‹œ<br/>Fallback?\"]\n        Q3[\"ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì¥ì•  ì‹œ<br/>Graceful Degradation?\"]\n    end\n```\n\n### ë©”íŠ¸ë¦­ ê¸°ë°˜ SPoF íƒì§€\n\n| ë©”íŠ¸ë¦­ | ì„ê³„ì¹˜ | ì˜ë¯¸ |\n|--------|-------|------|\n| ì˜ì¡´ì„±ë³„ ì—ëŸ¬ìœ¨ | > 50% | í•´ë‹¹ ì˜ì¡´ì„± ì¥ì•  |\n| DB ì»¤ë„¥ì…˜ í’€ ì‚¬ìš©ë¥  | > 90% | ì»¤ë„¥ì…˜ ê³ ê°ˆ ìœ„í—˜ |\n| Goroutine ìˆ˜ | ê¸‰ê²©í•œ ì¦ê°€ | ëˆ„ìˆ˜ ë˜ëŠ” ë¸”ë¡œí‚¹ |\n\n---\n\n## ì •ë¦¬\n\n| ìš”ì†Œ | ë„êµ¬ | ìš©ë„ |\n|------|------|------|\n| Logs | zap â†’ Loki | ë””ë²„ê¹…, ê°ì‚¬ |\n| Metrics | Prometheus â†’ Grafana | ëª¨ë‹ˆí„°ë§, Alert |\n| Traces | OpenTelemetry â†’ Tempo | ë³‘ëª© ë¶„ì„ |\n| Profile | pprof | ì„±ëŠ¥ ìµœì í™” |\n\n---\n\n## ì‹œë¦¬ì¦ˆ ë§ˆë¬´ë¦¬\n\n8í¸ì— ê±¸ì³ ì—”í„°í”„ë¼ì´ì¦ˆ Go ê°œë°œì˜ í•µì‹¬ì„ ë‹¤ë¤˜ìŠµë‹ˆë‹¤:\n\n| í¸ | ì£¼ì œ | í•µì‹¬ ìš”ì  |\n|----|------|----------|\n| 1 | í”„ë¡œì íŠ¸ ì„¤ê³„ | Hollow Main, internal/app, ì§„í™” ë‹¨ê³„ |\n| 2 | HTTP ì„œë²„ | ë¯¸ë“¤ì›¨ì–´ ìˆœì„œ, run.Group, Graceful Shutdown |\n| 3 | Context | íƒ€ì„ì•„ì›ƒ, ì·¨ì†Œ ì „íŒŒ, Request ID |\n| 4 | ë™ì‹œì„± | Goroutine ì œí•œ, errgroup |\n| 5 | ë°ì´í„°ë² ì´ìŠ¤ | WithTxë¡œ @Transactional ê²½í—˜ |\n| 6 | ì™¸ë¶€ í†µì‹  | íŒ¨í„´ ì¡°í•© ìˆœì„œ |\n| 7 | í…ŒìŠ¤íŠ¸ | Mock, Testcontainers, Ginkgo |\n| 8 | Observability | RED/USE, Grafana Alert |\n| 9 | Makefile | ê°œë°œ ì›Œí¬í”Œë¡œìš° ìë™í™”, FaÃ§ade |\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [uber/zap](https://github.com/uber-go/zap)\n- [Prometheus Go Client](https://github.com/prometheus/client_golang)\n- [OpenTelemetry Go](https://opentelemetry.io/docs/instrumentation/go/)\n- [Grafana Loki](https://grafana.com/oss/loki/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Enterprise",
      "Go",
      "Observability",
      "Performance"
    ],
    "readingTime": 5,
    "wordCount": 883,
    "isFeatured": false,
    "isPublic": true,
    "series": "enterprise-go",
    "date": "2026-01-01"
  },
  {
    "id": "enterprise-go-7-testing",
    "slug": "enterprise-go-7-testing",
    "path": "backend/go",
    "fullPath": "backend/go/enterprise-go-7-testing",
    "title": "Enterprise Go ì‹œë¦¬ì¦ˆ #7: í…ŒìŠ¤íŠ¸ ì „ëµê³¼ ì‹¤ì „",
    "excerpt": "JUnit, Jestì— ìµìˆ™í•œ ê°œë°œìë¥¼ ìœ„í•œ Go í…ŒìŠ¤íŠ¸ ìƒíƒœê³„ ê°€ì´ë“œ. Mock, í†µí•© í…ŒìŠ¤íŠ¸, BDD ìŠ¤íƒ€ì¼ì„ ë‹¤ë£¹ë‹ˆë‹¤.",
    "content": "# Enterprise Go ì‹œë¦¬ì¦ˆ #7: í…ŒìŠ¤íŠ¸ ì „ëµê³¼ ì‹¤ì „\n\n> **ë‹¤ë¥¸ ìƒíƒœê³„ ê²½í—˜ìë¥¼ ìœ„í•œ ë§¤í•‘**\n>\n> - Java: JUnit, Mockito, Testcontainers\n> - Node.js: Jest, Supertest\n> - Python: pytest, unittest.mock\n\n## í…ŒìŠ¤íŠ¸ ë„êµ¬ ë§¤í•‘\n\n| ê¸°ëŠ¥ | Java | Node.js | Go |\n|------|------|---------|-----|\n| Mock ìƒì„± | Mockito | jest.mock | **uber/gomock** |\n| í†µí•© í…ŒìŠ¤íŠ¸ | Testcontainers | Testcontainers | **Testcontainers-go** |\n| BDD ìŠ¤íƒ€ì¼ | Cucumber | Jest describe | **Ginkgo** |\n| ì–´ì„¤ì…˜ | AssertJ | Jest expect | **Gomega** |\n\n---\n\n## Mock: uber/gomock\n\n### Mockito vs gomock\n\n| Mockito | gomock |\n|---------|--------|\n| when(...).thenReturn(...) | EXPECT().Return(...) |\n| verify(..., times(n)) | EXPECT().Times(n) |\n| @Mock ì–´ë…¸í…Œì´ì…˜ | mockgen ì½”ë“œ ìƒì„± |\n\n### ì¸í„°í˜ì´ìŠ¤ ê¸°ë°˜ Mock\n\n```mermaid\ngraph LR\n    INTERFACE[UserRepository<br/>ì¸í„°í˜ì´ìŠ¤] --> IMPL[UserGormRepo<br/>ì‹¤ì œ êµ¬í˜„]\n    INTERFACE --> MOCK[MockUserRepository<br/>Mock êµ¬í˜„]\n```\n\n**Springê³¼ì˜ ì°¨ì´ì :**\n\n- Spring: @MockBeanìœ¼ë¡œ ëŸ°íƒ€ì„ ì£¼ì…\n- Go: **ì¸í„°í˜ì´ìŠ¤ ì •ì˜ í•„ìˆ˜**, ì»´íŒŒì¼ íƒ€ì„ íƒ€ì… ì²´í¬\n\n---\n\n## í†µí•© í…ŒìŠ¤íŠ¸: Testcontainers\n\n### Java vs Go\n\n| Java Testcontainers | Go Testcontainers |\n|---------------------|-------------------|\n| @Container ì–´ë…¸í…Œì´ì…˜ | container.Run() í•¨ìˆ˜ |\n| @DynamicPropertySource | ConnectionString() |\n| JUnit ìƒëª…ì£¼ê¸° | Ginkgo BeforeSuite/AfterSuite |\n\n### ì–¸ì œ ì‚¬ìš©í•˜ë‚˜?\n\n```mermaid\ngraph TD\n    Q{ë¬´ì—‡ì„ í…ŒìŠ¤íŠ¸?}\n    Q -->|ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§| MOCK[Mock ì‚¬ìš©]\n    Q -->|SQL ì¿¼ë¦¬| CONTAINER[Testcontainers]\n    Q -->|Repository í†µí•©| CONTAINER\n```\n\n---\n\n## BDD: Ginkgo\n\n### Jest vs Ginkgo\n\n| Jest | Ginkgo |\n|------|--------|\n| describe('...', () => {}) | Describe(\"...\", func() {}) |\n| beforeEach(() => {}) | BeforeEach(func() {}) |\n| it('should...', () => {}) | It(\"should...\", func() {}) |\n| expect(...).toBe(...) | Expect(...).To(Equal(...)) |\n\n### í…ŒìŠ¤íŠ¸ êµ¬ì¡°\n\n```mermaid\ngraph TB\n    DESCRIBE[\"Describe('UserService')\"]\n    DESCRIBE --> BEFORE[\"BeforeEach<br/>Mock ì„¤ì •\"]\n    DESCRIBE --> CONTEXT1[\"Context('ì¡´ì¬í•˜ëŠ” ì‚¬ìš©ì')\"]\n    DESCRIBE --> CONTEXT2[\"Context('ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì‚¬ìš©ì')\"]\n    CONTEXT1 --> IT1[\"It('ì •ë³´ ë°˜í™˜')\"]\n    CONTEXT2 --> IT2[\"It('ì—ëŸ¬ ë°˜í™˜')\"]\n```\n\n---\n\n## í…ŒìŠ¤íŠ¸ ë¶„ë¦¬: Label\n\n### JUnit @Tag vs Ginkgo Label\n\n| JUnit | Ginkgo |\n|-------|--------|\n| @Tag(\"integration\") | Label(\"integration\") |\n| -Dgroups=integration | --label-filter=\"integration\" |\n\n### CI íŒŒì´í”„ë¼ì¸\n\n```mermaid\ngraph LR\n    PR[PR Build] -->|ë¹ ë¥´ê²Œ| UNIT[\"--label-filter='!integration'\"]\n    MAIN[Main Build] -->|ì „ì²´| ALL[\"ginkgo -r\"]\n```\n\n---\n\n## ì •ë¦¬\n\n| ê¸°ëŠ¥ | ì¶”ì²œ ë„êµ¬ | ëŒ€ì‘ |\n|------|----------|------|\n| Mock | uber/gomock | Mockito |\n| í†µí•© í…ŒìŠ¤íŠ¸ | Testcontainers | @Testcontainers |\n| BDD | Ginkgo + Gomega | Jest describe |\n| ë¶„ë¦¬ ì‹¤í–‰ | Label | @Tag |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**8í¸: Observabilityì™€ Debugging**ì—ì„œëŠ” Micrometer, Winstonì— ëŒ€ì‘í•˜ëŠ” Goì˜ ê´€ì°°ê°€ëŠ¥ì„± ë„êµ¬ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [uber/gomock](https://github.com/uber-go/mock)\n- [Testcontainers Go](https://golang.testcontainers.org/)\n- [Ginkgo](https://onsi.github.io/ginkgo/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Enterprise",
      "Go",
      "Testing"
    ],
    "readingTime": 3,
    "wordCount": 401,
    "isFeatured": false,
    "isPublic": true,
    "series": "enterprise-go",
    "date": "2026-01-01"
  },
  {
    "id": "enterprise-go-6-resilient-communication",
    "slug": "enterprise-go-6-resilient-communication",
    "path": "backend/go",
    "fullPath": "backend/go/enterprise-go-6-resilient-communication",
    "title": "Enterprise Go ì‹œë¦¬ì¦ˆ #6: Resilientí•œ ì™¸ë¶€ í†µì‹ ",
    "excerpt": "Resilience4j, Pollyì— ìµìˆ™í•œ ê°œë°œìë¥¼ ìœ„í•œ Go Resilience íŒ¨í„´ ê°€ì´ë“œ. Circuit Breaker, Retry, Rate Limitingì„ ì˜¬ë°”ë¥´ê²Œ ì¡°í•©í•©ë‹ˆë‹¤.",
    "content": "# Enterprise Go ì‹œë¦¬ì¦ˆ #6: Resilientí•œ ì™¸ë¶€ í†µì‹ \n\n> **ë‹¤ë¥¸ ìƒíƒœê³„ ê²½í—˜ìë¥¼ ìœ„í•œ ë§¤í•‘**\n>\n> - Java: Resilience4j, Hystrix\n> - .NET: Polly\n> - Node.js: cockatiel, opossum\n\n## í•µì‹¬ ì§ˆë¬¸\n\në§ˆì´í¬ë¡œì„œë¹„ìŠ¤ í™˜ê²½ì—ì„œ:\n\n- ì™¸ë¶€ APIê°€ ëŠë ¤ì§€ë©´ ìš°ë¦¬ ì„œë¹„ìŠ¤ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜?\n- ì¬ì‹œë„ëŠ” ì–¸ì œ í•´ì•¼ í•˜ê³  ì–¸ì œ í•˜ë©´ ì•ˆ ë˜ë‚˜?\n- ì¥ì•  ì „íŒŒë¥¼ ì–´ë–»ê²Œ ë§‰ì„ ê²ƒì¸ê°€?\n\n---\n\n## Resilience íŒ¨í„´ ì¡°í•©\n\n### ì˜¬ë°”ë¥¸ ì ìš© ìˆœì„œ\n\n```mermaid\nflowchart LR\n    REQ[Request] --> RL[Rate Limit]\n    RL --> CB[Circuit Breaker]\n    CB --> RETRY[Retry]\n    RETRY --> TIMEOUT[Timeout]\n    TIMEOUT --> CLIENT[HTTP Client]\n```\n\n| ìˆœì„œ | íŒ¨í„´ | ëª©ì  | ìœ„ì¹˜ ì´ìœ  |\n|------|------|------|----------|\n| 1 | Rate Limit | ìš”ì²­ ì†ë„ ì œí•œ | ê°€ì¥ ë°”ê¹¥ì—ì„œ ê³¼ë¶€í•˜ ë°©ì§€ |\n| 2 | Circuit Breaker | ì¥ì•  ì„œë¹„ìŠ¤ ê²©ë¦¬ | ë¶ˆí•„ìš”í•œ ì¬ì‹œë„ ì°¨ë‹¨ |\n| 3 | Retry | ì¼ì‹œì  ì‹¤íŒ¨ ë³µêµ¬ | íƒ€ì„ì•„ì›ƒ ì „ì— ì¬ì‹œë„ ê²°ì • |\n| 4 | Timeout | ë¬´í•œ ëŒ€ê¸° ë°©ì§€ | ê°€ì¥ ì•ˆìª½ì—ì„œ ê°œë³„ ìš”ì²­ ì œì–´ |\n\n---\n\n## Timeout\n\n### ë‹¤ê³„ì¸µ íƒ€ì„ì•„ì›ƒ ì„¤ì •\n\n```mermaid\ngraph TB\n    subgraph \"íƒ€ì„ì•„ì›ƒ ê³„ì¸µ\"\n        DIAL[\"Connection: 5s\"]\n        TLS[\"TLS Handshake: 5s\"]\n        RESP[\"Response Header: 10s\"]\n        TOTAL[\"Total Request: 30s\"]\n    end\n    \n    DIAL --> TLS --> RESP --> TOTAL\n```\n\n| ì„¤ì • | ê¶Œì¥ê°’ | ëŒ€ì‘ ìƒí™© |\n|------|--------|----------|\n| ì—°ê²° íƒ€ì„ì•„ì›ƒ | 5ì´ˆ | DNS ì¥ì• , ë„¤íŠ¸ì›Œí¬ ë‹¨ì ˆ |\n| TLS í•¸ë“œì…°ì´í¬ | 5ì´ˆ | ì¸ì¦ì„œ ë¬¸ì œ |\n| ì‘ë‹µ í—¤ë” | 10ì´ˆ | ì„œë²„ ì²˜ë¦¬ ì§€ì—° |\n| ì „ì²´ ìš”ì²­ | 30ì´ˆ | ëŒ€ìš©ëŸ‰ ì‘ë‹µ í¬í•¨ |\n\n---\n\n## Retry\n\n### ì¬ì‹œë„ ê°€ëŠ¥ ì¡°ê±´\n\n```mermaid\ngraph TD\n    ERR{ì—ëŸ¬ ì¢…ë¥˜}\n    \n    ERR -->|5xx Server Error| YES[ì¬ì‹œë„ O]\n    ERR -->|408 Request Timeout| YES\n    ERR -->|429 Too Many Requests| WAIT[ëŒ€ê¸° í›„ ì¬ì‹œë„]\n    ERR -->|ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜| YES\n    \n    ERR -->|4xx Client Error| NO[ì¬ì‹œë„ X]\n    ERR -->|400, 401, 403, 404| NO\n```\n\n### Exponential Backoff with Jitter\n\n```mermaid\ngraph LR\n    R1[\"1ì°¨: 100ms\"] --> R2[\"2ì°¨: 200ms + jitter\"]\n    R2 --> R3[\"3ì°¨: 400ms + jitter\"]\n    R3 --> R4[\"4ì°¨: 800ms + jitter\"]\n    R4 --> FAIL[ìµœëŒ€ ë„ë‹¬]\n```\n\n**Jitterê°€ í•„ìš”í•œ ì´ìœ :**\n\n- ë™ì‹œì— ì‹¤íŒ¨í•œ í´ë¼ì´ì–¸íŠ¸ë“¤ì´ ë™ì‹œì— ì¬ì‹œë„ â†’ Thundering Herd\n- ëœë¤ ì§€ì—°ìœ¼ë¡œ ì¬ì‹œë„ ë¶„ì‚°\n\n---\n\n## Circuit Breaker\n\n### ìƒíƒœ ë¨¸ì‹ \n\n```mermaid\nstateDiagram-v2\n    [*] --> Closed: ì •ìƒ ë™ì‘\n    \n    Closed --> Open: ì‹¤íŒ¨ìœ¨ >= 60%<br/>(ìµœì†Œ 5íšŒ ìš”ì²­)\n    \n    Open --> HalfOpen: 30ì´ˆ ê²½ê³¼\n    \n    HalfOpen --> Closed: í…ŒìŠ¤íŠ¸ ì„±ê³µ\n    HalfOpen --> Open: í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨\n```\n\n### ìƒíƒœë³„ ë™ì‘\n\n| ìƒíƒœ | ìš”ì²­ ì²˜ë¦¬ | ë©”íŠ¸ë¦­ ìˆ˜ì§‘ |\n|------|----------|------------|\n| **Closed** | ì •ìƒ í†µê³¼ | O |\n| **Open** | ì¦‰ì‹œ ì‹¤íŒ¨ (Fallback) | X |\n| **Half-Open** | ì œí•œëœ ìˆ˜ë§Œ í†µê³¼ | O |\n\n### Resilience4j ëŒ€ì‘\n\n| Resilience4j | Go (sony/gobreaker) |\n|--------------|---------------------|\n| failureRateThreshold | ReadyToTrip ì½œë°± |\n| waitDurationInOpenState | Timeout |\n| permittedNumberOfCallsInHalfOpenState | MaxRequests |\n\n---\n\n## Rate Limiting\n\n### Token Bucket ì•Œê³ ë¦¬ì¦˜\n\n```mermaid\ngraph LR\n    FILL[\"í† í° ë³´ì¶©<br/>Nê°œ/ì´ˆ\"] --> BUCKET[\"ë²„í‚·<br/>ìµœëŒ€ Mê°œ\"]\n    BUCKET --> REQ[\"ìš”ì²­ ë„ì°©\"]\n    \n    REQ -->|í† í° ìˆìŒ| OK[ì²˜ë¦¬]\n    REQ -->|í† í° ì—†ìŒ| WAIT[ëŒ€ê¸°/ê±°ë¶€]\n```\n\n### ì™¸ë¶€ API Rate Limit ëŒ€ì‘\n\nì™¸ë¶€ APIê°€ 100 req/s ì œí•œì´ë¼ë©´:\n\n- **ìš°ë¦¬ ì„¤ì •**: 80 req/s (ì—¬ìœ ë¶„ 20%)\n- **ì´ìœ **: ë²„ìŠ¤íŠ¸ íŠ¸ë˜í”½, ë‹¤ë¥¸ í´ë¼ì´ì–¸íŠ¸ ê³ ë ¤\n\n---\n\n## íŒ¨í„´ ì¡°í•© ì „ëµ\n\n### ì–¸ì œ ì–´ë–¤ íŒ¨í„´ì„?\n\n| ìƒí™© | ì ìš© íŒ¨í„´ |\n|------|----------|\n| ë„¤íŠ¸ì›Œí¬ ì¼ì‹œ ì¥ì•  | Retry + Exponential Backoff |\n| ì™¸ë¶€ ì„œë¹„ìŠ¤ ì¥ì•  | Circuit Breaker |\n| ì™¸ë¶€ API í˜¸ì¶œëŸ‰ ì œí•œ | Rate Limiting |\n| ëª¨ë“  ì™¸ë¶€ í˜¸ì¶œ | Timeout (í•„ìˆ˜) |\n\n### ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ â†’ ì •ì‹ ì±„íƒ\n\në¹ ë¥¸ ê°œë°œì„ ìœ„í•´ Timeoutë§Œ ì„¤ì •í–ˆìœ¼ë‚˜, í”„ë¡œë•ì…˜ ë°°í¬ í›„ ê°œì„ :\n\n1. **1ì°¨**: Timeoutë§Œ ì ìš© â†’ ì™¸ë¶€ ì¥ì•  ì‹œ ìš”ì²­ ëˆ„ì \n2. **2ì°¨**: Retry ì¶”ê°€ â†’ ì¬ì‹œë„ í­í’ ë°œìƒ\n3. **3ì°¨**: Circuit Breaker ì¶”ê°€ â†’ ì¥ì•  ê²©ë¦¬ ì„±ê³µ\n4. **ìµœì¢…**: Rate Limitingìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´\n\n---\n\n## Go ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ\n\n| íŒ¨í„´ | ê¶Œì¥ ë¼ì´ë¸ŒëŸ¬ë¦¬ |\n|------|---------------|\n| Circuit Breaker | sony/gobreaker |\n| Retry | ì§ì ‘ êµ¬í˜„ ë˜ëŠ” avast/retry-go |\n| Rate Limiting | golang.org/x/time/rate |\n| í†µí•© | failsafe-go |\n\n---\n\n## ì •ë¦¬\n\n| íŒ¨í„´ | ëª©ì  | ì ìš© ìœ„ì¹˜ |\n|------|------|----------|\n| **Timeout** | ë¬´í•œ ëŒ€ê¸° ë°©ì§€ | í•„ìˆ˜, ëª¨ë“  ì™¸ë¶€ í˜¸ì¶œ |\n| **Retry** | ì¼ì‹œì  ì‹¤íŒ¨ ë³µêµ¬ | 5xx, ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ë§Œ |\n| **Circuit Breaker** | ì¥ì•  ì „íŒŒ ì°¨ë‹¨ | ì™¸ë¶€ ì˜ì¡´ì„± í˜¸ì¶œ |\n| **Rate Limiting** | ê³¼ë¶€í•˜ ë°©ì§€ | API í˜¸ì¶œëŸ‰ ì œí•œ ì‹œ |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**7í¸: í…ŒìŠ¤íŠ¸ ì „ëµê³¼ ì‹¤ì „**ì—ì„œëŠ” JUnit, Jestì— ëŒ€ì‘í•˜ëŠ” Goì˜ í…ŒìŠ¤íŠ¸ ìƒíƒœê³„ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [sony/gobreaker](https://github.com/sony/gobreaker)\n- [failsafe-go](https://failsafe-go.dev/)\n- [golang.org/x/time/rate](https://pkg.go.dev/golang.org/x/time/rate)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Enterprise",
      "Go",
      "HTTP",
      "Resilience"
    ],
    "readingTime": 4,
    "wordCount": 716,
    "isFeatured": false,
    "isPublic": true,
    "series": "enterprise-go",
    "date": "2026-01-01"
  },
  {
    "id": "enterprise-go-5-database",
    "slug": "enterprise-go-5-database",
    "path": "backend/go",
    "fullPath": "backend/go/enterprise-go-5-database",
    "title": "Enterprise Go ì‹œë¦¬ì¦ˆ #5: ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ íŒ¨í„´",
    "excerpt": "Springì˜ @Transactionalì²˜ëŸ¼ í¸ë¦¬í•˜ê²Œ íŠ¸ëœì­ì…˜ì„ ê´€ë¦¬í•˜ëŠ” Go íŒ¨í„´ì„ ë‹¤ë£¹ë‹ˆë‹¤. ì–´ë…¸í…Œì´ì…˜ í•˜ë‚˜ë¡œ í•´ê²°ë˜ë˜ ê²ƒì„ Goì—ì„œ ì–´ë–»ê²Œ êµ¬í˜„í• ê¹Œìš”?",
    "content": "# Enterprise Go ì‹œë¦¬ì¦ˆ #5: ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ íŒ¨í„´\n\n> **Spring ê°œë°œìë¥¼ ìœ„í•œ í•µì‹¬ ì§ˆë¬¸**\n> `@Transactional` ì–´ë…¸í…Œì´ì…˜ í•˜ë‚˜ë¡œ ë©”ì„œë“œë“¤ì„ í•˜ë‚˜ì˜ íŠ¸ëœì­ì…˜ìœ¼ë¡œ ë¬¶ë˜ í¸ë¦¬í•¨, Goì—ì„œë„ ê°€ëŠ¥í• ê¹Œìš”?\n\n## @Transactionalì˜ ë³¸ì§ˆ\n\n### Springì—ì„œì˜ ê²½í—˜\n\n```java\n@Transactional\npublic void transfer(String from, String to, BigDecimal amount) {\n    accountRepository.withdraw(from, amount);  // ê°™ì€ TX\n    accountRepository.deposit(to, amount);     // ê°™ì€ TX\n    logRepository.save(new TransferLog(...));  // ê°™ì€ TX\n}\n```\n\n**í•µì‹¬ í¸ì˜ì„±:**\n\n- ì–´ë…¸í…Œì´ì…˜ë§Œ ë¶™ì´ë©´ ë\n- ë©”ì„œë“œ ë‚´ ëª¨ë“  Repository í˜¸ì¶œì´ **ìë™ìœ¼ë¡œ ê°™ì€ íŠ¸ëœì­ì…˜**\n- ì˜ˆì™¸ ë°œìƒ ì‹œ **ìë™ ë¡¤ë°±**\n- ê°œë°œìëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì—ë§Œ ì§‘ì¤‘\n\n### Goì˜ í˜„ì‹¤\n\nGoì—ëŠ” AOPê°€ ì—†ìœ¼ë¯€ë¡œ `@Transactional`ê³¼ 100% ë™ì¼í•œ ê²½í—˜ì€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ **ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ í¸ì˜ì„±**ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n---\n\n## ëª©í‘œ: @Transactionalê³¼ ìœ ì‚¬í•œ ê²½í—˜\n\n### ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒ\n\n```mermaid\ngraph LR\n    subgraph \"Spring\"\n        A[\"@Transactional<br/>ì–´ë…¸í…Œì´ì…˜ í•œ ì¤„\"]\n    end\n    \n    subgraph \"Go ëª©í‘œ\"\n        B[\"WithTx(ctx, func(ctx) {...})<br/>ë˜í¼ í•¨ìˆ˜ í•œ ì¤„\"]\n    end\n    \n    A --> RESULT[Repositoryë“¤ì´<br/>ê°™ì€ TX ì‚¬ìš©]\n    B --> RESULT\n```\n\n### í•µì‹¬ ì•„ì´ë””ì–´\n\n```mermaid\nsequenceDiagram\n    participant Service\n    participant WithTx\n    participant Repository1\n    participant Repository2\n    participant DB\n    \n    Service->>WithTx: WithTx(ctx, func(ctx) error)\n    WithTx->>DB: BEGIN\n    WithTx->>WithTx: ctxì— TX ì‚½ì…\n    \n    WithTx->>Service: ctx (TX í¬í•¨)\n    \n    Service->>Repository1: Withdraw(ctx, ...)\n    Repository1->>Repository1: ctxì—ì„œ TX ì¶”ì¶œ\n    Repository1->>DB: UPDATE (TX ì‚¬ìš©)\n    \n    Service->>Repository2: Deposit(ctx, ...)\n    Repository2->>Repository2: ctxì—ì„œ TX ì¶”ì¶œ\n    Repository2->>DB: UPDATE (ê°™ì€ TX)\n    \n    alt ì„±ê³µ\n        WithTx->>DB: COMMIT\n    else ì—ëŸ¬\n        WithTx->>DB: ROLLBACK\n    end\n```\n\n---\n\n## êµ¬í˜„ íŒ¨í„´\n\n### ì‚¬ìš© ì½”ë“œ (UseCase)\n\nSpringì˜ `@Transactional`ê³¼ ìœ ì‚¬í•˜ê²Œ, **ë˜í¼ í•¨ìˆ˜ í•œ ì¤„**ë¡œ íŠ¸ëœì­ì…˜ ê²½ê³„ë¥¼ ì •ì˜í•©ë‹ˆë‹¤:\n\n```go\nfunc (u *TransferUseCase) Transfer(ctx context.Context, from, to string, amount int64) error {\n    // WithTx í•œ ì¤„ë¡œ íŠ¸ëœì­ì…˜ ì‹œì‘ - @Transactionalê³¼ ìœ ì‚¬!\n    return database.WithTx(ctx, u.db, func(ctx context.Context) error {\n        // ì´ ì•ˆì˜ ëª¨ë“  Repository í˜¸ì¶œì€ ê°™ì€ íŠ¸ëœì­ì…˜\n        if err := u.accountRepo.Withdraw(ctx, from, amount); err != nil {\n            return err  // ìë™ ë¡¤ë°±\n        }\n        if err := u.accountRepo.Deposit(ctx, to, amount); err != nil {\n            return err  // ìë™ ë¡¤ë°±\n        }\n        return u.logRepo.Save(ctx, &TransferLog{...})\n    })\n    // ì„±ê³µ ì‹œ ìë™ ì»¤ë°‹, ì‹¤íŒ¨ ì‹œ ìë™ ë¡¤ë°±\n}\n```\n\n### RepositoryëŠ” íŠ¸ëœì­ì…˜ì„ ëª¨ë¦„\n\n```go\nfunc (r *AccountRepository) Withdraw(ctx context.Context, id string, amount int64) error {\n    // ctxì—ì„œ TXê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì¼ë°˜ DB\n    db := database.GetDB(ctx, r.db)\n    return db.Model(&Account{}).Where(\"id = ?\", id).\n        Update(\"balance\", gorm.Expr(\"balance - ?\", amount)).Error\n}\n```\n\n---\n\n## Spring vs Go ë¹„êµ\n\n| ì¸¡ë©´ | Spring @Transactional | Go WithTx |\n|------|----------------------|-----------|\n| ë¬¸ë²• | ì–´ë…¸í…Œì´ì…˜ | ë˜í¼ í•¨ìˆ˜ |\n| ëª…ì‹œì„± | ì•”ë¬µì  | ëª…ì‹œì  |\n| íŠ¸ëœì­ì…˜ ì „íŒŒ | ì„ ì–¸ì  (REQUIRED ë“±) | Context ì „ë‹¬ |\n| ë¡¤ë°± ì¡°ê±´ | ì˜ˆì™¸ íƒ€ì… ê¸°ë°˜ | error ë°˜í™˜ |\n| í•™ìŠµ ê³¡ì„  | AOP ì´í•´ í•„ìš” | Context ì´í•´ í•„ìš” |\n\n### Goì˜ ì¥ì \n\n- **ëª…ì‹œì **: íŠ¸ëœì­ì…˜ ê²½ê³„ê°€ ì½”ë“œì— ë³´ì„\n- **í…ŒìŠ¤íŠ¸ ìš©ì´**: Mock Context ì£¼ì… ê°€ëŠ¥\n- **ë””ë²„ê¹… ìš©ì´**: ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ëª…í™•\n\n### Goì˜ ë‹¨ì \n\n- **ë°˜ë³µ ì½”ë“œ**: ë§¤ë²ˆ WithTx í˜¸ì¶œ í•„ìš”\n- **ê·œìœ¨ í•„ìš”**: GetDB í˜¸ì¶œ ëˆ„ë½ ì‹œ ë³„ë„ TX ì‚¬ìš©\n\n---\n\n## Connection Pool ì„¤ì •\n\n### í•µì‹¬ ì„¤ì •\n\n| ì„¤ì • | ê¶Œì¥ | ì´ìœ  |\n|------|------|------|\n| MaxOpenConns | 25-50 | DB ë™ì‹œ ì—°ê²° ì œí•œ ê³ ë ¤ |\n| MaxIdleConns | 10-25 | Openì˜ 40-50% |\n| ConnMaxLifetime | 5ë¶„ | ë°©í™”ë²½/LB íƒ€ì„ì•„ì›ƒ ê³ ë ¤ |\n\n### Spring ëŒ€ì‘\n\n| Spring | Go |\n|--------|-----|\n| HikariCP maximumPoolSize | MaxOpenConns |\n| minimumIdle | MaxIdleConns |\n| maxLifetime | ConnMaxLifetime |\n\n---\n\n## ì •ë¦¬\n\n| ìš”ì†Œ | ì—­í•  |\n|------|------|\n| **WithTx** | @Transactional ëŒ€ì‘, íŠ¸ëœì­ì…˜ ê²½ê³„ |\n| **GetDB** | Contextì—ì„œ TX ì¶”ì¶œ |\n| **Repository** | TXë¥¼ ëª¨ë¦„, Contextë§Œ ë°›ìŒ |\n\n**í•µì‹¬ ë©”ì‹œì§€**: Goì—ì„œë„ `WithTx` ë˜í¼ í•˜ë‚˜ë¡œ Springì˜ `@Transactional`ê³¼ ìœ ì‚¬í•œ í¸ì˜ì„±ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**6í¸: Resilientí•œ ì™¸ë¶€ í†µì‹ **ì—ì„œëŠ” Resilience4jì— ëŒ€ì‘í•˜ëŠ” Goì˜ Circuit Breaker, Retry íŒ¨í„´ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [GORM ê³µì‹ ë¬¸ì„œ](https://gorm.io/)\n- [database/sql](https://golang.org/pkg/database/sql/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Database",
      "Enterprise",
      "GORM",
      "Go",
      "Repository",
      "Transaction"
    ],
    "readingTime": 3,
    "wordCount": 594,
    "isFeatured": false,
    "isPublic": true,
    "series": "enterprise-go",
    "date": "2026-01-01"
  },
  {
    "id": "enterprise-go-4-goroutine-channel",
    "slug": "enterprise-go-4-goroutine-channel",
    "path": "backend/go",
    "fullPath": "backend/go/enterprise-go-4-goroutine-channel",
    "title": "Enterprise Go ì‹œë¦¬ì¦ˆ #4: Goroutineê³¼ Channel ì‹¤ì „ í™œìš©",
    "excerpt": "ExecutorService, Worker Threadsì— ìµìˆ™í•œ ê°œë°œìë¥¼ ìœ„í•œ Go ë™ì‹œì„± ëª¨ë¸ ê°€ì´ë“œ. Goroutineì˜ ë©”ëª¨ë¦¬ íŠ¹ì„±ê³¼ ì•ˆì „í•œ íŒ¨í„´ì„ ë‹¤ë£¹ë‹ˆë‹¤.",
    "content": "# Enterprise Go ì‹œë¦¬ì¦ˆ #4: Goroutineê³¼ Channel ì‹¤ì „ í™œìš©\n\n> **ë‹¤ë¥¸ ì–¸ì–´ ê²½í—˜ìë¥¼ ìœ„í•œ ë§¤í•‘**\n>\n> - Java: `ExecutorService`, `CompletableFuture`\n> - Node.js: Worker Threads, `Promise.all`\n> - Rust: `tokio::spawn`, `mpsc`\n\n## Goroutineì˜ ì‹¤ì²´\n\n### Java Thread vs Go Goroutine\n\n| íŠ¹ì„± | Java Thread | Go Goroutine |\n|------|------------|--------------|\n| ì´ˆê¸° ìŠ¤íƒ | ~1MB (OS í• ë‹¹) | **~2KB** (Go ëŸ°íƒ€ì„) |\n| ìµœëŒ€ ìŠ¤íƒ | ê³ ì • | **ë™ì  í™•ì¥** (ìµœëŒ€ 1GB) |\n| ìŠ¤ì¼€ì¤„ë§ | OS ì»¤ë„ | Go ëŸ°íƒ€ì„ (M:N) |\n| ìƒì„± ë¹„ìš© | ë†’ìŒ | ë‚®ìŒ |\n| ì‹¤ìš©ì  ìƒí•œ | ìˆ˜ì²œ ê°œ | **ìˆ˜ì‹­ë§Œ ê°œ** |\n\n### ì™œ ì œí•œì´ í•„ìš”í•œê°€?\n\nGoroutineì´ ê°€ë³ë‹¤ê³  í•´ì„œ ë¬´ì œí•œì€ ì•„ë‹™ë‹ˆë‹¤.\n\n**ì‹¤ì¦ì  ê·¼ê±°:**\n\n- ì´ˆê¸° ìŠ¤íƒ 2KB + ë””ìŠ¤í¬ë¦½í„° ~400ë°”ì´íŠ¸ â‰ˆ **ì•½ 2.4KB / goroutine**\n- 100,000ê°œ goroutine = **ìµœì†Œ 240MB** ë©”ëª¨ë¦¬\n- ì»¨í…Œì´ë„ˆ ë©”ëª¨ë¦¬ ì œí•œ(512MB~2GB)ì—ì„œ OOM ìœ„í—˜\n\n```mermaid\ngraph LR\n    subgraph \"ë©”ëª¨ë¦¬ ê³„ì‚°\"\n        G[100,000 goroutines] --> MEM[\"240MB+ ë©”ëª¨ë¦¬\"]\n        MEM --> LIMIT[\"ì»¨í…Œì´ë„ˆ ì œí•œ ì´ˆê³¼\"]\n        LIMIT --> OOM[\"OOM Killer\"]\n    end\n```\n\n**ì‹¤ì œ OOM ì‹œë‚˜ë¦¬ì˜¤:**\n\n1. ìš”ì²­ë§ˆë‹¤ goroutine ìƒì„±\n2. ì™¸ë¶€ API ì‘ë‹µ ì§€ì—°ìœ¼ë¡œ goroutine ëŒ€ê¸°\n3. ëŒ€ê¸° ì¤‘ì¸ goroutine ëˆ„ì \n4. ì»¨í…Œì´ë„ˆ ë©”ëª¨ë¦¬ ì œí•œ ë„ë‹¬ â†’ ê°•ì œ ì¢…ë£Œ\n\n---\n\n## Java ê°œë°œìë¥¼ ìœ„í•œ íŒ¨í„´ ë§¤í•‘\n\n### ExecutorService â†’ Worker Pool\n\n```mermaid\ngraph TB\n    subgraph \"Java ExecutorService\"\n        JES[newFixedThreadPool 10]\n        JQ[BlockingQueue]\n        JES --> JQ\n    end\n    \n    subgraph \"Go Worker Pool\"\n        GW[10 Worker Goroutines]\n        GC[Buffered Channel]\n        GW --> GC\n    end\n```\n\n| Java | Go |\n|------|-----|\n| `Executors.newFixedThreadPool(10)` | Worker Pool íŒ¨í„´ |\n| `executor.submit(task)` | `jobs <- task` |\n| `Future.get()` | `<-results` |\n\n### CompletableFuture.allOf â†’ errgroup\n\n```mermaid\nsequenceDiagram\n    participant Main\n    participant G1 as Goroutine 1\n    participant G2 as Goroutine 2\n    participant G3 as Goroutine 3\n    \n    Main->>G1: g.Go(fn1)\n    Main->>G2: g.Go(fn2)\n    Main->>G3: g.Go(fn3)\n    \n    G2->>Main: return error\n    Note over Main: Context ì·¨ì†Œ ì „íŒŒ\n    \n    G1-->>Main: ctx.Done() ê°ì§€\n    G3-->>Main: ctx.Done() ê°ì§€\n    \n    Main->>Main: g.Wait() - ì²« ì—ëŸ¬ ë°˜í™˜\n```\n\n**Javaì™€ì˜ ì°¨ì´ì :**\n\n- `CompletableFuture.allOf`ëŠ” ëª¨ë“  ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¼\n- `errgroup`ì€ ì²« ì—ëŸ¬ ì‹œ ë‚˜ë¨¸ì§€ **ì·¨ì†Œ ê°€ëŠ¥** (Context ì—°ë™)\n\n---\n\n## Channel ì„¤ê³„ ì›ì¹™\n\n### Buffered vs Unbuffered\n\n| íƒ€ì… | Java ëŒ€ì‘ | ì‚¬ìš© ì‹œì  |\n|------|----------|----------|\n| Unbuffered | `SynchronousQueue` | í•¸ë“œì…°ì´í¬, ë™ê¸° í†µì‹  |\n| Buffered | `ArrayBlockingQueue` | ë°±í”„ë ˆì…”, ë¹„ë™ê¸° |\n\n### ë‹«ê¸° ê·œì¹™\n\n```mermaid\ngraph LR\n    PRODUCER[\"Producer<br/>(Channel ìƒì„±ì)\"] -->|close| CH[Channel]\n    CH --> C1[Consumer 1]\n    CH --> C2[Consumer 2]\n```\n\n**Javaì™€ì˜ ì°¨ì´:**\n\n- Java: `BlockingQueue`ëŠ” ë³´í†µ ë‹«ì§€ ì•ŠìŒ\n- Go: **Producerê°€ close() í˜¸ì¶œ** (ê·œì¹™)\n\n---\n\n## ë™ê¸°í™” ë„êµ¬ ì„ íƒ\n\n### Mutex vs Channel\n\n```mermaid\ngraph TD\n    Q{ë¬´ì—‡ì´ í•„ìš”í•œê°€?}\n    Q -->|ìƒíƒœ ë³´í˜¸| MUTEX[sync.Mutex]\n    Q -->|ë°ì´í„° ì „ë‹¬| CHANNEL[Channel]\n    \n    MUTEX --> M1[\"ìºì‹œ, ì¹´ìš´í„°\"]\n    CHANNEL --> C1[\"ì‘ì—… ë¶„ë°°, ì´ë²¤íŠ¸\"]\n```\n\n| ìƒí™© | Java | Go |\n|------|------|-----|\n| ê³µìœ  ìƒíƒœ ë³´í˜¸ | `synchronized` | `sync.Mutex` |\n| ì‘ì—… ë¶„ë°° | `BlockingQueue` | Channel |\n| ë³‘ë ¬ ì‹¤í–‰ í›„ ìˆ˜ì§‘ | `CompletableFuture` | errgroup |\n\n---\n\n## Race Detector\n\nGoì˜ ê°•ë ¥í•œ ì¥ì  ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. Javaì˜ ThreadSanitizerì— ëŒ€ì‘ë˜ë‚˜, **í‘œì¤€ ë„êµ¬ë¡œ ë‚´ì¥**ë˜ì–´ ìˆìŠµë‹ˆë‹¤:\n\n```bash\ngo test -race ./...\ngo build -race ./cmd/myapp\n```\n\n**CI í•„ìˆ˜ ì ìš© ê¶Œì¥** - ëŸ°íƒ€ì„ ì˜¤ë²„í—¤ë“œê°€ ìˆìœ¼ë‚˜ í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œëŠ” ë¬¸ì œì—†ìŒ\n\n---\n\n## ì •ë¦¬\n\n| ê°œë… | Java ëŒ€ì‘ | Go |\n|------|----------|-----|\n| ìŠ¤ë ˆë“œ í’€ | ExecutorService | Worker Pool + Channel |\n| ë³‘ë ¬ ì‹¤í–‰ | CompletableFuture | errgroup |\n| ê³µìœ  ìƒíƒœ | synchronized | sync.Mutex |\n| ë©”ì‹œì§€ ì „ë‹¬ | BlockingQueue | Channel |\n| ê²½ìŸ ìƒíƒœ íƒì§€ | ThreadSanitizer | -race í”Œë˜ê·¸ |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**5í¸: ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ íŒ¨í„´**ì—ì„œëŠ” Springì˜ `@Transactional`ì— ëŒ€ì‘í•˜ëŠ” Goì˜ íŠ¸ëœì­ì…˜ ê´€ë¦¬ ì „ëµì„ ë‹¤ë£¹ë‹ˆë‹¤.\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Go Concurrency Patterns](https://talks.golang.org/2012/concurrency.slide)\n- [errgroup Package](https://pkg.go.dev/golang.org/x/sync/errgroup)\n- [Goroutine Stack Size](https://go.dev/blog/go1.4)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Concurrency",
      "Enterprise",
      "Go"
    ],
    "readingTime": 3,
    "wordCount": 583,
    "isFeatured": false,
    "isPublic": true,
    "series": "enterprise-go",
    "date": "2026-01-01"
  },
  {
    "id": "enterprise-go-3-context",
    "slug": "enterprise-go-3-context",
    "path": "backend/go",
    "fullPath": "backend/go/enterprise-go-3-context",
    "title": "Enterprise Go ì‹œë¦¬ì¦ˆ #3: Contextë¡œ ìš”ì²­ ìƒëª…ì£¼ê¸° ê´€ë¦¬",
    "excerpt": "Javaì˜ ThreadLocal, Node.jsì˜ AsyncLocalStorageì— ìµìˆ™í•œ ê°œë°œìë¥¼ ìœ„í•œ Go Context íŒ¨í„´ ê°€ì´ë“œì…ë‹ˆë‹¤.",
    "content": "# Enterprise Go ì‹œë¦¬ì¦ˆ #3: Contextë¡œ ìš”ì²­ ìƒëª…ì£¼ê¸° ê´€ë¦¬\n\n> **ë‹¤ë¥¸ ìƒíƒœê³„ ê²½í—˜ìë¥¼ ìœ„í•œ ë§¤í•‘**\n>\n> - Java: ThreadLocal, MDC\n> - Node.js: AsyncLocalStorage\n> - Python: contextvars\n\n## Contextì˜ ì—­í• \n\n### ë‹¤ë¥¸ ì–¸ì–´ì™€ì˜ ë¹„êµ\n\n| ê¸°ëŠ¥ | Java | Node.js | Go |\n|------|------|---------|-----|\n| ìš”ì²­ ë²”ìœ„ ê°’ | ThreadLocal | AsyncLocalStorage | context.WithValue |\n| íƒ€ì„ì•„ì›ƒ | ExecutorService timeout | AbortController | context.WithTimeout |\n| ì·¨ì†Œ ì „íŒŒ | Future.cancel() | AbortSignal | context.WithCancel |\n\n### í•µì‹¬ ì°¨ì´ì \n\nGo ContextëŠ” ì„¸ ê°€ì§€ ì—­í• ì„ **í•˜ë‚˜ì˜ ì¸í„°í˜ì´ìŠ¤**ì—ì„œ ì²˜ë¦¬:\n\n1. ê°’ ì „íŒŒ (Request ID, Trace ID)\n2. íƒ€ì„ì•„ì›ƒ\n3. ì·¨ì†Œ ì‹ í˜¸ ì „íŒŒ\n\n---\n\n## íƒ€ì„ì•„ì›ƒ ì„¤ê³„\n\n### ê³„ì¸µë³„ ì „ëµ\n\n```mermaid\ngraph TB\n    HTTP[\"HTTP ì „ì²´: 30s\"] --> UC[\"UseCase: 10s\"]\n    UC --> DB[\"DB ì¿¼ë¦¬: 3s\"]\n    UC --> EXT[\"ì™¸ë¶€ API: 5s\"]\n```\n\n**ì›ì¹™**: í•˜ìœ„ íƒ€ì„ì•„ì›ƒ í•© < ìƒìœ„ íƒ€ì„ì•„ì›ƒ\n\n### Spring vs Go\n\n| Spring | Go |\n|--------|-----|\n| @Transactional(timeout=3) | WithTimeout(ctx, 3*time.Second) |\n| RestTemplate.setConnectTimeout | http.Client Timeout |\n\n---\n\n## ì·¨ì†Œ ì „íŒŒ\n\n### ì™œ ì¤‘ìš”í•œê°€?\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant Server\n    participant DB\n    participant API\n    \n    Client->>Server: ìš”ì²­\n    Server->>DB: ì¿¼ë¦¬ ì‹œì‘\n    Server->>API: API í˜¸ì¶œ ì‹œì‘\n    Client-xServer: ì—°ê²° ëŠê¹€\n    \n    Note over DB,API: Context ì „íŒŒ O<br/>â†’ ëª¨ë‘ ì·¨ì†Œ\n    Note over DB,API: Context ì „íŒŒ X<br/>â†’ ê³„ì† ì‹¤í–‰ (ë‚­ë¹„)\n```\n\n### Java ëŒ€ì‘\n\n| Java | Go |\n|------|-----|\n| Future.cancel(true) | cancel() í˜¸ì¶œ |\n| InterruptedException | ctx.Done() ì±„ë„ |\n\n---\n\n## Request ID ì „íŒŒ\n\n### MDC vs Context\n\n```mermaid\ngraph LR\n    subgraph \"Java MDC\"\n        FILTER[Filter] --> MDC[\"MDC.put('requestId')\"]\n        MDC --> SERVICE[Service]\n        SERVICE --> LOG[\"ìë™ í¬í•¨\"]\n    end\n    \n    subgraph \"Go Context\"\n        MW[Middleware] --> CTX[\"WithValue(requestID)\"]\n        CTX --> HANDLER[Handler]\n        HANDLER --> LOGGER[\"logger.With(ctx)\"]\n    end\n```\n\n---\n\n## ì•ˆí‹°íŒ¨í„´\n\n### Contextë¥¼ êµ¬ì¡°ì²´ì— ì €ì¥\n\n```mermaid\ngraph TD\n    BAD[\"struct { ctx Context }\"] --> PROBLEM1[\"ìƒëª…ì£¼ê¸° ë¶ˆì¼ì¹˜\"]\n    BAD --> PROBLEM2[\"ì·¨ì†Œ ì‹ í˜¸ ë¬´ì‹œ\"]\n    \n    GOOD[\"func(ctx Context)\"] --> OK[\"ë§¤ë²ˆ ìµœì‹  ctx ì‚¬ìš©\"]\n```\n\n### Spring ê°œë°œì ì£¼ì˜ì \n\n| Spring ìŠµê´€ | Go ê·œì¹™ |\n|------------|---------|\n| @Autowiredë¡œ ì£¼ì… | **íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬** |\n| ThreadLocalì— ì €ì¥ | **ì €ì¥í•˜ì§€ ì•ŠìŒ** |\n| ì„ íƒì  ì‚¬ìš© | **ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„° (í•„ìˆ˜)** |\n\n---\n\n## ì •ë¦¬\n\n| ê°œë… | Java/Spring | Go |\n|------|-------------|-----|\n| ìš”ì²­ ë²”ìœ„ ê°’ | ThreadLocal/MDC | WithValue |\n| íƒ€ì„ì•„ì›ƒ | @Transactional timeout | WithTimeout |\n| ì·¨ì†Œ | Future.cancel | cancel() |\n| ì‚¬ìš© íŒ¨í„´ | ì•”ë¬µì  (ThreadLocal) | **ëª…ì‹œì  (íŒŒë¼ë¯¸í„°)** |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**4í¸: Goroutineê³¼ Channel ì‹¤ì „ í™œìš©**ì—ì„œëŠ” Javaì˜ ExecutorService, Node.jsì˜ Worker Threadsì— ëŒ€ì‘í•˜ëŠ” Go ë™ì‹œì„± íŒ¨í„´ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Go Blog: Context](https://blog.golang.org/context)\n- [context Package](https://pkg.go.dev/context)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Cancellation",
      "Concurrency",
      "Enterprise",
      "Go",
      "Observability",
      "Timeout"
    ],
    "readingTime": 3,
    "wordCount": 409,
    "isFeatured": false,
    "isPublic": true,
    "series": "enterprise-go",
    "date": "2026-01-01"
  },
  {
    "id": "enterprise-go-2-http-server",
    "slug": "enterprise-go-2-http-server",
    "path": "backend/go",
    "fullPath": "backend/go/enterprise-go-2-http-server",
    "title": "Enterprise Go ì‹œë¦¬ì¦ˆ #2: ê²¬ê³ í•œ HTTP ì„œë²„ êµ¬ì¶•",
    "excerpt": "Spring MVCì˜ Filter/Interceptor, Expressì˜ Middlewareì— ìµìˆ™í•œ ê°œë°œìë¥¼ ìœ„í•œ Echo ë¯¸ë“¤ì›¨ì–´ ì„¤ê³„ ê°€ì´ë“œì…ë‹ˆë‹¤.",
    "content": "# Enterprise Go ì‹œë¦¬ì¦ˆ #2: ê²¬ê³ í•œ HTTP ì„œë²„ êµ¬ì¶•\n\n> **ë‹¤ë¥¸ ìƒíƒœê³„ ê²½í—˜ìë¥¼ ìœ„í•œ ë§¤í•‘**\n>\n> - Java: Spring MVC Filter, HandlerInterceptor\n> - Node.js: Express Middleware\n> - Python: Django Middleware, FastAPI Middleware\n\n## ë¯¸ë“¤ì›¨ì–´ ì²´ì¸: ìˆœì„œê°€ ì¤‘ìš”í•˜ë‹¤\n\n### ì‹¤í–‰ íë¦„\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant Recover\n    participant RequestID\n    participant Logger\n    participant Auth\n    participant Handler\n    \n    Client->>Recover: Request\n    Recover->>RequestID: \n    RequestID->>Logger: \n    Logger->>Auth: \n    Auth->>Handler: \n    Handler-->>Auth: Response\n    Auth-->>Logger: \n    Logger-->>RequestID: \n    RequestID-->>Recover: \n    Recover-->>Client: \n```\n\n### ê¶Œì¥ ìˆœì„œì™€ ì´ìœ \n\n| ìˆœì„œ | ë¯¸ë“¤ì›¨ì–´ | Spring ëŒ€ì‘ | ìœ„ì¹˜ ì´ìœ  |\n|------|---------|------------|----------|\n| 1 | Recover | @ControllerAdvice | íŒ¨ë‹‰ â†’ 500 ë³€í™˜ |\n| 2 | Request ID | MDC ì„¤ì • | ë¡œê¹… ì „ ID í•„ìš” |\n| 3 | Logger | AccessLogFilter | ì¸ì¦ ì‹¤íŒ¨ë„ ë¡œê¹… |\n| 4 | CORS | CorsFilter | preflight ì²˜ë¦¬ |\n| 5 | Auth | SecurityFilter | í•¸ë“¤ëŸ¬ ë³´í˜¸ |\n\n### ê²½í—˜ë‹´\n\në¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ì„ ìœ„í•´ ë¯¸ë“¤ì›¨ì–´ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•Šì•˜ìœ¼ë‚˜, ì •ì‹ ì±„íƒ í›„:\n\n> Recoverë¥¼ Auth ë’¤ì— ë‘ì—ˆë”ë‹ˆ, ì¸ì¦ ë¡œì§ íŒ¨ë‹‰ ì‹œ ì‘ë‹µ ì—†ì´ ì—°ê²° ëŠê¹€\n\n> Loggerë¥¼ RequestID ì•ì— ë‘ì—ˆë”ë‹ˆ, ë¡œê·¸ì—ì„œ ìš”ì²­ ì¶”ì  ë¶ˆê°€\n\n---\n\n## ì—ëŸ¬ í•¸ë“¤ë§\n\n### ì¼ê´€ëœ ì‘ë‹µ êµ¬ì¡°\n\n```mermaid\ngraph TD\n    subgraph \"í‘œì¤€ ì—ëŸ¬ ì‘ë‹µ\"\n        CODE[\"code: 'VALIDATION_ERROR'\"]\n        MSG[\"message: 'ì´ë©”ì¼ í˜•ì‹ ì˜¤ë¥˜'\"]\n        DETAILS[\"details: {...}\"]\n    end\n```\n\n### ë„ë©”ì¸ ì—ëŸ¬ â†’ HTTP ë§¤í•‘\n\n| Domain Error | HTTP Status | Spring ëŒ€ì‘ |\n|--------------|-------------|------------|\n| ErrNotFound | 404 | @ResponseStatus |\n| ErrValidation | 400 | MethodArgumentNotValidException |\n| ErrUnauthorized | 401 | AuthenticationException |\n| ê¸°íƒ€ | 500 | @ExceptionHandler |\n\n---\n\n## Graceful Shutdown\n\n### ì™œ í•„ìš”í•œê°€?\n\n```mermaid\nsequenceDiagram\n    participant K8s\n    participant Pod\n    participant Client\n    \n    K8s->>Pod: SIGTERM\n    Pod->>Pod: ìƒˆ ìš”ì²­ ê±°ë¶€\n    Pod->>Client: ì§„í–‰ ì¤‘ ìš”ì²­ ì™„ë£Œ\n    Pod->>K8s: ì •ìƒ ì¢…ë£Œ\n```\n\n### Spring vs Go\n\n| ì¸¡ë©´ | Spring Boot | Go Echo |\n|------|-------------|---------|\n| ì„¤ì • | server.shutdown=graceful | ì§ì ‘ êµ¬í˜„ |\n| íƒ€ì„ì•„ì›ƒ | spring.lifecycle.timeout-per-shutdown-phase | WithTimeout |\n| ì‹œê·¸ë„ | ìë™ ì²˜ë¦¬ | signal.NotifyContext |\n\n### run.Group íŒ¨í„´ (Prometheus ë°©ì‹)\n\nì„œë²„ê°€ ì—¬ëŸ¬ ê³ ë£¨í‹´(ì›¹ ì„œë²„, ì›Œì»¤, í—¬ìŠ¤ì²´í¬ ë“±)ìœ¼ë¡œ êµ¬ì„±ë  ë•Œ:\n\n```go\nimport \"github.com/oklog/run\"\n\nvar g run.Group\n\n// HTTP ì„œë²„\ng.Add(func() error {\n    return server.ListenAndServe()\n}, func(err error) {\n    server.Shutdown(ctx)\n})\n\n// ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤\ng.Add(func() error {\n    return worker.Run()\n}, func(err error) {\n    worker.Stop()\n})\n\n// í•˜ë‚˜ë¼ë„ ì¢…ë£Œë˜ë©´ ì „ì²´ ì¢…ë£Œ\nif err := g.Run(); err != nil {\n    log.Error(err)\n}\n```\n\n**ì¥ì :** ì‹œê·¸ë„ ì²˜ë¦¬, ì—ëŸ¬ ì „íŒŒ, ì¢…ë£Œ ìˆœì„œë¥¼ í•œ ê³³ì—ì„œ ê´€ë¦¬\n\n---\n\n## ì •ë¦¬\n\n| ìš”ì†Œ | í•µì‹¬ |\n|------|------|\n| ë¯¸ë“¤ì›¨ì–´ ìˆœì„œ | Recover â†’ RequestID â†’ Logger â†’ Auth |\n| ì—ëŸ¬ í•¸ë“¤ë§ | ë„ë©”ì¸ ì—ëŸ¬ â†’ HTTP ìƒíƒœ ë§¤í•‘ |\n| Graceful Shutdown | SIGTERM ì²˜ë¦¬, run.Group íŒ¨í„´ |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**3í¸: Contextë¡œ ìš”ì²­ ìƒëª…ì£¼ê¸° ê´€ë¦¬**ì—ì„œëŠ” Javaì˜ ThreadLocal, Node.jsì˜ AsyncLocalStorageì— ëŒ€ì‘í•˜ëŠ” Context íŒ¨í„´ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Echo ê³µì‹ ë¬¸ì„œ](https://echo.labstack.com/)\n- [Graceful Shutdown](https://pkg.go.dev/net/http#Server.Shutdown)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Architecture",
      "Backend",
      "Echo",
      "Enterprise",
      "Go",
      "HTTP",
      "Resilience"
    ],
    "readingTime": 3,
    "wordCount": 460,
    "isFeatured": false,
    "isPublic": true,
    "series": "enterprise-go",
    "date": "2026-01-01"
  },
  {
    "id": "enterprise-go-1-project-structure",
    "slug": "enterprise-go-1-project-structure",
    "path": "backend/go",
    "fullPath": "backend/go/enterprise-go-1-project-structure",
    "title": "Enterprise Go ì‹œë¦¬ì¦ˆ #1: í”„ë¡œì íŠ¸ ì„¤ê³„ì™€ êµ¬ì¡°í™”",
    "excerpt": "Kubernetes, Docker CLI, Prometheus, Hugoì˜ ì†ŒìŠ¤ ì½”ë“œë¥¼ ë¶„ì„í•˜ì—¬ ë„ì¶œí•œ Go í”„ë¡œì íŠ¸ êµ¬ì¡° ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.",
    "content": "# Enterprise Go ì‹œë¦¬ì¦ˆ #1: í”„ë¡œì íŠ¸ ì„¤ê³„ì™€ êµ¬ì¡°í™”\n\n> **ëŒ€ìƒ ë…ì**: Java, Node.js ë“± ë‹¤ë¥¸ ìƒíƒœê³„ì—ì„œ ì¶©ë¶„í•œ ê²½í—˜ì„ ìŒ“ì€ í›„ Goë¡œ ì „í™˜í•˜ëŠ” ì‹œë‹ˆì–´ ì—”ì§€ë‹ˆì–´\n\n## ì‹œë¦¬ì¦ˆ ì†Œê°œ\n\n| # | ì£¼ì œ | ë‹¤ë¥¸ ì–¸ì–´ì—ì„œì˜ ëŒ€ì‘ ê°œë… |\n|---|------|------------------------|\n| **1** | í”„ë¡œì íŠ¸ ì„¤ê³„ | Maven ë©€í‹°ëª¨ë“ˆ, Gradle ì»¨ë²¤ì…˜ |\n| 2 | HTTP ì„œë²„ | Spring MVC, Express |\n| 3 | Context | ThreadLocal, AsyncLocalStorage |\n| 4 | ë™ì‹œì„± | ExecutorService, Worker Threads |\n| 5 | ë°ì´í„°ë² ì´ìŠ¤ | @Transactional, Sequelize |\n| 6 | ì™¸ë¶€ í†µì‹  | Resilience4j, Polly |\n| 7 | í…ŒìŠ¤íŠ¸ | JUnit, Jest |\n| 8 | Observability | Micrometer, Winston |\n| 9 | Makefile | npm scripts, Gradle tasks |\n\n---\n\n## ì‹¤ì œ í”„ë¡œë•ì…˜ í”„ë¡œì íŠ¸ ë¶„ì„\n\nì´ë¡ ì´ ì•„ë‹Œ **ì‹¤ì œ ì½”ë“œ**ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤:\n\n| í”„ë¡œì íŠ¸ | íŠ¹ì„± | Main ì—­í•  | ì¡°ë¦½ ì½”ë“œ ìœ„ì¹˜ |\n|---------|------|----------|--------------|\n| Kubernetes | ê±°ëŒ€ ëª¨ë†€ë¦¬ìŠ¤ | ìµœì†Œí™” | `cmd/<bin>/app/` |\n| Docker CLI | CLI ë„êµ¬ | ìµœì†Œí™” | `cli/command/` |\n| Prometheus | ë°ëª¬/ì„œë¹„ìŠ¤ | ì¤‘ê°„ | `main.go` ë‚´ë¶€ |\n| Hugo | ì»´íŒŒì¼ëŸ¬ | ìµœì†Œí™” | `commands/` |\n\n---\n\n## í•µì‹¬ ì¸ì‚¬ì´íŠ¸: Hollow Main íŒ¨í„´\n\n> **í”„ë¡œì íŠ¸ê°€ ì„±ìˆ™í• ìˆ˜ë¡ main í•¨ìˆ˜ëŠ” ë¹„ì–´ê°„ë‹¤.**\n\n### Fat Mainì˜ ë¬¸ì œ\n\n```go\n// âŒ ë‚˜ìœ ì˜ˆ: Fat Main\nfunc main() {\n    cfg := loadConfig()\n    db := connectDB(cfg)\n    userRepo := NewUserRepo(db)\n    userService := NewUserService(userRepo)\n    handler := NewHandler(userService)\n    http.ListenAndServe(\":8080\", handler)\n}\n```\n\n**ë¬¸ì œì :**\n\n- `main`ì€ í…ŒìŠ¤íŠ¸ ë¶ˆê°€\n- `main` íŒ¨í‚¤ì§€ëŠ” ë‹¤ë¥¸ ê³³ì—ì„œ import ë¶ˆê°€\n\n### Hollow Main (ê¶Œì¥)\n\n```go\n// âœ… ì¢‹ì€ ì˜ˆ: Hollow Main\npackage main\n\nimport \"myproject/internal/app\"\n\nfunc main() {\n    if err := app.Run(); err != nil {\n        os.Exit(1)\n    }\n}\n```\n\nëª¨ë“  ë¡œì§ì€ `internal/app`ìœ¼ë¡œ ì´ë™ â†’ **í…ŒìŠ¤íŠ¸ ê°€ëŠ¥**.\n\n---\n\n## ê¶Œì¥ ë””ë ‰í† ë¦¬ êµ¬ì¡°\n\n```\nproject/\nâ”œâ”€â”€ cmd/\nâ”‚   â””â”€â”€ myapp/\nâ”‚       â””â”€â”€ main.go         # í…… ë¹„ì–´ìˆìŒ\nâ”‚\nâ”œâ”€â”€ internal/\nâ”‚   â”œâ”€â”€ app/                # ì¡°ë¦½ ì½”ë“œ (Composition Root)\nâ”‚   â”‚   â”œâ”€â”€ app.go          # Run() í•¨ìˆ˜\nâ”‚   â”‚   â””â”€â”€ config.go       # ì„¤ì • ë¡œë“œ\nâ”‚   â”œâ”€â”€ api/                # HTTP í•¸ë“¤ëŸ¬, gRPC\nâ”‚   â”œâ”€â”€ biz/                # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§\nâ”‚   â””â”€â”€ data/               # DB, ì™¸ë¶€ API\nâ”‚\nâ”œâ”€â”€ pkg/                    # ì™¸ë¶€ ê³µê°œ (ì‹ ì¤‘í•˜ê²Œ)\nâ”œâ”€â”€ configs/                # ì„¤ì • íŒŒì¼\nâ”œâ”€â”€ api/                    # OpenAPI, Protobuf\nâ””â”€â”€ Makefile\n```\n\n---\n\n## internal/app: ì¡°ë¦½ì˜ ì¤‘ì‹¬\n\n```go\n// internal/app/app.go\nfunc Run() error {\n    cfg := LoadConfig()\n    \n    // ì˜ì¡´ì„± ì¡°ë¦½ (DI)\n    db := data.NewDatabase(cfg.DSN)\n    svc := biz.NewService(db)\n    server := api.NewServer(svc)\n    \n    // ì„œë²„ ì‹œì‘\n    return server.Start()\n}\n```\n\n**ì´ ìœ„ì¹˜ì˜ ì¥ì :**\n\n- `cmd/`ì—ì„œ ë¶„ë¦¬ â†’ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥\n- ì„¤ì • ë¡œë“œ, DI, ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬ ì§‘ì¤‘\n\n---\n\n## í”„ë¡œì íŠ¸ ì§„í™” ë‹¨ê³„\n\n### 1ë‹¨ê³„: í”„ë¡œí† íƒ€ì…\n\n```\nproject/\nâ”œâ”€â”€ main.go\nâ””â”€â”€ go.mod\n```\n\n500ì¤„ ë¯¸ë§Œì˜ PoC. ê´œì°®ìŠµë‹ˆë‹¤.\n\n### 2ë‹¨ê³„: ëª¨ë“ˆí™”\n\n```\nproject/\nâ”œâ”€â”€ cmd/myapp/main.go\nâ””â”€â”€ internal/\n    â”œâ”€â”€ app/\n    â”œâ”€â”€ api/\n    â””â”€â”€ data/\n```\n\n1000ì¤„ì„ ë„˜ì–´ê°€ê±°ë‚˜, DB ì½”ë“œì™€ í•¸ë“¤ëŸ¬ê°€ ì„ì´ê¸° ì‹œì‘í•˜ë©´.\n\n### 3ë‹¨ê³„: ë©€í‹° ë°”ì´ë„ˆë¦¬\n\n```\nproject/\nâ”œâ”€â”€ cmd/\nâ”‚   â”œâ”€â”€ api/\nâ”‚   â”œâ”€â”€ worker/\nâ”‚   â””â”€â”€ admin-cli/\nâ””â”€â”€ internal/        # ê³µìœ \n```\n\nì›¹ ì„œë²„ + ì›Œì»¤ + CLIê°€ ê°™ì€ ë¡œì§ì„ ê³µìœ í•  ë•Œ.\n\n---\n\n## CLI ë„êµ¬ìš© êµ¬ì¡°\n\nì›¹ ì„œë¹„ìŠ¤ê°€ ì•„ë‹Œ CLI ë„êµ¬ë¼ë©´:\n\n```\ncmd/myapp/\nâ””â”€â”€ main.go\n\ninternal/cli/\nâ”œâ”€â”€ root.go         # ë£¨íŠ¸ ì»¤ë§¨ë“œ\nâ”œâ”€â”€ serve.go        # serve ì„œë¸Œì»¤ë§¨ë“œ\nâ””â”€â”€ migrate.go      # migrate ì„œë¸Œì»¤ë§¨ë“œ\n```\n\n**íŒ¨í„´:** `cmd.Execute()` í•œ ì¤„ë¡œ ìœ„ì„\n\n---\n\n## internal/ ìš°ì„  ì „ëµ\n\n| ì–¸ì œ internal/ | ì–¸ì œ pkg/ |\n|---------------|----------|\n| ê¸°ë³¸ê°’ | ì™¸ë¶€ì—ì„œ import í•„ìš”í•  ë•Œ |\n| ë¦¬íŒ©í† ë§ ììœ  | API ì•ˆì •ì„± ì•½ì† |\n| ì´ˆê¸° ê°œë°œ | í”„ë¡œì íŠ¸ ì„±ìˆ™ í›„ |\n\n> **ê²½í—˜ì¹™:** ì²˜ìŒë¶€í„° pkg/ë¥¼ ì“°ì§€ ë§ˆì„¸ìš”. ë‚˜ì¤‘ì— í•„ìš”í•˜ë©´ ì˜®ê¸°ì„¸ìš”.\n\n---\n\n## ì •ë¦¬\n\n| ì›ì¹™ | ì„¤ëª… |\n|------|------|\n| **Hollow Main** | main.goëŠ” í…… ë¹„ì›Œë‘ì„¸ìš” |\n| **internal/app** | ì¡°ë¦½ ì½”ë“œëŠ” ì—¬ê¸°ì— |\n| **internal/ ìš°ì„ ** | pkg/ëŠ” ì„±ìˆ™ í›„ì— |\n| **ë‹¨ê³„ë³„ ì§„í™”** | í”„ë¡œí† íƒ€ì… â†’ ëª¨ë“ˆí™” â†’ ë©€í‹° ë°”ì´ë„ˆë¦¬ |\n\n---\n\n## ë‹¤ìŒ í¸ ì˜ˆê³ \n\n**2í¸: ê²¬ê³ í•œ HTTP ì„œë²„ êµ¬ì¶•**ì—ì„œëŠ” Echo ë¯¸ë“¤ì›¨ì–´ ì„¤ê³„ì™€ Graceful Shutdownì„ ë‹¤ë£¹ë‹ˆë‹¤.\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [golang-standards/project-layout](https://github.com/golang-standards/project-layout)\n- [Kubernetes cmd êµ¬ì¡°](https://github.com/kubernetes/kubernetes/tree/master/cmd)\n- [Docker CLI êµ¬ì¡°](https://github.com/docker/cli/tree/master/cmd)\n- [Google Wire](https://github.com/google/wire)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Architecture",
      "Backend",
      "DI",
      "Enterprise",
      "Go",
      "Tooling"
    ],
    "readingTime": 4,
    "wordCount": 646,
    "isFeatured": false,
    "isPublic": true,
    "series": "enterprise-go",
    "date": "2026-01-01"
  },
  {
    "id": "turf-polygon-operations",
    "slug": "turf-polygon-operations",
    "path": "frontend/maps",
    "fullPath": "frontend/maps/turf-polygon-operations",
    "title": "Turf.jsë¡œ í´ë¦¬ê³¤ ê³µê°„ ì—°ì‚° ìˆ˜í–‰í•˜ê¸°",
    "excerpt": "Turf.jsë¥¼ ì‚¬ìš©í•˜ì—¬ ë©€í‹°í´ë¦¬ê³¤ì˜ í•©ì§‘í•©, ì°¨ì§‘í•©, êµì§‘í•© ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# Turf.jsë¡œ í´ë¦¬ê³¤ ê³µê°„ ì—°ì‚° ìˆ˜í–‰í•˜ê¸°\n\n## ê°œìš”\n\n**Turf.js**ëŠ” ë¸Œë¼ìš°ì €ì™€ Node.jsì—ì„œ ì‹¤í–‰ë˜ëŠ” GeoJSON ê¸°ë°˜ ê³µê°„ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. v7ë¶€í„° `polygon-clipping` ì—”ì§„ìœ¼ë¡œ ì„±ëŠ¥ê³¼ ì •í™•ë„ê°€ í¬ê²Œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n![Union, Difference, Intersect ì—°ì‚° ê²°ê³¼ ì‹œê°í™”](/images/turf-operations-demo-image.png)\n\n## ì„¤ì¹˜\n\n```bash\n# ì „ì²´ ë¼ì´ë¸ŒëŸ¬ë¦¬\nnpm install @turf/turf\n\n# ë˜ëŠ” ê°œë³„ ëª¨ë“ˆ\nnpm install @turf/union @turf/difference @turf/intersect @turf/helpers\n```\n\n## ê¸°ë³¸ ë°ì´í„° ìƒì„±\n\n### í´ë¦¬ê³¤ ìƒì„±\n\n```javascript\nimport { polygon, multiPolygon, featureCollection } from '@turf/helpers';\n\n// ë‹¨ì¼ í´ë¦¬ê³¤\nconst poly1 = polygon([\n    [[0, 0], [2, 0], [2, 2], [0, 2], [0, 0]]\n]);\n\nconst poly2 = polygon([\n    [[1, 1], [3, 1], [3, 3], [1, 3], [1, 1]]\n]);\n\n// ë©€í‹°í´ë¦¬ê³¤\nconst multiPoly = multiPolygon([\n    [[[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]]],\n    [[[2, 2], [3, 2], [3, 3], [2, 3], [2, 2]]]\n]);\n```\n\n## í•©ì§‘í•© (Union)\n\nì—¬ëŸ¬ í´ë¦¬ê³¤ì„ í•˜ë‚˜ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.\n\n```javascript\nimport { union } from '@turf/union';\nimport { featureCollection } from '@turf/helpers';\n\n// v7: FeatureCollection ì…ë ¥\nconst merged = union(featureCollection([poly1, poly2]));\n// ê²°ê³¼: Polygon ë˜ëŠ” MultiPolygon (ì—°ê²°ë˜ì§€ ì•Šìœ¼ë©´)\n```\n\n### ì—¬ëŸ¬ í´ë¦¬ê³¤ ë³‘í•©\n\n```javascript\nimport { union } from '@turf/union';\nimport { featureCollection } from '@turf/helpers';\n\nconst polygons = [poly1, poly2, poly3, poly4];\nconst merged = union(featureCollection(polygons));\n```\n\n### ì‹¤ìš© ì˜ˆ: í–‰ì •êµ¬ì—­ ë³‘í•©\n\n```javascript\n// ê²½ê¸°ë„ ì‹œêµ°êµ¬ â†’ ê²½ê¸°ë„ ì „ì²´ ì˜ì—­\nconst gyeonggiCities = featureCollection([\n    suwonPolygon,\n    seongnamPolygon,\n    yonginPolygon,\n    // ...\n]);\n\nconst gyeonggiProvince = union(gyeonggiCities);\n```\n\n## ì°¨ì§‘í•© (Difference)\n\nê¸°ì¤€ í´ë¦¬ê³¤ì—ì„œ ë‹¤ë¥¸ í´ë¦¬ê³¤ ì˜ì—­ì„ ì œê±°í•©ë‹ˆë‹¤.\n\n```javascript\nimport { difference } from '@turf/difference';\nimport { featureCollection } from '@turf/helpers';\n\n// poly1ì—ì„œ poly2 ì˜ì—­ ì œê±°\nconst subtracted = difference(featureCollection([poly1, poly2]));\n// ê²°ê³¼: Polygon, MultiPolygon, ë˜ëŠ” null (ì™„ì „íˆ ë®ì´ë©´)\n```\n\n### ì—¬ëŸ¬ ì˜ì—­ ì œê±°\n\n```javascript\n// baseì—ì„œ exclude1, exclude2 ì˜ì—­ ëª¨ë‘ ì œê±°\nconst result = difference(featureCollection([base, exclude1, exclude2]));\n```\n\n### ì‹¤ìš© ì˜ˆ: íŠ¹ì • êµ¬ì—­ ì œì™¸\n\n```javascript\n// ì„œìš¸ì‹œ ì „ì²´ì—ì„œ í•œê°• ì˜ì—­ ì œì™¸\nconst seoulLandOnly = difference(featureCollection([\n    seoulBoundary,\n    hanRiverPolygon\n]));\n```\n\n## êµì§‘í•© (Intersect)\n\ní´ë¦¬ê³¤ë“¤ì˜ ê²¹ì¹˜ëŠ” ì˜ì—­ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n\n```javascript\nimport { intersect } from '@turf/intersect';\nimport { featureCollection } from '@turf/helpers';\n\nconst intersection = intersect(featureCollection([poly1, poly2]));\n// ê²°ê³¼: Polygon, MultiPolygon, ë˜ëŠ” null (ê²¹ì¹˜ì§€ ì•Šìœ¼ë©´)\n```\n\n### ì‹¤ìš© ì˜ˆ: ì„œë¹„ìŠ¤ ê°€ëŠ¥ ì˜ì—­\n\n```javascript\n// ë°°ë‹¬ ê°€ëŠ¥ ì˜ì—­ê³¼ ì‚¬ìš©ì ìœ„ì¹˜ ì£¼ë³€ ì˜ì—­ì˜ êµì§‘í•©\nconst deliverableArea = intersect(featureCollection([\n    restaurantDeliveryZone,\n    userRadiusCircle\n]));\n\nif (deliverableArea) {\n    console.log('ë°°ë‹¬ ê°€ëŠ¥í•œ ì˜ì—­ ì¡´ì¬');\n} else {\n    console.log('ë°°ë‹¬ ë¶ˆê°€');\n}\n```\n\n## ì—°ì‚° ê²°ê³¼ ê²€ì¦\n\n### Null ì²˜ë¦¬\n\nêµì§‘í•©ì´ë‚˜ ì°¨ì§‘í•© ê²°ê³¼ê°€ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n```javascript\nconst result = intersect(featureCollection([polyA, polyB]));\n\nif (result === null) {\n    console.log('ê²¹ì¹˜ëŠ” ì˜ì—­ ì—†ìŒ');\n    return;\n}\n\n// ê²°ê³¼ ì‚¬ìš©\nconsole.log(result.geometry.type); // 'Polygon' or 'MultiPolygon'\n```\n\n### ë©´ì  ê²€ì¦\n\n```javascript\nimport { area } from '@turf/area';\n\nconst merged = union(featureCollection(polygons));\nconst areaM2 = area(merged);\nconsole.log(`ë©´ì : ${(areaM2 / 1000000).toFixed(2)} kmÂ²`);\n```\n\n## ë³µí•© ì—°ì‚° ì˜ˆì‹œ\n\n### ë„ë„› í´ë¦¬ê³¤ ìƒì„±\n\n```javascript\nimport { difference } from '@turf/difference';\nimport { circle } from '@turf/circle';\nimport { featureCollection } from '@turf/helpers';\n\n// ì™¸ë¶€ ì› (ë°˜ê²½ 5km)\nconst outer = circle([127.0, 37.5], 5, { units: 'kilometers' });\n\n// ë‚´ë¶€ ì› (ë°˜ê²½ 2km)\nconst inner = circle([127.0, 37.5], 2, { units: 'kilometers' });\n\n// ë„ë„› í˜•íƒœ\nconst donut = difference(featureCollection([outer, inner]));\n```\n\n### ì—¬ëŸ¬ ì¡°ê±´ ì¡°í•©\n\n```javascript\nimport { union, difference, intersect } from '@turf/turf';\nimport { featureCollection } from '@turf/helpers';\n\n// 1. ëª¨ë“  ì„œë¹„ìŠ¤ ì§€ì—­ ë³‘í•©\nconst allServiceAreas = union(featureCollection(servicePolygons));\n\n// 2. ê¸ˆì§€ êµ¬ì—­ ì œê±°\nconst allowedAreas = difference(featureCollection([\n    allServiceAreas,\n    ...restrictedZones\n]));\n\n// 3. ì‚¬ìš©ì ë°˜ê²½ê³¼ êµì°¨\nconst finalServiceable = intersect(featureCollection([\n    allowedAreas,\n    userRadiusPolygon\n]));\n```\n\n## ì„±ëŠ¥ ê³ ë ¤ì‚¬í•­\n\n### ë³µì¡í•œ í´ë¦¬ê³¤\n\n```javascript\nimport { simplify } from '@turf/simplify';\n\n// ì—°ì‚° ì „ í´ë¦¬ê³¤ ë‹¨ìˆœí™” (ì •ì  ìˆ˜ ê°ì†Œ)\nconst simplified = simplify(complexPolygon, { tolerance: 0.001 });\nconst result = union(featureCollection([simplified, otherPolygon]));\n```\n\n### ìœ íš¨ì„± ê²€ì‚¬\n\n```javascript\nimport { kinks } from '@turf/kinks';\n\n// ìê¸° êµì°¨ ê²€ì‚¬\nconst selfIntersections = kinks(polygon);\nif (selfIntersections.features.length > 0) {\n    console.warn('ìê¸° êµì°¨í•˜ëŠ” í´ë¦¬ê³¤');\n}\n```\n\n## API ì •ë¦¬\n\n| í•¨ìˆ˜ | ì…ë ¥ | ì¶œë ¥ | ì„¤ëª… |\n|------|------|------|------|\n| `union` | FeatureCollection | Polygon/MultiPolygon | ëª¨ë“  í´ë¦¬ê³¤ ë³‘í•© |\n| `difference` | FeatureCollection | Polygon/MultiPolygon/null | ì²« ë²ˆì§¸ì—ì„œ ë‚˜ë¨¸ì§€ ì œê±° |\n| `intersect` | FeatureCollection | Polygon/MultiPolygon/null | ê³µí†µ ì˜ì—­ ì¶”ì¶œ |\n\n## ëª¨ë²” ì‚¬ë¡€\n\n1. **FeatureCollection ì‚¬ìš©**: v7ë¶€í„° ì—¬ëŸ¬ í´ë¦¬ê³¤ì€ FeatureCollectionìœ¼ë¡œ ì „ë‹¬\n2. **Null ì²´í¬ í•„ìˆ˜**: êµì§‘í•©/ì°¨ì§‘í•© ê²°ê³¼ê°€ ì—†ì„ ìˆ˜ ìˆìŒ\n3. **ë‹¨ìˆœí™” ë¨¼ì €**: ë³µì¡í•œ í´ë¦¬ê³¤ì€ `simplify` í›„ ì—°ì‚°\n4. **ì¢Œí‘œê³„ í†µì¼**: ëª¨ë“  ì…ë ¥ì€ ë™ì¼í•œ SRID (ë³´í†µ 4326)\n\n## ì°¸ê³  ìë£Œ\n\n- [Turf.js Documentation](https://turfjs.org/)\n- [Turf.js v7 Release Notes](https://github.com/Turfjs/turf/releases/tag/v7.0.0)",
    "docType": "original",
    "category": "Frontend",
    "tags": [
      "Geospatial",
      "JavaScript"
    ],
    "readingTime": 4,
    "wordCount": 714,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "mapbox-gl-guide",
    "slug": "mapbox-gl-guide",
    "path": "frontend/maps",
    "fullPath": "frontend/maps/mapbox-gl-guide",
    "title": "Mapbox GL JSë¡œ ì¸í„°ë™í‹°ë¸Œ ì§€ë„ êµ¬í˜„í•˜ê¸°",
    "excerpt": "Mapbox GL JS v3ë¡œ Earth2 ìŠ¤íƒ€ì¼ ê²©ì ê·¸ë¦¬ë“œì™€ ë„¤ì´ë²„ë¶€ë™ì‚° ìŠ¤íƒ€ì¼ ì˜ì—­ ë ˆì´ì–´ë¥¼ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# Mapbox GL JSë¡œ ì¸í„°ë™í‹°ë¸Œ ì§€ë„ êµ¬í˜„í•˜ê¸°\n\n## ê°œìš”\n\n**Mapbox GL JS**ëŠ” WebGL ê¸°ë°˜ ë²¡í„° íƒ€ì¼ ì§€ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ì´ ê¸€ì—ì„œëŠ” ë‘ ê°€ì§€ ì‹¤ìš©ì ì¸ íŒ¨í„´ì„ ë‹¤ë£¹ë‹ˆë‹¤:\n\n1. **ê²©ì ê·¸ë¦¬ë“œ ë ˆì´ì–´**: Earth2 ìŠ¤íƒ€ì¼ì˜ ë°”ë‘‘íŒ ì˜¤ë²„ë ˆì´\n2. **ì˜ì—­ í´ë¦¬ê³¤ ë ˆì´ì–´**: ë„¤ì´ë²„ë¶€ë™ì‚° ìŠ¤íƒ€ì¼ì˜ ì§€ì—­ í‘œì‹œ\n\n## ì„¤ì¹˜ ë° ì´ˆê¸°í™”\n\n```bash\nnpm install mapbox-gl @turf/turf\n```\n\n```javascript\nimport mapboxgl from 'mapbox-gl';\nimport 'mapbox-gl/dist/mapbox-gl.css';\n\nmapboxgl.accessToken = 'YOUR_ACCESS_TOKEN';\n\nconst map = new mapboxgl.Map({\n    container: 'map',\n    style: 'mapbox://styles/mapbox/dark-v11',  // ê²©ìê°€ ì˜ ë³´ì´ëŠ” ë‹¤í¬ í…Œë§ˆ\n    center: [127.0, 37.5],\n    zoom: 12\n});\n```\n\n---\n\n## ì˜ˆì‹œ 1: Earth2 ìŠ¤íƒ€ì¼ ê²©ì ê·¸ë¦¬ë“œ\n\nì „ ì„¸ê³„ë¥¼ ì¼ì •í•œ í¬ê¸°ì˜ ì…€ë¡œ ë‚˜ëˆ„ì–´ ë°”ë‘‘íŒì²˜ëŸ¼ í‘œì‹œí•©ë‹ˆë‹¤.\n\n![ê²©ì ê·¸ë¦¬ë“œ ë°ëª¨ - í´ë¦­ìœ¼ë¡œ ì…€ ì„ íƒ ê°€ëŠ¥](/images/mapbox-grid-demo-image.png)\n\n### ê²©ì ìƒì„± (Turf.js)\n\n```javascript\nimport { squareGrid, bboxPolygon } from '@turf/turf';\n\n// í˜„ì¬ ë·°í¬íŠ¸ ê¸°ì¤€ ê²©ì ìƒì„±\nfunction createGrid(bounds, cellSize = 0.01) {\n    const bbox = [bounds.getWest(), bounds.getSouth(), bounds.getEast(), bounds.getNorth()];\n    \n    // cellSize: ë„ ë‹¨ìœ„ (0.01ë„ â‰ˆ ì•½ 1km)\n    const grid = squareGrid(bbox, cellSize, { units: 'degrees' });\n    \n    // ê° ì…€ì— ê³ ìœ  ID ë¶€ì—¬\n    grid.features.forEach((cell, i) => {\n        cell.id = i;\n        cell.properties.cellId = `cell_${i}`;\n    });\n    \n    return grid;\n}\n```\n\n### ê²©ì ë ˆì´ì–´ ì¶”ê°€\n\n```javascript\nmap.on('load', () => {\n    const grid = createGrid(map.getBounds(), 0.005);\n    \n    // ì†ŒìŠ¤ ì¶”ê°€\n    map.addSource('grid', {\n        type: 'geojson',\n        data: grid,\n        generateId: true\n    });\n    \n    // ê²©ì ë©´ (Fill)\n    map.addLayer({\n        id: 'grid-fill',\n        type: 'fill',\n        source: 'grid',\n        paint: {\n            'fill-color': [\n                'case',\n                ['boolean', ['feature-state', 'selected'], false], '#00ff88',\n                ['boolean', ['feature-state', 'hover'], false], '#ffffff',\n                'transparent'\n            ],\n            'fill-opacity': [\n                'case',\n                ['boolean', ['feature-state', 'selected'], false], 0.6,\n                ['boolean', ['feature-state', 'hover'], false], 0.2,\n                0\n            ]\n        }\n    });\n    \n    // ê²©ì ì„  (Line)\n    map.addLayer({\n        id: 'grid-line',\n        type: 'line',\n        source: 'grid',\n        paint: {\n            'line-color': '#00ff88',\n            'line-width': 0.5,\n            'line-opacity': 0.5\n        }\n    });\n});\n```\n\n### ê²©ì ì¸í„°ë™ì…˜\n\n```javascript\nlet hoveredCellId = null;\nconst selectedCells = new Set();\n\n// í˜¸ë²„\nmap.on('mousemove', 'grid-fill', (e) => {\n    if (e.features.length > 0) {\n        if (hoveredCellId !== null) {\n            map.setFeatureState({ source: 'grid', id: hoveredCellId }, { hover: false });\n        }\n        hoveredCellId = e.features[0].id;\n        map.setFeatureState({ source: 'grid', id: hoveredCellId }, { hover: true });\n    }\n});\n\nmap.on('mouseleave', 'grid-fill', () => {\n    if (hoveredCellId !== null) {\n        map.setFeatureState({ source: 'grid', id: hoveredCellId }, { hover: false });\n    }\n    hoveredCellId = null;\n});\n\n// í´ë¦­ìœ¼ë¡œ ì…€ ì„ íƒ/í•´ì œ\nmap.on('click', 'grid-fill', (e) => {\n    const cellId = e.features[0].id;\n    const isSelected = selectedCells.has(cellId);\n    \n    if (isSelected) {\n        selectedCells.delete(cellId);\n        map.setFeatureState({ source: 'grid', id: cellId }, { selected: false });\n    } else {\n        selectedCells.add(cellId);\n        map.setFeatureState({ source: 'grid', id: cellId }, { selected: true });\n    }\n    \n    console.log('ì„ íƒëœ ì…€:', [...selectedCells]);\n});\n```\n\n### ë·°í¬íŠ¸ ë³€ê²½ ì‹œ ê²©ì ê°±ì‹ \n\n```javascript\nmap.on('moveend', () => {\n    const newGrid = createGrid(map.getBounds(), 0.005);\n    map.getSource('grid').setData(newGrid);\n});\n```\n\n---\n\n## ì˜ˆì‹œ 2: ë„¤ì´ë²„ë¶€ë™ì‚° ìŠ¤íƒ€ì¼ ì˜ì—­ ë ˆì´ì–´\n\ní–‰ì •êµ¬ì—­ë³„ë¡œ ìƒ‰ìƒê³¼ ë¼ë²¨ì„ í‘œì‹œí•˜ê³ , í´ë¦­ ì‹œ ìƒì„¸ ì •ë³´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n\n![ë¶€ë™ì‚° ìŠ¤íƒ€ì¼ ì˜ì—­ ë ˆì´ì–´ - ê°€ê²©ë³„ ìƒ‰ìƒê³¼ ë¼ë²¨](/images/mapbox-district-demo-image.png)\n\n### GeoJSON ë°ì´í„° êµ¬ì¡°\n\n```javascript\nconst districtsData = {\n    type: 'FeatureCollection',\n    features: [\n        {\n            type: 'Feature',\n            id: 1,\n            properties: {\n                name: 'ê°•ë‚¨êµ¬',\n                avgPrice: 15000000,  // í‰ë‹¹ ê°€ê²©\n                changeRate: 2.5      // ë³€ë™ë¥  %\n            },\n            geometry: {\n                type: 'Polygon',\n                coordinates: [[[127.0, 37.5], [127.1, 37.5], [127.1, 37.52], [127.0, 37.52], [127.0, 37.5]]]\n            }\n        },\n        // ...\n    ]\n};\n```\n\n### ì˜ì—­ ë ˆì´ì–´ ì¶”ê°€\n\n```javascript\nmap.on('load', () => {\n    map.addSource('districts', {\n        type: 'geojson',\n        data: districtsData,\n        generateId: true\n    });\n    \n    // ì˜ì—­ ë©´ - ê°€ê²©ì— ë”°ë¥¸ ìƒ‰ìƒ\n    map.addLayer({\n        id: 'districts-fill',\n        type: 'fill',\n        source: 'districts',\n        paint: {\n            'fill-color': [\n                'interpolate',\n                ['linear'],\n                ['get', 'avgPrice'],\n                5000000, '#fef0d9',    // ë‚®ì€ ê°€ê²©: ì—°í•œ ìƒ‰\n                10000000, '#fdbb84',\n                15000000, '#fc8d59',\n                20000000, '#d7301f'    // ë†’ì€ ê°€ê²©: ì§„í•œ ìƒ‰\n            ],\n            'fill-opacity': [\n                'case',\n                ['boolean', ['feature-state', 'hover'], false], 0.9,\n                0.6\n            ]\n        }\n    });\n    \n    // ì˜ì—­ ê²½ê³„ì„ \n    map.addLayer({\n        id: 'districts-outline',\n        type: 'line',\n        source: 'districts',\n        paint: {\n            'line-color': '#ffffff',\n            'line-width': [\n                'case',\n                ['boolean', ['feature-state', 'hover'], false], 3,\n                1\n            ]\n        }\n    });\n    \n    // ë¼ë²¨ (êµ¬ ì´ë¦„ + ê°€ê²©)\n    map.addLayer({\n        id: 'districts-label',\n        type: 'symbol',\n        source: 'districts',\n        layout: {\n            'text-field': [\n                'format',\n                ['get', 'name'], { 'font-scale': 1.2 },\n                '\\n', {},\n                ['concat', ['to-string', ['/', ['get', 'avgPrice'], 10000]], 'ë§Œì›'], { 'font-scale': 0.9 }\n            ],\n            'text-font': ['DIN Pro Medium', 'Arial Unicode MS Bold'],\n            'text-size': 12,\n            'text-anchor': 'center'\n        },\n        paint: {\n            'text-color': '#ffffff',\n            'text-halo-color': '#000000',\n            'text-halo-width': 1\n        }\n    });\n});\n```\n\n### í˜¸ë²„ ë° íŒì—…\n\n```javascript\nlet hoveredDistrictId = null;\n\nmap.on('mousemove', 'districts-fill', (e) => {\n    map.getCanvas().style.cursor = 'pointer';\n    \n    if (e.features.length > 0) {\n        if (hoveredDistrictId !== null) {\n            map.setFeatureState({ source: 'districts', id: hoveredDistrictId }, { hover: false });\n        }\n        hoveredDistrictId = e.features[0].id;\n        map.setFeatureState({ source: 'districts', id: hoveredDistrictId }, { hover: true });\n    }\n});\n\nmap.on('mouseleave', 'districts-fill', () => {\n    map.getCanvas().style.cursor = '';\n    if (hoveredDistrictId !== null) {\n        map.setFeatureState({ source: 'districts', id: hoveredDistrictId }, { hover: false });\n    }\n    hoveredDistrictId = null;\n});\n\n// í´ë¦­ ì‹œ ìƒì„¸ íŒì—…\nmap.on('click', 'districts-fill', (e) => {\n    const props = e.features[0].properties;\n    const changeColor = props.changeRate >= 0 ? '#ff4444' : '#4444ff';\n    const changeSign = props.changeRate >= 0 ? '+' : '';\n    \n    new mapboxgl.Popup()\n        .setLngLat(e.lngLat)\n        .setHTML(`\n            <div style=\"padding: 8px;\">\n                <h3 style=\"margin: 0 0 8px 0;\">${props.name}</h3>\n                <p style=\"margin: 4px 0;\">í‰ë‹¹ ê°€ê²©: <b>${(props.avgPrice / 10000).toFixed(0)}ë§Œì›</b></p>\n                <p style=\"margin: 4px 0; color: ${changeColor};\">\n                    ë³€ë™ë¥ : ${changeSign}${props.changeRate}%\n                </p>\n            </div>\n        `)\n        .addTo(map);\n});\n```\n\n### ë²”ë¡€ ì¶”ê°€\n\n```html\n<div id=\"legend\" style=\"position: absolute; bottom: 20px; left: 20px; background: white; padding: 10px; border-radius: 4px;\">\n    <h4>í‰ë‹¹ ê°€ê²©</h4>\n    <div><span style=\"background: #fef0d9; width: 20px; height: 10px; display: inline-block;\"></span> 500ë§Œ ì´í•˜</div>\n    <div><span style=\"background: #fdbb84; width: 20px; height: 10px; display: inline-block;\"></span> 500-1000ë§Œ</div>\n    <div><span style=\"background: #fc8d59; width: 20px; height: 10px; display: inline-block;\"></span> 1000-1500ë§Œ</div>\n    <div><span style=\"background: #d7301f; width: 20px; height: 10px; display: inline-block;\"></span> 1500ë§Œ ì´ìƒ</div>\n</div>\n```\n\n---\n\n## ê³µí†µ: ë™ì  ë°ì´í„° ë¡œë“œ\n\n### APIì—ì„œ GeoJSON ë¡œë“œ\n\n```javascript\nasync function loadDistricts() {\n    const response = await fetch('/api/districts?city=seoul');\n    const geojson = await response.json();\n    \n    if (map.getSource('districts')) {\n        map.getSource('districts').setData(geojson);\n    } else {\n        map.addSource('districts', { type: 'geojson', data: geojson, generateId: true });\n        // ë ˆì´ì–´ ì¶”ê°€...\n    }\n}\n```\n\n### í•„í„°ë§\n\n```javascript\n// íŠ¹ì • ì¡°ê±´ë§Œ í‘œì‹œ\nmap.setFilter('districts-fill', ['>', ['get', 'avgPrice'], 10000000]);\n\n// í•„í„° í•´ì œ\nmap.setFilter('districts-fill', null);\n```\n\n## ëª¨ë²” ì‚¬ë¡€\n\n1. **generateId í•„ìˆ˜**: Feature State í™œìš© ì‹œ í•„ìš”\n2. **load ì´ë²¤íŠ¸ í›„ ì‘ì—…**: ì†ŒìŠ¤/ë ˆì´ì–´ëŠ” ë°˜ë“œì‹œ `map.on('load', ...)` ë‚´ì—ì„œ\n3. **í˜¸ë²„ ìƒíƒœ ì •ë¦¬**: `mouseleave`ì—ì„œ ìƒíƒœ ì´ˆê¸°í™” í•„ìˆ˜\n4. **ì„±ëŠ¥**: ê²©ìëŠ” ë·°í¬íŠ¸ ë²”ìœ„ë¡œ ì œí•œ, ì „ì²´ ë¡œë“œ ê¸ˆì§€\n5. **ë©”ëª¨ë¦¬**: ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ `map.remove()` í˜¸ì¶œ\n\n## ì°¸ê³  ìë£Œ\n\n- [Mapbox GL JS v3 Documentation](https://docs.mapbox.com/mapbox-gl-js/)\n- [Turf.js squareGrid](https://turfjs.org/docs/#squareGrid)",
    "docType": "original",
    "category": "Frontend",
    "tags": [
      "Geospatial",
      "JavaScript",
      "Visualization"
    ],
    "readingTime": 5,
    "wordCount": 998,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "streams-audit-pipeline",
    "slug": "streams-audit-pipeline",
    "path": "database/redis",
    "fullPath": "database/redis/streams-audit-pipeline",
    "title": "Redis Streams ê¸°ë°˜ ë¹„ë™ê¸° ê°ì‚¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•",
    "excerpt": "Redis Streamsì™€ Consumer Groupì„ í™œìš©í•˜ì—¬ At-least-once ì „ë‹¬ê³¼ Dead Letter ì²˜ë¦¬ë¥¼ ì§€ì›í•˜ëŠ” ë¹„ë™ê¸° ê°ì‚¬ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# Redis Streams ê¸°ë°˜ ë¹„ë™ê¸° ê°ì‚¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\n\n## ê°œìš”\n\në°ì´í„° ë³€ê²½ ì´ë²¤íŠ¸ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ” ê°ì‚¬(Audit) ì‹œìŠ¤í…œì—ì„œ **Redis Streams**ëŠ” ê°•ë ¥í•œ ì„ íƒì…ë‹ˆë‹¤. ì´ ê¸€ì—ì„œëŠ” ë©”ì‹œì§€ ìœ ì‹¤ ì—†ëŠ” ê°ì‚¬ íŒŒì´í”„ë¼ì¸ì„ ì„¤ê³„í•˜ê³ , íŠ¹íˆ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œì˜ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ì„ í¬í•¨í•œ êµ¬í˜„ ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n\n## ì™œ Redis Streamsì¸ê°€?\n\n### ì¥ì \n\n| íŠ¹ì„± | ì„¤ëª… |\n|------|------|\n| **At-least-once ì „ë‹¬** | ACK ë©”ì»¤ë‹ˆì¦˜ê³¼ PELì„ í†µí•œ ë©”ì‹œì§€ ìœ ì‹¤ ë°©ì§€ |\n| **ìˆœì„œ ë³´ì¥** | ìŠ¤íŠ¸ë¦¼ ë‚´ ë©”ì‹œì§€ ì¸ì… ìˆœì„œ ìœ ì§€ |\n| **ê³ ì„±ëŠ¥** | ì¸ë©”ëª¨ë¦¬ ê¸°ë°˜ ë†’ì€ ì²˜ë¦¬ëŸ‰ |\n| **ì˜ì†ì„±** | AOF/RDB ì„¤ì •ìœ¼ë¡œ ë°ì´í„° ì§€ì†ì„± ë³´ì¥ |\n| **Consumer Group** | ì—¬ëŸ¬ ì›Œì»¤ ê°„ ìˆ˜í‰ í™•ì¥ ë° ë¶€í•˜ ë¶„ì‚° |\n\n### ë‹¨ì \n\n| íŠ¹ì„± | ì„¤ëª… |\n|------|------|\n| **ë©”ëª¨ë¦¬ ì••ë°•** | ë°ì´í„°ê°€ ëˆ„ì ë˜ë¯€ë¡œ `XTRIM` ë“±ì„ í†µí•œ ê´€ë¦¬ê°€ í•„ìˆ˜ì ì„ |\n| **ìƒíƒœ ê´€ë¦¬ ë³µì¡ì„±** | Pending ë©”ì‹œì§€(PEL) ë° ì¬ì‹œë„ ë¡œì§ì„ ì§ì ‘ ì œì–´í•´ì•¼ í•¨ |\n\n## ì•„í‚¤í…ì²˜ ì„¤ê³„\n\n### íŒŒì´í”„ë¼ì¸ íë¦„\n\n```mermaid\ngraph TD\n    %% Nodes\n    API[\"API Server\"]\n    RS[\"Redis Stream<br/>(audit-events)\"]\n    AW[\"Audit Worker<br/>(Consumer)\"]\n    DLS[\"Dead Letter Stream\"]\n    DLW[\"Dead Letter Worker\"]\n\n    %% Flow\n    API --> RS\n    RS --> AW\n    \n    %% Error Flow\n    AW -- \"ì‹¤íŒ¨ ì‹œ\" --> DLS\n    RS -. \"íŒŒì‹± ì‹¤íŒ¨ ì‹œ\" .-> DLS\n    \n    DLS --> DLW\n\n    %% Styling\n    style API fill:#f9f9f9,stroke:#333\n    style RS fill:#e1f5fe,stroke:#01579b\n    style AW fill:#e1f5fe,stroke:#01579b\n    style DLS fill:#fff3e0,stroke:#e65100\n    style DLW fill:#fff3e0,stroke:#e65100\n```\n\n### ë©”ì‹œì§€ êµ¬ì¡°\n\nRedis Streamsì˜ ë©”ì‹œì§€ëŠ” ë¶ˆë³€(Immutable)ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì¬ì‹œë„ íšŸìˆ˜ ê°™ì€ ìƒíƒœê°’ì€ ë©”ì‹œì§€ ë‚´ë¶€ì— ë‹´ê¸°ë³´ë‹¤ Redisê°€ ì œê³µí•˜ëŠ” metadata(delivery count)ë¥¼ í™œìš©í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì…ë‹ˆë‹¤.\n\n```go\npackage audit\n\nimport \"time\"\n\n// EventMessageëŠ” ê°ì‚¬ ì´ë²¤íŠ¸ ë©”ì‹œì§€ì…ë‹ˆë‹¤.\ntype EventMessage struct {\n    ID            string                 `json:\"id\"`\n    Collection    string                 `json:\"collection\"`\n    DocumentURI   string                 `json:\"document_uri\"`\n    Action        string                 `json:\"action\"` // CREATE, UPDATE, DELETE\n    Version       int32                  `json:\"version\"`\n    Payload       map[string]interface{} `json:\"payload\"`\n    Timestamp     time.Time              `json:\"timestamp\"`\n}\n\n```\n\n## í•µì‹¬ êµ¬í˜„\n\n### ë©”ì‹œì§€ ë°œí–‰ì (Producer)\n\n```go\npackage audit\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"fmt\"\n    \n    \"[github.com/redis/go-redis/v9](https://github.com/redis/go-redis/v9)\"\n)\n\nconst (\n    AuditStreamKey = \"audit-events-stream\"\n)\n\ntype EventProducer struct {\n    client redis.UniversalClient\n}\n\nfunc NewEventProducer(client redis.UniversalClient) *EventProducer {\n    return &EventProducer{client: client}\n}\n\n// PublishëŠ” ì´ë²¤íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¼ì— ë°œí–‰í•©ë‹ˆë‹¤.\nfunc (p *EventProducer) Publish(ctx context.Context, event *EventMessage) (string, error) {\n    payload, err := json.Marshal(event)\n    if err != nil {\n        return \"\", fmt.Errorf(\"marshal event: %w\", err)\n    }\n    \n    messageID, err := p.client.XAdd(ctx, &redis.XAddArgs{\n        Stream: AuditStreamKey,\n        Values: map[string]interface{}{\n            \"payload\": payload,\n        },\n    }).Result()\n    \n    if err != nil {\n        return \"\", fmt.Errorf(\"xadd: %w\", err)\n    }\n    \n    return messageID, nil\n}\n\n```\n\n### ì›Œì»¤ (Consumer)\n\nRedis Streamsì—ì„œ `>` IDëŠ” ìƒˆ ë©”ì‹œì§€ë§Œì„ ì˜ë¯¸í•©ë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ ì¬ì‹œì‘ ì‹œ ì²˜ë¦¬ë˜ì§€ ëª»í•œ ë©”ì‹œì§€ë¥¼ ë³µêµ¬í•˜ë ¤ë©´ `0` IDë¥¼ í†µí•´ PEL(Pending Entries List)ì„ ë¨¼ì € í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.\n\n```go\n// processMessagesëŠ” ìƒˆ ë©”ì‹œì§€ì™€ Pending ë©”ì‹œì§€ë¥¼ í•¨ê»˜ ì²˜ë¦¬í•©ë‹ˆë‹¤.\nfunc (w *StreamWorker) processMessages(ctx context.Context) {\n    // ì „ëµ: PEL(Pending Entries List)ì´ ë¹„ì›Œì§ˆ ë•Œê¹Œì§€ ìš°ì„  ì²˜ë¦¬í•œ í›„ ìƒˆ ë©”ì‹œì§€ë¡œ ì´ë™\n    \n    for {\n        // 1. ë³¸ì¸ì˜ Pending ë©”ì‹œì§€(\"0\") í™•ì¸\n        msgs, err := w.readBatch(ctx, \"0\")\n        if err != nil || len(msgs) == 0 {\n            break // ë” ì´ìƒ ì²˜ë¦¬í•  Pending ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´ íƒˆì¶œ\n        }\n        \n        for _, m := range msgs {\n            w.handleMessage(ctx, m)\n        }\n        \n        // BatchSizeë³´ë‹¤ ì ê²Œ ê°€ì ¸ì™”ë‹¤ë©´ PELì´ ê±°ì˜ ë¹„ì–´ìˆë‹¤ëŠ” ëœ»ì´ë¯€ë¡œ ë‹¤ìŒìœ¼ë¡œ ì§„í–‰\n        if int64(len(msgs)) < w.config.BatchSize {\n            break\n        }\n    }\n\n    // 2. ì´ì œ ìƒˆ ë©”ì‹œì§€(\">\") ì²˜ë¦¬\n    msgs, _ := w.readBatch(ctx, \">\")\n    for _, m := range msgs {\n        w.handleMessage(ctx, m)\n    }\n}\n\n// ê³µí†µ ì½ê¸° ë¡œì§ ë¶„ë¦¬\nfunc (w *StreamWorker) readBatch(ctx context.Context, id string) ([]redis.XMessage, error) {\n    streams, err := w.client.XReadGroup(ctx, &redis.XReadGroupArgs{\n        Group:    w.config.Group,\n        Consumer: w.config.Consumer,\n        Streams:  []string{w.config.Stream, id},\n        Count:    w.config.BatchSize,\n        Block:    w.config.PollInterval,\n    }).Result()\n    \n    if err != nil || len(streams) == 0 {\n        return nil, err\n    }\n    return streams[0].Messages, nil\n}\n\nfunc (w *StreamWorker) handleMessage(ctx context.Context, msg redis.XMessage) {\n    var event EventMessage\n    \n    // ì•ˆì „í•œ íƒ€ì… ë‹¨ì–¸\n    payloadRaw, ok := msg.Values[\"payload\"].(string)\n    if !ok {\n        w.moveToDeadLetter(ctx, msg, fmt.Errorf(\"invalid payload type\"))\n        return\n    }\n\n    if err := json.Unmarshal([]byte(payloadRaw), &event); err != nil {\n        w.moveToDeadLetter(ctx, msg, err)\n        return\n    }\n\n    // í•¸ë“¤ëŸ¬ í˜¸ì¶œ\n    errs := w.handler.Handle(ctx, []*EventMessage{&event})\n    \n    if len(errs) > 0 && errs[0] != nil {\n        w.handleFailure(ctx, msg, errs[0])\n        return\n    }\n    \n    // ì„±ê³µ ì‹œ ACKë¥¼ ë³´ë‚´ PELì—ì„œ ì œê±°\n    w.client.XAck(ctx, w.config.Stream, w.config.Group, msg.ID)\n}\n\nfunc (w *StreamWorker) handleFailure(ctx context.Context, msg redis.XMessage, err error) {\n    // XPENDINGìœ¼ë¡œ í˜„ì¬ ë©”ì‹œì§€ì˜ ì „ë‹¬ íšŸìˆ˜(delivery count) í™•ì¸\n    pends, _ := w.client.XPendingExt(ctx, &redis.XPendingExtArgs{\n        Stream: w.config.Stream,\n        Group:  w.config.Group,\n        Start:  msg.ID,\n        End:    msg.ID,\n        Count:  1,\n    }).Result()\n\n    if len(pends) > 0 && int(pends[0].Count) >= w.config.MaxRetries {\n        // ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼ ì‹œ Dead Letter ì´ë™ í›„ ACK\n        w.moveToDeadLetter(ctx, msg, err)\n        w.client.XAck(ctx, w.config.Stream, w.config.Group, msg.ID)\n        return\n    }\n    \n    // ACKë¥¼ í•˜ì§€ ì•Šìœ¼ë©´ ë‹¤ìŒ \"0\" ID ì¡°íšŒ ì‹œ ì¬ì „ë‹¬ë¨\n}\n\nfunc (w *StreamWorker) moveToDeadLetter(ctx context.Context, msg redis.XMessage, reason error) {\n    w.client.XAdd(ctx, &redis.XAddArgs{\n        Stream: w.config.DeadLetterStream,\n        Values: map[string]interface{}{\n            \"original_id\": msg.ID,\n            \"payload\":     msg.Values[\"payload\"],\n            \"error\":       reason.Error(),\n            \"failed_at\":   time.Now().Format(time.RFC3339),\n        },\n    })\n}\n\n```\n\n## ëª¨ë²” ì‚¬ë¡€\n\n1. **ID \"0\"ê³¼ \">\"ì˜ ì¡°í•©**: `>`ë§Œ ì‚¬ìš©í•˜ë©´ ì¥ì•  ë°œìƒ ì‹œ ì²˜ë¦¬ ì¤‘ì´ë˜ ë©”ì‹œì§€ê°€ ì˜ì›íˆ Pending ìƒíƒœë¡œ ë‚¨ê²Œ ë©ë‹ˆë‹¤. ë°˜ë“œì‹œ `0` ID ì¡°íšŒë¥¼ ë³‘í–‰í•˜ì„¸ìš”.\n2. **ACK ì‹ ì¤‘íˆ**: ë¡œì§ì´ ì™„ì „íˆ ì„±ê³µí•œ í›„ì—ë§Œ `XACK`ë¥¼ í˜¸ì¶œí•´ì•¼ At-least-onceë¥¼ ë³´ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n3. **íƒ€ì… ì•ˆì „ì„±**: Redis ë°ì´í„° ìœ íš¨ì„±ì„ ë¯¿ì§€ ë§ˆì„¸ìš”. íƒ€ì… ë‹¨ì–¸ ì‹œ ë°˜ë“œì‹œ `ok` íŒ¨í„´ì„ ì‚¬ìš©í•´ íŒ¨ë‹‰ì„ ë°©ì§€í•´ì•¼ í•©ë‹ˆë‹¤.\n4. **XTRIM ì •ê¸° ì‹¤í–‰**: ê°ì‚¬ ë¡œê·¸ëŠ” ì–‘ì´ ë§¤ìš° ë§ìœ¼ë¯€ë¡œ ìŠ¤íŠ¸ë¦¼ ìƒì„± ì‹œ í˜¹ì€ ì£¼ê¸°ì ìœ¼ë¡œ `XTRIM`ì„ ìˆ˜í–‰í•´ ë©”ëª¨ë¦¬ë¥¼ ê´€ë¦¬í•˜ì„¸ìš”.\n\n## ì°¸ê³  ìë£Œ\n\n* [Redis Streams ê³µì‹ ë¬¸ì„œ](https://redis.io/docs/data-types/streams/)\n* [go-redis ë¼ì´ë¸ŒëŸ¬ë¦¬](https://github.com/redis/go-redis)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Architecture",
      "Database",
      "Go",
      "Kafka",
      "Redis",
      "Streaming"
    ],
    "readingTime": 5,
    "wordCount": 819,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "postgis-geojson-guide",
    "slug": "postgis-geojson-guide",
    "path": "database/postgresql",
    "fullPath": "database/postgresql/postgis-geojson-guide",
    "title": "PostGISì™€ GeoJSONì„ í™œìš©í•œ ê³µê°„ ë°ì´í„° ê´€ë¦¬",
    "excerpt": "PostGISì—ì„œ GeoJSON í˜•ì‹ì˜ ê³µê°„ ë°ì´í„°ë¥¼ ì €ì¥, ë³€í™˜, ì¡°íšŒí•˜ëŠ” í•µì‹¬ í•¨ìˆ˜ì™€ ì¸ë±ì‹± ì „ëµì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# PostGISì™€ GeoJSONì„ í™œìš©í•œ ê³µê°„ ë°ì´í„° ê´€ë¦¬\n\n## ê°œìš”\n\n**PostGIS**ëŠ” PostgreSQLì˜ ê³µê°„ í™•ì¥ìœ¼ë¡œ, ì§€ë¦¬ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ì¿¼ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. **GeoJSON**ì€ ì§€ë¦¬ ì •ë³´ë¥¼ í‘œí˜„í•˜ëŠ” JSON ê¸°ë°˜ í‘œì¤€(RFC 7946)ì…ë‹ˆë‹¤.\n\n## í…Œì´ë¸” ì„¤ê³„\n\n### ê¸°ë³¸ ê³µê°„ í…Œì´ë¸”\n\n```sql\nCREATE EXTENSION IF NOT EXISTS postgis;\n\nCREATE TABLE regions (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    -- SRID 4326 = WGS84 (ìœ„ê²½ë„)\n    geom GEOMETRY(MultiPolygon, 4326)\n);\n\n-- ê³µê°„ ì¸ë±ìŠ¤ í•„ìˆ˜\nCREATE INDEX idx_regions_geom ON regions USING GIST(geom);\n```\n\n### SRID ì„ íƒ\n\n| SRID | ì¢Œí‘œê³„ | ìš©ë„ |\n|------|--------|------|\n| 4326 | WGS84 ìœ„ê²½ë„ | GeoJSON í‘œì¤€, GPS |\n| 3857 | Web Mercator | ì›¹ ì§€ë„ íƒ€ì¼ |\n| 5186 | Korea TM | í•œêµ­ ì¸¡ëŸ‰ |\n\n## GeoJSON â†’ PostGIS ì €ì¥\n\n### ST_GeomFromGeoJSON\n\nGeoJSON ë¬¸ìì—´ì„ Geometryë¡œ ë³€í™˜:\n\n```sql\n-- ë‹¨ì¼ í´ë¦¬ê³¤ ì‚½ì…\nINSERT INTO regions (name, geom)\nVALUES (\n    'ì„œìš¸ì‹œ',\n    ST_GeomFromGeoJSON('{\n        \"type\": \"Polygon\",\n        \"coordinates\": [[[126.9, 37.5], [127.1, 37.5], [127.1, 37.6], [126.9, 37.6], [126.9, 37.5]]]\n    }')\n);\n\n-- SRID ëª…ì‹œ (PostGIS 3.0+ëŠ” ê¸°ë³¸ 4326)\nINSERT INTO regions (name, geom)\nVALUES (\n    'ë¶€ì‚°ì‹œ',\n    ST_SetSRID(ST_GeomFromGeoJSON('{\"type\": \"Polygon\", \"coordinates\": ...}'), 4326)\n);\n```\n\n### ë°°ì¹˜ ì‚½ì…\n\n```sql\n-- JSON ë°°ì—´ì—ì„œ ì¼ê´„ ì‚½ì…\nINSERT INTO regions (name, geom)\nSELECT \n    feature->>'name',\n    ST_GeomFromGeoJSON(feature->'geometry')\nFROM jsonb_array_elements(:geojson_features::jsonb) AS feature;\n```\n\n## PostGIS â†’ GeoJSON ì¡°íšŒ\n\n### ST_AsGeoJSON\n\nGeometryë¥¼ GeoJSON ë¬¸ìì—´ë¡œ ë³€í™˜:\n\n```sql\n-- ê¸°ë³¸ ë³€í™˜\nSELECT id, name, ST_AsGeoJSON(geom) AS geojson\nFROM regions;\n\n-- ì¢Œí‘œ ì†Œìˆ˜ì  6ìë¦¬ë¡œ ì œí•œ (ìœ„ê²½ë„ ì•½ 0.1m ì •ë°€ë„)\nSELECT ST_AsGeoJSON(geom, 6) FROM regions;\n\n-- BBox í¬í•¨ (ì˜µì…˜ 1)\nSELECT ST_AsGeoJSON(geom, 6, 1) FROM regions;\n```\n\n### FeatureCollection ìƒì„±\n\ní´ë¼ì´ì–¸íŠ¸ì— ì „ë‹¬í•  ì™„ì „í•œ GeoJSON ìƒì„±:\n\n```sql\nSELECT json_build_object(\n    'type', 'FeatureCollection',\n    'features', json_agg(\n        json_build_object(\n            'type', 'Feature',\n            'id', id,\n            'geometry', ST_AsGeoJSON(geom, 6)::json,\n            'properties', json_build_object('name', name)\n        )\n    )\n) AS geojson\nFROM regions;\n```\n\n## ê³µê°„ ì¿¼ë¦¬\n\n### í¬í•¨ ê´€ê³„\n\n```sql\n-- íŠ¹ì • ì¢Œí‘œê°€ ì–´ëŠ ì§€ì—­ì— ì†í•˜ëŠ”ì§€\nSELECT name FROM regions\nWHERE ST_Contains(geom, ST_SetSRID(ST_Point(127.0, 37.5), 4326));\n```\n\n### ë°˜ê²½ ê²€ìƒ‰\n\n```sql\n-- ë°˜ê²½ 10km ë‚´ ì§€ì—­ (Geography íƒ€ì… í™œìš©)\nSELECT name, ST_Distance(geom::geography, ST_Point(127.0, 37.5)::geography) AS distance_m\nFROM regions\nWHERE ST_DWithin(geom::geography, ST_Point(127.0, 37.5)::geography, 10000)\nORDER BY distance_m;\n```\n\n### êµì°¨ ì˜ì—­\n\n```sql\n-- ë‘ ì§€ì—­ì˜ êµì§‘í•©\nSELECT ST_AsGeoJSON(ST_Intersection(a.geom, b.geom))\nFROM regions a, regions b\nWHERE a.name = 'ì„œìš¸ì‹œ' AND b.name = 'ê²½ê¸°ë„';\n```\n\n## ì¸ë±ìŠ¤ ì „ëµ\n\n### GIST ì¸ë±ìŠ¤\n\nê³µê°„ ì¿¼ë¦¬ ì„±ëŠ¥ì˜ í•µì‹¬:\n\n```sql\n-- ê¸°ë³¸ ê³µê°„ ì¸ë±ìŠ¤\nCREATE INDEX idx_geom ON regions USING GIST(geom);\n\n-- Geography ì¸ë±ìŠ¤ (ê±°ë¦¬ ê³„ì‚°ìš©)\nCREATE INDEX idx_geom_geog ON regions USING GIST((geom::geography));\n```\n\n### í´ëŸ¬ìŠ¤í„°ë§\n\nìì£¼ í•¨ê»˜ ì¡°íšŒë˜ëŠ” ë°ì´í„°ë¥¼ ë¬¼ë¦¬ì ìœ¼ë¡œ ì¸ì ‘ ë°°ì¹˜:\n\n```sql\nCLUSTER regions USING idx_regions_geom;\n```\n\n## ì¢Œí‘œê³„ ë³€í™˜\n\n```sql\n-- WGS84 â†’ Web Mercator\nSELECT ST_Transform(geom, 3857) FROM regions;\n\n-- ë©´ì  ê³„ì‚° (ì •í™•í•œ ê³„ì‚°ì„ ìœ„í•´ ì ì ˆí•œ íˆ¬ì˜ ì‚¬ìš©)\nSELECT name, ST_Area(ST_Transform(geom, 5186)) AS area_m2\nFROM regions;\n```\n\n## í´ë¦¬ê³¤ ìœ íš¨ì„±\n\n```sql\n-- ìœ íš¨ì„± ê²€ì‚¬\nSELECT name, ST_IsValid(geom), ST_IsValidReason(geom)\nFROM regions\nWHERE NOT ST_IsValid(geom);\n\n-- ìë™ ìˆ˜ì •\nUPDATE regions SET geom = ST_MakeValid(geom) WHERE NOT ST_IsValid(geom);\n\n-- GeoJSON í‘œì¤€ ì¤€ìˆ˜ (Right-Hand Rule)\nUPDATE regions SET geom = ST_ForcePolygonCCW(geom);\n```\n\n## ëª¨ë²” ì‚¬ë¡€\n\n1. **SRID 4326**: GeoJSONê³¼ í˜¸í™˜, ì›¹ ì„œë¹„ìŠ¤ í‘œì¤€\n2. **GIST ì¸ë±ìŠ¤ í•„ìˆ˜**: ê³µê°„ ì¿¼ë¦¬ ì„±ëŠ¥ ê²°ì •\n3. **ì¢Œí‘œ ì •ë°€ë„ ì œí•œ**: `ST_AsGeoJSON(geom, 6)`ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ì •ë°€ë„ ì œê±°\n4. **ìœ íš¨ì„± ê²€ì‚¬**: ì‚½ì… ì „ `ST_IsValid` í™•ì¸\n5. **Geography íƒ€ì…**: ê±°ë¦¬ ê³„ì‚°ì´ ì¤‘ìš”í•˜ë©´ Geography ì‚¬ìš©\n\n## ì°¸ê³  ìë£Œ\n\n- [PostGIS 3.4 Documentation](https://postgis.net/docs/)\n- [GeoJSON RFC 7946](https://datatracker.ietf.org/doc/html/rfc7946)",
    "docType": "original",
    "category": "Database",
    "tags": [
      "Database",
      "Geospatial",
      "PostgreSQL"
    ],
    "readingTime": 3,
    "wordCount": 538,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "wiredtiger-storage-engine",
    "slug": "wiredtiger-storage-engine",
    "path": "database/mongodb",
    "fullPath": "database/mongodb/wiredtiger-storage-engine",
    "title": "MongoDB WiredTiger ìŠ¤í† ë¦¬ì§€ ì—”ì§„ ì´í•´í•˜ê¸°",
    "excerpt": "MongoDBì˜ ê¸°ë³¸ ìŠ¤í† ë¦¬ì§€ ì—”ì§„ì¸ WiredTigerì˜ ì•„í‚¤í…ì²˜, ìºì‹œ, ì²´í¬í¬ì¸íŠ¸, Lock-Free ì•Œê³ ë¦¬ì¦˜ê¹Œì§€ ê¹Šì´ ìˆê²Œ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# MongoDB WiredTiger ìŠ¤í† ë¦¬ì§€ ì—”ì§„ ì´í•´í•˜ê¸°\n\n## ê°œìš”\n\n**WiredTiger**ëŠ” MongoDB 3.2ë¶€í„° ê¸°ë³¸ ìŠ¤í† ë¦¬ì§€ ì—”ì§„ìœ¼ë¡œ ì±„íƒëœ ê³ ì„±ëŠ¥ ìŠ¤í† ë¦¬ì§€ ì—”ì§„ì…ë‹ˆë‹¤. ë¬¸ì„œ ìˆ˜ì¤€ ë™ì‹œì„± ì œì–´ì™€ ì••ì¶•ì„ ì§€ì›í•˜ë©°, ëŒ€ë¶€ë¶„ì˜ ì›Œí¬ë¡œë“œì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n\n## í•µì‹¬ ì•„í‚¤í…ì²˜\n\n### ë¬¸ì„œ ìˆ˜ì¤€ ì ê¸ˆ (Document-Level Locking)\n\nWiredTigerì˜ ê°€ì¥ í° ì¥ì ì€ **ë¬¸ì„œ ìˆ˜ì¤€ì˜ ë™ì‹œì„± ì œì–´**ì…ë‹ˆë‹¤.\n\n| ìŠ¤í† ë¦¬ì§€ ì—”ì§„ | ì ê¸ˆ ìˆ˜ì¤€ | ë™ì‹œì„± |\n|-------------|---------|-------|\n| MMAPv1 (ë ˆê±°ì‹œ) | ì»¬ë ‰ì…˜ ìˆ˜ì¤€ | ë‚®ìŒ |\n| WiredTiger | ë¬¸ì„œ ìˆ˜ì¤€ | ë†’ìŒ |\n\n```javascript\n// ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œì— ëŒ€í•œ ë™ì‹œ ì“°ê¸°ê°€ ë³‘ë ¬ë¡œ ì²˜ë¦¬ë¨\ndb.users.updateOne({ _id: 1 }, { $set: { name: \"Alice\" } })\ndb.users.updateOne({ _id: 2 }, { $set: { name: \"Bob\" } })  // ë¸”ë¡œí‚¹ ì—†ìŒ\n```\n\n### MVCC (Multi-Version Concurrency Control)\n\nWiredTigerëŠ” **MVCC**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì½ê¸°ì™€ ì“°ê¸° ì‘ì—…ì´ ì„œë¡œë¥¼ ì°¨ë‹¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n- ì½ê¸° ì‘ì—…: ì‹œì‘ ì‹œì ì˜ ìŠ¤ëƒ…ìƒ·ì„ ì°¸ì¡°\n- ì“°ê¸° ì‘ì—…: ìƒˆ ë²„ì „ ìƒì„±\n- ì¶©ëŒ ì‹œ: ìë™ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜\n\n---\n\n## ë°ì´í„° ì ìš© ìˆœì„œ\n\nMongoDBì—ì„œ ë°ì´í„°ê°€ ì €ì¥ë˜ëŠ” ìˆœì„œë¥¼ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤:\n\n```\n1. WAL (Write-Ahead Log) ê¸°ë¡\n2. Data Memory ì ìš© (ê³µìœ  ìºì‹œ)\n3. OpLog ê¸°ë¡ (Replica Setìš©)\n4. Disk Flush (ì²´í¬í¬ì¸íŠ¸)\n```\n\nì´ ìˆœì„œ ë•ë¶„ì— ì¥ì•  ì‹œì—ë„ WALì„ í†µí•´ ë°ì´í„°ë¥¼ ë³µêµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n---\n\n## ê³µìœ  ìºì‹œ (Shared Cache)\n\nWiredTigerì˜ ê³µìœ  ìºì‹œëŠ” MySQLì˜ Buffer Poolê³¼ ìœ ì‚¬í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\n\n### MySQL Buffer Poolê³¼ì˜ ì°¨ì´\n\n| íŠ¹ì„± | MySQL Buffer Pool | WiredTiger Cache |\n|-----|-------------------|------------------|\n| ìºì‹± ë°©ì‹ | B-Tree ìƒì˜ ì£¼ì†Œ ì‚¬ìš© | ë©”ëª¨ë¦¬ ì£¼ì†Œë¡œ ë³€í™˜í•˜ì—¬ ì ì¬ |\n| ìºì‹± ì†ë„ | ë¹ ë¦„ | ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦¼ (ë³€í™˜ ê³¼ì •) |\n| ì½ê¸° ì„±ëŠ¥ | ë³´í†µ | ìºì‹± í›„ ë” ë¹ ë¦„ |\n\nWiredTigerëŠ” ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ì í•©í•œ íŠ¸ë¦¬ í˜•íƒœë¡œ **ì¬êµ¬ì„±**í•˜ì—¬ ì ì¬í•©ë‹ˆë‹¤. ì´ ë³€í™˜ ê³¼ì •ìœ¼ë¡œ ì¸í•´ ì´ˆê¸° ìºì‹±ì€ ëŠë¦´ ìˆ˜ ìˆì§€ë§Œ, ì¼ë‹¨ ìºì‹±ë˜ë©´ RDBë³´ë‹¤ ë¹ ë¥¸ ì½ê¸° ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n\n### ìºì‹œ ì„¤ì •\n\n```yaml\n# mongod.conf\nstorage:\n  wiredTiger:\n    engineConfig:\n      cacheSizeGB: 4  # ê¶Œì¥: (RAM - 1GB) / 2\n```\n\n**ê¸°ë³¸ ìºì‹œ í¬ê¸° ê³„ì‚°:**\n\n- (RAM - 1GB) Ã— 50%\n- ë˜ëŠ” 256MB ì¤‘ í° ê°’\n\n---\n\n## Lock-Free ì•Œê³ ë¦¬ì¦˜\n\nWiredTigerëŠ” ë†’ì€ ë™ì‹œì„±ì„ ìœ„í•´ **Lock-Free ì•Œê³ ë¦¬ì¦˜**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\n### Hazard Pointer\n\n- í˜„ì¬ ë°ì´í„°ë¥¼ ì°¸ì¡°í•˜ê³  ìˆëŠ” ë©”ëª¨ë¦¬ ì£¼ì†Œë¥¼ Hazard Pointerì— ë“±ë¡\n- ìºì‹œì—ì„œ ì œê±°(Eviction) ì‹œ Hazard Pointerì— ë“±ë¡ëœ ë°ì´í„°ëŠ” ë³´í˜¸\n- Disk Flush ì—¬ë¶€ë„ Hazard Pointerë¥¼ í†µí•´ ê²°ì •\n\n### Skip List\n\n- Undo Logë¥¼ Skip Listë¡œ ê´€ë¦¬\n- ë ˆì½”ë“œ ë³€ê²½ ì‹œ ì´ì „ ë²„ì „ì„ Skip Listì— ì¶”ê°€\n- MVCC êµ¬í˜„ì˜ í•µì‹¬ ìë£Œêµ¬ì¡°\n\n---\n\n## Cache Eviction\n\nìºì‹œì˜ ë¹ˆ ê³µê°„ì„ ì ì ˆíˆ ìœ ì§€í•˜ì—¬ ìƒˆ ë°ì´í„°ë¥¼ ì ì¬í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n\n### ë™ì‘ ë°©ì‹\n\n- **ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ**: í‰ìƒì‹œ Eviction ì²˜ë¦¬\n- **í¬ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ**: ë°±ê·¸ë¼ìš´ë“œê°€ ê³µê°„ í™•ë³´ ì‹¤íŒ¨ ì‹œ ì§ì ‘ ìˆ˜í–‰ (ì„±ëŠ¥ ì €í•˜ ë°œìƒ)\n\n### íŠœë‹ íŒŒë¼ë¯¸í„°\n\n| íŒŒë¼ë¯¸í„° | ì„¤ëª… | ê¸°ë³¸ê°’ |\n|---------|------|-------|\n| `threads_max` | Eviction ìŠ¤ë ˆë“œ ìµœëŒ€ ê°œìˆ˜ | 4 (1~20) |\n| `threads_min` | Eviction ìŠ¤ë ˆë“œ ìµœì†Œ ê°œìˆ˜ | 1 (1~20) |\n| `eviction_dirty_target` | ë”í‹° í˜ì´ì§€ ë¹„ìœ¨ ìœ ì§€ ëª©í‘œ | 5% |\n| `eviction_target` | ì „ì²´ ìºì‹œ ì‚¬ìš©ë¥  ëª©í‘œ | 80% |\n\n```yaml\n# ê³ ê¸‰ Eviction ì„¤ì •\nstorage:\n  wiredTiger:\n    engineConfig:\n      eviction:\n        threads_max: 8\n        threads_min: 4\n```\n\n> **ì£¼ì˜**: Evictionì´ í¬ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ë©´ ì“°ê¸° ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ì €í•˜ë©ë‹ˆë‹¤. `cache eviction` ê´€ë ¨ ì§€í‘œë¥¼ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”.\n\n---\n\n## ë°ì´í„° ì••ì¶•\n\nWiredTigerëŠ” ë‘ ìˆ˜ì¤€ì˜ ì••ì¶•ì„ ì§€ì›í•©ë‹ˆë‹¤:\n\n### ì»¬ë ‰ì…˜ ë°ì´í„° ì••ì¶•\n\n```javascript\n// ì»¬ë ‰ì…˜ ìƒì„± ì‹œ ì••ì¶• ì•Œê³ ë¦¬ì¦˜ ì§€ì •\ndb.createCollection(\"logs\", {\n  storageEngine: {\n    wiredTiger: {\n      configString: \"block_compressor=zstd\"\n    }\n  }\n})\n```\n\n| ì•Œê³ ë¦¬ì¦˜ | ì••ì¶•ë¥  | ì†ë„ | ìš©ë„ |\n|---------|-------|-----|------|\n| `snappy` (ê¸°ë³¸) | ë³´í†µ | ë¹ ë¦„ | ë²”ìš© |\n| `zlib` | ë†’ìŒ | ëŠë¦¼ | ì•„ì¹´ì´ë¸Œ |\n| `zstd` | ë†’ìŒ | ë¹ ë¦„ | **ê¶Œì¥** (4.2+) |\n| `none` | - | - | ì‹¤ì‹œê°„ ì²˜ë¦¬ |\n\n> **zstd ê¶Œì¥ ì´ìœ **: ë¬´ì†ì‹¤ ì••ì¶•ì´ë©´ì„œ ì••ì¶•/í•´ì œ ì†ë„ê°€ ì¤€ìˆ˜í•¨\n\n### ì¸ë±ìŠ¤ í”„ë¦¬í”½ìŠ¤ ì••ì¶•\n\nì¸ë±ìŠ¤ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ **í”„ë¦¬í”½ìŠ¤ ì••ì¶•**ì´ ì ìš©ë©ë‹ˆë‹¤:\n\n```javascript\n// ì¸ë±ìŠ¤ ì••ì¶• ë¹„í™œì„±í™” ì˜ˆì‹œ\ndb.collection.createIndex(\n  { field: 1 },\n  { storageEngine: { wiredTiger: { configString: \"prefix_compression=false\" } } }\n)\n```\n\n---\n\n## ì²´í¬í¬ì¸íŠ¸ (Checkpoint)\n\nì²´í¬í¬ì¸íŠ¸ëŠ” **ë°ì´í„° íŒŒì¼ê³¼ íŠ¸ëœì­ì…˜ ë¡œê·¸ê°€ ë™ê¸°í™”ë˜ëŠ” ì‹œì **ì…ë‹ˆë‹¤. DB ì¥ì•  ì‹œ ë³µêµ¬ ì‹œì ì„ ê²°ì •í•˜ëŠ” ê¸°ì¤€ì´ ë©ë‹ˆë‹¤.\n\n### Sharp Checkpoint\n\nMongoDBëŠ” **Sharp Checkpoint** ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:\n\n- ì²´í¬í¬ì¸íŠ¸ ì‹¤í–‰ ì‹œì ì— ë”í‹° í˜ì´ì§€ë¥¼ **í•œ ë²ˆì— ëª¨ì•„ì„œ** ë””ìŠ¤í¬ì— ë‚´ë ¤ì”€\n- Fuzzy Checkpoint(ì ì§„ì  ë°©ì‹)ì™€ ëŒ€ë¹„ë˜ëŠ” ê°œë…\n\n### ì²´í¬í¬ì¸íŠ¸ íŠ¸ë¦¬ê±°\n\nê¸°ë³¸ì ìœ¼ë¡œ ë‹¤ìŒ ì¡°ê±´ì—ì„œ ì²´í¬í¬ì¸íŠ¸ê°€ ë°œìƒí•©ë‹ˆë‹¤:\n\n- **60ì´ˆ** ê²½ê³¼\n- **2GB** ì €ë„ ë°ì´í„° ëˆ„ì \n\n### ì²´í¬í¬ì¸íŠ¸ ì˜µì…˜\n\n| ì˜µì…˜ | ì„¤ëª… | ê¸°ë³¸ê°’ |\n|-----|------|-------|\n| `log_size` | ì´ í¬ê¸°ë§Œí¼ íŠ¸ëœì­ì…˜ ë¡œê·¸ ì“°ë©´ ì²´í¬í¬ì¸íŠ¸ ì‹¤í–‰ | 0 (ìë™) |\n| `wait` | ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ê°„ê²© (ì´ˆ) | 0 (ìë™) |\n\n```yaml\n# mongod.conf\nstorage:\n  wiredTiger:\n    engineConfig:\n      checkpointSizeMB: 1024\n```\n\n---\n\n## ì €ë„ë§ (Journaling)\n\nWrite-Ahead Logging(WAL)ìœ¼ë¡œ ë°ì´í„° ë‚´êµ¬ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.\n\n- Journal LogëŠ” ë°ì´í„° ë””ë ‰í† ë¦¬ í•˜ìœ„ `journal/` í´ë”ì— ì €ì¥\n- ì¥ì•  ë°œìƒ ì‹œ Journal Logë¥¼ ì‚¬ìš©í•´ ë°ì´í„° ë³µêµ¬\n\n```yaml\nstorage:\n  journal:\n    enabled: true\n    commitIntervalMs: 100  # ê¸°ë³¸ê°’\n```\n\n---\n\n## ì„±ëŠ¥ íŠœë‹\n\n### í”„ë¡œë•ì…˜ ê¶Œì¥ ì„¤ì •\n\n```yaml\n# mongod.conf\nstorage:\n  wiredTiger:\n    engineConfig:\n      cacheSizeGB: 8\n      journalCompressor: snappy\n    collectionConfig:\n      blockCompressor: zstd\n    indexConfig:\n      prefixCompressionEnabled: true\n```\n\n### ëª¨ë‹ˆí„°ë§ ëª…ë ¹ì–´\n\n```javascript\n// WiredTiger ì „ì²´ í†µê³„\ndb.serverStatus().wiredTiger\n\n// ìºì‹œ ìƒíƒœ\ndb.serverStatus().wiredTiger.cache\n\n// ì»¬ë ‰ì…˜ë³„ í†µê³„\ndb.collection.stats().wiredTiger\n```\n\n---\n\n## ì£¼ì˜ì‚¬í•­\n\n1. **ìºì‹œ í¬ê¸°**: ì‹œìŠ¤í…œ RAMì˜ 50% ì´í•˜ ê¶Œì¥\n2. **ì••ì¶•**: CPU ì˜¤ë²„í—¤ë“œì™€ ì €ì¥ ê³µê°„ íŠ¸ë ˆì´ë“œì˜¤í”„ ê³ ë ¤\n3. **ì €ë„ë§**: ë¹„í™œì„±í™” ì‹œ ë°ì´í„° ì†ì‹¤ ìœ„í—˜\n4. **Eviction ëª¨ë‹ˆí„°ë§**: í¬ê·¸ë¼ìš´ë“œ Eviction ë°œìƒ ì‹œ ì„±ëŠ¥ ì €í•˜\n5. **ì²´í¬í¬ì¸íŠ¸**: Sharp Checkpointë¡œ ì¸í•œ ì¼ì‹œì  I/O ìŠ¤íŒŒì´í¬ ê³ ë ¤\n\n## ì°¸ê³  ìë£Œ\n\n- [MongoDB WiredTiger ê³µì‹ ë¬¸ì„œ](https://www.mongodb.com/docs/manual/core/wiredtiger/)\n- [WiredTiger GitHub](https://github.com/wiredtiger/wiredtiger)\n- [MongoDB ì´ ì •ë¦¬ (liltdevs)](https://liltdevs.tistory.com/216)",
    "docType": "original",
    "category": "Database",
    "tags": [
      "Database",
      "MongoDB",
      "WiredTiger"
    ],
    "readingTime": 5,
    "wordCount": 865,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "sharding-guide",
    "slug": "sharding-guide",
    "path": "database/mongodb",
    "fullPath": "database/mongodb/sharding-guide",
    "title": "MongoDB ìƒ¤ë”©(Sharding) ì™„ë²½ ê°€ì´ë“œ",
    "excerpt": "MongoDB ìƒ¤ë”©ì˜ ê°œë…ë¶€í„° ìƒ¤ë“œ í‚¤ ì„ íƒ, í´ëŸ¬ìŠ¤í„° êµ¬ì„±ê¹Œì§€ ì‹¤ì „ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
    "content": "# MongoDB ìƒ¤ë”©(Sharding) ì™„ë²½ ê°€ì´ë“œ\n\n## ìƒ¤ë”©ì´ë€?\n\n**ìƒ¤ë”©**ì€ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ì„œë²„ì— ë¶„ì‚° ì €ì¥í•˜ëŠ” ìˆ˜í‰ í™•ì¥(Horizontal Scaling) ë°©ì‹ì…ë‹ˆë‹¤. MongoDBëŠ” ìë™ ìƒ¤ë”©ì„ ì§€ì›í•˜ì—¬ ë°ì´í„°ê°€ ì¦ê°€í•´ë„ ì„±ëŠ¥ì„ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## ìƒ¤ë”© ì•„í‚¤í…ì²˜\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                      Application                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â”‚\n                            â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                       mongos                             â”‚\nâ”‚                   (Query Router)                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â”‚\n        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â–¼                   â–¼                   â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Shard 1     â”‚   â”‚   Shard 2     â”‚   â”‚   Shard 3     â”‚\nâ”‚  (Replica Set)â”‚   â”‚  (Replica Set)â”‚   â”‚  (Replica Set)â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â”‚\n                            â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              Config Servers (Replica Set)                â”‚\nâ”‚                  (ë©”íƒ€ë°ì´í„° ì €ì¥)                        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### êµ¬ì„± ìš”ì†Œ\n\n| êµ¬ì„± ìš”ì†Œ | ì—­í•  |\n|---------|------|\n| **mongos** | í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ì„ ì ì ˆí•œ ìƒ¤ë“œë¡œ ë¼ìš°íŒ… |\n| **Shard** | ì‹¤ì œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ë ˆí”Œë¦¬ì¹´ ì…‹ |\n| **Config Server** | í´ëŸ¬ìŠ¤í„° ë©”íƒ€ë°ì´í„° ë° ì²­í¬ ì •ë³´ ì €ì¥ |\n\n## ìƒ¤ë“œ í‚¤ (Shard Key)\n\nìƒ¤ë“œ í‚¤ëŠ” ë°ì´í„°ë¥¼ ë¶„ì‚°í•˜ëŠ” ê¸°ì¤€ì´ ë˜ëŠ” í•„ë“œì…ë‹ˆë‹¤. **ìƒ¤ë“œ í‚¤ ì„ íƒì€ ì„±ëŠ¥ì— ì§ì ‘ì ì¸ ì˜í–¥**ì„ ë¯¸ì¹©ë‹ˆë‹¤.\n\n### ì¢‹ì€ ìƒ¤ë“œ í‚¤ì˜ ì¡°ê±´\n\n1. **ë†’ì€ ì¹´ë””ë„ë¦¬í‹°**: ë‹¤ì–‘í•œ ê°’ì„ ê°€ì ¸ì•¼ í•¨\n2. **ê· ë“±í•œ ë¶„í¬**: ë°ì´í„°ê°€ ê³¨ê³ ë£¨ ë¶„ì‚°ë˜ì–´ì•¼ í•¨\n3. **ì¿¼ë¦¬ íŒ¨í„´ ë¶€í•©**: ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¿¼ë¦¬ ì¡°ê±´ê³¼ ì¼ì¹˜\n\n### ìƒ¤ë“œ í‚¤ ì˜ˆì‹œ\n\n```javascript\n// ì¢‹ì€ ì˜ˆ: ë†’ì€ ì¹´ë””ë„ë¦¬í‹° + ê· ë“± ë¶„í¬\nsh.shardCollection(\"mydb.orders\", { orderId: \"hashed\" })\n\n// ë²”ìœ„ ìƒ¤ë”©: ë²”ìœ„ ì¿¼ë¦¬ì— ìœ ë¦¬\nsh.shardCollection(\"mydb.logs\", { timestamp: 1 })\n\n// ë³µí•© ìƒ¤ë“œ í‚¤: ë” ì„¸ë°€í•œ ë¶„ì‚°\nsh.shardCollection(\"mydb.users\", { country: 1, createdAt: 1 })\n```\n\n### ìƒ¤ë“œ í‚¤ ì „ëµ\n\n| ì „ëµ | ì¥ì  | ë‹¨ì  | ì í•©í•œ ê²½ìš° |\n|-----|------|------|-----------|\n| **Hashed** | ê· ë“± ë¶„ì‚° | ë²”ìœ„ ì¿¼ë¦¬ ë¹„íš¨ìœ¨ | ëœë¤ ì•¡ì„¸ìŠ¤ |\n| **Ranged** | ë²”ìœ„ ì¿¼ë¦¬ íš¨ìœ¨ì  | í•«ìŠ¤íŒŸ ìœ„í—˜ | ì‹œê³„ì—´ ë°ì´í„° |\n| **Compound** | ìœ ì—°ì„± | ì„¤ê³„ ë³µì¡ | ë³µì¡í•œ ì¿¼ë¦¬ |\n\n## ì²­í¬ (Chunk)\n\në°ì´í„°ëŠ” **ì²­í¬** ë‹¨ìœ„ë¡œ ìƒ¤ë“œì— ë¶„ì‚°ë©ë‹ˆë‹¤.\n\n```javascript\n// ì²­í¬ í¬ê¸° í™•ì¸\nuse config\ndb.settings.find({ _id: \"chunksize\" })\n\n// ì²­í¬ í¬ê¸° ë³€ê²½ (MB ë‹¨ìœ„, ê¸°ë³¸ 128MB)\ndb.settings.updateOne(\n  { _id: \"chunksize\" },\n  { $set: { value: 64 } },\n  { upsert: true }\n)\n```\n\n### ì²­í¬ ë°¸ëŸ°ì‹±\n\nMongoDBëŠ” **ë°¸ëŸ°ì„œ**ë¥¼ í†µí•´ ì²­í¬ë¥¼ ìë™ìœ¼ë¡œ ê· í˜• ë¶„ë°°í•©ë‹ˆë‹¤:\n\n```javascript\n// ë°¸ëŸ°ì„œ ìƒíƒœ í™•ì¸\nsh.getBalancerState()\n\n// ë°¸ëŸ°ì„œ í™œì„±í™”/ë¹„í™œì„±í™”\nsh.startBalancer()\nsh.stopBalancer()\n\n// íŠ¹ì • ì‹œê°„ëŒ€ì—ë§Œ ë°¸ëŸ°ì‹±\ndb.settings.updateOne(\n  { _id: \"balancer\" },\n  { $set: { activeWindow: { start: \"02:00\", stop: \"06:00\" } } }\n)\n```\n\n## ìƒ¤ë”© í´ëŸ¬ìŠ¤í„° êµ¬ì„±\n\n### 1. Config Server ì„¤ì •\n\n```yaml\n# config-server.conf\nsharding:\n  clusterRole: configsvr\nreplication:\n  replSetName: configReplSet\nnet:\n  port: 27019\n```\n\n### 2. Shard ì„¤ì •\n\n```yaml\n# shard.conf\nsharding:\n  clusterRole: shardsvr\nreplication:\n  replSetName: shard1ReplSet\nnet:\n  port: 27018\n```\n\n### 3. mongos ì„¤ì •\n\n```yaml\n# mongos.conf\nsharding:\n  configDB: configReplSet/config1:27019,config2:27019,config3:27019\nnet:\n  port: 27017\n```\n\n### 4. ìƒ¤ë“œ ì¶”ê°€\n\n```javascript\n// mongosì— ì ‘ì†í•˜ì—¬ ìƒ¤ë“œ ì¶”ê°€\nsh.addShard(\"shard1ReplSet/shard1-1:27018,shard1-2:27018\")\nsh.addShard(\"shard2ReplSet/shard2-1:27018,shard2-2:27018\")\n\n// ìƒ¤ë”© í™œì„±í™”\nsh.enableSharding(\"mydb\")\n\n// ì»¬ë ‰ì…˜ ìƒ¤ë”©\nsh.shardCollection(\"mydb.orders\", { customerId: \"hashed\" })\n```\n\n## ì¿¼ë¦¬ ë¼ìš°íŒ…\n\n### Targeted Query (íš¨ìœ¨ì )\n\nìƒ¤ë“œ í‚¤ë¥¼ í¬í•¨í•œ ì¿¼ë¦¬ëŠ” íŠ¹ì • ìƒ¤ë“œë¡œë§Œ ì „ë‹¬ë©ë‹ˆë‹¤:\n\n```javascript\n// customerIdê°€ ìƒ¤ë“œ í‚¤ì¼ ë•Œ - íŠ¹ì • ìƒ¤ë“œë§Œ ì¡°íšŒ\ndb.orders.find({ customerId: \"user123\" })\n```\n\n### Scatter-Gather Query (ë¹„íš¨ìœ¨ì )\n\nìƒ¤ë“œ í‚¤ê°€ ì—†ìœ¼ë©´ ëª¨ë“  ìƒ¤ë“œì— ì¿¼ë¦¬ ì „ì†¡:\n\n```javascript\n// ëª¨ë“  ìƒ¤ë“œì— ì¿¼ë¦¬ ì „ì†¡ í›„ ê²°ê³¼ ë³‘í•©\ndb.orders.find({ status: \"pending\" })\n```\n\n## ëª¨ë‹ˆí„°ë§\n\n```javascript\n// ìƒ¤ë”© ìƒíƒœ í™•ì¸\nsh.status()\n\n// ê° ìƒ¤ë“œë³„ ë°ì´í„° ë¶„í¬\ndb.orders.getShardDistribution()\n\n// ì²­í¬ ì •ë³´\nuse config\ndb.chunks.find({ ns: \"mydb.orders\" }).pretty()\n```\n\n## ì£¼ì˜ì‚¬í•­\n\n1. **ìƒ¤ë“œ í‚¤ëŠ” ë³€ê²½ ë¶ˆê°€**: ì„¤ê³„ ë‹¨ê³„ì—ì„œ ì‹ ì¤‘íˆ ì„ íƒ\n2. **ìƒ¤ë“œ í‚¤ ê°’ì€ ë¶ˆë³€**: í•œë²ˆ ì„¤ì •ëœ ë¬¸ì„œì˜ ìƒ¤ë“œ í‚¤ ë³€ê²½ ë¶ˆê°€\n3. **íŠ¸ëœì­ì…˜ ì œí•œ**: ë‹¤ì¤‘ ìƒ¤ë“œ íŠ¸ëœì­ì…˜ì€ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥\n4. **ì¸ë±ìŠ¤ í•„ìˆ˜**: ìƒ¤ë“œ í‚¤ì—ëŠ” ë°˜ë“œì‹œ ì¸ë±ìŠ¤ ì¡´ì¬í•´ì•¼ í•¨\n\n## ì°¸ê³  ìë£Œ\n\n- [MongoDB Sharding ê³µì‹ ë¬¸ì„œ](https://www.mongodb.com/docs/manual/sharding/)\n- [ìƒ¤ë“œ í‚¤ ì„ íƒ ê°€ì´ë“œ](https://www.mongodb.com/docs/manual/core/sharding-choose-a-shard-key/)",
    "docType": "original",
    "category": "Database",
    "tags": [
      "Database",
      "Distributed Systems",
      "MongoDB",
      "Scaling"
    ],
    "readingTime": 3,
    "wordCount": 577,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "merkle-trie-integrity",
    "slug": "merkle-trie-integrity",
    "path": "blockchain/ethereum",
    "fullPath": "blockchain/ethereum/merkle-trie-integrity",
    "title": "go-ethereum Merkle Trieë¥¼ í™œìš©í•œ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦",
    "excerpt": "go-ethereumì˜ Merkle Trie êµ¬í˜„ì„ í™œìš©í•˜ì—¬ ëŒ€ëŸ‰ ë°ì´í„°ì˜ ë¬´ê²°ì„±ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê²€ì¦í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# go-ethereum Merkle Trieë¥¼ í™œìš©í•œ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦\n\n## ê°œìš”\n\n**Merkle Trie**ëŠ” ëŒ€ëŸ‰ì˜ ë°ì´í„° ë¬´ê²°ì„±ì„ ë‹¨ì¼ í•´ì‹œê°’(Merkle Root)ìœ¼ë¡œ ì¦ëª…í•  ìˆ˜ ìˆëŠ” ìë£Œêµ¬ì¡°ì…ë‹ˆë‹¤. ì´ ê¸€ì—ì„œëŠ” go-ethereumì˜ Trie íŒ¨í‚¤ì§€ë¥¼ í™œìš©í•˜ì—¬ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n\n## Merkle Trieë€?\n\n### í•µì‹¬ ê°œë…\n\n```mermaid\ngraph TD\n    %% ë…¸ë“œ ì •ì˜\n    Root[\"Merkle Root (H_root)\"]\n    HAB[\"H(AB)\"]\n    HCD[\"H(CD)\"]\n    HA[\"H(A)\"]\n    HB[\"H(B)\"]\n    HC[\"H(C)\"]\n    HD[\"H(D)\"]\n    A[\"A\"]\n    B[\"B\"]\n    C[\"C\"]\n    D[\"D\"]\n\n    %% ì—°ê²° ì •ì˜ (í•´ì‹œ ê³¼ì •: ì•„ë˜ì—ì„œ ìœ„ë¡œ)\n    HAB --> Root\n    HCD --> Root\n    HA --> HAB\n    HB --> HAB\n    HC --> HCD\n    HD --> HCD\n    A --> HA\n    B --> HB\n    C --> HC\n    D --> HD\n\n    %% ìŠ¤íƒ€ì¼ë§ (ì„ íƒì‚¬í•­: ë£¨íŠ¸ì™€ ë¦¬í”„ ë…¸ë“œ ê°•ì¡°)\n    style Root fill:#f9f,stroke:#333,stroke-width:2px,color:white\n    style A fill:#fff,stroke:#333,stroke-dasharray: 5 5\n    style B fill:#fff,stroke:#333,stroke-dasharray: 5 5\n    style C fill:#fff,stroke:#333,stroke-dasharray: 5 5\n    style D fill:#fff,stroke:#333,stroke-dasharray: 5 5\n```\n\n- **Leaf ë…¸ë“œ**: ì›ë³¸ ë°ì´í„°ì˜ í•´ì‹œ\n- **ë‚´ë¶€ ë…¸ë“œ**: ìì‹ ë…¸ë“œë“¤ì˜ í•´ì‹œ ì¡°í•©\n- **Root ë…¸ë“œ**: ì „ì²´ ë°ì´í„°ë¥¼ ëŒ€í‘œí•˜ëŠ” ë‹¨ì¼ í•´ì‹œ\n\n### ì™œ Merkle Trieì¸ê°€?\n\n| ì¥ì  | ì„¤ëª… |\n|------|------|\n| **íš¨ìœ¨ì  ê²€ì¦** | O(log n) ë³µì¡ë„ë¡œ íŠ¹ì • ë°ì´í„° í¬í•¨ ì¦ëª… |\n| **ë°°ì¹˜ ê²€ì¦** | ìˆ˜ì²œ ê°œ ë°ì´í„°ë¥¼ ë‹¨ì¼ ë£¨íŠ¸ë¡œ ê²€ì¦ |\n| **ë³€ì¡° ê°ì§€** | í•˜ë‚˜ë¼ë„ ë³€ê²½ë˜ë©´ ë£¨íŠ¸ í•´ì‹œ ë³€ê²½ |\n| **ë¸”ë¡ì²´ì¸ í˜¸í™˜** | ëŒ€ë¶€ë¶„ì˜ ë¸”ë¡ì²´ì¸ì´ ì‚¬ìš©í•˜ëŠ” í‘œì¤€ êµ¬ì¡° |\n\n## go-ethereum Trie íŒ¨í‚¤ì§€\n\n### ì„¤ì¹˜\n\n```bash\ngo get github.com/ethereum/go-ethereum\n```\n\n### í•µì‹¬ ì¸í„°í˜ì´ìŠ¤\n\n```go\npackage integrity\n\nimport (\n    \"github.com/ethereum/go-ethereum/common\"\n    \"github.com/ethereum/go-ethereum/core/rawdb\"\n    ethtrie \"github.com/ethereum/go-ethereum/trie\"\n    \"github.com/ethereum/go-ethereum/triedb\"\n)\n```\n\n## í•µì‹¬ êµ¬í˜„\n\n### Trie ë˜í¼\n\n```go\npackage integrity\n\nimport (\n    \"errors\"\n    \"fmt\"\n    \"sync\"\n    \n    \"github.com/ethereum/go-ethereum/common\"\n    \"github.com/ethereum/go-ethereum/core/rawdb\"\n    ethtrie \"github.com/ethereum/go-ethereum/trie\"\n    \"github.com/ethereum/go-ethereum/triedb\"\n)\n\nvar (\n    ErrNodeNotFound = errors.New(\"node not found in trie\")\n    ErrInvalidProof = errors.New(\"invalid merkle proof\")\n)\n\n// LeafNodeëŠ” Trieì˜ ë¦¬í”„ ë…¸ë“œì…ë‹ˆë‹¤.\ntype LeafNode struct {\n    key   []byte\n    value []byte\n}\n\nfunc NewLeafNode(key, value []byte) *LeafNode {\n    return &LeafNode{key: key, value: value}\n}\n\nfunc (n *LeafNode) Key() []byte   { return n.key }\nfunc (n *LeafNode) Value() []byte { return n.value }\nfunc (n *LeafNode) Hash() []byte  { return n.value }\n\n// IntegrityTrieëŠ” ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ì„ ìœ„í•œ Merkle Trieì…ë‹ˆë‹¤.\ntype IntegrityTrie struct {\n    trie *ethtrie.Trie\n    db   *triedb.Database\n    mu   sync.RWMutex\n}\n\n// NewIntegrityTrieëŠ” ìƒˆ Trieë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\nfunc NewIntegrityTrie(seedNodes []*LeafNode) (*IntegrityTrie, error) {\n    // ì¸ë©”ëª¨ë¦¬ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±\n    memDB := rawdb.NewMemoryDatabase()\n    trieDB := triedb.NewDatabase(memDB, nil)\n    \n    // ë¹ˆ Trie ìƒì„±\n    t := ethtrie.NewEmpty(trieDB)\n    \n    integrityTrie := &IntegrityTrie{\n        trie: t,\n        db:   trieDB,\n    }\n    \n    // ì´ˆê¸° ë…¸ë“œ ì‚½ì…\n    for _, node := range seedNodes {\n        if err := integrityTrie.Insert(node.Key(), node.Value()); err != nil {\n            return nil, fmt.Errorf(\"insert seed node: %w\", err)\n        }\n    }\n    \n    return integrityTrie, nil\n}\n\n// InsertëŠ” í‚¤-ê°’ ìŒì„ Trieì— ì¶”ê°€í•©ë‹ˆë‹¤.\nfunc (t *IntegrityTrie) Insert(key, value []byte) error {\n    t.mu.Lock()\n    defer t.mu.Unlock()\n    \n    return t.trie.Update(key, value)\n}\n\n// RootHashëŠ” í˜„ì¬ Merkle Rootë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\nfunc (t *IntegrityTrie) RootHash() common.Hash {\n    t.mu.RLock()\n    defer t.mu.RUnlock()\n    \n    return t.trie.Hash()\n}\n\n// Commitì€ Trie ìƒíƒœë¥¼ ì €ì¥í•˜ê³  Rootë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\nfunc (t *IntegrityTrie) Commit() (common.Hash, error) {\n    t.mu.Lock()\n    defer t.mu.Unlock()\n    \n    // Trie ì»¤ë°‹\n    rootHash, _, err := t.trie.Commit(false)\n    if err != nil {\n        return common.Hash{}, fmt.Errorf(\"trie commit: %w\", err)\n    }\n    \n    // ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥\n    if err := t.db.Commit(rootHash, false); err != nil {\n        return common.Hash{}, fmt.Errorf(\"db commit: %w\", err)\n    }\n    \n    return rootHash, nil\n}\n```\n\n### ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬\n\nê°ì‚¬ ë¡œê·¸ ë“± ëŒ€ëŸ‰ ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬:\n\n```go\n// BatchDataëŠ” ë°°ì¹˜ ì²˜ë¦¬í•  ë°ì´í„° í•­ëª©ì…ë‹ˆë‹¤.\ntype BatchData struct {\n    ID      string\n    Content []byte\n}\n\n// ComputeBatchRootëŠ” ë°°ì¹˜ ë°ì´í„°ì˜ Merkle Rootë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\nfunc ComputeBatchRoot(items []BatchData) (common.Hash, error) {\n    nodes := make([]*LeafNode, len(items))\n    \n    for i, item := range items {\n        // í‚¤: ê³ ìœ  IDì˜ í•´ì‹œ\n        key := crypto.Keccak256([]byte(item.ID))\n        // ê°’: ì½˜í…ì¸ ì˜ í•´ì‹œ\n        value := crypto.Keccak256(item.Content)\n        \n        nodes[i] = NewLeafNode(key, value)\n    }\n    \n    trie, err := NewIntegrityTrie(nodes)\n    if err != nil {\n        return common.Hash{}, err\n    }\n    \n    return trie.Commit()\n}\n```\n\n### ê°ì‚¬ ë¡œê·¸ í†µí•© ì˜ˆì‹œ\n\n```go\npackage audit\n\nimport (\n    \"context\"\n    \"time\"\n    \"encoding/json\"\n)\n\n// AuditBatchëŠ” ê°ì‚¬ ë¡œê·¸ ë°°ì¹˜ì…ë‹ˆë‹¤.\ntype AuditBatch struct {\n    ID        string       `json:\"id\"`\n    Logs      []AuditLog   `json:\"logs\"`\n    CreatedAt time.Time    `json:\"created_at\"`\n}\n\n// AuditLogëŠ” ê°œë³„ ê°ì‚¬ ë¡œê·¸ì…ë‹ˆë‹¤.\ntype AuditLog struct {\n    DocumentID string `json:\"document_id\"`\n    Action     string `json:\"action\"`\n    Version    int32  `json:\"version\"`\n    Hash       string `json:\"hash\"`\n}\n\n// AnchoringServiceëŠ” ë¸”ë¡ì²´ì¸ ì•µì»¤ë§ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.\ntype AnchoringService struct {\n    blockchain BlockchainClient\n}\n\n// ProcessBatchëŠ” ë°°ì¹˜ ê°ì‚¬ ë¡œê·¸ë¥¼ ì²˜ë¦¬í•˜ê³  ì•µì»¤ë§í•©ë‹ˆë‹¤.\nfunc (s *AnchoringService) ProcessBatch(ctx context.Context, logs []AuditLog) (*AnchorResult, error) {\n    // 1. ê° ë¡œê·¸ë¥¼ Merkle Trie ë…¸ë“œë¡œ ë³€í™˜\n    items := make([]BatchData, len(logs))\n    for i, log := range logs {\n        content, _ := json.Marshal(log)\n        items[i] = BatchData{\n            ID:      log.DocumentID + \"_\" + fmt.Sprint(log.Version),\n            Content: content,\n        }\n    }\n    \n    // 2. Merkle Root ê³„ì‚°\n    merkleRoot, err := ComputeBatchRoot(items)\n    if err != nil {\n        return nil, fmt.Errorf(\"compute merkle root: %w\", err)\n    }\n    \n    // 3. ë¸”ë¡ì²´ì¸ì— ì•µì»¤ë§\n    txHash, err := s.blockchain.SubmitAnchor(ctx, merkleRoot.Hex())\n    if err != nil {\n        return nil, fmt.Errorf(\"submit anchor: %w\", err)\n    }\n    \n    return &AnchorResult{\n        MerkleRoot:      merkleRoot.Hex(),\n        TransactionHash: txHash,\n        AnchoredAt:      time.Now(),\n        LogCount:        len(logs),\n    }, nil\n}\n\ntype AnchorResult struct {\n    MerkleRoot      string    `json:\"merkle_root\"`\n    TransactionHash string    `json:\"transaction_hash\"`\n    AnchoredAt      time.Time `json:\"anchored_at\"`\n    LogCount        int       `json:\"log_count\"`\n}\n```\n\n## Merkle Proof ìƒì„± ë° ê²€ì¦\n\níŠ¹ì • ë°ì´í„°ê°€ Merkle Rootì— í¬í•¨ë˜ì–´ ìˆìŒì„ ì¦ëª…:\n\n```go\nimport (\n    \"github.com/ethereum/go-ethereum/trie\"\n)\n\n// GenerateProofëŠ” íŠ¹ì • í‚¤ì— ëŒ€í•œ Merkle Proofë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\nfunc (t *IntegrityTrie) GenerateProof(key []byte) ([][]byte, error) {\n    t.mu.RLock()\n    defer t.mu.RUnlock()\n    \n    // Proof ë…¸ë“œë“¤ì„ ë‹´ì„ ë©”ëª¨ë¦¬ DB\n    proofDB := rawdb.NewMemoryDatabase()\n    \n    // Proof ìƒì„±\n    if err := t.trie.Prove(key, proofDB); err != nil {\n        return nil, fmt.Errorf(\"generate proof: %w\", err)\n    }\n    \n    // Proof ë°ì´í„° ì¶”ì¶œ\n    var proofNodes [][]byte\n    it := proofDB.NewIterator(nil, nil)\n    defer it.Release()\n    \n    for it.Next() {\n        proofNodes = append(proofNodes, common.CopyBytes(it.Value()))\n    }\n    \n    return proofNodes, nil\n}\n\n// VerifyProofëŠ” Merkle Proofë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.\nfunc VerifyProof(rootHash common.Hash, key []byte, proofNodes [][]byte) ([]byte, error) {\n    // Proof DB êµ¬ì„±\n    proofDB := rawdb.NewMemoryDatabase()\n    for _, node := range proofNodes {\n        hash := crypto.Keccak256(node)\n        proofDB.Put(hash, node)\n    }\n    \n    // Proof ê²€ì¦ ë° ê°’ ë°˜í™˜\n    value, err := trie.VerifyProof(rootHash, key, proofDB)\n    if err != nil {\n        return nil, ErrInvalidProof\n    }\n    \n    return value, nil\n}\n```\n\n### ì‚¬ìš© ì˜ˆì‹œ\n\n```go\nfunc ExampleMerkleProof() {\n    // 1. ë°ì´í„° ì¤€ë¹„\n    items := []BatchData{\n        {ID: \"doc-001\", Content: []byte(`{\"action\":\"CREATE\"}`)},\n        {ID: \"doc-002\", Content: []byte(`{\"action\":\"UPDATE\"}`)},\n        {ID: \"doc-003\", Content: []byte(`{\"action\":\"DELETE\"}`)},\n    }\n    \n    nodes := make([]*LeafNode, len(items))\n    for i, item := range items {\n        key := crypto.Keccak256([]byte(item.ID))\n        value := crypto.Keccak256(item.Content)\n        nodes[i] = NewLeafNode(key, value)\n    }\n    \n    // 2. Trie ìƒì„± ë° Root ê³„ì‚°\n    trie, _ := NewIntegrityTrie(nodes)\n    rootHash, _ := trie.Commit()\n    \n    fmt.Printf(\"Merkle Root: %s\\n\", rootHash.Hex())\n    \n    // 3. íŠ¹ì • ë¬¸ì„œì˜ Proof ìƒì„±\n    targetKey := crypto.Keccak256([]byte(\"doc-002\"))\n    proof, _ := trie.GenerateProof(targetKey)\n    \n    fmt.Printf(\"Proof nodes: %d\\n\", len(proof))\n    \n    // 4. Proof ê²€ì¦ (ë‹¤ë¥¸ ì‹œìŠ¤í…œì—ì„œ)\n    expectedValue := crypto.Keccak256([]byte(`{\"action\":\"UPDATE\"}`))\n    verifiedValue, err := VerifyProof(rootHash, targetKey, proof)\n    \n    if err != nil {\n        fmt.Println(\"Proof invalid!\")\n    } else if bytes.Equal(verifiedValue, expectedValue) {\n        fmt.Println(\"Proof verified! Data integrity confirmed.\")\n    }\n}\n```\n\n## ë¬´ê²°ì„± ê²€ì¦ API\n\nì™¸ë¶€ì—ì„œ ë°ì´í„° ë¬´ê²°ì„±ì„ ê²€ì¦í•  ìˆ˜ ìˆëŠ” API:\n\n```go\npackage api\n\ntype IntegrityVerification struct {\n    DocumentID   string `json:\"document_id\"`\n    Version      int32  `json:\"version\"`\n    ContentHash  string `json:\"content_hash\"`\n    MerkleRoot   string `json:\"merkle_root\"`\n    BlockchainTx string `json:\"blockchain_tx\"`\n    Verified     bool   `json:\"verified\"`\n    VerifiedAt   string `json:\"verified_at\"`\n}\n\nfunc (s *APIServer) VerifyIntegrity(ctx context.Context, req VerifyRequest) (*IntegrityVerification, error) {\n    // 1. ë¬¸ì„œ ì¡°íšŒ\n    doc, err := s.docService.FindByVersion(ctx, req.Collection, req.URI, req.Version)\n    if err != nil {\n        return nil, err\n    }\n    \n    // 2. ë¬¸ì„œì˜ ì•µì»¤ë§ ë©”íƒ€ë°ì´í„° í™•ì¸\n    anchor := doc.AnchoringMetadata\n    if anchor.Status != \"COMPLETED\" {\n        return nil, errors.New(\"document not yet anchored\")\n    }\n    \n    // 3. ë¸”ë¡ì²´ì¸ì—ì„œ Merkle Root ì¡°íšŒ\n    blockchainRoot, err := s.blockchain.GetAnchor(ctx, anchor.TransactionHash)\n    if err != nil {\n        return nil, err\n    }\n    \n    // 4. í˜„ì¬ ë°ì´í„°ë¡œ í•´ì‹œ ì¬ê³„ì‚°\n    currentHash := computeDocumentHash(doc)\n    \n    // 5. ì €ì¥ëœ Merkle Rootì™€ ë¸”ë¡ì²´ì¸ Root ë¹„êµ\n    verified := anchor.MerkleRoot == blockchainRoot\n    \n    return &IntegrityVerification{\n        DocumentID:   doc.URI,\n        Version:      doc.Version,\n        ContentHash:  currentHash,\n        MerkleRoot:   anchor.MerkleRoot,\n        BlockchainTx: anchor.TransactionHash,\n        Verified:     verified,\n        VerifiedAt:   time.Now().Format(time.RFC3339),\n    }, nil\n}\n```\n\n## ì„±ëŠ¥ ìµœì í™”\n\n### ë°°ì¹˜ í¬ê¸° ì¡°ì •\n\n```go\nconst (\n    OptimalBatchSize = 1000 // Trie ì—°ì‚°ì— ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸°\n    MaxBatchSize     = 10000\n)\n\nfunc ProcessWithOptimalBatch(items []BatchData) ([]common.Hash, error) {\n    var roots []common.Hash\n    \n    for i := 0; i < len(items); i += OptimalBatchSize {\n        end := i + OptimalBatchSize\n        if end > len(items) {\n            end = len(items)\n        }\n        \n        batch := items[i:end]\n        root, err := ComputeBatchRoot(batch)\n        if err != nil {\n            return nil, err\n        }\n        \n        roots = append(roots, root)\n    }\n    \n    return roots, nil\n}\n```\n\n### ìºì‹±\n\n```go\ntype TrieCache struct {\n    mu    sync.RWMutex\n    roots map[string]common.Hash // batchID -> rootHash\n}\n\nfunc (c *TrieCache) Get(batchID string) (common.Hash, bool) {\n    c.mu.RLock()\n    defer c.mu.RUnlock()\n    root, ok := c.roots[batchID]\n    return root, ok\n}\n\nfunc (c *TrieCache) Set(batchID string, root common.Hash) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    c.roots[batchID] = root\n}\n```\n\n## ëª¨ë²” ì‚¬ë¡€\n\n1. **ë°°ì¹˜ ì²˜ë¦¬**: ê°œë³„ ë°ì´í„°ë§ˆë‹¤ Trie ìƒì„±í•˜ì§€ ë§ê³  ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬\n2. **Root ì €ì¥**: ê³„ì‚°ëœ Merkle RootëŠ” ì˜êµ¬ ì €ì¥ì†Œì— ì €ì¥\n3. **Proof ìºì‹±**: ìì£¼ ì¡°íšŒë˜ëŠ” ë°ì´í„°ì˜ ProofëŠ” ìºì‹±\n4. **í•´ì‹œ ì¼ê´€ì„±**: ë™ì¼í•œ í•´ì‹œ í•¨ìˆ˜(Keccak256) ì¼ê´€ë˜ê²Œ ì‚¬ìš©\n5. **ë™ì‹œì„± ì œì–´**: Trie ì ‘ê·¼ ì‹œ ë®¤í…ìŠ¤ë¡œ ë³´í˜¸\n\n## ì°¸ê³  ìë£Œ\n\n- [go-ethereum Trie íŒ¨í‚¤ì§€](https://pkg.go.dev/github.com/ethereum/go-ethereum/trie)\n- [Merkle Tree ìœ„í‚¤í”¼ë””ì•„](https://en.wikipedia.org/wiki/Merkle_tree)\n- [Ethereum Yellow Paper](https://ethereum.github.io/yellowpaper/paper.pdf)",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain",
      "Go",
      "MerkleTrie",
      "Transaction"
    ],
    "readingTime": 8,
    "wordCount": 1409,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "functional-options-pattern",
    "slug": "functional-options-pattern",
    "path": "backend/patterns",
    "fullPath": "backend/patterns/functional-options-pattern",
    "title": "Functional Options íŒ¨í„´ì„ í™œìš©í•œ Go ì„¤ì • ê´€ë¦¬",
    "excerpt": "Goì˜ Functional Options íŒ¨í„´ì„ í™œìš©í•˜ì—¬ í•„ìˆ˜ íŒŒë¼ë¯¸í„°ì™€ ì„ íƒì  ì„¤ì •ì„ ëª…í™•íˆ ë¶„ë¦¬í•˜ê³ , í•©ë¦¬ì ì¸ ê¸°ë³¸ê°’ ìœ„ì— ìœ ì—°í•œ ì»¤ìŠ¤í„°ë§ˆì´ì§•ì„ ì œê³µí•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# Functional Options íŒ¨í„´ì„ í™œìš©í•œ Go ì„¤ì • ê´€ë¦¬\n\n## ê°œìš”\n\n**Functional Options íŒ¨í„´**ì€ Goì—ì„œ ê°ì²´ ìƒì„± ì‹œ **í•„ìˆ˜ íŒŒë¼ë¯¸í„°**ì™€ **ì„ íƒì  ì„¤ì •**ì„ ëª…í™•íˆ ë¶„ë¦¬í•˜ëŠ” ê´€ìš©ì ì¸ íŒ¨í„´ì…ë‹ˆë‹¤.\n\ní•µì‹¬ ì›ì¹™:\n\n- **í•„ìˆ˜ê°’**: ìƒì„±ìì˜ ëª…ì‹œì  íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬ (ì—†ìœ¼ë©´ ì»´íŒŒì¼ ì—ëŸ¬)\n- **ì„ íƒê°’**: í•©ë¦¬ì ì¸ ê¸°ë³¸ê°’ì„ ì •ì˜í•˜ê³ , `With...` ì˜µì…˜ í•¨ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ\n\n## íŒ¨í„´ì˜ í•µì‹¬: í•„ìˆ˜ vs ì„ íƒ\n\n### ì„¤ê³„ ì˜ë„\n\n```\nNewWorker(client, handler, opts...)\n         â†‘       â†‘        â†‘\n      í•„ìˆ˜ê°’   í•„ìˆ˜ê°’   ì„ íƒì  ì˜µì…˜ë“¤\n      \nìƒì„±ì ì‹œê·¸ë‹ˆì²˜ê°€ \"ë¬´ì—‡ì´ í•„ìˆ˜ì¸ì§€\"ë¥¼ ëª…í™•íˆ ì„ ì–¸í•©ë‹ˆë‹¤.\nì˜µì…˜ì„ ì „ë‹¬í•˜ì§€ ì•Šì•„ë„ ê¸°ë³¸ê°’ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.\n```\n\n| êµ¬ë¶„ | ì „ë‹¬ ë°©ì‹ | íŠ¹ì§• |\n|------|----------|------|\n| **í•„ìˆ˜ íŒŒë¼ë¯¸í„°** | ìƒì„±ìì˜ ëª…ì‹œì  ì¸ì | ëˆ„ë½ ì‹œ ì»´íŒŒì¼ ì—ëŸ¬, ê¸°ë³¸ê°’ ì—†ìŒ |\n| **ì„ íƒì  ì„¤ì •** | `...WorkerOption` ê°€ë³€ ì¸ì | ê¸°ë³¸ê°’ ì¡´ì¬, í•„ìš” ì‹œ ì˜¤ë²„ë¼ì´ë“œ |\n\n### ì™œ ì´ êµ¬ë¶„ì´ ì¤‘ìš”í•œê°€?\n\n```go\n// âŒ ë‚˜ìœ ì˜ˆ: ëª¨ë“  ê²ƒì´ ì˜µì…˜\n// clientê°€ nilì´ì–´ë„ ì»´íŒŒì¼ì€ í†µê³¼ â†’ ëŸ°íƒ€ì„ ì—ëŸ¬\nworker := NewWorker(\n    WithClient(client),     // í•„ìˆ˜ì¸ë° ì˜µì…˜ì²˜ëŸ¼ ë³´ì„\n    WithHandler(handler),   // í•„ìˆ˜ì¸ë° ì˜µì…˜ì²˜ëŸ¼ ë³´ì„\n    WithBatchSize(100),\n)\n\n// âœ… ì¢‹ì€ ì˜ˆ: í•„ìˆ˜ê°’ì€ ëª…ì‹œì  íŒŒë¼ë¯¸í„°\n// clientë‚˜ handler ëˆ„ë½ ì‹œ ì»´íŒŒì¼ ì—ëŸ¬\nworker := NewWorker(\n    client,                 // í•„ìˆ˜: ì²« ë²ˆì§¸ ì¸ì\n    handler,                // í•„ìˆ˜: ë‘ ë²ˆì§¸ ì¸ì  \n    WithBatchSize(100),     // ì„ íƒ: ê¸°ë³¸ê°’ 10ì„ 100ìœ¼ë¡œ ì˜¤ë²„ë¼ì´ë“œ\n)\n```\n\n## ê¸°ë³¸ êµ¬í˜„\n\n### 1. ë‚´ë¶€ ì„¤ì • êµ¬ì¡°ì²´ (ê¸°ë³¸ê°’ ì •ì˜)\n\n```go\npackage worker\n\nimport \"time\"\n\n// workerConfigëŠ” Workerì˜ \"ì„ íƒì \" ì„¤ì •ì„ ë‹´ìŠµë‹ˆë‹¤.\n// í•„ìˆ˜ê°’(client, handler)ì€ ì—¬ê¸°ì— í¬í•¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\ntype workerConfig struct {\n    batchSize        int\n    pollInterval     time.Duration\n    maxRetries       int\n    shutdownTimeout  time.Duration\n    deadLetterStream string\n}\n\n// defaultConfigëŠ” í•©ë¦¬ì ì¸ ê¸°ë³¸ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n// ì˜µì…˜ì„ ì „í˜€ ì „ë‹¬í•˜ì§€ ì•Šì•„ë„ ì´ ê°’ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.\nfunc defaultConfig() workerConfig {\n    return workerConfig{\n        batchSize:        10,                    // ê¸°ë³¸: 10ê°œì”© ì²˜ë¦¬\n        pollInterval:     100 * time.Millisecond, // ê¸°ë³¸: 100ms í´ë§\n        maxRetries:       3,                     // ê¸°ë³¸: 3íšŒ ì¬ì‹œë„\n        shutdownTimeout:  30 * time.Second,      // ê¸°ë³¸: 30ì´ˆ ì¢…ë£Œ ëŒ€ê¸°\n        deadLetterStream: \"dead-letter-stream\",  // ê¸°ë³¸ DLQ ì´ë¦„\n    }\n}\n```\n\n### 2. ì˜µì…˜ íƒ€ì…ê³¼ With í•¨ìˆ˜ë“¤ (ê¸°ë³¸ê°’ ì˜¤ë²„ë¼ì´ë“œ)\n\n```go\n// WorkerOptionì€ ê¸°ë³¸ ì„¤ì •ì„ ë³€ê²½í•˜ëŠ” í•¨ìˆ˜ íƒ€ì…ì…ë‹ˆë‹¤.\ntype WorkerOption func(*workerConfig)\n\n// WithBatchSizeëŠ” ê¸°ë³¸ê°’(10)ì„ ì˜¤ë²„ë¼ì´ë“œí•©ë‹ˆë‹¤.\nfunc WithBatchSize(size int) WorkerOption {\n    return func(c *workerConfig) {\n        if size > 0 {\n            c.batchSize = size\n        }\n    }\n}\n\n// WithPollIntervalì€ ê¸°ë³¸ê°’(100ms)ì„ ì˜¤ë²„ë¼ì´ë“œí•©ë‹ˆë‹¤.\nfunc WithPollInterval(interval time.Duration) WorkerOption {\n    return func(c *workerConfig) {\n        if interval > 0 {\n            c.pollInterval = interval\n        }\n    }\n}\n\n// WithMaxRetriesëŠ” ê¸°ë³¸ê°’(3)ì„ ì˜¤ë²„ë¼ì´ë“œí•©ë‹ˆë‹¤.\nfunc WithMaxRetries(retries int) WorkerOption {\n    return func(c *workerConfig) {\n        c.maxRetries = retries\n    }\n}\n\n// WithShutdownTimeoutì€ ê¸°ë³¸ê°’(30s)ì„ ì˜¤ë²„ë¼ì´ë“œí•©ë‹ˆë‹¤.\nfunc WithShutdownTimeout(timeout time.Duration) WorkerOption {\n    return func(c *workerConfig) {\n        c.shutdownTimeout = timeout\n    }\n}\n\n// WithDeadLetterStreamì€ ê¸°ë³¸ê°’ì„ ì˜¤ë²„ë¼ì´ë“œí•©ë‹ˆë‹¤.\nfunc WithDeadLetterStream(stream string) WorkerOption {\n    return func(c *workerConfig) {\n        c.deadLetterStream = stream\n    }\n}\n```\n\n### 3. ìƒì„±ì (í•„ìˆ˜ íŒŒë¼ë¯¸í„° + ì„ íƒì  ì˜µì…˜)\n\n```go\n// StreamWorkerëŠ” ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¼ì„ ì²˜ë¦¬í•˜ëŠ” ì›Œì»¤ì…ë‹ˆë‹¤.\ntype StreamWorker struct {\n    // í•„ìˆ˜ ì˜ì¡´ì„± (ìƒì„±ì íŒŒë¼ë¯¸í„°ë¡œ ì£¼ì…)\n    client  redis.UniversalClient\n    handler MessageHandler\n    \n    // ì„ íƒì  ì„¤ì • (ê¸°ë³¸ê°’ + ì˜µì…˜ìœ¼ë¡œ êµ¬ì„±)\n    config  workerConfig\n    \n    stopCh  chan struct{}\n}\n\n// NewStreamWorkerëŠ” ìƒˆ ì›Œì»¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n//\n// íŒŒë¼ë¯¸í„°:\n//   - client: Redis í´ë¼ì´ì–¸íŠ¸ (í•„ìˆ˜)\n//   - handler: ë©”ì‹œì§€ í•¸ë“¤ëŸ¬ (í•„ìˆ˜)\n//   - opts: ì„ íƒì  ì„¤ì • (ê¸°ë³¸ê°’ ì¡´ì¬, í•„ìš” ì‹œ ì˜¤ë²„ë¼ì´ë“œ)\nfunc NewStreamWorker(\n    client redis.UniversalClient,   // í•„ìˆ˜: ì—†ìœ¼ë©´ ì»´íŒŒì¼ ì—ëŸ¬\n    handler MessageHandler,          // í•„ìˆ˜: ì—†ìœ¼ë©´ ì»´íŒŒì¼ ì—ëŸ¬\n    opts ...WorkerOption,            // ì„ íƒ: ì—†ì–´ë„ ê¸°ë³¸ê°’ìœ¼ë¡œ ë™ì‘\n) *StreamWorker {\n    // 1. ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹œì‘\n    cfg := defaultConfig()\n    \n    // 2. ì „ë‹¬ëœ ì˜µì…˜ë“¤ë¡œ ê¸°ë³¸ê°’ ì˜¤ë²„ë¼ì´ë“œ\n    for _, opt := range opts {\n        opt(&cfg)\n    }\n    \n    return &StreamWorker{\n        client:  client,\n        handler: handler,\n        config:  cfg,\n        stopCh:  make(chan struct{}),\n    }\n}\n```\n\n### 4. ì‚¬ìš© ì˜ˆì‹œ\n\n```go\nfunc main() {\n    // í•„ìˆ˜ ì˜ì¡´ì„± ì¤€ë¹„\n    client := redis.NewClusterClient(&redis.ClusterOptions{\n        Addrs: []string{\"localhost:7001\"},\n    })\n    handler := &AuditHandler{}\n    \n    // Case 1: ê¸°ë³¸ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì˜µì…˜ ì—†ìŒ)\n    // batchSize=10, pollInterval=100ms, maxRetries=3 ...\n    worker1 := NewStreamWorker(client, handler)\n    \n    // Case 2: ì¼ë¶€ ì„¤ì •ë§Œ ì˜¤ë²„ë¼ì´ë“œ\n    // batchSize=100 (ì˜¤ë²„ë¼ì´ë“œ), ë‚˜ë¨¸ì§€ëŠ” ê¸°ë³¸ê°’\n    worker2 := NewStreamWorker(client, handler,\n        WithBatchSize(100),\n    )\n    \n    // Case 3: ì—¬ëŸ¬ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ\n    worker3 := NewStreamWorker(client, handler,\n        WithBatchSize(100),              // 10 â†’ 100\n        WithPollInterval(50*time.Millisecond), // 100ms â†’ 50ms\n        WithMaxRetries(5),               // 3 â†’ 5\n    )\n    \n    worker3.Start(context.Background())\n}\n```\n\n## ì‹¤ì œ í™œìš© ì˜ˆì‹œ\n\n### ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸\n\n```go\ntype dbConfig struct {\n    // ì„ íƒì  ì„¤ì • (ê¸°ë³¸ê°’ ì¡´ì¬)\n    host       string\n    port       int\n    poolSize   int\n    timeout    time.Duration\n}\n\nfunc defaultDBConfig() dbConfig {\n    return dbConfig{\n        host:     \"localhost\",\n        port:     5432,\n        poolSize: 10,\n        timeout:  5 * time.Second,\n    }\n}\n\ntype DBOption func(*dbConfig)\n\nfunc WithHost(host string) DBOption {\n    return func(c *dbConfig) { c.host = host }\n}\n\nfunc WithPort(port int) DBOption {\n    return func(c *dbConfig) { c.port = port }\n}\n\nfunc WithPoolSize(size int) DBOption {\n    return func(c *dbConfig) { c.poolSize = size }\n}\n\n// NewDBClient: databaseëŠ” í•„ìˆ˜, ë‚˜ë¨¸ì§€ëŠ” ì„ íƒ\nfunc NewDBClient(database string, opts ...DBOption) (*DBClient, error) {\n    // database: í•„ìˆ˜ íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’ ì—†ìŒ)\n    if database == \"\" {\n        return nil, errors.New(\"database name is required\")\n    }\n    \n    // ì„ íƒì  ì„¤ì •: ê¸°ë³¸ê°’ + ì˜µì…˜ ì˜¤ë²„ë¼ì´ë“œ\n    cfg := defaultDBConfig()\n    for _, opt := range opts {\n        opt(&cfg)\n    }\n    \n    return &DBClient{\n        database: database,  // í•„ìˆ˜\n        config:   cfg,       // ì„ íƒ (ê¸°ë³¸ê°’ + ì˜¤ë²„ë¼ì´ë“œ)\n    }, nil\n}\n\n// ì‚¬ìš©\ndb, _ := NewDBClient(\"mydb\")                           // ê¸°ë³¸ê°’ ì‚¬ìš©\ndb, _ := NewDBClient(\"mydb\", WithHost(\"db.prod.com\"))  // hostë§Œ ì˜¤ë²„ë¼ì´ë“œ\ndb, _ := NewDBClient(\"mydb\", WithHost(\"db.prod.com\"), WithPoolSize(50))\n```\n\n## ì‘ìš© íŒ¨í„´\n\n### ì˜µì…˜ í”„ë¦¬ì…‹ (ìì£¼ ì“°ëŠ” ì„¤ì • ì¡°í•©)\n\n```go\n// ProductionOptionsëŠ” í”„ë¡œë•ì…˜ í™˜ê²½ìš© ì„¤ì • í”„ë¦¬ì…‹ì…ë‹ˆë‹¤.\nfunc ProductionOptions() []WorkerOption {\n    return []WorkerOption{\n        WithBatchSize(100),\n        WithMaxRetries(5),\n        WithShutdownTimeout(60 * time.Second),\n    }\n}\n\n// í”„ë¦¬ì…‹ ì‚¬ìš©\nworker := NewStreamWorker(client, handler, ProductionOptions()...)\n\n// í”„ë¦¬ì…‹ + ì¶”ê°€ ì˜¤ë²„ë¼ì´ë“œ\nopts := append(ProductionOptions(), WithBatchSize(200))\nworker := NewStreamWorker(client, handler, opts...)\n```\n\n### ê²€ì¦ í¬í•¨ ì˜µì…˜\n\n```go\nfunc WithBatchSize(size int) WorkerOption {\n    return func(c *workerConfig) {\n        // ê²€ì¦: ë²”ìœ„ ì œí•œ\n        if size < 1 {\n            size = 1\n        }\n        if size > 1000 {\n            size = 1000\n        }\n        c.batchSize = size\n    }\n}\n```\n\n### ì—ëŸ¬ ë°˜í™˜ ì˜µì…˜ (ê³ ê¸‰)\n\n```go\ntype WorkerOptionWithError func(*workerConfig) error\n\nfunc NewStreamWorkerSafe(\n    client redis.UniversalClient,\n    handler MessageHandler,\n    opts ...WorkerOptionWithError,\n) (*StreamWorker, error) {\n    cfg := defaultConfig()\n    \n    for _, opt := range opts {\n        if err := opt(&cfg); err != nil {\n            return nil, err\n        }\n    }\n    \n    return &StreamWorker{config: cfg}, nil\n}\n\nfunc WithStreamFromEnv(key string) WorkerOptionWithError {\n    return func(c *workerConfig) error {\n        value := os.Getenv(key)\n        if value == \"\" {\n            return fmt.Errorf(\"env %s is not set\", key)\n        }\n        // ì„¤ì • ì ìš©\n        return nil\n    }\n}\n```\n\n## í…ŒìŠ¤íŠ¸ ìš©ì´ì„±\n\nì˜µì…˜ íŒ¨í„´ì€ í…ŒìŠ¤íŠ¸ì—ì„œ íŠ¹íˆ ìœ ìš©í•©ë‹ˆë‹¤:\n\n```go\nfunc TestWorker_ProcessMessages(t *testing.T) {\n    // í…ŒìŠ¤íŠ¸ìš© ì§§ì€ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì˜¤ë²„ë¼ì´ë“œ\n    worker := NewStreamWorker(\n        mockClient,\n        mockHandler,\n        WithBatchSize(1),                        // ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 1ê°œì”©\n        WithPollInterval(10*time.Millisecond),   // ë¹ ë¥¸ í´ë§\n        WithMaxRetries(1),                       // ì¬ì‹œë„ ìµœì†Œí™”\n        WithShutdownTimeout(100*time.Millisecond),\n    )\n    \n    // í…ŒìŠ¤íŠ¸ ë¡œì§...\n}\n```\n\n## í•µì‹¬ ì •ë¦¬\n\n```mermaid\nflowchart TD\n    subgraph Constructor [\"NewStreamWorker(client, handler, opts...)\"]\n        direction LR\n        P1[\"client\"] --> |\"í•„ìˆ˜\"| REQ[\"ì—†ìœ¼ë©´<br/>ì»´íŒŒì¼ ì—ëŸ¬\"]\n        P2[\"handler\"] --> |\"í•„ìˆ˜\"| REQ\n        P3[\"opts...\"] --> |\"ì„ íƒ\"| OPT[\"ì—†ìœ¼ë©´<br/>ê¸°ë³¸ê°’ ì‚¬ìš©\"]\n    end\n\n    subgraph Internal [\"ë‚´ë¶€ ë™ì‘\"]\n        direction TB\n        S1[\"1. cfg := defaultConfig()\"] --> |\"ê¸°ë³¸ê°’ ì´ˆê¸°í™”\"| S2\n        S2[\"2. for opt := range opts\"] --> |\"ì˜¤ë²„ë¼ì´ë“œ\"| S3\n        S3[\"opt(&cfg)\"]\n    end\n\n    Constructor --> Internal\n\n    style REQ fill:#ffebee,stroke:#c62828\n    style OPT fill:#e8f5e9,stroke:#2e7d32\n```\n\n## ëª¨ë²” ì‚¬ë¡€\n\n1. **í•„ìˆ˜/ì„ íƒ ë¶„ë¦¬**: ì—†ìœ¼ë©´ ì•ˆ ë˜ëŠ” ê²ƒì€ íŒŒë¼ë¯¸í„°, ê¸°ë³¸ê°’ì´ ìˆëŠ” ê²ƒì€ ì˜µì…˜\n2. **ê¸°ë³¸ê°’ ëª…ì‹œ**: `defaultConfig()` í•¨ìˆ˜ë¡œ í•©ë¦¬ì ì¸ ê¸°ë³¸ê°’ ì •ì˜\n3. **With ì ‘ë‘ì‚¬**: ì˜µì…˜ í•¨ìˆ˜ëŠ” `With...` ë„¤ì´ë° ì»¨ë²¤ì…˜ ì¤€ìˆ˜\n4. **ê²€ì¦ í¬í•¨**: ì˜µì…˜ í•¨ìˆ˜ ë‚´ì—ì„œ ê°’ ê²€ì¦\n5. **ë¶ˆë³€ì„±**: ë‚´ë¶€ config êµ¬ì¡°ì²´ëŠ” ì™¸ë¶€ì— ë…¸ì¶œí•˜ì§€ ì•ŠìŒ\n\n## ì°¸ê³  ìë£Œ\n\n- [Dave Cheney - Functional Options](https://dave.cheney.net/2014/10/17/functional-options-for-friendly-apis)\n- [Rob Pike - Self-referential functions](https://commandcenter.blogspot.com/2014/01/self-referential-functions-and-design.html)\n- [Uber Go Style Guide](https://github.com/uber-go/guide/blob/master/style.md#functional-options)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Architecture",
      "Design Patterns",
      "Go"
    ],
    "readingTime": 6,
    "wordCount": 1163,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "append-only-versioning",
    "slug": "append-only-versioning",
    "path": "backend/patterns",
    "fullPath": "backend/patterns/append-only-versioning",
    "title": "Append-Only ë¬¸ì„œ ë²„ì €ë‹ ì„¤ê³„ ë° êµ¬í˜„",
    "excerpt": "ë°ì´í„° ë¶ˆë³€ì„±ê³¼ ì™„ì „í•œ ê°ì‚¬ ì¶”ì ì„ ë³´ì¥í•˜ëŠ” Append-Only ë¬¸ì„œ ë²„ì €ë‹ ì•„í‚¤í…ì²˜ì˜ ì„¤ê³„ ì›ì¹™ê³¼ Go êµ¬í˜„ ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# Append-Only ë¬¸ì„œ ë²„ì €ë‹ ì„¤ê³„ ë° êµ¬í˜„\n\n## ê°œìš”\n\në°ì´í„°ì˜ ë¶ˆë³€ì„±(Immutability)ê³¼ ì™„ì „í•œ ê°ì‚¬ ì¶”ì (Audit Trail)ì´ ìš”êµ¬ë˜ëŠ” ì‹œìŠ¤í…œì—ì„œëŠ” Append-Only ì•„í‚¤í…ì²˜ê°€ íš¨ê³¼ì ì…ë‹ˆë‹¤. ì´ ê¸€ì—ì„œëŠ” ë¬¸ì„œ ë²„ì €ë‹ ì‹œìŠ¤í…œì„ ì„¤ê³„í•˜ê³  êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n\n## ì™œ Append-Onlyì¸ê°€?\n\n### ì¥ì \n\n| íŠ¹ì„± | ì„¤ëª… |\n|------|------|\n| **ë¶ˆë³€ì„± ë³´ì¥** | ê¸°ì¡´ ë°ì´í„°ë¥¼ ìˆ˜ì •í•˜ì§€ ì•Šì•„ ë°ì´í„° ë¬´ê²°ì„± ìœ ì§€ |\n| **ê°ì‚¬ ì¶”ì ** | ëª¨ë“  ë³€ê²½ ì´ë ¥ì´ ìë™ìœ¼ë¡œ ë³´ì¡´ |\n| **ì¶©ëŒ ë°©ì§€** | ë™ì‹œ ìˆ˜ì •ìœ¼ë¡œ ì¸í•œ ë°ì´í„° ì†ì‹¤ ìœ„í—˜ ê°ì†Œ |\n| **ë³µêµ¬ ìš©ì´** | íŠ¹ì • ì‹œì ìœ¼ë¡œ ë¡¤ë°± ê°€ëŠ¥ |\n| **ê·œì • ì¤€ìˆ˜** | ê¸ˆìœµ/ì˜ë£Œ ë“± ê·œì œ ì‚°ì—…ì˜ ë°ì´í„° ë³´ì¡´ ìš”ê±´ ì¶©ì¡± |\n\n### ë‹¨ì \n\n| íŠ¹ì„± | ì„¤ëª… |\n|------|------|\n| **ì €ì¥ ê³µê°„** | ë²„ì „ë§ˆë‹¤ ì „ì²´ ìŠ¤ëƒ…ìƒ· ì €ì¥ìœ¼ë¡œ ìš©ëŸ‰ ì¦ê°€ |\n| **ì¿¼ë¦¬ ë³µì¡ë„** | ìµœì‹  ë²„ì „ ì¡°íšŒ ì‹œ ì¶”ê°€ ë¡œì§ í•„ìš” |\n| **ì‚­ì œ ì²˜ë¦¬** | ë¬¼ë¦¬ì  ì‚­ì œ ëŒ€ì‹  ì†Œí”„íŠ¸ ì‚­ì œ í•„ìš” |\n\n## ì•„í‚¤í…ì²˜ ì„¤ê³„\n\n### ë“€ì–¼ ì»¬ë ‰ì…˜ êµ¬ì¡°\n\n```mermaid\ngraph LR\n    subgraph StorageLayer [Storage Layer]\n        direction TB\n        subgraph AllVersions [\"&lt;collection&gt; (All Version Snapshots)\"]\n            direction TB\n            v1[\"doc_uri: 'user-001'<br/>version: 1<br/>created_at: ...\"]\n            v2[\"doc_uri: 'user-001'<br/>version: 2\"]\n            v3[\"doc_uri: 'user-001'<br/>version: 3\"]\n        end\n\n        subgraph Current [\"current_&lt;collection&gt; (Latest Only)\"]\n            direction TB\n            latest[\"doc_uri: 'user-001'<br/>version: 3 (latest)<br/>updated_at: ...\"]\n        end\n    end\n\n    %% ìŠ¤íƒ€ì¼ë§\n    style StorageLayer fill:#f9f9f9,stroke:#333,stroke-dasharray: 5 5\n    style AllVersions fill:#fff,stroke:#007bff\n    style Current fill:#fff,stroke:#28a745\n```\n\n**ë‘ ì»¬ë ‰ì…˜ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ :**\n\n- `<collection>`: íˆìŠ¤í† ë¦¬ ë³´ì¡´, ê°ì‚¬ ì¶”ì , ê·œì • ì¤€ìˆ˜\n- `current_<collection>`: ìµœì‹  ë°ì´í„° ë¹ ë¥¸ ì¡°íšŒ, ì¸ë±ìŠ¤ ìµœì í™”\n\n### ë¬¸ì„œ ìƒíƒœ ê´€ë¦¬\n\n```go\ntype DocumentState string\n\nconst (\n    DocStateActive  DocumentState = \"ACTIVE\"\n    DocStateDeleted DocumentState = \"DELETED\"\n    DocStatePending DocumentState = \"PENDING\"\n)\n```\n\n## í•µì‹¬ êµ¬í˜„\n\n### ë¬¸ì„œ ëª¨ë¸\n\n```go\npackage model\n\nimport (\n    \"time\"\n)\n\n// VersionedDocumentëŠ” ë²„ì „ ê´€ë¦¬ë˜ëŠ” ë¬¸ì„œì˜ ê¸°ë³¸ êµ¬ì¡°ì…ë‹ˆë‹¤.\ntype VersionedDocument struct {\n    ID        string                 `bson:\"_id,omitempty\"`\n    URI       string                 `bson:\"uri\"`\n    Version   int32                  `bson:\"version\"`\n    DocStatus DocumentState          `bson:\"doc_status\"`\n    Fields    map[string]interface{} `bson:\"fields\"`\n    CreatedAt time.Time              `bson:\"created_at\"`\n    UpdatedAt time.Time              `bson:\"updated_at\"`\n}\n\n// AuditEntryëŠ” ë³€ê²½ ê°ì‚¬ ë¡œê·¸ í•­ëª©ì…ë‹ˆë‹¤.\ntype AuditEntry struct {\n    ID            string                 `bson:\"_id,omitempty\"`\n    Collection    string                 `bson:\"collection\"`\n    DocumentURI   string                 `bson:\"document_uri\"`\n    Action        string                 `bson:\"action\"` // CREATE, UPDATE, DELETE\n    ChangedBy     string                 `bson:\"changed_by\"`\n    ChangedAt     time.Time              `bson:\"changed_at\"`\n    PreviousData  map[string]interface{} `bson:\"previous_data,omitempty\"`\n    NewData       map[string]interface{} `bson:\"new_data,omitempty\"`\n    ChangedFields []string               `bson:\"changed_fields,omitempty\"`\n}\n```\n\n### ì„œë¹„ìŠ¤ ì¸í„°í˜ì´ìŠ¤\n\n```go\npackage document\n\nimport (\n    \"context\"\n)\n\n// VersionedServiceëŠ” ë²„ì „ ê´€ë¦¬ ë¬¸ì„œ ì„œë¹„ìŠ¤ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.\ntype VersionedService interface {\n    // CreateëŠ” ë²„ì „ 1ì˜ ìƒˆ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n    Create(ctx context.Context, collection string, data BsonDocument) (*BsonDocument, error)\n    \n    // FindLatestëŠ” URIì˜ ìµœì‹  ë²„ì „ ë¬¸ì„œë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\n    FindLatest(ctx context.Context, collection, uri string) (*BsonDocument, error)\n    \n    // FindByVersionëŠ” íŠ¹ì • ë²„ì „ì˜ ë¬¸ì„œë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\n    FindByVersion(ctx context.Context, collection, uri string, version int32) (*BsonDocument, error)\n    \n    // SoftUpdateëŠ” ìƒˆ ë²„ì „ì„ ìƒì„±í•˜ì—¬ ë¬¸ì„œë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n    SoftUpdate(ctx context.Context, collection, uri string, updates BsonDocument) (*BsonDocument, error)\n    \n    // SoftDeleteëŠ” ì‚­ì œ ìƒíƒœì˜ ìƒˆ ë²„ì „ì„ ìƒì„±í•©ë‹ˆë‹¤.\n    SoftDelete(ctx context.Context, collection, uri string) (*BsonDocument, error)\n    \n    // GetHistoryëŠ” ë¬¸ì„œì˜ ì „ì²´ ë²„ì „ íˆìŠ¤í† ë¦¬ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\n    GetHistory(ctx context.Context, collection, uri string) ([]BsonDocument, error)\n}\n```\n\n### ë¬¸ì„œ ìƒì„±\n\n```go\nfunc (s *versionService) Create(ctx context.Context, collection string, data BsonDocument) (*BsonDocument, error) {\n    now := time.Now()\n    \n    doc := &VersionedDocument{\n        URI:       data.URI,\n        Version:   1, // ì²« ë²„ì „\n        DocStatus: DocStateActive,\n        Fields:    data.Fields,\n        CreatedAt: now,\n        UpdatedAt: now,\n    }\n    \n    // íŠ¸ëœì­ì…˜ìœ¼ë¡œ ë‘ ì»¬ë ‰ì…˜ì— ë™ì‹œ ì €ì¥\n    _, err := s.WithTransaction(ctx, func(sessCtx context.Context) (interface{}, error) {\n        // 1. ë²„ì „ íˆìŠ¤í† ë¦¬ ì»¬ë ‰ì…˜ì— ì €ì¥\n        if _, err := s.collection(collection).InsertOne(sessCtx, doc); err != nil {\n            return nil, err\n        }\n        \n        // 2. ìµœì‹  ë²„ì „ ì»¬ë ‰ì…˜ì—ë„ ì €ì¥\n        currentColl := s.currentCollection(collection)\n        if _, err := currentColl.InsertOne(sessCtx, doc); err != nil {\n            return nil, err\n        }\n        \n        // 3. ê°ì‚¬ ë¡œê·¸ ìƒì„±\n        if err := s.createAuditLog(sessCtx, collection, doc, \"CREATE\", nil); err != nil {\n            return nil, err\n        }\n        \n        return doc, nil\n    })\n    \n    if err != nil {\n        return nil, err\n    }\n    \n    return doc, nil\n}\n```\n\n### ì†Œí”„íŠ¸ ì—…ë°ì´íŠ¸\n\nê¸°ì¡´ ë²„ì „ì€ ê·¸ëŒ€ë¡œ ë‘ê³  ìƒˆ ë²„ì „ì„ ì¶”ê°€í•©ë‹ˆë‹¤:\n\n```go\nfunc (s *versionService) SoftUpdate(ctx context.Context, collection, uri string, updates BsonDocument) (*BsonDocument, error) {\n    // í˜„ì¬ ìµœì‹  ë²„ì „ ì¡°íšŒ\n    current, err := s.FindLatest(ctx, collection, uri)\n    if err != nil {\n        return nil, err\n    }\n    \n    if current.DocStatus == DocStateDeleted {\n        return nil, ErrDocumentDeleted\n    }\n    \n    now := time.Now()\n    \n    // ìƒˆ ë²„ì „ ë¬¸ì„œ ìƒì„± (ê¸°ì¡´ í•„ë“œ + ì—…ë°ì´íŠ¸ í•„ë“œ)\n    newFields := mergeFields(current.Fields, updates.Fields)\n    changedFields := detectChangedFields(current.Fields, newFields)\n    \n    newDoc := &VersionedDocument{\n        URI:       uri,\n        Version:   current.Version + 1, // ë²„ì „ ì¦ê°€\n        DocStatus: DocStateActive,\n        Fields:    newFields,\n        CreatedAt: current.CreatedAt, // ì›ë³¸ ìƒì„± ì‹œê°„ ìœ ì§€\n        UpdatedAt: now,\n    }\n    \n    _, err = s.WithTransaction(ctx, func(sessCtx context.Context) (interface{}, error) {\n        // 1. ìƒˆ ë²„ì „ì„ íˆìŠ¤í† ë¦¬ ì»¬ë ‰ì…˜ì— ì¶”ê°€ (Append)\n        if _, err := s.collection(collection).InsertOne(sessCtx, newDoc); err != nil {\n            return nil, err\n        }\n        \n        // 2. ìµœì‹  ë²„ì „ ì»¬ë ‰ì…˜ ì—…ë°ì´íŠ¸ (Replace)\n        filter := bson.M{\"uri\": uri}\n        if _, err := s.currentCollection(collection).ReplaceOne(sessCtx, filter, newDoc); err != nil {\n            return nil, err\n        }\n        \n        // 3. ê°ì‚¬ ë¡œê·¸\n        auditLog := &AuditEntry{\n            Collection:    collection,\n            DocumentURI:   uri,\n            Action:        \"UPDATE\",\n            ChangedAt:     now,\n            PreviousData:  current.Fields,\n            NewData:       newFields,\n            ChangedFields: changedFields,\n        }\n        if err := s.insertAuditLog(sessCtx, auditLog); err != nil {\n            return nil, err\n        }\n        \n        return newDoc, nil\n    })\n    \n    if err != nil {\n        return nil, err\n    }\n    \n    return newDoc, nil\n}\n```\n\n### ì†Œí”„íŠ¸ ì‚­ì œ\n\në¬¼ë¦¬ì  ì‚­ì œ ëŒ€ì‹  ì‚­ì œ ìƒíƒœì˜ ìƒˆ ë²„ì „ì„ ìƒì„±í•©ë‹ˆë‹¤:\n\n```go\nfunc (s *versionService) SoftDelete(ctx context.Context, collection, uri string) (*BsonDocument, error) {\n    current, err := s.FindLatest(ctx, collection, uri)\n    if err != nil {\n        return nil, err\n    }\n    \n    now := time.Now()\n    \n    // ì‚­ì œ ìƒíƒœì˜ ìƒˆ ë²„ì „ ìƒì„±\n    deletedDoc := &VersionedDocument{\n        URI:       uri,\n        Version:   current.Version + 1,\n        DocStatus: DocStateDeleted, // DELETED ìƒíƒœ\n        Fields:    current.Fields,  // ë§ˆì§€ë§‰ ë°ì´í„° ë³´ì¡´\n        CreatedAt: current.CreatedAt,\n        UpdatedAt: now,\n    }\n    \n    _, err = s.WithTransaction(ctx, func(sessCtx context.Context) (interface{}, error) {\n        // 1. ì‚­ì œ ë²„ì „ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€\n        if _, err := s.collection(collection).InsertOne(sessCtx, deletedDoc); err != nil {\n            return nil, err\n        }\n        \n        // 2. ìµœì‹  ë²„ì „ ì»¬ë ‰ì…˜ë„ ì‚­ì œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸\n        filter := bson.M{\"uri\": uri}\n        if _, err := s.currentCollection(collection).ReplaceOne(sessCtx, filter, deletedDoc); err != nil {\n            return nil, err\n        }\n        \n        // 3. ê°ì‚¬ ë¡œê·¸\n        if err := s.createAuditLog(sessCtx, collection, deletedDoc, \"DELETE\", current.Fields); err != nil {\n            return nil, err\n        }\n        \n        return deletedDoc, nil\n    })\n    \n    return deletedDoc, err\n}\n```\n\n### ë³€ê²½ í•„ë“œ ê°ì§€\n\n```go\nfunc detectChangedFields(previous, current map[string]interface{}) []string {\n    var changed []string\n    \n    for key, newVal := range current {\n        oldVal, exists := previous[key]\n        if !exists || !reflect.DeepEqual(oldVal, newVal) {\n            changed = append(changed, key)\n        }\n    }\n    \n    // ì‚­ì œëœ í•„ë“œ ê°ì§€\n    for key := range previous {\n        if _, exists := current[key]; !exists {\n            changed = append(changed, key)\n        }\n    }\n    \n    return changed\n}\n```\n\n## Optimistic Locking\n\në™ì‹œ ì—…ë°ì´íŠ¸ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•œ ë‚™ê´€ì  ì ê¸ˆ:\n\n```go\nfunc (s *versionService) SoftUpdateWithLock(\n    ctx context.Context, \n    collection, uri string, \n    expectedVersion int32,\n    updates BsonDocument,\n) (*BsonDocument, error) {\n    current, err := s.FindLatest(ctx, collection, uri)\n    if err != nil {\n        return nil, err\n    }\n    \n    // ë²„ì „ ë¶ˆì¼ì¹˜ ì‹œ ì¶©ëŒ ì—ëŸ¬\n    if current.Version != expectedVersion {\n        return nil, fmt.Errorf(\n            \"version conflict: expected %d, actual %d\",\n            expectedVersion, current.Version,\n        )\n    }\n    \n    return s.SoftUpdate(ctx, collection, uri, updates)\n}\n```\n\n## ì¿¼ë¦¬ íŒ¨í„´\n\n### ìµœì‹  ë²„ì „ ì¡°íšŒ (ë¹ ë¦„)\n\n```go\n// current_<collection>ì—ì„œ ì§ì ‘ ì¡°íšŒ\nfunc (s *versionService) FindLatest(ctx context.Context, collection, uri string) (*BsonDocument, error) {\n    filter := bson.M{\n        \"uri\":        uri,\n        \"doc_status\": bson.M{\"$ne\": DocStateDeleted},\n    }\n    \n    var doc VersionedDocument\n    err := s.currentCollection(collection).FindOne(ctx, filter).Decode(&doc)\n    if err != nil {\n        return nil, err\n    }\n    \n    return &doc, nil\n}\n```\n\n### íŠ¹ì • ë²„ì „ ì¡°íšŒ\n\n```go\nfunc (s *versionService) FindByVersion(ctx context.Context, collection, uri string, version int32) (*BsonDocument, error) {\n    filter := bson.M{\n        \"uri\":     uri,\n        \"version\": version,\n    }\n    \n    var doc VersionedDocument\n    err := s.collection(collection).FindOne(ctx, filter).Decode(&doc)\n    if err != nil {\n        return nil, err\n    }\n    \n    return &doc, nil\n}\n```\n\n### íˆìŠ¤í† ë¦¬ ì¡°íšŒ\n\n```go\nfunc (s *versionService) GetHistory(ctx context.Context, collection, uri string) ([]BsonDocument, error) {\n    filter := bson.M{\"uri\": uri}\n    opts := options.Find().SetSort(bson.D{{Key: \"version\", Value: 1}})\n    \n    cursor, err := s.collection(collection).Find(ctx, filter, opts)\n    if err != nil {\n        return nil, err\n    }\n    defer cursor.Close(ctx)\n    \n    var docs []VersionedDocument\n    if err := cursor.All(ctx, &docs); err != nil {\n        return nil, err\n    }\n    \n    return docs, nil\n}\n```\n\n## ì¸ë±ìŠ¤ ì „ëµ\n\n```go\n// íˆìŠ¤í† ë¦¬ ì»¬ë ‰ì…˜ ì¸ë±ìŠ¤\nindexes := []mongo.IndexModel{\n    {\n        Keys: bson.D{{Key: \"uri\", Value: 1}, {Key: \"version\", Value: -1}},\n        Options: options.Index().SetUnique(true),\n    },\n    {\n        Keys: bson.D{{Key: \"created_at\", Value: -1}},\n    },\n}\n\n// ìµœì‹  ë²„ì „ ì»¬ë ‰ì…˜ ì¸ë±ìŠ¤\ncurrentIndexes := []mongo.IndexModel{\n    {\n        Keys: bson.D{{Key: \"uri\", Value: 1}},\n        Options: options.Index().SetUnique(true),\n    },\n    {\n        Keys: bson.D{{Key: \"doc_status\", Value: 1}},\n    },\n}\n```\n\n## ìŠ¤í† ë¦¬ì§€ ìµœì í™”\n\në²„ì „ ëˆ„ì ìœ¼ë¡œ ì¸í•œ ì €ì¥ ê³µê°„ ì¦ê°€ ëŒ€ì‘:\n\n### 1. í•„ë“œ ì••ì¶•\n\n```go\n// ë³€ê²½ëœ í•„ë“œë§Œ ì €ì¥í•˜ëŠ” Delta ë°©ì‹ (ì„ íƒì )\ntype DeltaDocument struct {\n    URI         string                 `bson:\"uri\"`\n    Version     int32                  `bson:\"version\"`\n    BaseVersion int32                  `bson:\"base_version,omitempty\"`\n    Delta       map[string]interface{} `bson:\"delta,omitempty\"`\n    IsSnapshot  bool                   `bson:\"is_snapshot\"`\n}\n```\n\n### 2. TTL ì •ì±…\n\n```go\n// ì˜¤ë˜ëœ ë²„ì „ ìë™ ì •ë¦¬ (ìµœì‹  Nê°œ ìœ ì§€)\nindexes := mongo.IndexModel{\n    Keys:    bson.D{{Key: \"created_at\", Value: 1}},\n    Options: options.Index().SetExpireAfterSeconds(86400 * 365), // 1ë…„\n}\n```\n\n## ëª¨ë²” ì‚¬ë¡€\n\n1. **íŠ¸ëœì­ì…˜ í•„ìˆ˜**: ë‘ ì»¬ë ‰ì…˜ ë™ì‹œ ì—…ë°ì´íŠ¸ ì‹œ ì›ìì„± ë³´ì¥\n2. **URI ë¶ˆë³€ì„±**: ë¬¸ì„œ ì‹ë³„ì(URI)ëŠ” ìƒì„± í›„ ë³€ê²½ ê¸ˆì§€\n3. **ê°ì‚¬ ë¡œê·¸ ë¶„ë¦¬**: ê°ì‚¬ ë¡œê·¸ëŠ” ë³„ë„ ì»¬ë ‰ì…˜ì— ì €ì¥í•˜ì—¬ ë…ë¦½ì  ì ‘ê·¼\n4. **ì¸ë±ìŠ¤ ìµœì í™”**: ìµœì‹  ë²„ì „ ì¡°íšŒ íŒ¨í„´ì— ë§ëŠ” ì¸ë±ìŠ¤ ì„¤ê³„\n5. **ë²„ì „ ìƒí•œ ì„¤ì •**: ë¬´í•œ ë²„ì „ ì¦ê°€ ë°©ì§€ ì •ì±… ê³ ë ¤\n\n## ì°¸ê³  ìë£Œ\n\n- [Event Sourcing Pattern](https://martinfowler.com/eaaDev/EventSourcing.html)\n- [MongoDB Transactions](https://www.mongodb.com/docs/manual/core/transactions/)\n- [Immutable Data Patterns](https://www.cidrdb.org/cidr2015/Papers/CIDR15_Paper16.pdf)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Architecture",
      "Audit",
      "Go",
      "MongoDB"
    ],
    "readingTime": 8,
    "wordCount": 1447,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "grpc-gateway-dual-protocol",
    "slug": "grpc-gateway-dual-protocol",
    "path": "backend/http",
    "fullPath": "backend/http/grpc-gateway-dual-protocol",
    "title": "gRPC-Gatewayë¡œ ë‹¨ì¼ API ë“€ì–¼ í”„ë¡œí† ì½œ ì§€ì›",
    "excerpt": "í•˜ë‚˜ì˜ Proto ì •ì˜ë¡œ gRPCì™€ RESTful HTTP APIë¥¼ ë™ì‹œì— ì œê³µí•˜ëŠ” gRPC-Gateway êµ¬í˜„ ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# gRPC-Gatewayë¡œ ë‹¨ì¼ API ë“€ì–¼ í”„ë¡œí† ì½œ ì§€ì›\n\n## ê°œìš”\n\n**gRPC-Gateway**ëŠ” gRPC ì„œë¹„ìŠ¤ì— RESTful HTTP/JSON ì¸í„°í˜ì´ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ì¶”ê°€í•´ì£¼ëŠ” ë¦¬ë²„ìŠ¤ í”„ë¡ì‹œì…ë‹ˆë‹¤. í•˜ë‚˜ì˜ Proto ì •ì˜ë¡œ ë‘ í”„ë¡œí† ì½œì„ ëª¨ë‘ ì§€ì›í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## ì™œ ë“€ì–¼ í”„ë¡œí† ì½œì¸ê°€?\n\n### ì¥ì \n\n| í”„ë¡œí† ì½œ | ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ |\n|----------|--------------|\n| **gRPC** | ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê°„ í†µì‹ , ê³ ì„±ëŠ¥ í•„ìš”, ìŠ¤íŠ¸ë¦¬ë° |\n| **HTTP/JSON** | ì›¹ ë¸Œë¼ìš°ì €, ì™¸ë¶€ API í´ë¼ì´ì–¸íŠ¸, ë””ë²„ê¹… |\n\në‹¨ì¼ ì½”ë“œë² ì´ìŠ¤ë¡œ ë‘ ë‹ˆì¦ˆë¥¼ ëª¨ë‘ ì¶©ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n### ë‹¨ì \n\n| íŠ¹ì„± | ì„¤ëª… |\n|------|------|\n| ì¶”ê°€ ë ˆì´ì–´ | HTTP â†’ gRPC ë³€í™˜ ì˜¤ë²„í—¤ë“œ |\n| ê¸°ëŠ¥ ì œí•œ | HTTPì—ì„œ gRPC ìŠ¤íŠ¸ë¦¬ë° ì¼ë¶€ ì§€ì› ì œí•œ |\n| ë³µì¡ì„± | ë‘ í”„ë¡œí† ì½œì˜ ì—ëŸ¬ ì²˜ë¦¬ ë§¤í•‘ í•„ìš” |\n\n## ì•„í‚¤í…ì²˜\n\n```mermaid\ngraph LR\n    %% ì™¸ë¶€ í´ë¼ì´ì–¸íŠ¸ ì •ì˜\n    subgraph Clients [\"Clients\"]\n        HTTP_Client[\"HTTP Client<br/>(Browser, etc)\"]\n        gRPC_Client[\"gRPC Client<br/>(Microservice)\"]\n    end\n\n    %% API ì„œë²„ ë‚´ë¶€ êµ¬ì¡° ì •ì˜\n    subgraph APIServer [\"API Server\"]\n        direction TB\n        \n        subgraph Ports [\"Incoming Ports\"]\n            HTTP_Port[\":8080 HTTP\"]\n            gRPC_Port[\":9090 gRPC\"]\n        end\n\n        Mux[\"gRPC-Gateway Mux\"]\n        gRPC_Srv[\"gRPC Server<br/>(Core Logic)\"]\n        \n        subgraph Endpoints [\"Misc Endpoints\"]\n            OpenAPI[\"OpenAPI<br/>/openapi\"]\n            Health[\"Health<br/>/ready\"]\n        end\n\n        %% ë‚´ë¶€ íë¦„\n        HTTP_Port --> Mux\n        Mux --> gRPC_Srv\n        gRPC_Port --> gRPC_Srv\n    end\n\n    %% í´ë¼ì´ì–¸íŠ¸ ì—°ê²°\n    HTTP_Client -- \"JSON/REST\" --> HTTP_Port\n    gRPC_Client -- \"Protobuf\" --> gRPC_Port\n\n    %% ìŠ¤íƒ€ì¼ë§\n    style APIServer fill:#f5f5f5,stroke:#333,stroke-width:2px\n    style Mux fill:#e1f5fe,stroke:#01579b\n    style gRPC_Srv fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px\n    style HTTP_Client fill:#fff,stroke:#333\n    style gRPC_Client fill:#fff,stroke:#333\n```\n\n## Proto ì •ì˜\n\n### HTTP ì–´ë…¸í…Œì´ì…˜ ì¶”ê°€\n\n```protobuf\nsyntax = \"proto3\";\n\npackage v1beta;\n\nimport \"google/api/annotations.proto\";\nimport \"google/protobuf/struct.proto\";\n\nservice DocumentService {\n  // POST /v1beta/collections/{collection}/documents\n  rpc CreateDocument(CreateDocumentRequest) returns (CreateDocumentResponse) {\n    option (google.api.http) = {\n      post: \"/v1beta/collections/{collection}/documents\"\n      body: \"*\"\n    };\n  }\n  \n  // GET /v1beta/collections/{collection}/documents/{uri}\n  rpc GetDocument(GetDocumentRequest) returns (GetDocumentResponse) {\n    option (google.api.http) = {\n      get: \"/v1beta/collections/{collection}/documents/{uri}\"\n    };\n  }\n  \n  // PATCH /v1beta/collections/{collection}/documents/{uri}\n  rpc UpdateDocument(UpdateDocumentRequest) returns (UpdateDocumentResponse) {\n    option (google.api.http) = {\n      patch: \"/v1beta/collections/{collection}/documents/{uri}\"\n      body: \"*\"\n    };\n  }\n  \n  // DELETE /v1beta/collections/{collection}/documents/{uri}\n  rpc DeleteDocument(DeleteDocumentRequest) returns (DeleteDocumentResponse) {\n    option (google.api.http) = {\n      delete: \"/v1beta/collections/{collection}/documents/{uri}\"\n    };\n  }\n  \n  // POSTë¡œ ì¿¼ë¦¬ (URL ê¸¸ì´ ì œí•œ íšŒí”¼)\n  // POST /v1beta/collections/{collection}/documents:query\n  rpc QueryDocuments(QueryDocumentsRequest) returns (QueryDocumentsResponse) {\n    option (google.api.http) = {\n      post: \"/v1beta/collections/{collection}/documents:query\"\n      body: \"*\"\n    };\n  }\n  \n  // ë°°ì¹˜ ì‘ì—…\n  // POST /v1beta/collections/{collection}/documents:batchCreate\n  rpc BatchCreateDocuments(BatchCreateRequest) returns (BatchCreateResponse) {\n    option (google.api.http) = {\n      post: \"/v1beta/collections/{collection}/documents:batchCreate\"\n      body: \"*\"\n    };\n  }\n}\n\nmessage CreateDocumentRequest {\n  // URL ê²½ë¡œ íŒŒë¼ë¯¸í„°ë¡œ ì¶”ì¶œë¨\n  string collection = 1;\n  DocumentInput document = 2;\n}\n\nmessage GetDocumentRequest {\n  string collection = 1;\n  string uri = 2;\n  // ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°: ?version=3\n  optional int32 version = 3;\n  // ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°: ?skip_validation=true\n  optional bool skip_validation = 4;\n}\n\n// ... ë‚˜ë¨¸ì§€ ë©”ì‹œì§€ ì •ì˜\n```\n\n### URL ë§¤í•‘ ê·œì¹™\n\n| gRPC í•„ë“œ ìœ„ì¹˜ | HTTP ìœ„ì¹˜ |\n|---------------|-----------|\n| `{field}` in path | URL ê²½ë¡œ íŒŒë¼ë¯¸í„° |\n| `body: \"*\"` | ë‚˜ë¨¸ì§€ í•„ë“œëŠ” JSON body |\n| `body: \"field\"` | íŠ¹ì • í•„ë“œë§Œ body |\n| ê·¸ ì™¸ í•„ë“œ | ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° |\n\n## ì½”ë“œ ìƒì„± ì„¤ì •\n\n### buf.gen.yaml\n\n```yaml\nversion: v2\nplugins:\n  # gRPC ì„œë²„/í´ë¼ì´ì–¸íŠ¸\n  - remote: buf.build/grpc/go:v1.5.1\n    out: generated/go/proto\n    opt: paths=source_relative\n  \n  # gRPC-Gateway í•¸ë“¤ëŸ¬\n  - remote: buf.build/grpc-ecosystem/gateway:v2.25.1\n    out: generated/go/proto\n    opt:\n      - paths=source_relative\n      - standalone=true       # ë…ë¦½ íŒŒì¼ë¡œ ìƒì„±\n      - generate_unbound_methods=true\n```\n\n### ìƒì„± ì‹¤í–‰\n\n```bash\nbuf generate\n```\n\nìƒì„±ëœ íŒŒì¼:\n\n- `api_grpc.pb.go`: gRPC ì„œë²„/í´ë¼ì´ì–¸íŠ¸\n- `api.pb.gw.go`: HTTP Gateway í•¸ë“¤ëŸ¬\n\n## ì„œë²„ êµ¬í˜„\n\n### í†µí•© ì„œë²„\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"net\"\n    \"net/http\"\n    \"sync\"\n    \n    \"github.com/grpc-ecosystem/grpc-gateway/v2/runtime\"\n    \"google.golang.org/grpc\"\n    \"google.golang.org/grpc/credentials/insecure\"\n    \n    pb \"github.com/myorg/myservice/generated/go/proto/v1beta\"\n)\n\ntype Server struct {\n    grpcServer *grpc.Server\n    httpServer *http.Server\n    \n    grpcPort string\n    httpPort string\n}\n\nfunc NewServer(service DocumentService) *Server {\n    // gRPC ì„œë²„ ì„¤ì •\n    grpcServer := grpc.NewServer(\n        grpc.UnaryInterceptor(loggingInterceptor),\n    )\n    pb.RegisterDocumentServiceServer(grpcServer, &documentHandler{service})\n    \n    return &Server{\n        grpcServer: grpcServer,\n        grpcPort:   \":9090\",\n        httpPort:   \":8080\",\n    }\n}\n\nfunc (s *Server) Start(ctx context.Context) error {\n    var wg sync.WaitGroup\n    errCh := make(chan error, 2)\n    \n    // 1. gRPC ì„œë²„ ì‹œì‘\n    wg.Add(1)\n    go func() {\n        defer wg.Done()\n        lis, err := net.Listen(\"tcp\", s.grpcPort)\n        if err != nil {\n            errCh <- err\n            return\n        }\n        if err := s.grpcServer.Serve(lis); err != nil {\n            errCh <- err\n        }\n    }()\n    \n    // 2. HTTP Gateway ì‹œì‘\n    wg.Add(1)\n    go func() {\n        defer wg.Done()\n        if err := s.startHTTPGateway(ctx); err != nil {\n            errCh <- err\n        }\n    }()\n    \n    select {\n    case err := <-errCh:\n        return err\n    case <-ctx.Done():\n        s.Shutdown()\n        return ctx.Err()\n    }\n}\n\nfunc (s *Server) startHTTPGateway(ctx context.Context) error {\n    mux := runtime.NewServeMux(\n        // JSON í•„ë“œ ì´ë¦„ ì„¤ì •\n        runtime.WithMarshalerOption(runtime.MIMEWildcard, &runtime.JSONPb{\n            MarshalOptions: protojson.MarshalOptions{\n                UseProtoNames:   true,\n                EmitUnpopulated: true,\n            },\n            UnmarshalOptions: protojson.UnmarshalOptions{\n                DiscardUnknown: true,\n            },\n        }),\n        // ì—ëŸ¬ í•¸ë“¤ë§ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆ\n        runtime.WithErrorHandler(customErrorHandler),\n    )\n    \n    opts := []grpc.DialOption{grpc.WithTransportCredentials(insecure.NewCredentials())}\n    \n    // gRPC ì„œë²„ì— ì—°ê²°\n    err := pb.RegisterDocumentServiceHandlerFromEndpoint(ctx, mux, \"localhost\"+s.grpcPort, opts)\n    if err != nil {\n        return err\n    }\n    \n    // ì¶”ê°€ HTTP ì—”ë“œí¬ì¸íŠ¸\n    handler := http.NewServeMux()\n    handler.Handle(\"/\", mux)\n    handler.HandleFunc(\"/openapi.yaml\", serveOpenAPI)\n    handler.HandleFunc(\"/ready\", healthCheck)\n    \n    s.httpServer = &http.Server{\n        Addr:    s.httpPort,\n        Handler: handler,\n    }\n    \n    return s.httpServer.ListenAndServe()\n}\n\nfunc (s *Server) Shutdown() {\n    s.grpcServer.GracefulStop()\n    if s.httpServer != nil {\n        s.httpServer.Shutdown(context.Background())\n    }\n}\n```\n\n### ì—ëŸ¬ í•¸ë“¤ë§\n\ngRPC ìƒíƒœ ì½”ë“œë¥¼ HTTP ìƒíƒœ ì½”ë“œë¡œ ë§¤í•‘:\n\n```go\nfunc customErrorHandler(\n    ctx context.Context,\n    mux *runtime.ServeMux,\n    marshaler runtime.Marshaler,\n    w http.ResponseWriter,\n    r *http.Request,\n    err error,\n) {\n    // gRPC ì—ëŸ¬ì—ì„œ ìƒíƒœ ì¶”ì¶œ\n    st, _ := status.FromError(err)\n    \n    // HTTP ìƒíƒœ ì½”ë“œ ë§¤í•‘\n    httpStatus := runtime.HTTPStatusFromCode(st.Code())\n    \n    // ì»¤ìŠ¤í…€ ì—ëŸ¬ ì‘ë‹µ\n    response := map[string]interface{}{\n        \"code\":    st.Code().String(),\n        \"message\": st.Message(),\n    }\n    \n    if details := st.Details(); len(details) > 0 {\n        response[\"details\"] = details\n    }\n    \n    w.Header().Set(\"Content-Type\", \"application/json\")\n    w.WriteHeader(httpStatus)\n    json.NewEncoder(w).Encode(response)\n}\n```\n\n## ì¸-í”„ë¡œì„¸ìŠ¤ vs ë„¤íŠ¸ì›Œí¬ ì—°ê²°\n\n### ë„¤íŠ¸ì›Œí¬ ì—°ê²° (ê¸°ë³¸)\n\n```go\n// gRPC ì„œë²„ì— ë„¤íŠ¸ì›Œí¬ë¡œ ì—°ê²°\npb.RegisterDocumentServiceHandlerFromEndpoint(ctx, mux, \"localhost:9090\", opts)\n```\n\n### ì¸-í”„ë¡œì„¸ìŠ¤ ì—°ê²° (ê¶Œì¥)\n\në„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ ì—†ì´ ì§ì ‘ ì—°ê²°:\n\n```go\nfunc (s *Server) startHTTPGatewayInProcess(ctx context.Context) error {\n    mux := runtime.NewServeMux()\n    \n    // ì„œë²„ êµ¬í˜„ì²´ë¥¼ ì§ì ‘ ë“±ë¡ (ë„¤íŠ¸ì›Œí¬ ê²½ìœ  ì—†ìŒ)\n    err := pb.RegisterDocumentServiceHandlerServer(ctx, mux, s.documentHandler)\n    if err != nil {\n        return err\n    }\n    \n    s.httpServer = &http.Server{\n        Addr:    s.httpPort,\n        Handler: mux,\n    }\n    \n    return s.httpServer.ListenAndServe()\n}\n```\n\n## ë¯¸ë“¤ì›¨ì–´\n\n### HTTP ë¯¸ë“¤ì›¨ì–´ ì²´ì¸\n\n```go\nfunc (s *Server) buildHTTPHandler(mux *runtime.ServeMux) http.Handler {\n    // ë¯¸ë“¤ì›¨ì–´ ì²´ì¸\n    handler := corsMiddleware(mux)\n    handler = loggingMiddleware(handler)\n    handler = metricsMiddleware(handler)\n    \n    return handler\n}\n\nfunc corsMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        w.Header().Set(\"Access-Control-Allow-Origin\", \"*\")\n        w.Header().Set(\"Access-Control-Allow-Methods\", \"GET, POST, PUT, PATCH, DELETE, OPTIONS\")\n        w.Header().Set(\"Access-Control-Allow-Headers\", \"Content-Type, Authorization\")\n        \n        if r.Method == \"OPTIONS\" {\n            w.WriteHeader(http.StatusNoContent)\n            return\n        }\n        \n        next.ServeHTTP(w, r)\n    })\n}\n\nfunc loggingMiddleware(next http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        start := time.Now()\n        \n        wrapped := &responseWriter{ResponseWriter: w, status: 200}\n        next.ServeHTTP(wrapped, r)\n        \n        log.Printf(\"%s %s %d %v\", r.Method, r.URL.Path, wrapped.status, time.Since(start))\n    })\n}\n```\n\n### gRPC ì¸í„°ì…‰í„°\n\n```go\nfunc loggingInterceptor(\n    ctx context.Context,\n    req interface{},\n    info *grpc.UnaryServerInfo,\n    handler grpc.UnaryHandler,\n) (interface{}, error) {\n    start := time.Now()\n    \n    resp, err := handler(ctx, req)\n    \n    log.Printf(\"gRPC %s %v err=%v\", info.FullMethod, time.Since(start), err)\n    \n    return resp, err\n}\n```\n\n## OpenAPI ìŠ¤í™ ì œê³µ\n\n### ìë™ ìƒì„±ëœ ìŠ¤í™ ì œê³µ\n\n```go\n//go:embed generated/docs/openapi.yaml\nvar openAPISpec []byte\n\nfunc serveOpenAPI(w http.ResponseWriter, r *http.Request) {\n    w.Header().Set(\"Content-Type\", \"application/x-yaml\")\n    w.Write(openAPISpec)\n}\n```\n\n### Stoplight Elements í†µí•©\n\nì •ì  HTMLê³¼ Stoplight Elements ì›¹ ì»´í¬ë„ŒíŠ¸ë¡œ ì¸í„°ë™í‹°ë¸Œ API ë¬¸ì„œë¥¼ ì œê³µí•©ë‹ˆë‹¤:\n\n```html\n<!-- docs/openapi.html -->\n<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"utf-8\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n    <title>API Documentation</title>\n    <script src=\"https://unpkg.com/@stoplight/elements/web-components.min.js\"></script>\n    <link rel=\"stylesheet\" href=\"https://unpkg.com/@stoplight/elements/styles.min.css\" />\n  </head>\n  <body>\n    <elements-api\n      apiDescriptionUrl=\"openapi.swagger.json\"\n      router=\"hash\"\n      layout=\"sidebar\"\n      hideInternal=\"true\"\n    />\n  </body>\n</html>\n```\n\nProto í”„ë¡œì íŠ¸ì—ì„œ embed.FSë¡œ ì •ì  íŒŒì¼ ì œê³µ:\n\n```go\n// generated/go/docs/embed.go\npackage docs\n\nimport \"embed\"\n\n//go:embed openapi.html openapi.swagger.json\nvar StaticFiles embed.FS\n```\n\ngRPC-Gateway Muxì— ì§ì ‘ ê²½ë¡œ ë“±ë¡:\n\n```go\nimport (\n    \"io/fs\"\n    \"net/http\"\n    \n    \"github.com/grpc-ecosystem/grpc-gateway/v2/runtime\"\n    \"myproject/generated/go/docs\"\n)\n\nfunc setupAPIRoutes(gatewayMux *runtime.ServeMux) {\n    // íŒŒì¼ ì„œë¹™ í—¬í¼\n    serveEmbeddedFile := func(w http.ResponseWriter, r *http.Request, fsys fs.FS, name string) {\n        fileServer := http.FileServer(http.FS(fsys))\n        r.URL.Path = name\n        fileServer.ServeHTTP(w, r)\n    }\n    \n    // OpenAPI JSON ìŠ¤í™\n    gatewayMux.HandlePath(\"GET\", \"/openapi.swagger.json\", \n        func(w http.ResponseWriter, r *http.Request, _ map[string]string) {\n            serveEmbeddedFile(w, r, docs.StaticFiles, \"openapi.swagger.json\")\n        })\n    \n    // Stoplight Elements UI (HTML)\n    gatewayMux.HandlePath(\"GET\", \"/openapi.html\", \n        func(w http.ResponseWriter, r *http.Request, _ map[string]string) {\n            serveEmbeddedFile(w, r, docs.StaticFiles, \"openapi.html\")\n        })\n    \n    // ë£¨íŠ¸ ì ‘ì† ì‹œ ë¬¸ì„œë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸\n    gatewayMux.HandlePath(\"GET\", \"/\", \n        func(w http.ResponseWriter, r *http.Request, _ map[string]string) {\n            http.Redirect(w, r, \"/openapi.html\", http.StatusFound)\n        })\n}\n```\n\n## ìš”ì²­/ì‘ë‹µ ì˜ˆì‹œ\n\n### HTTP ìš”ì²­\n\n```bash\n# ë¬¸ì„œ ìƒì„±\ncurl -X POST http://localhost:8080/v1beta/collections/users/documents \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"document\": {\n      \"uri\": \"user-001\",\n      \"fields\": {\n        \"name\": \"John Doe\",\n        \"email\": \"john@example.com\"\n      }\n    }\n  }'\n\n# ë¬¸ì„œ ì¡°íšŒ\ncurl http://localhost:8080/v1beta/collections/users/documents/user-001?version=1\n\n# ë¬¸ì„œ ì—…ë°ì´íŠ¸\ncurl -X PATCH http://localhost:8080/v1beta/collections/users/documents/user-001 \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"fields\": {\"name\": \"Jane Doe\"}}'\n```\n\n### gRPC ìš”ì²­ (grpcurl)\n\n```bash\n# ë¬¸ì„œ ìƒì„±\ngrpcurl -plaintext -d '{\n  \"collection\": \"users\",\n  \"document\": {\n    \"uri\": \"user-001\",\n    \"fields\": {\"name\": \"John Doe\"}\n  }\n}' localhost:9090 v1beta.DocumentService/CreateDocument\n\n# ë¬¸ì„œ ì¡°íšŒ\ngrpcurl -plaintext -d '{\n  \"collection\": \"users\",\n  \"uri\": \"user-001\"\n}' localhost:9090 v1beta.DocumentService/GetDocument\n```\n\n## ëª¨ë²” ì‚¬ë¡€\n\n1. **ì¸-í”„ë¡œì„¸ìŠ¤ ì—°ê²°**: ê°€ëŠ¥í•˜ë©´ `RegisterHandlerServer` ì‚¬ìš©\n2. **ì¼ê´€ëœ ì—ëŸ¬ ì²˜ë¦¬**: gRPC/HTTP ì—ëŸ¬ ë§¤í•‘ í†µì¼\n3. **OpenAPI í†µí•©**: ìë™ ìƒì„± ìŠ¤í™ìœ¼ë¡œ ë¬¸ì„œí™”\n4. **ë¯¸ë“¤ì›¨ì–´ ë¶„ë¦¬**: HTTP/gRPC ê°ê° ì ì ˆí•œ ë¯¸ë“¤ì›¨ì–´ ì ìš©\n5. **Health Check**: `/ready` ì—”ë“œí¬ì¸íŠ¸ë¡œ í—¬ìŠ¤ì²´í¬ ë¶„ë¦¬\n\n## ì°¸ê³  ìë£Œ\n\n- [gRPC-Gateway ê³µì‹ ë¬¸ì„œ](https://grpc-ecosystem.github.io/grpc-gateway/)\n- [google.api.http ì–´ë…¸í…Œì´ì…˜](https://cloud.google.com/endpoints/docs/grpc/transcoding)\n- [runtime.ServeMux ì˜µì…˜](https://pkg.go.dev/github.com/grpc-ecosystem/grpc-gateway/v2/runtime)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Architecture",
      "Backend",
      "Go",
      "HTTP",
      "gRPC"
    ],
    "readingTime": 7,
    "wordCount": 1375,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "wire-dependency-injection",
    "slug": "wire-dependency-injection",
    "path": "backend/go",
    "fullPath": "backend/go/wire-dependency-injection",
    "title": "Wireë¥¼ í™œìš©í•œ Go ì˜ì¡´ì„± ì£¼ì…(DI) êµ¬í˜„",
    "excerpt": "Googleì˜ Wireë¥¼ ì‚¬ìš©í•˜ì—¬ Go ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì»´íŒŒì¼ íƒ€ì„ ì˜ì¡´ì„± ì£¼ì…ì„ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# Wireë¥¼ í™œìš©í•œ Go ì˜ì¡´ì„± ì£¼ì…(DI) êµ¬í˜„\n\n## ê°œìš”\n\n**Wire**ëŠ” Googleì—ì„œ ê°œë°œí•œ Goìš© ì»´íŒŒì¼ íƒ€ì„ ì˜ì¡´ì„± ì£¼ì… ë„êµ¬ì…ë‹ˆë‹¤. ë¦¬í”Œë ‰ì…˜ ì—†ì´ ì½”ë“œ ìƒì„±ì„ í†µí•´ DIë¥¼ êµ¬í˜„í•˜ë¯€ë¡œ ëŸ°íƒ€ì„ ì˜¤ë²„í—¤ë“œê°€ ì—†ìŠµë‹ˆë‹¤.\n\n## ì™œ Wireì¸ê°€?\n\n| ë„êµ¬ | ë°©ì‹ | ì¥ì  | ë‹¨ì  |\n|-----|------|------|------|\n| ìˆ˜ë™ DI | ì§ì ‘ êµ¬ì„± | ë‹¨ìˆœ, ëª…ì‹œì  | ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ |\n| dig/fx | ëŸ°íƒ€ì„ ë¦¬í”Œë ‰ì…˜ | ìœ ì—° | ëŸ°íƒ€ì„ ì˜¤ë²„í—¤ë“œ, ë””ë²„ê¹… ì–´ë ¤ì›€ |\n| **Wire** | ì»´íŒŒì¼ íƒ€ì„ ì½”ë“œ ìƒì„± | íƒ€ì… ì•ˆì „, ì˜¤ë²„í—¤ë“œ ì—†ìŒ | ì´ˆê¸° ì„¤ì • í•„ìš” |\n\n## ì„¤ì¹˜\n\n```bash\ngo install github.com/google/wire/cmd/wire@latest\n```\n\n## ê¸°ë³¸ ê°œë…\n\n### Provider\n\n**Provider**ëŠ” ì˜ì¡´ì„±ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤:\n\n```go\n// providers.go\npackage main\n\ntype Config struct {\n    DBHost string\n    DBPort int\n}\n\nfunc NewConfig() *Config {\n    return &Config{\n        DBHost: \"localhost\",\n        DBPort: 5432,\n    }\n}\n\ntype Database struct {\n    config *Config\n}\n\nfunc NewDatabase(cfg *Config) (*Database, error) {\n    return &Database{config: cfg}, nil\n}\n\ntype UserRepository struct {\n    db *Database\n}\n\nfunc NewUserRepository(db *Database) *UserRepository {\n    return &UserRepository{db: db}\n}\n\ntype UserService struct {\n    repo *UserRepository\n}\n\nfunc NewUserService(repo *UserRepository) *UserService {\n    return &UserService{repo: repo}\n}\n```\n\n### Injector\n\n**Injector**ëŠ” ì˜ì¡´ì„± ê·¸ë˜í”„ë¥¼ ì •ì˜í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤:\n\n```go\n// wire.go\n//go:build wireinject\n\npackage main\n\nimport \"github.com/google/wire\"\n\nfunc InitializeUserService() (*UserService, error) {\n    wire.Build(\n        NewConfig,\n        NewDatabase,\n        NewUserRepository,\n        NewUserService,\n    )\n    return nil, nil // Wireê°€ ì´ ë¶€ë¶„ì„ ìƒì„±ëœ ì½”ë“œë¡œ ëŒ€ì²´\n}\n```\n\n### ì½”ë“œ ìƒì„±\n\n```bash\nwire ./...\n\n# ìƒì„±ëœ íŒŒì¼: wire_gen.go\n```\n\nìƒì„±ëœ `wire_gen.go`:\n\n```go\n// Code generated by Wire. DO NOT EDIT.\n//go:build !wireinject\n\npackage main\n\nfunc InitializeUserService() (*UserService, error) {\n    config := NewConfig()\n    database, err := NewDatabase(config)\n    if err != nil {\n        return nil, err\n    }\n    userRepository := NewUserRepository(database)\n    userService := NewUserService(userRepository)\n    return userService, nil\n}\n```\n\n## Provider Set\n\nê´€ë ¨ Providerë“¤ì„ ê·¸ë£¹í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n```go\n// infrastructure/wire.go\npackage infrastructure\n\nimport \"github.com/google/wire\"\n\nvar InfraSet = wire.NewSet(\n    NewConfig,\n    NewDatabase,\n    NewRedisClient,\n    NewLogger,\n)\n```\n\n```go\n// repository/wire.go\npackage repository\n\nimport \"github.com/google/wire\"\n\nvar RepositorySet = wire.NewSet(\n    NewUserRepository,\n    NewOrderRepository,\n    NewProductRepository,\n)\n```\n\n```go\n// service/wire.go\npackage service\n\nimport \"github.com/google/wire\"\n\nvar ServiceSet = wire.NewSet(\n    NewUserService,\n    NewOrderService,\n    NewProductService,\n)\n```\n\n```go\n// wire.go\npackage main\n\nimport \"github.com/google/wire\"\n\nfunc InitializeApp() (*App, error) {\n    wire.Build(\n        infrastructure.InfraSet,\n        repository.RepositorySet,\n        service.ServiceSet,\n        NewApp,\n    )\n    return nil, nil\n}\n```\n\n## ì¸í„°í˜ì´ìŠ¤ ë°”ì¸ë”©\n\nì¸í„°í˜ì´ìŠ¤ì— êµ¬í˜„ì²´ë¥¼ ë°”ì¸ë”©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n```go\n// ì¸í„°í˜ì´ìŠ¤ ì •ì˜\ntype UserRepository interface {\n    FindByID(id string) (*User, error)\n    Save(user *User) error\n}\n\n// êµ¬í˜„ì²´\ntype userRepositoryImpl struct {\n    db *Database\n}\n\nfunc NewUserRepository(db *Database) *userRepositoryImpl {\n    return &userRepositoryImpl{db: db}\n}\n\n// Wire ë°”ì¸ë”©\nvar RepositorySet = wire.NewSet(\n    NewUserRepository,\n    wire.Bind(new(UserRepository), new(*userRepositoryImpl)),\n)\n```\n\n## Struct Provider\n\nêµ¬ì¡°ì²´ í•„ë“œë¥¼ ì§ì ‘ ì£¼ì…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n```go\ntype App struct {\n    UserService  *UserService\n    OrderService *OrderService\n    Logger       *Logger\n}\n\nvar AppSet = wire.NewSet(\n    wire.Struct(new(App), \"*\"), // ëª¨ë“  í•„ë“œ ì£¼ì…\n    // ë˜ëŠ” íŠ¹ì • í•„ë“œë§Œ\n    // wire.Struct(new(App), \"UserService\", \"Logger\"),\n)\n```\n\n## Value Provider\n\nì •ì  ê°’ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n```go\nfunc InitializeApp(cfg *Config) (*App, error) {\n    wire.Build(\n        wire.Value(&Config{DBHost: \"localhost\"}), // ì •ì  ê°’\n        NewDatabase,\n        NewApp,\n    )\n    return nil, nil\n}\n```\n\n## ì‹¤ì „ í”„ë¡œì íŠ¸ êµ¬ì¡°\n\n```\nmyapp/\nâ”œâ”€â”€ cmd/\nâ”‚   â””â”€â”€ api/\nâ”‚       â”œâ”€â”€ main.go\nâ”‚       â”œâ”€â”€ wire.go          # Injector ì •ì˜\nâ”‚       â””â”€â”€ wire_gen.go      # ìƒì„±ëœ ì½”ë“œ\nâ”œâ”€â”€ internal/\nâ”‚   â”œâ”€â”€ config/\nâ”‚   â”‚   â””â”€â”€ config.go\nâ”‚   â”œâ”€â”€ infrastructure/\nâ”‚   â”‚   â”œâ”€â”€ database.go\nâ”‚   â”‚   â”œâ”€â”€ redis.go\nâ”‚   â”‚   â””â”€â”€ wire.go          # Provider Set\nâ”‚   â”œâ”€â”€ repository/\nâ”‚   â”‚   â”œâ”€â”€ user_repository.go\nâ”‚   â”‚   â””â”€â”€ wire.go\nâ”‚   â”œâ”€â”€ service/\nâ”‚   â”‚   â”œâ”€â”€ user_service.go\nâ”‚   â”‚   â””â”€â”€ wire.go\nâ”‚   â””â”€â”€ handler/\nâ”‚       â”œâ”€â”€ user_handler.go\nâ”‚       â””â”€â”€ wire.go\nâ””â”€â”€ go.mod\n```\n\n### cmd/api/wire.go\n\n```go\n//go:build wireinject\n\npackage main\n\nimport (\n    \"github.com/google/wire\"\n    \"myapp/internal/config\"\n    \"myapp/internal/infrastructure\"\n    \"myapp/internal/repository\"\n    \"myapp/internal/service\"\n    \"myapp/internal/handler\"\n)\n\nfunc InitializeServer(cfg *config.Config) (*Server, error) {\n    wire.Build(\n        infrastructure.InfraSet,\n        repository.RepositorySet,\n        service.ServiceSet,\n        handler.HandlerSet,\n        NewServer,\n    )\n    return nil, nil\n}\n```\n\n### cmd/api/main.go\n\n```go\npackage main\n\nfunc main() {\n    cfg := config.Load()\n    \n    server, err := InitializeServer(cfg)\n    if err != nil {\n        log.Fatal(err)\n    }\n    \n    server.Run()\n}\n```\n\n## í…ŒìŠ¤íŠ¸ì—ì„œ Wire í™œìš©\n\n```go\n// wire_test.go\n//go:build wireinject\n\npackage main\n\nimport \"github.com/google/wire\"\n\nfunc InitializeTestUserService(mockDB *MockDatabase) *UserService {\n    wire.Build(\n        NewUserRepository,\n        NewUserService,\n    )\n    return nil\n}\n```\n\n## Makefile í†µí•©\n\n```makefile\n.PHONY: generate wire\n\nwire:\n wire ./...\n\ngenerate: wire\n go generate ./...\n\nbuild: generate\n go build -o bin/app ./cmd/api\n```\n\n## ì£¼ì˜ì‚¬í•­\n\n1. **ë¹Œë“œ íƒœê·¸ í•„ìˆ˜**: `//go:build wireinject` ìŠì§€ ë§ ê²ƒ\n2. **wire_gen.go ì»¤ë°‹**: ìƒì„±ëœ ì½”ë“œëŠ” ë²„ì „ ê´€ë¦¬ì— í¬í•¨\n3. **ìˆœí™˜ ì˜ì¡´ì„± ë¶ˆê°€**: Wireê°€ ì—ëŸ¬ë¡œ ê°ì§€\n4. **ì—ëŸ¬ ì²˜ë¦¬**: Providerê°€ errorë¥¼ ë°˜í™˜í•˜ë©´ ìë™ ì „íŒŒ\n\n## ì°¸ê³  ìë£Œ\n\n- [Wire GitHub](https://github.com/google/wire)\n- [Wire Tutorial](https://github.com/google/wire/blob/main/docs/guide.md)\n- [Wire Best Practices](https://github.com/google/wire/blob/main/docs/best-practices.md)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Architecture",
      "Backend",
      "DI",
      "Go"
    ],
    "readingTime": 4,
    "wordCount": 720,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "pprof-profiling-guide",
    "slug": "pprof-profiling-guide",
    "path": "backend/go",
    "fullPath": "backend/go/pprof-profiling-guide",
    "title": "Go pprofë¥¼ í™œìš©í•œ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ê°€ì´ë“œ",
    "excerpt": "Goì˜ ë‚´ì¥ í”„ë¡œíŒŒì¼ë§ ë„êµ¬ pprofë¥¼ í™œìš©í•˜ì—¬ CPU, ë©”ëª¨ë¦¬, ê³ ë£¨í‹´ ë³‘ëª©ì„ ë¶„ì„í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# Go pprofë¥¼ í™œìš©í•œ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ê°€ì´ë“œ\n\n## ê°œìš”\n\n**pprof**ëŠ” Goì— ë‚´ì¥ëœ í”„ë¡œíŒŒì¼ë§ ë„êµ¬ë¡œ, CPU ì‚¬ìš©ëŸ‰, ë©”ëª¨ë¦¬ í• ë‹¹, ê³ ë£¨í‹´ ìƒíƒœ ë“±ì„ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œë„ ì•ˆì „í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ ì„±ëŠ¥ ìµœì í™”ì— í•„ìˆ˜ì ì…ë‹ˆë‹¤.\n\n## ê¸°ë³¸ ì„¤ì •\n\n### HTTP ì„œë²„ì— pprof ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€\n\n```go\npackage main\n\nimport (\n    \"net/http\"\n    _ \"net/http/pprof\" // ìë™ìœ¼ë¡œ /debug/pprof/* ì—”ë“œí¬ì¸íŠ¸ ë“±ë¡\n)\n\nfunc main() {\n    go func() {\n        // ë³„ë„ í¬íŠ¸ì—ì„œ pprof ì„œë²„ ì‹¤í–‰ (ë³´ì•ˆìƒ ê¶Œì¥)\n        http.ListenAndServe(\"localhost:6060\", nil)\n    }()\n    \n    // ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œì§\n    // ...\n}\n```\n\n### ì œê³µë˜ëŠ” ì—”ë“œí¬ì¸íŠ¸\n\n| ì—”ë“œí¬ì¸íŠ¸ | ì„¤ëª… |\n|----------|------|\n| `/debug/pprof/` | í”„ë¡œíŒŒì¼ ì¸ë±ìŠ¤ í˜ì´ì§€ |\n| `/debug/pprof/heap` | ë©”ëª¨ë¦¬ í• ë‹¹ í”„ë¡œíŒŒì¼ |\n| `/debug/pprof/goroutine` | ê³ ë£¨í‹´ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ |\n| `/debug/pprof/profile` | CPU í”„ë¡œíŒŒì¼ (30ì´ˆ) |\n| `/debug/pprof/trace` | ì‹¤í–‰ íŠ¸ë ˆì´ìŠ¤ |\n\n## CPU í”„ë¡œíŒŒì¼ë§\n\n### í”„ë¡œíŒŒì¼ ìˆ˜ì§‘\n\n```bash\n# 30ì´ˆê°„ CPU í”„ë¡œíŒŒì¼ ìˆ˜ì§‘\ngo tool pprof http://localhost:6060/debug/pprof/profile?seconds=30\n\n# ë˜ëŠ” íŒŒì¼ë¡œ ì €ì¥\ncurl -o cpu.prof http://localhost:6060/debug/pprof/profile?seconds=30\ngo tool pprof cpu.prof\n```\n\n### ì£¼ìš” ëª…ë ¹ì–´\n\n```bash\n# pprof ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œì—ì„œ\n(pprof) top10          # ìƒìœ„ 10ê°œ í•¨ìˆ˜\n(pprof) list funcName  # íŠ¹ì • í•¨ìˆ˜ì˜ ë¼ì¸ë³„ ë¶„ì„\n(pprof) web            # ê·¸ë˜í”„ ì‹œê°í™” (ë¸Œë¼ìš°ì €)\n(pprof) pdf            # PDFë¡œ ì €ì¥\n```\n\n### ì¶œë ¥ ì˜ˆì‹œ\n\n```\n      flat  flat%   sum%        cum   cum%\n     2.50s 25.00% 25.00%      4.00s 40.00%  main.processData\n     1.50s 15.00% 40.00%      1.50s 15.00%  runtime.mallocgc\n     1.00s 10.00% 50.00%      3.00s 30.00%  encoding/json.Marshal\n```\n\n| ì»¬ëŸ¼ | ì„¤ëª… |\n|-----|------|\n| `flat` | í•´ë‹¹ í•¨ìˆ˜ ìì²´ ì‹¤í–‰ ì‹œê°„ |\n| `cum` | í•´ë‹¹ í•¨ìˆ˜ + í˜¸ì¶œí•œ í•¨ìˆ˜ í•©ê³„ ì‹œê°„ |\n\n## ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§\n\n### í™ í”„ë¡œíŒŒì¼ ìˆ˜ì§‘\n\n```bash\n# í˜„ì¬ í™ ìƒíƒœ\ngo tool pprof http://localhost:6060/debug/pprof/heap\n\n# í• ë‹¹ëœ ê°ì²´ ìˆ˜ ê¸°ì¤€\ngo tool pprof -alloc_objects http://localhost:6060/debug/pprof/heap\n\n# í• ë‹¹ëœ ë©”ëª¨ë¦¬ í¬ê¸° ê¸°ì¤€\ngo tool pprof -alloc_space http://localhost:6060/debug/pprof/heap\n```\n\n### ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íƒì§€\n\n```go\n// runtime.MemStats í™œìš©\nimport \"runtime\"\n\nfunc printMemStats() {\n    var m runtime.MemStats\n    runtime.ReadMemStats(&m)\n    \n    fmt.Printf(\"Alloc = %v MiB\\n\", m.Alloc / 1024 / 1024)\n    fmt.Printf(\"TotalAlloc = %v MiB\\n\", m.TotalAlloc / 1024 / 1024)\n    fmt.Printf(\"Sys = %v MiB\\n\", m.Sys / 1024 / 1024)\n    fmt.Printf(\"NumGC = %v\\n\", m.NumGC)\n}\n```\n\n## ê³ ë£¨í‹´ í”„ë¡œíŒŒì¼ë§\n\n```bash\n# í˜„ì¬ ê³ ë£¨í‹´ ìƒíƒœ í™•ì¸\ngo tool pprof http://localhost:6060/debug/pprof/goroutine\n\n# ë¤í”„ íŒŒì¼ë¡œ ì €ì¥\ncurl http://localhost:6060/debug/pprof/goroutine?debug=2 > goroutines.txt\n```\n\nê³ ë£¨í‹´ ëˆ„ìˆ˜ ì§•í›„:\n\n- ê³ ë£¨í‹´ ìˆ˜ê°€ ì§€ì†ì ìœ¼ë¡œ ì¦ê°€\n- íŠ¹ì • í•¨ìˆ˜ì—ì„œ ëŒ€ê¸° ì¤‘ì¸ ê³ ë£¨í‹´ì´ ë§ìŒ\n\n## ì›¹ UIë¡œ ì‹œê°í™”\n\n**pprof ì›¹ ì¸í„°í˜ì´ìŠ¤** (Go 1.10+):\n\n```bash\ngo tool pprof -http=:8080 cpu.prof\n```\n\në¸Œë¼ìš°ì €ì—ì„œ `http://localhost:8080`ìœ¼ë¡œ ì ‘ì†í•˜ë©´:\n\n- **Top**: í•¨ìˆ˜ë³„ CPU/ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰\n- **Graph**: í˜¸ì¶œ ê·¸ë˜í”„\n- **Flame Graph**: í”Œë ˆì„ ì°¨íŠ¸\n- **Source**: ì†ŒìŠ¤ ì½”ë“œ ë ˆë²¨ ë¶„ì„\n\n## í”„ë¡œë•ì…˜ í™˜ê²½ ê³ ë„í™”\n\n### 1. ë³´ì•ˆ ì„¤ì •\n\n```go\npackage main\n\nimport (\n    \"net/http\"\n    \"net/http/pprof\"\n)\n\nfunc main() {\n    // ë³„ë„ ì„œë²„ì—ì„œ ì¸ì¦ ì¶”ê°€\n    pprofMux := http.NewServeMux()\n    pprofMux.HandleFunc(\"/debug/pprof/\", pprof.Index)\n    pprofMux.HandleFunc(\"/debug/pprof/cmdline\", pprof.Cmdline)\n    pprofMux.HandleFunc(\"/debug/pprof/profile\", pprof.Profile)\n    pprofMux.HandleFunc(\"/debug/pprof/symbol\", pprof.Symbol)\n    pprofMux.HandleFunc(\"/debug/pprof/trace\", pprof.Trace)\n    \n    // ì¸ì¦ ë¯¸ë“¤ì›¨ì–´ ì ìš©\n    go http.ListenAndServe(\"localhost:6060\", authMiddleware(pprofMux))\n}\n```\n\n### 2. ì—°ì† í”„ë¡œíŒŒì¼ë§ (Continuous Profiling)\n\n```go\nimport (\n    \"os\"\n    \"runtime/pprof\"\n    \"time\"\n)\n\nfunc startContinuousProfiling() {\n    ticker := time.NewTicker(10 * time.Minute)\n    for range ticker.C {\n        f, _ := os.Create(fmt.Sprintf(\"heap_%s.prof\", time.Now().Format(\"20060102_150405\")))\n        pprof.WriteHeapProfile(f)\n        f.Close()\n    }\n}\n```\n\n### 3. ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—°ë™\n\n- **Pyroscope**: ì˜¤í”ˆì†ŒìŠ¤ ì—°ì† í”„ë¡œíŒŒì¼ë§\n- **Datadog Continuous Profiler**: ìƒìš© APM\n- **Google Cloud Profiler**: GCP í†µí•©\n\n```go\n// Pyroscope ì˜ˆì‹œ\nimport \"github.com/grafana/pyroscope-go\"\n\nfunc main() {\n    pyroscope.Start(pyroscope.Config{\n        ApplicationName: \"my-app\",\n        ServerAddress:   \"http://pyroscope:4040\",\n    })\n}\n```\n\n## ë²¤ì¹˜ë§ˆí¬ì™€ í•¨ê»˜ ì‚¬ìš©\n\n```bash\n# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ + CPU í”„ë¡œíŒŒì¼\ngo test -bench=. -cpuprofile=cpu.prof\n\n# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ + ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼\ngo test -bench=. -memprofile=mem.prof\n\n# ë¶„ì„\ngo tool pprof cpu.prof\n```\n\n## ì£¼ì˜ì‚¬í•­\n\n1. **í”„ë¡œë•ì…˜ í¬íŠ¸ ë¶„ë¦¬**: pprof ì—”ë“œí¬ì¸íŠ¸ëŠ” ë³„ë„ í¬íŠ¸ì—ì„œ ì‹¤í–‰\n2. **ì¸ì¦ í•„ìˆ˜**: ì™¸ë¶€ ë…¸ì¶œ ì‹œ ë°˜ë“œì‹œ ì¸ì¦ ì ìš©\n3. **ì˜¤ë²„í—¤ë“œ**: CPU í”„ë¡œíŒŒì¼ë§ì€ ì•½ 5% ì˜¤ë²„í—¤ë“œ ë°œìƒ\n4. **ìƒ˜í”Œë§ ì£¼ê¸°**: `runtime.SetCPUProfileRate()`ë¡œ ì¡°ì ˆ ê°€ëŠ¥\n\n## ì°¸ê³  ìë£Œ\n\n- [Go pprof ê³µì‹ ë¬¸ì„œ](https://pkg.go.dev/net/http/pprof)\n- [Profiling Go Programs](https://go.dev/blog/pprof)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Go",
      "Performance"
    ],
    "readingTime": 4,
    "wordCount": 603,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "ginkgo-bdd-testing",
    "slug": "ginkgo-bdd-testing",
    "path": "backend/go",
    "fullPath": "backend/go/ginkgo-bdd-testing",
    "title": "Ginkgoì™€ Testcontainersë¥¼ í™œìš©í•œ í†µí•© í…ŒìŠ¤íŠ¸ ì „ëµ",
    "excerpt": "Ginkgo BDD í”„ë ˆì„ì›Œí¬ì™€ Testcontainersë¥¼ ê²°í•©í•˜ì—¬ ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ì‹ ë¢°ì„± ë†’ì€ í†µí•© í…ŒìŠ¤íŠ¸ë¥¼ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# Ginkgoì™€ Testcontainersë¥¼ í™œìš©í•œ í†µí•© í…ŒìŠ¤íŠ¸ ì „ëµ\n\n## ê°œìš”\n\n**Ginkgo**ëŠ” Goì˜ BDD í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ì´ë©°, **Testcontainers**ëŠ” í…ŒìŠ¤íŠ¸ì—ì„œ Docker ì»¨í…Œì´ë„ˆë¥¼ í”„ë¡œê·¸ë˜ë§¤í‹±í•˜ê²Œ ê´€ë¦¬í•©ë‹ˆë‹¤. ì´ ì¡°í•©ìœ¼ë¡œ Mock ì—†ì´ ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” í†µí•© í…ŒìŠ¤íŠ¸ë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## ì™œ ì´ ì¡°í•©ì¸ê°€?\n\n### ì¥ì \n\n| íŠ¹ì„± | ì„¤ëª… |\n|------|------|\n| **ì‹¤ì œ í™˜ê²½** | Mock ëŒ€ì‹  ì‹¤ì œ DBë¡œ í…ŒìŠ¤íŠ¸ â†’ ë†’ì€ ì‹ ë¢°ë„ |\n| **ê²©ë¦¬ì„±** | í…ŒìŠ¤íŠ¸ë§ˆë‹¤ ê¹¨ë—í•œ ì»¨í…Œì´ë„ˆ í™˜ê²½ |\n| **BDD ê°€ë…ì„±** | Describe/Context/Itìœ¼ë¡œ ì˜ë„ ëª…í™•íˆ í‘œí˜„ |\n| **ë³‘ë ¬ ì‹¤í–‰** | ì»¨í…Œì´ë„ˆ ê²©ë¦¬ë¡œ ì•ˆì „í•œ ë³‘ë ¬ í…ŒìŠ¤íŠ¸ |\n\n### ë‹¨ì \n\n| íŠ¹ì„± | ì„¤ëª… |\n|------|------|\n| **ì†ë„** | ì»¨í…Œì´ë„ˆ ì‹œì‘ ì‹œê°„ìœ¼ë¡œ ìœ ë‹› í…ŒìŠ¤íŠ¸ë³´ë‹¤ ëŠë¦¼ |\n| **ë¦¬ì†ŒìŠ¤** | Docker ì‹¤í–‰ í•„ìš”, CIì—ì„œ ì¶”ê°€ ì„¤ì • í•„ìš” |\n| **ë³µì¡ì„±** | ì»¨í…Œì´ë„ˆ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬ í•„ìš” |\n\n## ì„¤ì¹˜\n\n```bash\n# Ginkgo CLI ë° ë¼ì´ë¸ŒëŸ¬ë¦¬\ngo install github.com/onsi/ginkgo/v2/ginkgo@latest\ngo get github.com/onsi/gomega/...\n\n# Testcontainers\ngo get github.com/testcontainers/testcontainers-go\ngo get github.com/testcontainers/testcontainers-go/modules/mongodb\ngo get github.com/testcontainers/testcontainers-go/modules/redis\n```\n\n## í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì¡°\n\n### í…ŒìŠ¤íŠ¸ í—¬í¼\n\n```go\n// testutils/mongodb.go\npackage testutils\n\nimport (\n    \"context\"\n    \"os\"\n    \n    \"github.com/testcontainers/testcontainers-go\"\n    \"github.com/testcontainers/testcontainers-go/modules/mongodb\"\n    \"go.mongodb.org/mongo-driver/mongo\"\n    \"go.mongodb.org/mongo-driver/mongo/options\"\n)\n\n// MongoDBFixtureëŠ” MongoDB í…ŒìŠ¤íŠ¸ í™˜ê²½ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.\ntype MongoDBFixture struct {\n    container *mongodb.MongoDBContainer\n    client    *mongo.Client\n    cleanup   func()\n}\n\nfunc (f *MongoDBFixture) GetConnectionString() (string, error) {\n    return f.container.ConnectionString(context.Background())\n}\n\nfunc (f *MongoDBFixture) GetClient() *mongo.Client {\n    return f.client\n}\n\nfunc (f *MongoDBFixture) Cleanup() {\n    f.cleanup()\n}\n\n// GetMongoDBFixtureëŠ” MongoDB í…ŒìŠ¤íŠ¸ Fixtureë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n// packageNameì„ ì „ë‹¬í•˜ë©´ ë™ì¼ ì´ë¦„ì˜ ì»¨í…Œì´ë„ˆë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.\nfunc GetMongoDBFixture(ctx context.Context, packageName string) (*MongoDBFixture, error) {\n    // Ryuk(ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì»¨í…Œì´ë„ˆ) ë¹„í™œì„±í™” - CI í™˜ê²½ì—ì„œ ê¶Œì¥\n    os.Setenv(\"TESTCONTAINERS_RYUK_DISABLED\", \"true\")\n    \n    // ReplicaSet í™œì„±í™” - íŠ¸ëœì­ì…˜ í…ŒìŠ¤íŠ¸ì— í•„ìš”\n    replicaSetName := \"rs0\"\n    \n    mongoContainer, err := mongodb.Run(ctx,\n        \"public.ecr.aws/docker/library/mongo:8\",  // ê³µì‹ ECR ì´ë¯¸ì§€\n        mongodb.WithReplicaSet(replicaSetName),   // íŠ¸ëœì­ì…˜ ì§€ì›\n        testcontainers.WithReuseByName(packageName), // ì»¨í…Œì´ë„ˆ ì¬ì‚¬ìš©ìœ¼ë¡œ ì†ë„ í–¥ìƒ\n    )\n    if err != nil {\n        return nil, err\n    }\n    \n    // ì—°ê²° ë¬¸ìì—´ ê°€ì ¸ì˜¤ê¸°\n    connString, err := mongoContainer.ConnectionString(ctx)\n    if err != nil {\n        return nil, err\n    }\n    \n    // ReplicaSet ì‚¬ìš© ì‹œ Direct ì—°ê²° í•„ìš”\n    clientOpts := options.Client().ApplyURI(connString)\n    clientOpts.SetDirect(true)\n    \n    mongoClient, err := mongo.Connect(ctx, clientOpts)\n    if err != nil {\n        return nil, err\n    }\n    \n    // ì—°ê²° í™•ì¸\n    if err := mongoClient.Ping(ctx, nil); err != nil {\n        return nil, err\n    }\n    \n    // cleanup í´ë¡œì € - ì»¨í…Œì´ë„ˆì™€ í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬\n    cleanup := func(client *mongo.Client, container *mongodb.MongoDBContainer) func() {\n        return func() {\n            client.Disconnect(ctx)\n            container.Terminate(ctx)\n        }\n    }(mongoClient, mongoContainer)\n    \n    return &MongoDBFixture{\n        container: mongoContainer,\n        client:    mongoClient,\n        cleanup:   cleanup,\n    }, nil\n}\n```\n\n### Redis Fixture\n\n```go\n// testutils/redis.go\npackage testutils\n\nimport (\n    \"context\"\n    \"os\"\n    \n    goredis \"github.com/redis/go-redis/v9\"\n    \"github.com/testcontainers/testcontainers-go/modules/redis\"\n)\n\ntype RedisFixture struct {\n    container *redis.RedisContainer\n    client    *goredis.Client\n    cleanup   func()\n}\n\nfunc (f *RedisFixture) GetClient() *goredis.Client {\n    return f.client\n}\n\nfunc (f *RedisFixture) Cleanup() {\n    f.cleanup()\n}\n\nfunc (f *RedisFixture) GetConnectionString() (string, error) {\n    return f.container.Endpoint(context.Background(), \"\")\n}\n\nfunc GetRedisFixture(ctx context.Context, packageName string) (*RedisFixture, error) {\n    os.Setenv(\"TESTCONTAINERS_RYUK_DISABLED\", \"true\")\n    \n    redisContainer, err := redis.Run(ctx,\n        \"public.ecr.aws/docker/library/redis:alpine\",\n    )\n    if err != nil {\n        return nil, err\n    }\n    \n    // Endpointë¡œ host:port í˜•íƒœì˜ ì£¼ì†Œ íšë“\n    redisAddr, err := redisContainer.Endpoint(ctx, \"\")\n    if err != nil {\n        return nil, err\n    }\n    \n    // Optionsì— ì§ì ‘ Addr ì„¤ì •\n    redisClient := goredis.NewClient(&goredis.Options{\n        Addr: redisAddr,\n    })\n    \n    if err := redisClient.Ping(ctx).Err(); err != nil {\n        return nil, err\n    }\n    \n    cleanup := func(client *goredis.Client, container *redis.RedisContainer) func() {\n        return func() {\n            client.Close()\n            container.Terminate(ctx)\n        }\n    }(redisClient, redisContainer)\n    \n    return &RedisFixture{\n        container: redisContainer,\n        client:    redisClient,\n        cleanup:   cleanup,\n    }, nil\n}\n```\n\n### í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì„¤ì •\n\n```go\n// internal/document/document_suite_test.go\npackage document_test\n\nimport (\n    \"context\"\n    \"testing\"\n    \n    . \"github.com/onsi/ginkgo/v2\"\n    . \"github.com/onsi/gomega\"\n    \n    \"myapp/testutil\"\n)\n\nvar (\n    testEnv *testutil.TestEnvironment\n    testCtx context.Context\n)\n\nfunc TestDocument(t *testing.T) {\n    RegisterFailHandler(Fail)\n    RunSpecs(t, \"Document Suite\")\n}\n\nvar _ = BeforeSuite(func() {\n    var err error\n    testCtx = context.Background()\n    \n    By(\"í…ŒìŠ¤íŠ¸ í™˜ê²½ ì´ˆê¸°í™”\")\n    testEnv, err = testutil.NewTestEnvironment(testCtx)\n    Expect(err).NotTo(HaveOccurred())\n    \n    By(\"MongoDB ì—°ê²° í™•ì¸\")\n    // ... ì—°ê²° í…ŒìŠ¤íŠ¸\n})\n\nvar _ = AfterSuite(func() {\n    By(\"í…ŒìŠ¤íŠ¸ í™˜ê²½ ì •ë¦¬\")\n    testEnv.Cleanup(testCtx)\n})\n```\n\n## í…ŒìŠ¤íŠ¸ ì‘ì„±\n\n### ë¬¸ì„œ ì„œë¹„ìŠ¤ í†µí•© í…ŒìŠ¤íŠ¸\n\n```go\n// internal/document/service_integration_test.go\npackage document_test\n\nimport (\n    \"context\"\n    \n    . \"github.com/onsi/ginkgo/v2\"\n    . \"github.com/onsi/gomega\"\n    \n    \"go.mongodb.org/mongo-driver/mongo\"\n    \"go.mongodb.org/mongo-driver/mongo/options\"\n    \n    \"myapp/internal/document\"\n)\n\nvar _ = Describe(\"DocumentService í†µí•© í…ŒìŠ¤íŠ¸\", Label(\"integration\"), func() {\n    var (\n        service    document.Service\n        collection *mongo.Collection\n        ctx        context.Context\n    )\n    \n    BeforeEach(func() {\n        ctx = context.Background()\n        \n        // í…ŒìŠ¤íŠ¸ìš© MongoDB í´ë¼ì´ì–¸íŠ¸\n        client, err := mongo.Connect(ctx, options.Client().ApplyURI(testEnv.MongoURI))\n        Expect(err).NotTo(HaveOccurred())\n        \n        // í…ŒìŠ¤íŠ¸ë§ˆë‹¤ ìƒˆ ì»¬ë ‰ì…˜ ì‚¬ìš©\n        dbName := \"test_db\"\n        collName := \"test_collection_\" + GinkgoParallelProcess()\n        collection = client.Database(dbName).Collection(collName)\n        \n        // ì„œë¹„ìŠ¤ ìƒì„±\n        service = document.NewService(client, dbName)\n    })\n    \n    AfterEach(func() {\n        // í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬\n        collection.Drop(ctx)\n    })\n    \n    Describe(\"Create\", func() {\n        Context(\"ìœ íš¨í•œ ë¬¸ì„œê°€ ì£¼ì–´ì¡Œì„ ë•Œ\", func() {\n            It(\"ë²„ì „ 1ì˜ ìƒˆ ë¬¸ì„œë¥¼ ìƒì„±í•œë‹¤\", func() {\n                input := document.CreateInput{\n                    URI: \"doc-001\",\n                    Fields: map[string]interface{}{\n                        \"name\":  \"Test Document\",\n                        \"value\": 42,\n                    },\n                }\n                \n                doc, err := service.Create(ctx, \"test_collection\", input)\n                \n                Expect(err).NotTo(HaveOccurred())\n                Expect(doc.URI).To(Equal(\"doc-001\"))\n                Expect(doc.Version).To(Equal(int32(1)))\n                Expect(doc.Fields[\"name\"]).To(Equal(\"Test Document\"))\n            })\n        })\n        \n        Context(\"ì¤‘ë³µ URIê°€ ì£¼ì–´ì¡Œì„ ë•Œ\", func() {\n            BeforeEach(func() {\n                _, err := service.Create(ctx, \"test_collection\", document.CreateInput{\n                    URI:    \"doc-001\",\n                    Fields: map[string]interface{}{},\n                })\n                Expect(err).NotTo(HaveOccurred())\n            })\n            \n            It(\"ì—ëŸ¬ë¥¼ ë°˜í™˜í•œë‹¤\", func() {\n                _, err := service.Create(ctx, \"test_collection\", document.CreateInput{\n                    URI:    \"doc-001\",\n                    Fields: map[string]interface{}{},\n                })\n                \n                Expect(err).To(HaveOccurred())\n                Expect(err).To(MatchError(ContainSubstring(\"duplicate\")))\n            })\n        })\n    })\n    \n    Describe(\"SoftUpdate\", func() {\n        var existingDoc *document.Document\n        \n        BeforeEach(func() {\n            var err error\n            existingDoc, err = service.Create(ctx, \"test_collection\", document.CreateInput{\n                URI:    \"doc-update-test\",\n                Fields: map[string]interface{}{\"name\": \"Original\"},\n            })\n            Expect(err).NotTo(HaveOccurred())\n        })\n        \n        Context(\"ì •ìƒì ì¸ ì—…ë°ì´íŠ¸ ìš”ì²­ì¼ ë•Œ\", func() {\n            It(\"ìƒˆ ë²„ì „ì„ ìƒì„±í•˜ê³  ê¸°ì¡´ ë²„ì „ì„ ë³´ì¡´í•œë‹¤\", func() {\n                updated, err := service.SoftUpdate(ctx, \"test_collection\", \"doc-update-test\", \n                    document.UpdateInput{\n                        Fields: map[string]interface{}{\"name\": \"Updated\"},\n                    },\n                )\n                \n                Expect(err).NotTo(HaveOccurred())\n                Expect(updated.Version).To(Equal(int32(2)))\n                Expect(updated.Fields[\"name\"]).To(Equal(\"Updated\"))\n                \n                // ì´ì „ ë²„ì „ì´ ë³´ì¡´ë˜ëŠ”ì§€ í™•ì¸\n                history, err := service.GetHistory(ctx, \"test_collection\", \"doc-update-test\")\n                Expect(err).NotTo(HaveOccurred())\n                Expect(history).To(HaveLen(2))\n                Expect(history[0].Version).To(Equal(int32(1)))\n                Expect(history[1].Version).To(Equal(int32(2)))\n            })\n        })\n    })\n    \n    Describe(\"SoftDelete\", func() {\n        BeforeEach(func() {\n            _, err := service.Create(ctx, \"test_collection\", document.CreateInput{\n                URI:    \"doc-delete-test\",\n                Fields: map[string]interface{}{\"data\": \"value\"},\n            })\n            Expect(err).NotTo(HaveOccurred())\n        })\n        \n        It(\"ë¬¸ì„œë¥¼ DELETED ìƒíƒœë¡œ ë§ˆí‚¹í•œë‹¤\", func() {\n            deleted, err := service.SoftDelete(ctx, \"test_collection\", \"doc-delete-test\")\n            \n            Expect(err).NotTo(HaveOccurred())\n            Expect(deleted.Status).To(Equal(document.StatusDeleted))\n            \n            // ìµœì‹  ë²„ì „ ì¡°íšŒ ì‹œ ì°¾ì„ ìˆ˜ ì—†ìŒ\n            _, err = service.FindLatest(ctx, \"test_collection\", \"doc-delete-test\")\n            Expect(err).To(MatchError(document.ErrNotFound))\n        })\n    })\n})\n```\n\n### Redis í†µí•© í…ŒìŠ¤íŠ¸\n\n```go\n// internal/worker/worker_integration_test.go\npackage worker_test\n\nimport (\n    \"context\"\n    \"time\"\n    \n    . \"github.com/onsi/ginkgo/v2\"\n    . \"github.com/onsi/gomega\"\n    \n    \"github.com/redis/go-redis/v9\"\n    \n    \"myapp/internal/worker\"\n)\n\nvar _ = Describe(\"StreamWorker í†µí•© í…ŒìŠ¤íŠ¸\", Label(\"integration\"), func() {\n    var (\n        redisClient redis.UniversalClient\n        ctx         context.Context\n    )\n    \n    BeforeEach(func() {\n        ctx = context.Background()\n        \n        redisClient = redis.NewClient(&redis.Options{\n            Addr: testEnv.RedisAddr,\n        })\n        \n        // ì´ì „ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬\n        redisClient.FlushAll(ctx)\n    })\n    \n    AfterEach(func() {\n        redisClient.Close()\n    })\n    \n    Describe(\"ë©”ì‹œì§€ ì²˜ë¦¬\", func() {\n        Context(\"ì •ìƒ ë©”ì‹œì§€ê°€ ë°œí–‰ë˜ì—ˆì„ ë•Œ\", func() {\n            It(\"í•¸ë“¤ëŸ¬ê°€ í˜¸ì¶œë˜ê³  ACK ì²˜ë¦¬ëœë‹¤\", func() {\n                processed := make(chan string, 1)\n                \n                handler := &testHandler{\n                    onHandle: func(msgs []*worker.Message) []error {\n                        for _, m := range msgs {\n                            processed <- m.ID\n                        }\n                        return nil\n                    },\n                }\n                \n                w := worker.NewStreamWorker(\n                    redisClient,\n                    handler,\n                    worker.WithStream(\"test-stream\"),\n                    worker.WithGroup(\"test-group\"),\n                    worker.WithBatchSize(1),\n                    worker.WithPollInterval(50*time.Millisecond),\n                )\n                \n                w.Start(ctx)\n                defer w.Stop()\n                \n                // ë©”ì‹œì§€ ë°œí–‰\n                redisClient.XAdd(ctx, &redis.XAddArgs{\n                    Stream: \"test-stream\",\n                    Values: map[string]interface{}{\"data\": \"test\"},\n                })\n                \n                // ì²˜ë¦¬ í™•ì¸\n                Eventually(processed).Should(Receive())\n                \n                // ACK í™•ì¸ (Pending ì—†ìŒ)\n                pending, _ := redisClient.XPending(ctx, \"test-stream\", \"test-group\").Result()\n                Expect(pending.Count).To(BeZero())\n            })\n        })\n        \n        Context(\"ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ\", func() {\n            It(\"Dead Letter ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì´ë™í•œë‹¤\", func() {\n                handler := &testHandler{\n                    onHandle: func(msgs []*worker.Message) []error {\n                        return []error{errors.New(\"processing failed\")}\n                    },\n                }\n                \n                w := worker.NewStreamWorker(\n                    redisClient,\n                    handler,\n                    worker.WithStream(\"test-stream\"),\n                    worker.WithGroup(\"test-group\"),\n                    worker.WithMaxRetries(1),\n                    worker.WithDeadLetterStream(\"dead-letters\"),\n                )\n                \n                w.Start(ctx)\n                defer w.Stop()\n                \n                // ë©”ì‹œì§€ ë°œí–‰\n                redisClient.XAdd(ctx, &redis.XAddArgs{\n                    Stream: \"test-stream\",\n                    Values: map[string]interface{}{\"data\": \"fail\"},\n                })\n                \n                // Dead Letter í™•ì¸\n                Eventually(func() int64 {\n                    len, _ := redisClient.XLen(ctx, \"dead-letters\").Result()\n                    return len\n                }).Should(BeNumerically(\">\", 0))\n            })\n        })\n    })\n})\n\ntype testHandler struct {\n    onHandle func([]*worker.Message) []error\n}\n\nfunc (h *testHandler) Handle(ctx context.Context, msgs []*worker.Message) []error {\n    return h.onHandle(msgs)\n}\n```\n\n## í…Œì´ë¸” ë“œë¦¬ë¸ í…ŒìŠ¤íŠ¸\n\nGinkgoì˜ `DescribeTable`ë¡œ ë‹¤ì–‘í•œ ì¼€ì´ìŠ¤ ì»¤ë²„:\n\n```go\nvar _ = Describe(\"ìŠ¤í‚¤ë§ˆ ê²€ì¦\", func() {\n    DescribeTable(\"ìœ íš¨í•œ ë¬¸ì„œ\",\n        func(fields map[string]interface{}, expectValid bool) {\n            err := validator.Validate(schema, fields)\n            \n            if expectValid {\n                Expect(err).NotTo(HaveOccurred())\n            } else {\n                Expect(err).To(HaveOccurred())\n            }\n        },\n        Entry(\"ëª¨ë“  í•„ìˆ˜ í•„ë“œ ì¡´ì¬\", map[string]interface{}{\n            \"name\": \"test\", \"email\": \"test@example.com\",\n        }, true),\n        Entry(\"í•„ìˆ˜ í•„ë“œ ëˆ„ë½\", map[string]interface{}{\n            \"name\": \"test\",\n        }, false),\n        Entry(\"ì˜ëª»ëœ íƒ€ì…\", map[string]interface{}{\n            \"name\": 123, \"email\": \"test@example.com\",\n        }, false),\n    )\n})\n```\n\n## í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n\n### ê¸°ë³¸ ì‹¤í–‰\n\n```bash\n# ëª¨ë“  í…ŒìŠ¤íŠ¸\nginkgo ./...\n\n# ìƒì„¸ ì¶œë ¥\nginkgo -v ./...\n\n# í†µí•© í…ŒìŠ¤íŠ¸ë§Œ\nginkgo --label-filter=\"integration\" ./...\n\n# ìœ ë‹› í…ŒìŠ¤íŠ¸ë§Œ (í†µí•© ì œì™¸)\nginkgo --label-filter=\"!integration\" ./...\n```\n\n### ë³‘ë ¬ ì‹¤í–‰\n\n```bash\n# í”„ë¡œì„¸ìŠ¤ ìë™ ê²°ì •\nginkgo -p ./...\n\n# í”„ë¡œì„¸ìŠ¤ ìˆ˜ ì§€ì •\nginkgo -procs=4 ./...\n```\n\n### ì»¤ë²„ë¦¬ì§€\n\n```bash\nginkgo -cover -coverprofile=coverage.out ./...\ngo tool cover -html=coverage.out -o coverage.html\n```\n\n## Makefile í†µí•©\n\n```makefile\n# ê¸°ë³¸ í…ŒìŠ¤íŠ¸ í”Œë˜ê·¸ + ì¶”ê°€ ì¸ìëŠ” TESTFLAGSë¡œ ì „ë‹¬\nTEST_FLAGS = --skip-package \"./deps\"\nTEST_FLAGS += $(TESTFLAGS)\nTEST_TIMEOUT = 30m\nCOVERAGE_OUT = coverage.out\n\n.PHONY: test test-verbose unit-test integration-test coverage-test cov-html\n\ntest:\n ginkgo -r $(TEST_FLAGS) --timeout=$(TEST_TIMEOUT)\n\ntest-verbose:\n ginkgo -r $(TEST_FLAGS) --timeout=$(TEST_TIMEOUT) -v\n\nunit-test:\n ginkgo -r $(TEST_FLAGS) --label-filter=\"!integration\" --junit-report=unit-test-report.xml --timeout=$(TEST_TIMEOUT)\n\nintegration-test:\n ginkgo -r $(TEST_FLAGS) --label-filter=\"integration\" --junit-report=integration-test-report.xml --timeout=$(TEST_TIMEOUT)\n\ncoverage-test:\n ginkgo -r -cover --coverprofile=$(COVERAGE_OUT) --timeout=$(TEST_TIMEOUT)\n\ncov-html: coverage-test\n go tool cover -html=$(COVERAGE_OUT) -o coverage.html\n```\n\n### ì‚¬ìš© ì˜ˆì‹œ\n\n```bash\n# ì „ì²´ í…ŒìŠ¤íŠ¸\nmake test\n\n# ìƒì„¸ ì¶œë ¥\nmake test-verbose\n\n# ìœ ë‹› í…ŒìŠ¤íŠ¸ë§Œ\nmake unit-test\n\n# í†µí•© í…ŒìŠ¤íŠ¸ë§Œ\nmake integration-test\n\n# ì»¤ë²„ë¦¬ì§€ HTML ë¦¬í¬íŠ¸\nmake cov-html\n\n# ì¶”ê°€ ì¸ì ì „ë‹¬ (íŠ¹ì • íŒ¨í‚¤ì§€, focus ë“±)\nmake test TESTFLAGS=\"./internal/document/...\"\nmake test TESTFLAGS='--focus=\"CreateDocument\"'\nmake test-verbose TESTFLAGS=\"-p\"\n```\n\n## CI ì„¤ì •\n\n```yaml\n# .github/workflows/test.yml\nname: Tests\n\non: [push, pull_request]\n\njobs:\n  unit-tests:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-go@v5\n        with:\n          go-version: '1.24'\n      \n      - name: Install Ginkgo\n        run: go install github.com/onsi/ginkgo/v2/ginkgo@latest\n      \n      - name: Run Unit Tests\n        run: ginkgo -r --label-filter=\"!integration\"\n  \n  integration-tests:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-go@v5\n        with:\n          go-version: '1.24'\n      \n      - name: Install Ginkgo\n        run: go install github.com/onsi/ginkgo/v2/ginkgo@latest\n      \n      - name: Run Integration Tests\n        run: ginkgo -r --label-filter=\"integration\" --timeout=30m\n```\n\n## ëª¨ë²” ì‚¬ë¡€\n\n1. **Label ì‚¬ìš©**: `integration` ë¼ë²¨ë¡œ ìœ ë‹›/í†µí•© í…ŒìŠ¤íŠ¸ ë¶„ë¦¬\n2. **ë³‘ë ¬ ì•ˆì „**: `GinkgoParallelProcess()`ë¡œ ë¦¬ì†ŒìŠ¤ ì´ë¦„ ë¶„ë¦¬\n3. **ì •ë¦¬ ì² ì €**: `AfterEach`ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë°˜ë“œì‹œ ì •ë¦¬\n4. **íƒ€ì„ì•„ì›ƒ ì„¤ì •**: í†µí•© í…ŒìŠ¤íŠ¸ëŠ” ì¶©ë¶„í•œ íƒ€ì„ì•„ì›ƒ ì„¤ì •\n5. **ì‹¤íŒ¨ ê²©ë¦¬**: í•œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ê°€ ë‹¤ë¥¸ í…ŒìŠ¤íŠ¸ì— ì˜í–¥ ì—†ë„ë¡\n\n## ì°¸ê³  ìë£Œ\n\n- [Ginkgo ê³µì‹ ë¬¸ì„œ](https://onsi.github.io/ginkgo/)\n- [Gomega ê³µì‹ ë¬¸ì„œ](https://onsi.github.io/gomega/)\n- [Testcontainers Go](https://golang.testcontainers.org/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Docker",
      "Go",
      "MongoDB",
      "Redis",
      "Testing"
    ],
    "readingTime": 8,
    "wordCount": 1520,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "gc-tuning-experience",
    "slug": "gc-tuning-experience",
    "path": "backend/go",
    "fullPath": "backend/go/gc-tuning-experience",
    "title": "Go ê°€ë¹„ì§€ ì»¬ë ‰í„°(GC) ì´í•´ì™€ íŠœë‹ ê²½í—˜",
    "excerpt": "Goì˜ ê°€ë¹„ì§€ ì»¬ë ‰í„° ë™ì‘ ì›ë¦¬ì™€ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œì˜ GC íŠœë‹ ê²½í—˜ì„ ê³µìœ í•©ë‹ˆë‹¤.",
    "content": "# Go ê°€ë¹„ì§€ ì»¬ë ‰í„°(GC) ì´í•´ì™€ íŠœë‹ ê²½í—˜\n\n## ê°œìš”\n\nGoëŠ” **Concurrent Mark-and-Sweep** ë°©ì‹ì˜ ê°€ë¹„ì§€ ì»¬ë ‰í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. Go 1.5 ì´í›„ STW(Stop-The-World) ì‹œê°„ì„ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì§€ì†ì ìœ¼ë¡œ ê°œì„ ë˜ì–´, ëŒ€ë¶€ë¶„ì˜ ê²½ìš° 1ms ì´í•˜ì˜ pause timeì„ ë‹¬ì„±í•©ë‹ˆë‹¤.\n\n## GC ë™ì‘ ì›ë¦¬\n\n### Tricolor Mark-and-Sweep\n\nGo GCëŠ” **ì‚¼ìƒ‰ ë§ˆí‚¹ ì•Œê³ ë¦¬ì¦˜**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:\n\n| ìƒ‰ìƒ | ì˜ë¯¸ |\n|-----|------|\n| **White** | ì•„ì§ ìŠ¤ìº”ë˜ì§€ ì•ŠìŒ (ìˆ˜ì§‘ ëŒ€ìƒ í›„ë³´) |\n| **Gray** | ìŠ¤ìº” ì¤‘, ì°¸ì¡° ê°ì²´ í™•ì¸ í•„ìš” |\n| **Black** | ìŠ¤ìº” ì™„ë£Œ, ì°¸ì¡° ê°ì²´ ëª¨ë‘ í™•ì¸ë¨ |\n\n### GC ë‹¨ê³„\n\n```\n1. Mark Setup (STW)     â†’ 0.1ms ë¯¸ë§Œ\n2. Concurrent Marking   â†’ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰\n3. Mark Termination (STW) â†’ 0.1ms ë¯¸ë§Œ\n4. Concurrent Sweeping  â†’ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰\n```\n\n> Go 1.8+ë¶€í„° ëŒ€ë¶€ë¶„ì˜ STW ì‹œê°„ì´ **sub-millisecond** ìˆ˜ì¤€ì…ë‹ˆë‹¤.\n\n## GC ëª¨ë‹ˆí„°ë§\n\n### runtime íŒ¨í‚¤ì§€ í™œìš©\n\n```go\npackage main\n\nimport (\n    \"fmt\"\n    \"runtime\"\n    \"time\"\n)\n\nfunc printGCStats() {\n    var stats runtime.MemStats\n    runtime.ReadMemStats(&stats)\n    \n    fmt.Printf(\"Alloc = %v MiB\\n\", stats.Alloc/1024/1024)\n    fmt.Printf(\"HeapAlloc = %v MiB\\n\", stats.HeapAlloc/1024/1024)\n    fmt.Printf(\"HeapSys = %v MiB\\n\", stats.HeapSys/1024/1024)\n    fmt.Printf(\"NumGC = %v\\n\", stats.NumGC)\n    fmt.Printf(\"PauseTotalNs = %v ms\\n\", stats.PauseTotalNs/1e6)\n    fmt.Printf(\"LastGC = %v\\n\", time.Unix(0, int64(stats.LastGC)))\n}\n```\n\n### GODEBUG í™˜ê²½ë³€ìˆ˜\n\n```bash\n# GC íŠ¸ë ˆì´ìŠ¤ í™œì„±í™”\nGODEBUG=gctrace=1 ./myapp\n\n# ì¶œë ¥ ì˜ˆì‹œ:\n# gc 1 @0.012s 2%: 0.018+1.2+0.014 ms clock, 0.14+0.8/1.0/0+0.11 ms cpu, 4->4->1 MB, 5 MB goal, 8 P\n```\n\n| í•„ë“œ | ì˜ë¯¸ |\n|-----|------|\n| `gc 1` | GC ë²ˆí˜¸ |\n| `2%` | CPU ì‚¬ìš©ë¥  |\n| `0.018+1.2+0.014 ms` | STW + concurrent + STW ì‹œê°„ |\n| `4->4->1 MB` | í™: ì‹œì‘ â†’ ì¢…ë£Œ â†’ ë¼ì´ë¸Œ |\n| `5 MB goal` | ë‹¤ìŒ GC ëª©í‘œ í™ í¬ê¸° |\n\n## GOGC íŠœë‹\n\n### GOGC í™˜ê²½ë³€ìˆ˜\n\n**GOGC**ëŠ” GC íŠ¸ë¦¬ê±° ì„ê³„ê°’ì„ ì¡°ì ˆí•©ë‹ˆë‹¤:\n\n```bash\n# ê¸°ë³¸ê°’: 100 (í™ì´ 2ë°°ê°€ ë˜ë©´ GC)\nGOGC=100 ./myapp\n\n# ë” ìì£¼ GC (ë©”ëª¨ë¦¬ ì ˆì•½, CPU ì¦ê°€)\nGOGC=50 ./myapp\n\n# ëœ ìì£¼ GC (ë©”ëª¨ë¦¬ ì¦ê°€, CPU ì ˆì•½)\nGOGC=200 ./myapp\n\n# GC ë¹„í™œì„±í™” (ê·¹ë‹¨ì  ì¼€ì´ìŠ¤)\nGOGC=off ./myapp\n```\n\n### ëŸ°íƒ€ì„ì—ì„œ ì¡°ì ˆ\n\n```go\nimport \"runtime/debug\"\n\n// GOGC ê°’ ë³€ê²½\ndebug.SetGCPercent(50)\n\n// í˜„ì¬ ê°’ í™•ì¸ ë° ë³€ê²½\nold := debug.SetGCPercent(200)\nfmt.Printf(\"Previous GOGC: %d\\n\", old)\n```\n\n## ë©”ëª¨ë¦¬ ì œí•œ (Go 1.19+)\n\n### GOMEMLIMIT\n\nGo 1.19ì—ì„œ ë„ì…ëœ **ì†Œí”„íŠ¸ ë©”ëª¨ë¦¬ ì œí•œ**:\n\n```bash\n# ìµœëŒ€ 4GB í™ ì œí•œ\nGOMEMLIMIT=4GiB ./myapp\n```\n\n```go\nimport \"runtime/debug\"\n\n// ëŸ°íƒ€ì„ì—ì„œ ì„¤ì •\ndebug.SetMemoryLimit(4 * 1024 * 1024 * 1024) // 4GB\n```\n\n### GOGC + GOMEMLIMIT ì¡°í•©\n\n```bash\n# ê¶Œì¥: GOGC=off + GOMEMLIMIT\n# ë©”ëª¨ë¦¬ ì œí•œì— ë„ë‹¬í•˜ë©´ ìë™ìœ¼ë¡œ GC\nGOGC=off GOMEMLIMIT=2GiB ./myapp\n```\n\n## í”„ë¡œë•ì…˜ íŠœë‹ ê²½í—˜\n\n### Case 1: ê³ ë¹ˆë„ í• ë‹¹ ì„œë¹„ìŠ¤\n\n**ë¬¸ì œ**: ì´ˆë‹¹ 10ë§Œ ê±´ ìš”ì²­ ì²˜ë¦¬, GC pauseê°€ p99 ë ˆì´í„´ì‹œì— ì˜í–¥\n\n**í•´ê²°**:\n\n```go\n// sync.Poolë¡œ ê°ì²´ ì¬ì‚¬ìš©\nvar bufferPool = sync.Pool{\n    New: func() interface{} {\n        return make([]byte, 4096)\n    },\n}\n\nfunc handleRequest() {\n    buf := bufferPool.Get().([]byte)\n    defer bufferPool.Put(buf)\n    \n    // buf ì‚¬ìš©\n}\n```\n\n**ê²°ê³¼**: í• ë‹¹ëŸ‰ 70% ê°ì†Œ, GC ë¹ˆë„ 50% ê°ì†Œ\n\n### Case 2: ëŒ€ìš©ëŸ‰ ìºì‹œ ì„œë¹„ìŠ¤\n\n**ë¬¸ì œ**: 32GB í™, GC marking ì‹œê°„ì´ ê¸¸ì–´ì§\n\n**í•´ê²°**:\n\n```bash\n# ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •ìœ¼ë¡œ ì˜ˆì¸¡ ê°€ëŠ¥í•œ GC\nGOMEMLIMIT=30GiB GOGC=100 ./cache-server\n```\n\në˜í•œ **ì™¸ë¶€ ìºì‹œ**(Redis, Memcached)ë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì˜¤í”„ë¡œë“œ\n\n### Case 3: ë°°ì¹˜ ì²˜ë¦¬ ì›Œì»¤\n\n**ë¬¸ì œ**: ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ GCê°€ ì²˜ë¦¬ ì‹œê°„ì— ì˜í–¥\n\n**í•´ê²°**:\n\n```go\nfunc processBatch(items []Item) {\n    // ë°°ì¹˜ ì²˜ë¦¬ ì „ GC ê°•ì œ ì‹¤í–‰\n    runtime.GC()\n    \n    // ì²˜ë¦¬ ì¤‘ GC ë¹„í™œì„±í™”\n    debug.SetGCPercent(-1)\n    defer debug.SetGCPercent(100)\n    \n    for _, item := range items {\n        process(item)\n    }\n}\n```\n\n## ë©”ëª¨ë¦¬ í• ë‹¹ ìµœì í™”\n\n### 1. ì‚¬ì „ í• ë‹¹\n\n```go\n// Bad\nvar result []int\nfor i := 0; i < 1000; i++ {\n    result = append(result, i)\n}\n\n// Good\nresult := make([]int, 0, 1000)\nfor i := 0; i < 1000; i++ {\n    result = append(result, i)\n}\n```\n\n### 2. í¬ì¸í„° íšŒí”¼\n\n```go\n// í™ í• ë‹¹ ìœ ë°œ\ntype Bad struct {\n    data *int\n}\n\n// ìŠ¤íƒ í• ë‹¹ ê°€ëŠ¥\ntype Good struct {\n    data int\n}\n```\n\n### 3. Escape Analysis í™œìš©\n\n```bash\n# ì´ìŠ¤ì¼€ì´í”„ ë¶„ì„ ê²°ê³¼ í™•ì¸\ngo build -gcflags=\"-m\" ./...\n```\n\n## Ballast ê¸°ë²• (ë ˆê±°ì‹œ)\n\n> **Note**: Go 1.19+ GOMEMLIMIT ë„ì… ì´í›„ ballast ê¸°ë²•ì€ ê¶Œì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n```go\n// ë ˆê±°ì‹œ: í° ë°°ì—´ë¡œ í™ í¬ê¸° ìœ ì§€\nvar ballast = make([]byte, 1<<30) // 1GB\n\nfunc main() {\n    _ = ballast // ë³€ìˆ˜ ìœ ì§€\n    // ...\n}\n```\n\n## ëª¨ë‹ˆí„°ë§ ì§€í‘œ\n\ní”„ë¡œë•ì…˜ì—ì„œ ì¶”ì í•´ì•¼ í•  GC ê´€ë ¨ ì§€í‘œ:\n\n| ì§€í‘œ | ì„¤ëª… | ì„ê³„ê°’ |\n|-----|------|-------|\n| `go_gc_duration_seconds` | GC pause ì‹œê°„ | p99 < 10ms |\n| `go_memstats_heap_alloc_bytes` | í˜„ì¬ í™ ì‚¬ìš©ëŸ‰ | GOMEMLIMITì˜ 80% |\n| `go_memstats_gc_cpu_fraction` | GC CPU ì‚¬ìš©ë¥  | < 5% |\n\n## ì°¸ê³  ìë£Œ\n\n- [Go GC Guide](https://tip.golang.org/doc/gc-guide)\n- [Go 1.19 Memory Limit](https://go.dev/blog/go1.19)\n- [Getting to Go: The Journey of Go's Garbage Collector](https://go.dev/blog/ismmkeynote)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Go",
      "Performance"
    ],
    "readingTime": 4,
    "wordCount": 740,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "buf-protobuf-management",
    "slug": "buf-protobuf-management",
    "path": "backend/go",
    "fullPath": "backend/go/buf-protobuf-management",
    "title": "Buf v2 ê¸°ë°˜ Proto ê´€ë¦¬ ë° ì½”ë“œ ìë™ ìƒì„±",
    "excerpt": "Buf v2ë¥¼ í™œìš©í•˜ì—¬ Protobuf ìŠ¤í‚¤ë§ˆë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³ , gRPC ì„œë²„/í´ë¼ì´ì–¸íŠ¸, HTTP Gateway, OpenAPI ìŠ¤í™ì„ ìë™ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# Buf v2 ê¸°ë°˜ Proto ê´€ë¦¬ ë° ì½”ë“œ ìë™ ìƒì„±\n\n## ê°œìš”\n\nëŒ€ê·œëª¨ gRPC ì„œë¹„ìŠ¤ì—ì„œ **Protobuf ìŠ¤í‚¤ë§ˆ**ëŠ” ì„œë¹„ìŠ¤ ê³„ì•½ì˜ í•µì‹¬ì…ë‹ˆë‹¤. **Buf v2**ëŠ” Proto íŒŒì¼ì˜ ë¦°íŠ¸, Breaking Change ê°ì§€, ë‹¤ì¤‘ ì–¸ì–´ ì½”ë“œ ìƒì„±ì„ í†µí•© ê´€ë¦¬í•©ë‹ˆë‹¤.\n\n## ì™œ Bufì¸ê°€?\n\n### protoc ëŒ€ë¹„ ì¥ì \n\n| ê¸°ëŠ¥ | protoc | buf v2 |\n|------|--------|--------|\n| ì˜ì¡´ì„± ê´€ë¦¬ | ìˆ˜ë™ (include ê²½ë¡œ) | ìë™ (BSR/deps) |\n| ë¦°íŒ… | ë³„ë„ ë„êµ¬ í•„ìš” | ë‚´ì¥ + ì»¤ìŠ¤í…€ ê·œì¹™ |\n| Breaking Change | ì—†ìŒ | ìë™ ê°ì§€ |\n| í”ŒëŸ¬ê·¸ì¸ ê´€ë¦¬ | ë¡œì»¬ ì„¤ì¹˜ í•„ìˆ˜ | Remote Plugins ì§€ì› |\n| ì„¤ì • | ë³µì¡í•œ CLI í”Œë˜ê·¸ | YAML ì„¤ì • íŒŒì¼ |\n\n### ë‹¨ì \n\n| íŠ¹ì„± | ì„¤ëª… |\n|------|------|\n| í•™ìŠµ ê³¡ì„  | ì‹ ê·œ ì„¤ì • ì²´ê³„ ì´í•´ í•„ìš” |\n| BSR ì˜ì¡´ì„± | ì¼ë¶€ ê¸°ëŠ¥ì€ Buf Schema Registry í•„ìš” |\n| ë„¤íŠ¸ì›Œí¬ | Remote Plugins ì‚¬ìš© ì‹œ ì¸í„°ë„· ì—°ê²° í•„ìš” |\n\n## í”„ë¡œì íŠ¸ êµ¬ì¡°\n\n### Proto ì „ìš© ë ˆí¬ì§€í† ë¦¬\n\n```\nproto-service/\nâ”œâ”€â”€ buf.yaml              # ëª¨ë“ˆ ì„¤ì •\nâ”œâ”€â”€ buf.gen.yaml          # ì½”ë“œ ìƒì„± ì„¤ì •\nâ”œâ”€â”€ buf.lock              # ì˜ì¡´ì„± ë½ íŒŒì¼\nâ”œâ”€â”€ deps/                 # ë¡œì»¬ ì˜ì¡´ì„± (ì„ íƒ)\nâ”‚   â””â”€â”€ custom/\nâ”‚       â””â”€â”€ options.proto\nâ””â”€â”€ proto/\n    â””â”€â”€ v1beta/\n        â””â”€â”€ api.proto     # ì„œë¹„ìŠ¤ ì •ì˜\n```\n\n## ì„¤ì • íŒŒì¼\n\n### buf.yaml (ëª¨ë“ˆ ì„¤ì •)\n\n```yaml\n# buf.yaml\nversion: v2\nmodules:\n  - path: proto/v1beta\n    name: buf.build/myorg/myservice\n  - path: deps/custom  # ë¡œì»¬ ì˜ì¡´ì„± ëª¨ë“ˆ\ndeps:\n  # ì™¸ë¶€ ì˜ì¡´ì„± (Google APIs, gRPC-Gateway ë“±)\n  - buf.build/googleapis/googleapis\n  - buf.build/grpc-ecosystem/grpc-gateway\n  - buf.build/gnostic/gnostic\nlint:\n  use:\n    - STANDARD\n  except:\n    - FIELD_NOT_REQUIRED\n    - PACKAGE_NO_IMPORT_CYCLE\n  disallow_comment_ignores: true\nbreaking:\n  use:\n    - FILE\n  except:\n    - EXTENSION_NO_DELETE\n    - FIELD_SAME_DEFAULT\n```\n\n### buf.gen.yaml (ì½”ë“œ ìƒì„± ì„¤ì •)\n\n```yaml\n# buf.gen.yaml\nversion: v2\nmanaged:\n  enabled: true\n  disable:\n    - module: buf.build/googleapis/googleapis\n    - module: buf.build/grpc-ecosystem/grpc-gateway\n    - module: buf.build/gnostic/gnostic\n  override:\n    - file_option: go_package_prefix\n      value: github.com/myorg/myservice/generated/go/proto/\nplugins:\n  # Go Protobuf ë©”ì‹œì§€\n  - remote: buf.build/protocolbuffers/go:v1.36.2\n    out: generated/go/proto\n    opt: paths=source_relative\n  \n  # gRPC Go ì„œë²„/í´ë¼ì´ì–¸íŠ¸\n  - remote: buf.build/grpc/go:v1.5.1\n    out: generated/go/proto\n    opt: paths=source_relative\n  \n  # gRPC-Gateway (HTTP í•¸ë“¤ëŸ¬)\n  - remote: buf.build/grpc-ecosystem/gateway:v2.25.1\n    out: generated/go/proto/gateway\n    opt:\n      - paths=source_relative\n      - standalone=true\n  \n  # OpenAPI ìŠ¤í™ ìë™ ìƒì„±\n  - remote: buf.build/community/google-gnostic-openapi:v0.7.0\n    out: generated/docs\n    opt: paths=source_relative\n\ninputs:\n  - directory: proto/v1beta\n  - proto_file: deps/custom/options.proto  # íŠ¹ì • íŒŒì¼ë§Œ í¬í•¨\n```\n\n## Proto ì‘ì„± ì˜ˆì‹œ\n\n### ì„œë¹„ìŠ¤ ì •ì˜\n\n```protobuf\n// proto/v1beta/api.proto\nsyntax = \"proto3\";\n\npackage v1beta;\n\nimport \"gnostic/openapi/v3/annotations.proto\";\nimport \"google/api/annotations.proto\";\nimport \"google/protobuf/struct.proto\";\nimport \"google/protobuf/timestamp.proto\";\n\n// DocumentServiceëŠ” ë²„ì „ ê´€ë¦¬ ë¬¸ì„œë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.\nservice DocumentService {\n  // ìƒˆ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ë²„ì „ 1ë¡œ ì‹œì‘ë©ë‹ˆë‹¤.\n  rpc CreateDocument(CreateDocumentRequest) returns (CreateDocumentResponse) {\n    option (google.api.http) = {\n      post: \"/v1beta/collections/{collection}/documents\"\n      body: \"*\"\n    };\n  }\n  \n  // URIë¡œ ë¬¸ì„œë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤. ë²„ì „ ë¯¸ì§€ì • ì‹œ ìµœì‹  ë²„ì „ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n  rpc GetDocument(GetDocumentRequest) returns (GetDocumentResponse) {\n    option (google.api.http) = {\n      get: \"/v1beta/collections/{collection}/documents/{uri}\"\n    };\n  }\n  \n  // ë¬¸ì„œë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ìƒˆ ë²„ì „ì´ ìƒì„±ë©ë‹ˆë‹¤.\n  rpc UpdateDocument(UpdateDocumentRequest) returns (UpdateDocumentResponse) {\n    option (google.api.http) = {\n      patch: \"/v1beta/collections/{collection}/documents/{uri}\"\n      body: \"*\"\n    };\n  }\n  \n  // ë¬¸ì„œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤. ì†Œí”„íŠ¸ ì‚­ì œë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.\n  rpc DeleteDocument(DeleteDocumentRequest) returns (DeleteDocumentResponse) {\n    option (google.api.http) = {\n      delete: \"/v1beta/collections/{collection}/documents/{uri}\"\n    };\n  }\n}\n\n// ë¬¸ì„œ ìƒíƒœ\nenum DocumentStatus {\n  DOCUMENT_STATUS_UNSPECIFIED = 0;\n  DOCUMENT_STATUS_ACTIVE = 1;\n  DOCUMENT_STATUS_DELETED = 2;\n}\n\n// ë¬¸ì„œ ëª¨ë¸\nmessage Document {\n  // MongoDB ObjectID ë¬¸ìì—´\n  optional string id = 1 [(gnostic.openapi.v3.property) = {\n    description: \"ë¬¸ì„œì˜ ë°ì´í„°ë² ì´ìŠ¤ ID\"\n    nullable: true\n  }];\n  \n  // ë…¼ë¦¬ì  ë¬¸ì„œ ì‹ë³„ì (ë²„ì „ ì „ì²´ì—ì„œ ê³µìœ )\n  string uri = 2 [(gnostic.openapi.v3.property) = {\n    description: \"ë¬¸ì„œì˜ ê³ ìœ  ì‹ë³„ì\"\n    nullable: false\n  }];\n  \n  // JSON ìŠ¤í‚¤ë§ˆë¥¼ ì¤€ìˆ˜í•˜ëŠ” ë¬¸ì„œ ë°ì´í„°\n  google.protobuf.Struct fields = 3 [(gnostic.openapi.v3.property) = {\n    description: \"ë¬¸ì„œ í•„ë“œ ë°ì´í„°\"\n    nullable: false\n  }];\n  \n  // ë²„ì „ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘, ì—…ë°ì´íŠ¸ë§ˆë‹¤ ì¦ê°€)\n  int32 version = 4;\n  \n  // ë¬¸ì„œ ìƒíƒœ\n  DocumentStatus status = 5;\n  \n  // ìƒì„± ì‹œê°\n  google.protobuf.Timestamp created_at = 6;\n  \n  // ì—…ë°ì´íŠ¸ ì‹œê°\n  google.protobuf.Timestamp updated_at = 7;\n}\n\n// ìš”ì²­/ì‘ë‹µ ë©”ì‹œì§€ë“¤\nmessage CreateDocumentRequest {\n  string collection = 1;\n  DocumentInput document = 2;\n}\n\nmessage DocumentInput {\n  string uri = 1;\n  google.protobuf.Struct fields = 2;\n}\n\nmessage CreateDocumentResponse {\n  Document document = 1;\n}\n\nmessage GetDocumentRequest {\n  string collection = 1;\n  string uri = 2;\n  optional int32 version = 3;  // ë¯¸ì§€ì • ì‹œ ìµœì‹  ë²„ì „\n}\n\nmessage GetDocumentResponse {\n  Document document = 1;\n}\n\nmessage UpdateDocumentRequest {\n  string collection = 1;\n  string uri = 2;\n  google.protobuf.Struct fields = 3;\n  optional int32 expected_version = 4;  // Optimistic Locking\n}\n\nmessage UpdateDocumentResponse {\n  Document document = 1;\n}\n\nmessage DeleteDocumentRequest {\n  string collection = 1;\n  string uri = 2;\n}\n\nmessage DeleteDocumentResponse {\n  bool success = 1;\n}\n```\n\n## ì½”ë“œ ìƒì„±\n\n### ê¸°ë³¸ ìƒì„±\n\n```bash\n# ì˜ì¡´ì„± ì—…ë°ì´íŠ¸\nbuf mod update\n\n# ì½”ë“œ ìƒì„±\nbuf generate\n\n# ìƒì„± êµ¬ì¡°\ngenerated/\nâ”œâ”€â”€ go/\nâ”‚   â””â”€â”€ proto/\nâ”‚       â”œâ”€â”€ v1beta/\nâ”‚       â”‚   â”œâ”€â”€ api.pb.go         # ë©”ì‹œì§€ ì •ì˜\nâ”‚       â”‚   â””â”€â”€ api_grpc.pb.go    # gRPC ì„œë²„/í´ë¼ì´ì–¸íŠ¸\nâ”‚       â””â”€â”€ gateway/\nâ”‚           â””â”€â”€ v1beta/\nâ”‚               â””â”€â”€ api.pb.gw.go  # HTTP Gateway\nâ””â”€â”€ docs/\n    â””â”€â”€ v1beta/\n        â””â”€â”€ openapi.yaml          # OpenAPI ìŠ¤í™\n```\n\n### íŠ¹ì • ê²½ë¡œë§Œ ìƒì„±\n\n```bash\nbuf generate --path proto/v1beta/api.proto\n```\n\n## ìƒì„±ëœ ì½”ë“œ í™œìš©\n\n### gRPC ì„œë²„\n\n```go\npackage main\n\nimport (\n    \"net\"\n    \n    \"google.golang.org/grpc\"\n    pb \"github.com/myorg/myservice/generated/go/proto/v1beta\"\n)\n\ntype documentServer struct {\n    pb.UnimplementedDocumentServiceServer\n    service DocumentService\n}\n\nfunc (s *documentServer) CreateDocument(ctx context.Context, req *pb.CreateDocumentRequest) (*pb.CreateDocumentResponse, error) {\n    doc, err := s.service.Create(ctx, req.Collection, req.Document)\n    if err != nil {\n        return nil, err\n    }\n    return &pb.CreateDocumentResponse{Document: doc}, nil\n}\n\nfunc main() {\n    lis, _ := net.Listen(\"tcp\", \":9090\")\n    \n    grpcServer := grpc.NewServer()\n    pb.RegisterDocumentServiceServer(grpcServer, &documentServer{})\n    \n    grpcServer.Serve(lis)\n}\n```\n\n### HTTP Gateway\n\n```go\npackage main\n\nimport (\n    \"net/http\"\n    \n    \"github.com/grpc-ecosystem/grpc-gateway/v2/runtime\"\n    gw \"github.com/myorg/myservice/generated/go/proto/gateway/v1beta\"\n)\n\nfunc main() {\n    ctx := context.Background()\n    mux := runtime.NewServeMux()\n    \n    // gRPC ì„œë²„ì— ì—°ê²°í•˜ì—¬ HTTP ìš”ì²­ í”„ë¡ì‹œ\n    opts := []grpc.DialOption{grpc.WithTransportCredentials(insecure.NewCredentials())}\n    err := gw.RegisterDocumentServiceHandlerFromEndpoint(ctx, mux, \"localhost:9090\", opts)\n    if err != nil {\n        panic(err)\n    }\n    \n    // HTTP ì„œë²„ ì‹œì‘\n    http.ListenAndServe(\":8080\", mux)\n}\n```\n\n### OpenAPI ìŠ¤í™\n\nìƒì„±ëœ `openapi.yaml`ì„ Swagger UIì™€ í•¨ê»˜ ì œê³µ:\n\n```go\nfunc main() {\n    // ... Gateway ì„¤ì • ...\n    \n    // OpenAPI ìŠ¤í™ ì œê³µ\n    http.HandleFunc(\"/openapi.yaml\", func(w http.ResponseWriter, r *http.Request) {\n        http.ServeFile(w, r, \"generated/docs/v1beta/openapi.yaml\")\n    })\n}\n```\n\n## ë¦°íŒ… ë° Breaking Change ê°ì§€\n\n### ë¦°íŠ¸ ì‹¤í–‰\n\n```bash\nbuf lint\n\n# íŠ¹ì • íŒŒì¼ë§Œ\nbuf lint --path proto/v1beta/api.proto\n\n# ì—ëŸ¬ ì¶œë ¥ ì˜ˆì‹œ:\n# proto/v1beta/api.proto:15:3:Field \"id\" should be marked as optional.\n```\n\n### Breaking Change ê°ì§€\n\n```bash\n# í˜„ì¬ ë¸Œëœì¹˜ vs main\nbuf breaking --against '.git#branch=main'\n\n# í˜„ì¬ vs ì´ì „ ì»¤ë°‹\nbuf breaking --against '.git#ref=HEAD~1'\n\n# í˜„ì¬ vs BSR ìµœì‹  ë²„ì „\nbuf breaking --against 'buf.build/myorg/myservice'\n```\n\n## CI/CD í†µí•©\n\n### GitHub Actions\n\n```yaml\n# .github/workflows/proto.yml\nname: Proto CI\n\non:\n  push:\n    paths: ['proto/**', 'buf.*']\n  pull_request:\n    paths: ['proto/**', 'buf.*']\n\njobs:\n  lint-and-check:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n      - uses: bufbuild/buf-setup-action@v1\n        with:\n          version: latest\n      \n      - name: Lint\n        run: buf lint\n      \n      - name: Breaking Change Check\n        run: buf breaking --against 'https://github.com/${{ github.repository }}.git#branch=main'\n  \n  generate:\n    needs: lint-and-check\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n      - uses: bufbuild/buf-setup-action@v1\n      \n      - name: Generate Code\n        run: buf generate\n      \n      - name: Check for uncommitted changes\n        run: |\n          if [[ -n $(git status --porcelain generated/) ]]; then\n            echo \"Generated code is out of sync!\"\n            git diff generated/\n            exit 1\n          fi\n```\n\n## Makefile í†µí•©\n\n```makefile\n.PHONY: proto-deps proto-lint proto-breaking proto-gen proto-clean\n\n# ì˜ì¡´ì„± ì—…ë°ì´íŠ¸\nproto-deps:\n buf mod update\n\n# ë¦°íŠ¸\nproto-lint:\n buf lint\n\n# Breaking Change ê²€ì‚¬\nproto-breaking:\n buf breaking --against '.git#branch=main'\n\n# ì½”ë“œ ìƒì„±\nproto-gen:\n buf generate\n\n# ì •ë¦¬\nproto-clean:\n rm -rf generated/\n\n# ì „ì²´ ë¹Œë“œ\nproto-all: proto-deps proto-lint proto-gen\n```\n\n## ëª¨ë²” ì‚¬ë¡€\n\n1. **ë²„ì „ ë„¤ì´ë°**: íŒ¨í‚¤ì§€ì— `v1`, `v1beta` ë“± ë²„ì „ í¬í•¨\n2. **Breaking Change CI**: PRë§ˆë‹¤ ìë™ ê²€ì‚¬\n3. **ìƒì„± ì½”ë“œ ì»¤ë°‹**: `.gitignore`ì— `generated/` ì¶”ê°€ ê¶Œì¥\n4. **Proto ì£¼ì„**: ì„œë¹„ìŠ¤/ë©”ì‹œì§€ ì£¼ì„ì€ ìƒì„± ì½”ë“œì™€ OpenAPIì— ë°˜ì˜ë¨\n5. **Optional ëª…ì‹œ**: Proto3ì—ì„œ `optional` í‚¤ì›Œë“œë¡œ nullable ëª…í™•íˆ í‘œí˜„\n\n## ì°¸ê³  ìë£Œ\n\n- [Buf ê³µì‹ ë¬¸ì„œ](https://buf.build/docs/)\n- [Buf Schema Registry](https://buf.build/docs/bsr/introduction)\n- [gRPC-Gateway](https://grpc-ecosystem.github.io/grpc-gateway/)\n- [gnostic OpenAPI](https://github.com/google/gnostic)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "Go",
      "OpenAPI",
      "buf",
      "gRPC"
    ],
    "readingTime": 7,
    "wordCount": 1210,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "ìŠ¤íŒŒì´í¬-íŠ¸ë˜í”½-ëŒ€ì‘-ì „ëµ",
    "slug": "seupaikeu-teuraepig-daeeung-jeonryag",
    "path": "backend/devops",
    "fullPath": "backend/devops/seupaikeu-teuraepig-daeeung-jeonryag",
    "title": "ìŠ¤íŒŒì´í¬ íŠ¸ë˜í”½ ëŒ€ì‘ ì „ëµ",
    "excerpt": "í‹°ì¼“íŒ…, ì´ë²¤íŠ¸ ì‘ëª¨, í”Œë˜ì‹œ ì„¸ì¼ ë“± ìˆœê°„ì ìœ¼ë¡œ í­ë°œí•˜ëŠ” ì“°ê¸° ìš”ì²­ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì•„í‚¤í…ì²˜ ì „ëµì„ ë‹¤ë£¹ë‹ˆë‹¤.",
    "content": "# ìŠ¤íŒŒì´í¬ íŠ¸ë˜í”½ ëŒ€ì‘ ì „ëµ\n\nì¼ë°˜ì ì¸ ì„œë¹„ìŠ¤ ìŠ¤ì¼€ì¼ë§ ì „ëµì€ **ì½ê¸° ì¤‘ì‹¬**ì˜ ì ì§„ì ì¸ ì„±ì¥ì„ ê°€ì •í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ë‹¤ìŒê³¼ ê°™ì€ ì„œë¹„ìŠ¤ëŠ” ì™„ì „íˆ ë‹¤ë¥¸ íŒ¨í„´ì„ ë³´ì…ë‹ˆë‹¤:\n\n- **í‹°ì¼“íŒ… ì‹œìŠ¤í…œ**: ì½˜ì„œíŠ¸ í‹°ì¼“ ì˜¤í”ˆ ì‹œ ìˆ˜ì‹­ë§Œ ëª…ì´ ë™ì‹œì— êµ¬ë§¤ ì‹œë„\n- **ì´ë²¤íŠ¸ ì‘ëª¨**: ì„ ì°©ìˆœ ì¿ í° ë°œê¸‰\n- **í”Œë˜ì‹œ ì„¸ì¼**: íŠ¹ì • ì‹œê°„ì— í• ì¸ ìƒí’ˆ íŒë§¤\n- **ì‹¤ì‹œê°„ íˆ¬í‘œ/ì„¤ë¬¸**: ë°©ì†¡ ì¤‘ ì‹œì²­ì íˆ¬í‘œ\n\nì´ëŸ° ì„œë¹„ìŠ¤ì˜ íŠ¹ì§•ì€ **ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì‹œì ì— í­ë°œì ì¸ ì“°ê¸° ìš”ì²­**ì´ ë°œìƒí•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.\n\n## ë¬¸ì œ: ì™œ ì¼ë°˜ì ì¸ ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ë¶€ì¡±í•œê°€?\n\n### ì½ê¸° vs ì“°ê¸° í™•ì¥ì˜ ì°¨ì´\n\n```mermaid\nflowchart TB\n    subgraph ì½ê¸°í™•ì¥[\"ì½ê¸° í™•ì¥ (ì‰¬ì›€)\"]\n        Cache[Redis Cache]\n        R1[(Read Replica 1)]\n        R2[(Read Replica 2)]\n        R3[(Read Replica 3)]\n    end\n    \n    subgraph ì“°ê¸°í™•ì¥[\"ì“°ê¸° í™•ì¥ (ì–´ë ¤ì›€)\"]\n        Master[(Master DB)]\n        Lock[Row Lock]\n        Constraint[Unique Constraint]\n    end\n```\n\n**ì½ê¸° í™•ì¥**ì€ ìºì‹œì™€ ë³µì œë³¸ìœ¼ë¡œ ê±°ì˜ ë¬´í•œëŒ€ë¡œ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ë©´ **ì“°ê¸° í™•ì¥**ì€:\n\n- ëª¨ë“  ì“°ê¸°ê°€ Master DBë¡œ ì§‘ì¤‘\n- ë™ì‹œì„± ì œì–´ë¥¼ ìœ„í•œ ë½(Lock) ê²½í•©\n- ë°ì´í„° ì •í•©ì„± ë³´ì¥ í•„ìš”\n\n### ì‹¤ì œ ë³‘ëª© ì‚¬ë¡€\n\n```python\n# ì„ ì°©ìˆœ 100ëª… ì¿ í° ë°œê¸‰ - ë¬¸ì œê°€ ìˆëŠ” ì½”ë“œ\ndef issue_coupon(user_id):\n    coupon = db.query(\"SELECT * FROM coupons WHERE issued = false LIMIT 1 FOR UPDATE\")\n    if coupon:\n        db.execute(\"UPDATE coupons SET issued = true, user_id = ? WHERE id = ?\", \n                   user_id, coupon.id)\n        return coupon\n    return None\n```\n\n10ë§Œ ëª…ì´ ë™ì‹œì— ìš”ì²­í•˜ë©´:\n\n- `FOR UPDATE` ë½ìœ¼ë¡œ ì¸í•œ ëŒ€ê¸°ì—´ í˜•ì„±\n- DB ì—°ê²° í’€ ê³ ê°ˆ\n- íƒ€ì„ì•„ì›ƒ ë°œìƒ\n- ì„œë¹„ìŠ¤ ì „ì²´ ë§ˆë¹„\n\n## ì „ëµ 1: ë©”ì‹œì§€ íë¡œ ë¶€í•˜ ë¶„ì‚°\n\nê°€ì¥ ê¸°ë³¸ì ì´ë©´ì„œ íš¨ê³¼ì ì¸ ì „ëµì…ë‹ˆë‹¤. ë™ê¸° ì²˜ë¦¬ë¥¼ ë¹„ë™ê¸°ë¡œ ì „í™˜í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    Client[Client] --> API[API Server]\n    API -->|ì¦‰ì‹œ ì‘ë‹µ| Client\n    API -->|Enqueue| Queue[(Message Queue)]\n    Queue --> Worker1[Worker 1]\n    Queue --> Worker2[Worker 2]\n    Queue --> Worker3[Worker 3]\n    Worker1 --> DB[(DB)]\n    Worker2 --> DB\n    Worker3 --> DB\n```\n\n### êµ¬í˜„ ì˜ˆì‹œ\n\n```python\n# API ë ˆì´ì–´ - ì¦‰ì‹œ ì‘ë‹µ\nasync def apply_coupon(user_id: str):\n    request_id = uuid.uuid4()\n    await redis.lpush(\"coupon:queue\", json.dumps({\n        \"request_id\": request_id,\n        \"user_id\": user_id,\n        \"timestamp\": time.time()\n    }))\n    return {\"status\": \"queued\", \"request_id\": request_id}\n\n# Worker - ìˆœì°¨ ì²˜ë¦¬\nasync def process_coupon_queue():\n    while True:\n        request = await redis.brpop(\"coupon:queue\")\n        data = json.loads(request)\n        \n        # ì´ë¯¸ ë°œê¸‰ë°›ì•˜ëŠ”ì§€ í™•ì¸\n        if await redis.sismember(\"coupon:issued_users\", data[\"user_id\"]):\n            continue\n            \n        # ì¬ê³  í™•ì¸ ë° ë°œê¸‰ (ì›ìì  ì—°ì‚°)\n        remaining = await redis.decr(\"coupon:remaining\")\n        if remaining >= 0:\n            await redis.sadd(\"coupon:issued_users\", data[\"user_id\"])\n            await notify_user(data[\"user_id\"], \"ì¿ í° ë°œê¸‰ ì™„ë£Œ!\")\n        else:\n            await redis.incr(\"coupon:remaining\")  # ë¡¤ë°±\n            await notify_user(data[\"user_id\"], \"ì¿ í°ì´ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤\")\n```\n\n**ì¥ì :**\n\n- API ì„œë²„ëŠ” ì¦‰ì‹œ ì‘ë‹µ â†’ íƒ€ì„ì•„ì›ƒ ë°©ì§€\n- ì²˜ë¦¬ëŸ‰ì„ Worker ìˆ˜ë¡œ ì œì–´ ê°€ëŠ¥\n- DB ë¶€í•˜ ì˜ˆì¸¡ ê°€ëŠ¥\n\n**ë‹¨ì :**\n\n- ì‚¬ìš©ìì—ê²Œ ì¦‰ì‹œ ê²°ê³¼ë¥¼ ì•Œë ¤ì£¼ê¸° ì–´ë ¤ì›€\n- ì¶”ê°€ ì¸í”„ë¼(í, Worker) í•„ìš”\n\n## ì „ëµ 2: ìš”ì²­ ì œí•œ (Rate Limiting + ê°€ìƒ ëŒ€ê¸°ì—´)\n\nì‚¬ìš©ì ê²½í—˜ì„ ê°œì„ í•˜ë©´ì„œ ì‹œìŠ¤í…œì„ ë³´í˜¸í•˜ëŠ” ì „ëµì…ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    Users[10ë§Œ ëª… ë™ì‹œ ì ‘ì†] --> Gate[Rate Limiter]\n    Gate -->|ì´ˆë‹¹ 1000ëª…| Queue[ê°€ìƒ ëŒ€ê¸°ì—´]\n    Gate -->|ë‚˜ë¨¸ì§€| Waiting[ëŒ€ê¸° í™”ë©´]\n    Queue --> API[API Server]\n    API --> DB[(DB)]\n    \n    subgraph ì‚¬ìš©ìí™”ë©´[\"ì‚¬ìš©ì í™”ë©´\"]\n        Waiting -->|10ì´ˆë§ˆë‹¤ í´ë§| Position[ëŒ€ê¸° ìˆœë²ˆ í‘œì‹œ]\n    end\n```\n\n### ê°€ìƒ ëŒ€ê¸°ì—´ êµ¬í˜„\n\n```python\n# ì…ì¥ í† í° ë°œê¸‰\nasync def enter_queue(user_id: str):\n    queue_position = await redis.incr(\"queue:position\")\n    token = jwt.encode({\n        \"user_id\": user_id,\n        \"position\": queue_position,\n        \"exp\": time.time() + 600  # 10ë¶„ ìœ íš¨\n    }, SECRET_KEY)\n    return {\"token\": token, \"position\": queue_position}\n\n# í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ìˆœë²ˆ í™•ì¸\nasync def get_current_position():\n    return await redis.get(\"queue:current\") or 0\n\n# ì‹¤ì œ êµ¬ë§¤ í˜ì´ì§€ ì§„ì… í—ˆìš© ì—¬ë¶€\nasync def can_enter(token: str):\n    data = jwt.decode(token, SECRET_KEY)\n    current = await get_current_position()\n    \n    # ë‚´ ìˆœë²ˆì´ í˜„ì¬ ì²˜ë¦¬ ìˆœë²ˆ + 100 ì´ë‚´ë©´ ì§„ì… í—ˆìš©\n    if data[\"position\"] <= current + 100:\n        return True\n    return False\n```\n\n**ì¥ì :**\n\n- ì‚¬ìš©ìì—ê²Œ ëŒ€ê¸° ìƒí™© íˆ¬ëª…í•˜ê²Œ ê³µê°œ\n- ì‹œìŠ¤í…œ ê³¼ë¶€í•˜ ë°©ì§€\n- ê³µì •í•œ ìˆœì„œ ë³´ì¥\n\n## ì „ëµ 3: ì‚¬ì „ ì¬ê³  í• ë‹¹ (Quota Pre-allocation)\n\nì¬ê³  ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë¶„í• í•˜ì—¬ ë½ ê²½í•©ì„ ì œê±°í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart TB\n    subgraph ì‚¬ì „í• ë‹¹[\"ì‚¬ì „ ì¬ê³  í• ë‹¹\"]\n        Total[ì´ ì¬ê³ : 1000ê°œ]\n        Total --> S1[Server 1: 200ê°œ]\n        Total --> S2[Server 2: 200ê°œ]\n        Total --> S3[Server 3: 200ê°œ]\n        Total --> S4[Server 4: 200ê°œ]\n        Total --> S5[Server 5: 200ê°œ]\n    end\n    \n    User1[User] --> S1\n    User2[User] --> S2\n    User3[User] --> S3\n```\n\n### êµ¬í˜„ ì˜ˆì‹œ\n\n```python\n# ì„œë²„ ì‹œì‘ ì‹œ ì¬ê³  í• ë‹¹\nSERVER_ID = os.environ.get(\"SERVER_ID\")\nLOCAL_QUOTA_KEY = f\"quota:{SERVER_ID}\"\n\nasync def initialize_quota():\n    # Coordinatorì—ì„œ í• ë‹¹ë°›ì€ ìˆ˜ëŸ‰\n    allocated = await coordinator.allocate_quota(SERVER_ID, 200)\n    await redis.set(LOCAL_QUOTA_KEY, allocated)\n\n# êµ¬ë§¤ ì²˜ë¦¬ - ë¡œì»¬ Redisë§Œ ì‚¬ìš©\nasync def purchase(user_id: str):\n    remaining = await redis.decr(LOCAL_QUOTA_KEY)\n    if remaining >= 0:\n        await redis.sadd(\"purchased_users\", user_id)\n        return {\"success\": True}\n    else:\n        await redis.incr(LOCAL_QUOTA_KEY)  # ë¡¤ë°±\n        \n        # ë‹¤ë¥¸ ì„œë²„ì— ì¬ê³ ê°€ ë‚¨ì•˜ëŠ”ì§€ í™•ì¸\n        for server_id in await get_other_servers():\n            result = await try_other_server(server_id, user_id)\n            if result:\n                return result\n        \n        return {\"success\": False, \"message\": \"í’ˆì ˆ\"}\n```\n\n**ì¥ì :**\n\n- DB ë½ ê²½í•© ì™„ì „ ì œê±°\n- ê° ì„œë²„ê°€ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ â†’ ìˆ˜í‰ í™•ì¥ ìš©ì´\n- ì´ˆë‹¹ ìˆ˜ë§Œ ê±´ ì²˜ë¦¬ ê°€ëŠ¥\n\n**ë‹¨ì :**\n\n- ì¬ê³  ë¶ˆê· í˜• ë°œìƒ ê°€ëŠ¥\n- êµ¬í˜„ ë³µì¡ë„ ì¦ê°€\n\n## ì „ëµ 4: ì´ë²¤íŠ¸ ì†Œì‹± + CQRS\n\në³µì¡í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì´ í•„ìš”í•œ ê²½ìš°, ì“°ê¸°ì™€ ì½ê¸°ë¥¼ ì™„ì „íˆ ë¶„ë¦¬í•©ë‹ˆë‹¤.\n\n```mermaid\nflowchart LR\n    subgraph Command[\"Command (ì“°ê¸°)\"]\n        API[API] --> EventStore[(Event Store)]\n        EventStore --> Processor[Event Processor]\n    end\n    \n    subgraph Query[\"Query (ì½ê¸°)\"]\n        Processor --> ReadDB[(Read DB)]\n        ReadDB --> QueryAPI[Query API]\n    end\n    \n    Client[Client] --> API\n    Client --> QueryAPI\n```\n\n### í•µì‹¬ ê°œë…\n\n```python\n# ì´ë²¤íŠ¸ ì €ì¥ (Append Only)\nasync def apply_for_event(user_id: str, event_id: str):\n    event = {\n        \"type\": \"APPLICATION_SUBMITTED\",\n        \"user_id\": user_id,\n        \"event_id\": event_id,\n        \"timestamp\": datetime.utcnow().isoformat(),\n        \"sequence\": await redis.incr(f\"event:{event_id}:seq\")\n    }\n    \n    # Kafka, EventStore ë“±ì— ì €ì¥\n    await kafka.send(\"applications\", event)\n    return {\"status\": \"accepted\", \"sequence\": event[\"sequence\"]}\n\n# ì´ë²¤íŠ¸ ì²˜ë¦¬ (ë¹„ë™ê¸°)\nasync def process_application_event(event):\n    if event[\"type\"] == \"APPLICATION_SUBMITTED\":\n        # ì¤‘ë³µ ì²´í¬, ìê²© í™•ì¸ ë“± ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§\n        if await is_eligible(event[\"user_id\"], event[\"event_id\"]):\n            await update_read_model(event)\n```\n\n**ì¥ì :**\n\n- ì“°ê¸° ì²˜ë¦¬ëŸ‰ ê·¹ëŒ€í™” (Append Only)\n- ì™„ë²½í•œ ê°ì‚¬ ë¡œê·¸\n- ì‹œê°„ ì—¬í–‰(íŠ¹ì • ì‹œì  ìƒíƒœ ë³µì›) ê°€ëŠ¥\n\n**ë‹¨ì :**\n\n- ìµœì¢… ì¼ê´€ì„±(Eventual Consistency)\n- í•™ìŠµ ê³¡ì„ ì´ ë†’ìŒ\n\n## ì „ëµ 5: í•˜ë“œì›¨ì–´ ë ˆë²¨ ìµœì í™”\n\nì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…ì²˜ ì™¸ì—ë„ í•˜ë“œì›¨ì–´ ì„ íƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n\n### ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ\n\n| ìš”êµ¬ì‚¬í•­ | ê¶Œì¥ DB |\n|----------|---------|\n| ë‹¨ìˆœ ì¹´ìš´í„°/ì¬ê³  | Redis (INCR/DECR) |\n| íŠ¸ëœì­ì…˜ í•„ìˆ˜ | PostgreSQL, MySQL |\n| ëŒ€ìš©ëŸ‰ ì“°ê¸° | ScyllaDB, Cassandra |\n| ì´ë²¤íŠ¸ ë¡œê·¸ | Kafka, Apache Pulsar |\n\n### ë„¤íŠ¸ì›Œí¬ ìµœì í™”\n\n```mermaid\nflowchart LR\n    User[User] --> CDN[CDN Edge]\n    CDN -->|Static| Static[ì •ì  ìì‚°]\n    CDN -->|Dynamic| Origin[Origin Server]\n    \n    subgraph Region[\"Same Region\"]\n        Origin --> Redis[(Redis)]\n        Origin --> DB[(DB)]\n    end\n```\n\n- **ê°™ì€ ë¦¬ì „ì— ëª¨ë“  ì¸í”„ë¼ ë°°ì¹˜**: ë„¤íŠ¸ì›Œí¬ ì§€ì—° ìµœì†Œí™”\n- **Connection Pooling**: DB ì—°ê²° ì¬ì‚¬ìš©\n- **Keep-Alive**: HTTP ì—°ê²° ì¬ì‚¬ìš©\n\n## ì‹¤ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸\n\níŠ¸ë˜í”½ ìŠ¤íŒŒì´í¬ì— ëŒ€ë¹„í•œ ì²´í¬ë¦¬ìŠ¤íŠ¸:\n\n| í•­ëª© | ì²´í¬ |\n|------|------|\n| Rate Limiting ì„¤ì • | â˜ |\n| ë©”ì‹œì§€ í ë„ì… | â˜ |\n| ìºì‹œ ì›Œë°ì—… ì™„ë£Œ | â˜ |\n| DB ì»¤ë„¥ì…˜ í’€ í¬ê¸° ì¡°ì • | â˜ |\n| ì˜¤í† ìŠ¤ì¼€ì¼ë§ í…ŒìŠ¤íŠ¸ | â˜ |\n| ë¶€í•˜ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰ | â˜ |\n| ì¥ì•  ë°œìƒ ì‹œ Fallback ì¤€ë¹„ | â˜ |\n| ëª¨ë‹ˆí„°ë§/ì•Œë¦¼ ì„¤ì • | â˜ |\n\n## ì •ë¦¬\n\nìŠ¤íŒŒì´í¬ì„± ì“°ê¸° íŠ¸ë˜í”½ ëŒ€ì‘ì˜ í•µì‹¬ ì›ì¹™:\n\n1. **ë™ê¸° â†’ ë¹„ë™ê¸° ì „í™˜**: ì¦‰ì‹œ ì‘ë‹µí•˜ê³  ë‚˜ì¤‘ì— ì²˜ë¦¬\n2. **ë½ ê²½í•© ìµœì†Œí™”**: ì‚¬ì „ í• ë‹¹, ë‚™ê´€ì  ë½ í™œìš©\n3. **ì‚¬ìš©ì ê²½í—˜ ê³ ë ¤**: ëŒ€ê¸°ì—´ ìˆœë²ˆ í‘œì‹œ, ì§„í–‰ ìƒí™© ì•Œë¦¼\n4. **Fail-safe ì„¤ê³„**: ê³¼ë¶€í•˜ ì‹œ ìš°ì•„í•œ ì„±ëŠ¥ ì €í•˜(Graceful Degradation)\n\n---\n\n## Related Posts\n\n- [ì„œë¹„ìŠ¤ ê·œëª¨ì— ë”°ë¥¸ ìŠ¤ì¼€ì¼ë§ ì „ëµ](/blog/backend/devops/ì„œë¹„ìŠ¤-ê·œëª¨ì—-ë”°ë¥¸-ìŠ¤ì¼€ì¼ë§-ì „ëµ) - ì¼ë°˜ì ì¸ ì½ê¸° ì¤‘ì‹¬ ì„œë¹„ìŠ¤ì˜ ìŠ¤ì¼€ì¼ë§\n\n## References\n\n- [Designing Data-Intensive Applications - Martin Kleppmann](https://dataintensive.net/)\n- [The Twelve-Factor App](https://12factor.net/)",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [
      "Architecture",
      "Backend",
      "DevOps",
      "Scaling",
      "Testing",
      "queue"
    ],
    "readingTime": 6,
    "wordCount": 1122,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "locust-e2e-testing",
    "slug": "locust-e2e-testing",
    "path": "backend/devops",
    "fullPath": "backend/devops/locust-e2e-testing",
    "title": "Locust ê¸°ë°˜ í™˜ê²½ë³„ E2E í…ŒìŠ¤íŠ¸ ìë™í™”",
    "excerpt": "Locustë¥¼ í™œìš©í•˜ì—¬ ê°œë°œë¶€í„° í”„ë¡œë•ì…˜ê¹Œì§€ í™˜ê²½ë³„ E2E í…ŒìŠ¤íŠ¸ë¥¼ ìë™í™”í•˜ê³  Kubernetesì—ì„œ ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# Locust ê¸°ë°˜ í™˜ê²½ë³„ E2E í…ŒìŠ¤íŠ¸ ìë™í™”\n\n## ê°œìš”\n\n**Locust**ëŠ” Python ê¸°ë°˜ì˜ ì˜¤í”ˆì†ŒìŠ¤ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ë„êµ¬ì…ë‹ˆë‹¤. ì½”ë“œë¡œ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‘ì„±í•˜ê³ , ë‹¤ì–‘í•œ í™˜ê²½(INT/STAGE/PROD)ì—ì„œ ì¼ê´€ëœ E2E í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## ì™œ Locustì¸ê°€?\n\n### ì¥ì \n\n| íŠ¹ì„± | ì„¤ëª… |\n|------|------|\n| **ì½”ë“œ ê¸°ë°˜** | Pythonìœ¼ë¡œ ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤ ì‘ì„± |\n| **ë¶„ì‚° ì‹¤í–‰** | ì—¬ëŸ¬ ì›Œì»¤ë¡œ ëŒ€ê·œëª¨ ë¶€í•˜ ìƒì„± |\n| **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§** | Web UIë¡œ ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ í™•ì¸ |\n| **ìœ ì—°ì„±** | ë‹¤ì–‘í•œ í”„ë¡œí† ì½œ ì§€ì› (HTTP, gRPC ë“±) |\n| **Kubernetes ì¹œí™”** | Job/Podìœ¼ë¡œ ì‰½ê²Œ ë°°í¬ |\n\n### ë‹¨ì \n\n| íŠ¹ì„± | ì„¤ëª… |\n|------|------|\n| **Python ì˜ì¡´** | Python í™˜ê²½ í•„ìš” |\n| **ì´ˆê¸° ì„¤ì •** | ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤ëŠ” ì½”ë“œ ì‘ì„± í•„ìš” |\n\n## í”„ë¡œì íŠ¸ êµ¬ì¡°\n\n```\ne2e/\nâ”œâ”€â”€ .python-version          # Python ë²„ì „\nâ”œâ”€â”€ pyproject.toml           # ì˜ì¡´ì„± ì •ì˜\nâ”œâ”€â”€ Makefile                 # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸\nâ”œâ”€â”€ config/\nâ”‚   â”œâ”€â”€ local.yaml           # ë¡œì»¬ í™˜ê²½ ì„¤ì •\nâ”‚   â”œâ”€â”€ int.yaml             # í†µí•© í™˜ê²½ ì„¤ì •\nâ”‚   â”œâ”€â”€ stage.yaml           # ìŠ¤í…Œì´ì§€ ì„¤ì •\nâ”‚   â””â”€â”€ prod.yaml            # í”„ë¡œë•ì…˜ ì„¤ì •\nâ”œâ”€â”€ suites/\nâ”‚   â”œâ”€â”€ smoke.py             # ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸\nâ”‚   â”œâ”€â”€ functional.py        # ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸\nâ”‚   â”œâ”€â”€ performance.py       # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\nâ”‚   â””â”€â”€ stress.py            # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸\nâ”œâ”€â”€ utils/\nâ”‚   â”œâ”€â”€ client.py            # API í´ë¼ì´ì–¸íŠ¸\nâ”‚   â””â”€â”€ data_generator.py    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±\nâ””â”€â”€ locustfile.py            # ë©”ì¸ ì§„ì…ì \n```\n\n## ì„¤ì • íŒŒì¼\n\n### pyproject.toml\n\n```toml\n[project]\nname = \"e2e-tests\"\nversion = \"1.0.0\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"locust>=2.20.0\",\n    \"pyyaml>=6.0\",\n    \"grpcio>=1.60.0\",\n    \"grpcio-tools>=1.60.0\",\n]\n\n[tool.uv]\ndev-dependencies = [\n    \"pytest>=8.0.0\",\n]\n```\n\n### í™˜ê²½ë³„ ì„¤ì •\n\n```yaml\n# config/int.yaml\nenvironment: int\nbase_url: https://api-int.example.com\ngrpc_host: grpc-int.example.com:443\n\nsettings:\n  default_timeout: 30\n  max_retries: 3\n\ntest_data:\n  collection_prefix: \"e2e_test_\"\n  cleanup_after: true\n```\n\n## í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ êµ¬í˜„\n\n### ê¸°ë³¸ í´ë¼ì´ì–¸íŠ¸\n\n```python\n# utils/client.py\nfrom typing import Any\nimport grpc\nfrom locust import events\n\nclass APIClient:\n    def __init__(self, base_url: str, timeout: int = 30):\n        self.base_url = base_url\n        self.timeout = timeout\n    \n    def create_document(self, collection: str, uri: str, fields: dict) -> dict:\n        \"\"\"ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n        response = self._post(\n            f\"/v1beta/collections/{collection}/documents\",\n            json={\n                \"document\": {\n                    \"uri\": uri,\n                    \"fields\": fields\n                }\n            }\n        )\n        return response.json()\n    \n    def get_document(self, collection: str, uri: str) -> dict:\n        \"\"\"ë¬¸ì„œë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n        response = self._get(f\"/v1beta/collections/{collection}/documents/{uri}\")\n        return response.json()\n    \n    def update_document(self, collection: str, uri: str, fields: dict) -> dict:\n        \"\"\"ë¬¸ì„œë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\"\"\"\n        response = self._patch(\n            f\"/v1beta/collections/{collection}/documents/{uri}\",\n            json={\"fields\": fields}\n        )\n        return response.json()\n    \n    def delete_document(self, collection: str, uri: str) -> bool:\n        \"\"\"ë¬¸ì„œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.\"\"\"\n        response = self._delete(f\"/v1beta/collections/{collection}/documents/{uri}\")\n        return response.json().get(\"success\", False)\n```\n\n### ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸\n\në¹ ë¥¸ í—¬ìŠ¤ì²´í¬ ë° ê¸°ë³¸ ê¸°ëŠ¥ í™•ì¸:\n\n```python\n# suites/smoke.py\nfrom locust import HttpUser, task, between\nimport uuid\n\nclass SmokeTestUser(HttpUser):\n    \"\"\"30ì´ˆ ë‚´ í•µì‹¬ ê¸°ëŠ¥ ê²€ì¦\"\"\"\n    \n    wait_time = between(0.5, 1)\n    \n    def on_start(self):\n        \"\"\"í…ŒìŠ¤íŠ¸ ì‹œì‘ ì „ ì´ˆê¸°í™”\"\"\"\n        self.collection = f\"smoke_test_{uuid.uuid4().hex[:8]}\"\n        self.created_docs = []\n    \n    @task(3)\n    def create_and_get_document(self):\n        \"\"\"ë¬¸ì„œ ìƒì„± ë° ì¡°íšŒ í…ŒìŠ¤íŠ¸\"\"\"\n        doc_uri = f\"doc-{uuid.uuid4().hex[:8]}\"\n        \n        # ìƒì„±\n        with self.client.post(\n            f\"/v1beta/collections/{self.collection}/documents\",\n            json={\n                \"document\": {\n                    \"uri\": doc_uri,\n                    \"fields\": {\"test\": True, \"timestamp\": str(time.time())}\n                }\n            },\n            catch_response=True\n        ) as response:\n            if response.status_code == 200:\n                self.created_docs.append(doc_uri)\n                response.success()\n            else:\n                response.failure(f\"Create failed: {response.text}\")\n        \n        # ì¡°íšŒ\n        with self.client.get(\n            f\"/v1beta/collections/{self.collection}/documents/{doc_uri}\",\n            catch_response=True\n        ) as response:\n            if response.status_code == 200:\n                data = response.json()\n                if data.get(\"document\", {}).get(\"uri\") == doc_uri:\n                    response.success()\n                else:\n                    response.failure(\"URI mismatch\")\n            else:\n                response.failure(f\"Get failed: {response.text}\")\n    \n    @task(1)\n    def health_check(self):\n        \"\"\"í—¬ìŠ¤ì²´í¬\"\"\"\n        self.client.get(\"/ready\")\n    \n    def on_stop(self):\n        \"\"\"í…ŒìŠ¤íŠ¸ ì¢…ë£Œ í›„ ì •ë¦¬\"\"\"\n        for uri in self.created_docs:\n            self.client.delete(f\"/v1beta/collections/{self.collection}/documents/{uri}\")\n```\n\n### ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸\n\nCRUD ì „ì²´ íë¦„ ë° ì—£ì§€ ì¼€ì´ìŠ¤:\n\n```python\n# suites/functional.py\nfrom locust import HttpUser, task, between, SequentialTaskSet\nimport uuid\n\nclass DocumentCRUDFlow(SequentialTaskSet):\n    \"\"\"ìˆœì°¨ì  CRUD í”Œë¡œìš° í…ŒìŠ¤íŠ¸\"\"\"\n    \n    def on_start(self):\n        self.doc_uri = f\"crud-test-{uuid.uuid4().hex[:8]}\"\n        self.version = 0\n    \n    @task\n    def step1_create(self):\n        \"\"\"1. ë¬¸ì„œ ìƒì„±\"\"\"\n        response = self.client.post(\n            f\"/v1beta/collections/functional_test/documents\",\n            json={\n                \"document\": {\n                    \"uri\": self.doc_uri,\n                    \"fields\": {\"step\": 1, \"status\": \"created\"}\n                }\n            }\n        )\n        if response.status_code == 200:\n            self.version = response.json()[\"document\"][\"version\"]\n    \n    @task\n    def step2_read(self):\n        \"\"\"2. ë¬¸ì„œ ì¡°íšŒ\"\"\"\n        response = self.client.get(\n            f\"/v1beta/collections/functional_test/documents/{self.doc_uri}\"\n        )\n        assert response.json()[\"document\"][\"version\"] == self.version\n    \n    @task\n    def step3_update(self):\n        \"\"\"3. ë¬¸ì„œ ì—…ë°ì´íŠ¸\"\"\"\n        response = self.client.patch(\n            f\"/v1beta/collections/functional_test/documents/{self.doc_uri}\",\n            json={\n                \"fields\": {\"step\": 2, \"status\": \"updated\"},\n                \"expected_version\": self.version\n            }\n        )\n        if response.status_code == 200:\n            self.version = response.json()[\"document\"][\"version\"]\n    \n    @task\n    def step4_verify_history(self):\n        \"\"\"4. íˆìŠ¤í† ë¦¬ í™•ì¸\"\"\"\n        response = self.client.get(\n            f\"/v1beta/collections/functional_test/documents/{self.doc_uri}/history\"\n        )\n        history = response.json()[\"documents\"]\n        assert len(history) == 2  # ë²„ì „ 1, 2\n    \n    @task\n    def step5_delete(self):\n        \"\"\"5. ë¬¸ì„œ ì‚­ì œ\"\"\"\n        response = self.client.delete(\n            f\"/v1beta/collections/functional_test/documents/{self.doc_uri}\"\n        )\n        assert response.json()[\"success\"] == True\n        self.interrupt()  # í”Œë¡œìš° ì¢…ë£Œ\n\n\nclass FunctionalTestUser(HttpUser):\n    wait_time = between(1, 3)\n    tasks = [DocumentCRUDFlow]\n```\n\n### ì„±ëŠ¥/ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸\n\n```python\n# suites/performance.py\nfrom locust import HttpUser, task, between, LoadTestShape\nimport uuid\n\nclass PerformanceTestUser(HttpUser):\n    \"\"\"ê³ ë¶€í•˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\"\"\"\n    \n    wait_time = between(0.1, 0.5)  # ë¹ ë¥¸ ìš”ì²­\n    \n    @task(5)\n    def batch_create(self):\n        \"\"\"ë°°ì¹˜ ìƒì„±\"\"\"\n        docs = [\n            {\"uri\": f\"perf-{uuid.uuid4().hex[:8]}\", \"fields\": {\"batch\": True}}\n            for _ in range(10)\n        ]\n        self.client.post(\n            \"/v1beta/collections/perf_test/documents:batchCreate\",\n            json={\"documents\": docs}\n        )\n    \n    @task(10)\n    def query_documents(self):\n        \"\"\"ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸\"\"\"\n        self.client.post(\n            \"/v1beta/collections/perf_test/documents:query\",\n            json={\n                \"query\": {\"filter\": {}},\n                \"page_size\": 100\n            }\n        )\n\n\nclass StressTestShape(LoadTestShape):\n    \"\"\"ì ì§„ì  ë¶€í•˜ ì¦ê°€ í…ŒìŠ¤íŠ¸\"\"\"\n    \n    stages = [\n        {\"duration\": 60, \"users\": 10, \"spawn_rate\": 2},    # ë¨í”„ì—…\n        {\"duration\": 120, \"users\": 50, \"spawn_rate\": 5},   # ìœ ì§€\n        {\"duration\": 60, \"users\": 100, \"spawn_rate\": 10},  # í”¼í¬\n        {\"duration\": 60, \"users\": 50, \"spawn_rate\": 10},   # ë‹¤ìš´\n    ]\n    \n    def tick(self):\n        run_time = self.get_run_time()\n        \n        for stage in self.stages:\n            if run_time < stage[\"duration\"]:\n                return (stage[\"users\"], stage[\"spawn_rate\"])\n            run_time -= stage[\"duration\"]\n        \n        return None  # í…ŒìŠ¤íŠ¸ ì¢…ë£Œ\n```\n\n### ë©”ì¸ ì§„ì…ì \n\n```python\n# locustfile.py\nimport os\nimport yaml\nfrom locust import events\n\nfrom suites.smoke import SmokeTestUser\nfrom suites.functional import FunctionalTestUser\nfrom suites.performance import PerformanceTestUser\n\n# í™˜ê²½ ì„¤ì • ë¡œë“œ\ndef load_config():\n    env = os.getenv(\"TEST_ENV\", \"local\")\n    config_path = f\"config/{env}.yaml\"\n    \n    with open(config_path) as f:\n        return yaml.safe_load(f)\n\nCONFIG = load_config()\n\n@events.init.add_listener\ndef on_locust_init(environment, **kwargs):\n    \"\"\"í…ŒìŠ¤íŠ¸ ì´ˆê¸°í™”\"\"\"\n    environment.host = CONFIG[\"base_url\"]\n    print(f\"Testing against: {CONFIG['environment']}\")\n\n# í…ŒìŠ¤íŠ¸ ëª¨ë“œì— ë”°ë¥¸ User í´ë˜ìŠ¤ ì„ íƒ\nTEST_MODE = os.getenv(\"TEST_MODE\", \"smoke\")\n\nif TEST_MODE == \"smoke\":\n    class User(SmokeTestUser):\n        pass\nelif TEST_MODE == \"functional\":\n    class User(FunctionalTestUser):\n        pass\nelif TEST_MODE == \"performance\":\n    class User(PerformanceTestUser):\n        pass\n```\n\n## Makefile\n\n```makefile\n# e2e/Makefile\n\n.PHONY: install smoke functional performance stress\n\nLOCUST_FLAGS = --headless --only-summary\n\ninstall:\n uv sync\n\n# ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸ (30ì´ˆ, 1 ì‚¬ìš©ì)\nsmoke:\n TEST_MODE=smoke TEST_ENV=$(ENV) locust \\\n  $(LOCUST_FLAGS) \\\n  -u 1 -r 1 -t 30s\n\n# ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ (5ë¶„, 10 ì‚¬ìš©ì)\nfunctional:\n TEST_MODE=functional TEST_ENV=$(ENV) locust \\\n  $(LOCUST_FLAGS) \\\n  -u 10 -r 2 -t 300s\n\n# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (10ë¶„, 50 ì‚¬ìš©ì)\nperformance:\n TEST_MODE=performance TEST_ENV=$(ENV) locust \\\n  $(LOCUST_FLAGS) \\\n  -u 50 -r 5 -t 600s\n\n# ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ (10ë¶„, 100 ì‚¬ìš©ì)\nstress:\n TEST_MODE=stress TEST_ENV=$(ENV) locust \\\n  $(LOCUST_FLAGS) \\\n  -u 100 -r 10 -t 600s\n\n# ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ (Web UI)\ninteractive:\n TEST_MODE=$(MODE) TEST_ENV=$(ENV) locust\n\n# í™˜ê²½ë³„ ì‹¤í–‰\ntest-local:\n $(MAKE) smoke ENV=local\n\ntest-int:\n $(MAKE) functional ENV=int\n\ntest-stage:\n $(MAKE) performance ENV=stage\n```\n\n## Kubernetes ë°°í¬\n\n### ConfigMap\n\n```yaml\n# k8s/tests/locust/configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: locust-test-config\ndata:\n  test_mode: \"smoke\"\n  test_env: \"int\"\n  users: \"10\"\n  spawn_rate: \"2\"\n  duration: \"300s\"\n```\n\n### Job\n\n```yaml\n# k8s/tests/locust/job.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: locust-e2e-test\nspec:\n  ttlSecondsAfterFinished: 86400\n  template:\n    spec:\n      containers:\n        - name: locust\n          image: myregistry/e2e-tests:latest\n          command:\n            - locust\n            - --headless\n            - --only-summary\n            - -u\n            - $(USERS)\n            - -r\n            - $(SPAWN_RATE)\n            - -t\n            - $(DURATION)\n          env:\n            - name: TEST_MODE\n              valueFrom:\n                configMapKeyRef:\n                  name: locust-test-config\n                  key: test_mode\n            - name: TEST_ENV\n              valueFrom:\n                configMapKeyRef:\n                  name: locust-test-config\n                  key: test_env\n            - name: USERS\n              valueFrom:\n                configMapKeyRef:\n                  name: locust-test-config\n                  key: users\n            - name: SPAWN_RATE\n              valueFrom:\n                configMapKeyRef:\n                  name: locust-test-config\n                  key: spawn_rate\n            - name: DURATION\n              valueFrom:\n                configMapKeyRef:\n                  name: locust-test-config\n                  key: duration\n      restartPolicy: Never\n  backoffLimit: 0\n```\n\n### í™˜ê²½ë³„ ì‹¤í–‰\n\n```bash\n# INT í™˜ê²½ ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸\nkubectl -n testing patch configmap locust-test-config \\\n  --type=merge -p '{\"data\":{\"test_mode\":\"smoke\",\"test_env\":\"int\",\"users\":\"1\",\"duration\":\"30s\"}}'\nkubectl -n testing apply -f k8s/tests/locust/job.yaml\n\n# STAGE í™˜ê²½ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\nkubectl -n testing patch configmap locust-test-config \\\n  --type=merge -p '{\"data\":{\"test_mode\":\"performance\",\"test_env\":\"stage\",\"users\":\"50\",\"duration\":\"600s\"}}'\nkubectl delete job locust-e2e-test -n testing --ignore-not-found\nkubectl -n testing apply -f k8s/tests/locust/job.yaml\n\n# ë¡œê·¸ í™•ì¸\nkubectl -n testing logs -f job/locust-e2e-test\n```\n\n## í”„ë¡œë•ì…˜ í…ŒìŠ¤íŠ¸ ì•ˆì „ ì¥ì¹˜\n\n```makefile\n# í”„ë¡œë•ì…˜ í…ŒìŠ¤íŠ¸ (ê·¹ë„ë¡œ ì œí•œëœ ì„¤ì •)\ntest-prod:\n @echo \"âš ï¸  WARNING: Production test!\"\n @read -p \"Type 'I understand the risks': \" confirm && \\\n  [ \"$$confirm\" = \"I understand the risks\" ] || (echo \"Cancelled.\" && exit 1)\n TEST_MODE=smoke TEST_ENV=prod locust \\\n  $(LOCUST_FLAGS) \\\n  -u 1 -r 1 -t 60s  # 1ëª…, 1ë¶„ë§Œ\n```\n\n## ëª¨ë²” ì‚¬ë¡€\n\n1. **í™˜ê²½ ë¶„ë¦¬**: í™˜ê²½ë³„ ì„¤ì • íŒŒì¼ë¡œ ì—”ë“œí¬ì¸íŠ¸/ì„¤ì • ê´€ë¦¬\n2. **í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬**: `on_stop`ì—ì„œ ìƒì„±í•œ ë°ì´í„° ì‚­ì œ\n3. **ì ì§„ì  ë¶€í•˜**: `LoadTestShape`ë¡œ ê¸‰ê²©í•œ ë¶€í•˜ ë°©ì§€\n4. **í”„ë¡œë•ì…˜ ë³´í˜¸**: í”„ë¡œë•ì…˜ í…ŒìŠ¤íŠ¸ëŠ” ê·¹ë„ë¡œ ì œí•œ\n5. **ê²°ê³¼ ì €ì¥**: `--csv` ì˜µì…˜ìœ¼ë¡œ ê²°ê³¼ ê¸°ë¡\n\n## ì°¸ê³  ìë£Œ\n\n- [Locust ê³µì‹ ë¬¸ì„œ](https://docs.locust.io/)\n- [Locust Kubernetes ë°°í¬](https://docs.locust.io/en/stable/running-distributed.html)\n- [LoadTestShape ê°€ì´ë“œ](https://docs.locust.io/en/stable/custom-load-shape.html)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "Kubernetes",
      "Performance",
      "Python",
      "Testing"
    ],
    "readingTime": 7,
    "wordCount": 1308,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "linux-ulimit-guide",
    "slug": "linux-ulimit-guide",
    "path": "backend/devops",
    "fullPath": "backend/devops/linux-ulimit-guide",
    "title": "Linux íŒŒì¼ ë””ìŠ¤í¬ë¦½í„° ì œí•œ (ulimit) ì™„ë²½ ê°€ì´ë“œ",
    "excerpt": "ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ ì„œë²„ì—ì„œ 'Too many open files' ì˜¤ë¥˜ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ulimit ì„¤ì • ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# Linux íŒŒì¼ ë””ìŠ¤í¬ë¦½í„° ì œí•œ (ulimit) ì™„ë²½ ê°€ì´ë“œ\n\n## ê°œìš”\n\nê³ íŠ¸ë˜í”½ ì„œë²„ë¥¼ ìš´ì˜í•˜ë‹¤ ë³´ë©´ `Too many open files` ì—ëŸ¬ë¥¼ ë§ˆì£¼ì¹˜ê²Œ ë©ë‹ˆë‹¤. ì´ëŠ” Linuxì˜ **íŒŒì¼ ë””ìŠ¤í¬ë¦½í„°(File Descriptor)** ì œí•œ ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ ê¸€ì—ì„œëŠ” ulimitì˜ ê°œë…ê³¼ ì‹¤ì „ ì„¤ì • ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n\n## íŒŒì¼ ë””ìŠ¤í¬ë¦½í„°ë€?\n\níŒŒì¼ ë””ìŠ¤í¬ë¦½í„°ëŠ” Linuxì—ì„œ ì—´ë¦° íŒŒì¼, ì†Œì¼“, íŒŒì´í”„ ë“±ì„ ë‚˜íƒ€ë‚´ëŠ” ì •ìˆ˜ ê°’ì…ë‹ˆë‹¤. ëª¨ë“  I/O ì‘ì—…ì€ íŒŒì¼ ë””ìŠ¤í¬ë¦½í„°ë¥¼ í†µí•´ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.\n\n```bash\n# í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ ì—´ë¦° íŒŒì¼ ë””ìŠ¤í¬ë¦½í„° í™•ì¸\nls -la /proc/self/fd/\n```\n\n## ì œí•œ ì¢…ë¥˜\n\n### ì‹œìŠ¤í…œ ì „ì²´ ì œí•œ\n\n```bash\n# ì‹œìŠ¤í…œ ì „ì²´ ìµœëŒ€ íŒŒì¼ ë””ìŠ¤í¬ë¦½í„° ìˆ˜\ncat /proc/sys/fs/file-max\n\n# í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ íŒŒì¼ ë””ìŠ¤í¬ë¦½í„° ìˆ˜\ncat /proc/sys/fs/file-nr\n```\n\n### í”„ë¡œì„¸ìŠ¤ë³„ ì œí•œ\n\n```bash\n# í˜„ì¬ ì‰˜ì˜ ì œí•œ í™•ì¸\nulimit -n       # soft limit\nulimit -Hn      # hard limit\n```\n\n| êµ¬ë¶„ | ì„¤ëª… |\n|-----|------|\n| **Soft Limit** | ì‹¤ì œ ì ìš©ë˜ëŠ” ì œí•œ, ì‚¬ìš©ìê°€ ë³€ê²½ ê°€ëŠ¥ |\n| **Hard Limit** | ìµœëŒ€ ìƒí•œì„ , rootë§Œ ì¦ê°€ ê°€ëŠ¥ |\n\n## ì¼ì‹œì  ë³€ê²½ (í˜„ì¬ ì„¸ì…˜ë§Œ)\n\n```bash\n# Soft limit ë³€ê²½ (hard limit ë²”ìœ„ ë‚´ì—ì„œ)\nulimit -n 65535\n\n# Hard limit ë³€ê²½ (root ê¶Œí•œ í•„ìš”)\nsudo ulimit -Hn 100000\n```\n\n## ì˜êµ¬ì  ë³€ê²½\n\n### 1. limits.conf ì„¤ì •\n\n`/etc/security/limits.conf` íŒŒì¼ì„ ìˆ˜ì •í•©ë‹ˆë‹¤:\n\n```bash\n# /etc/security/limits.conf\n# <domain>  <type>  <item>  <value>\n\n*           soft    nofile  65535\n*           hard    nofile  100000\nroot        soft    nofile  65535\nroot        hard    nofile  100000\n```\n\n| í•„ë“œ | ì„¤ëª… | ì˜ˆì‹œ |\n|-----|------|------|\n| domain | ì ìš© ëŒ€ìƒ | `*` (ëª¨ë“  ì‚¬ìš©ì), `root`, `@group` |\n| type | ì œí•œ ìœ í˜• | `soft`, `hard`, `-` (ë‘˜ ë‹¤) |\n| item | ì œí•œ í•­ëª© | `nofile` (íŒŒì¼ ìˆ˜), `nproc` (í”„ë¡œì„¸ìŠ¤ ìˆ˜) |\n| value | ì œí•œ ê°’ | ìˆ«ì ë˜ëŠ” `unlimited` |\n\n### 2. systemd ì„œë¹„ìŠ¤ ì„¤ì •\n\nsystemdë¡œ ê´€ë¦¬ë˜ëŠ” ì„œë¹„ìŠ¤ëŠ” ë³„ë„ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤:\n\n```ini\n# /etc/systemd/system/myapp.service\n[Service]\nLimitNOFILE=65535\nLimitNPROC=65535\n```\n\në˜ëŠ” ì „ì—­ ì„¤ì •:\n\n```ini\n# /etc/systemd/system.conf\nDefaultLimitNOFILE=65535\n```\n\nì„¤ì • í›„ ì¬ì‹œì‘:\n\n```bash\nsudo systemctl daemon-reload\nsudo systemctl restart myapp\n```\n\n### 3. sysctlë¡œ ì‹œìŠ¤í…œ ì „ì²´ ì œí•œ ë³€ê²½\n\n```bash\n# /etc/sysctl.conf\nfs.file-max = 2097152\nfs.nr_open = 2097152\n\n# ì ìš©\nsudo sysctl -p\n```\n\n## ì‹¤ì „ ì˜ˆì‹œ: Nginx ì„¤ì •\n\n```nginx\n# /etc/nginx/nginx.conf\nworker_rlimit_nofile 65535;\n\nevents {\n    worker_connections 65535;\n}\n```\n\n## ë¬¸ì œ í•´ê²°\n\n### ì„¤ì •ì´ ì ìš©ë˜ì§€ ì•Šì„ ë•Œ\n\n```bash\n# PAM ëª¨ë“ˆ í™•ì¸\ngrep pam_limits /etc/pam.d/common-session\n# session required pam_limits.so ê°€ ìˆì–´ì•¼ í•¨\n```\n\n### í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ ì œí•œ í™•ì¸\n\n```bash\n# íŠ¹ì • í”„ë¡œì„¸ìŠ¤ì˜ ì œí•œ í™•ì¸\ncat /proc/<PID>/limits\n```\n\n## ê¶Œì¥ ì„¤ì •ê°’\n\n| ì„œë²„ ìš©ë„ | nofile ê¶Œì¥ê°’ |\n|---------|--------------|\n| ì¼ë°˜ ì›¹ ì„œë²„ | 65,535 |\n| ê³ íŠ¸ë˜í”½ API ì„œë²„ | 100,000+ |\n| ë°ì´í„°ë² ì´ìŠ¤ | 65,535 ~ 100,000 |\n| ë©”ì‹œì§€ ë¸Œë¡œì»¤ | 500,000+ |\n\n## ì£¼ì˜ì‚¬í•­\n\n- **ê³¼ë„í•œ ê°’ ì„¤ì • ê¸ˆì§€**: ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œ ë°œìƒ ê°€ëŠ¥\n- **soft â‰¤ hard**: soft limitì€ hard limitì„ ì´ˆê³¼í•  ìˆ˜ ì—†ìŒ\n- **ì¬ë¶€íŒ… í›„ í™•ì¸**: ì˜êµ¬ ì„¤ì • ì ìš© ì—¬ë¶€ ê²€ì¦ í•„ìˆ˜\n\n## ì°¸ê³  ìë£Œ\n\n- [Linux man page: limits.conf](https://man7.org/linux/man-pages/man5/limits.conf.5.html)\n- [systemd LimitNOFILE](https://www.freedesktop.org/software/systemd/man/systemd.exec.html)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "Infrastructure",
      "Performance"
    ],
    "readingTime": 3,
    "wordCount": 458,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "helm-environment-management",
    "slug": "helm-environment-management",
    "path": "backend/devops",
    "fullPath": "backend/devops/helm-environment-management",
    "title": "Helm í™˜ê²½ë³„ Values ì˜¤ë²„ë¼ì´ë“œ ì „ëµ",
    "excerpt": "Helm Chartì—ì„œ ê³µí†µ ì„¤ì •ê³¼ í™˜ê²½ë³„ ì„¤ì •ì„ ë¶„ë¦¬í•˜ì—¬ GitOps ë°©ì‹ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# Helm í™˜ê²½ë³„ Values ì˜¤ë²„ë¼ì´ë“œ ì „ëµ\n\n## ê°œìš”\n\nKubernetes í™˜ê²½ì—ì„œ ë™ì¼í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ INT/STAGE/REAL ë“± ì—¬ëŸ¬ í™˜ê²½ì— ë°°í¬í•  ë•Œ, **ê³µí†µ ì„¤ì •**ê³¼ **í™˜ê²½ë³„ ì„¤ì •**ì„ ë¶„ë¦¬í•˜ë©´ ìœ ì§€ë³´ìˆ˜ì„±ì´ í¬ê²Œ í–¥ìƒë©ë‹ˆë‹¤.\n\n## í•µì‹¬ ê°œë…\n\n```mermaid\nflowchart LR\n    subgraph Common [\"ê³µí†µ ì„¤ì •\"]\n        VApp[\"values.application.yaml\"]\n        VInfra[\"values.infra.yaml\"]\n    end\n    \n    subgraph EnvOverride [\"í™˜ê²½ë³„ ì˜¤ë²„ë¼ì´ë“œ\"]\n        Int[\"int/values.application.yaml\"]\n        Stage[\"stage/values.application.yaml\"]\n        Real[\"real/values.application.yaml\"]\n    end\n    \n    VApp --> Int\n    VApp --> Stage\n    VApp --> Real\n    \n    Int --> Merge[\"helm template ë³‘í•©\"]\n    Stage --> Merge\n    Real --> Merge\n```\n\n- **ê³µí†µ ì„¤ì •**: ëª¨ë“  í™˜ê²½ì—ì„œ ë™ì¼í•œ ê¸°ë³¸ê°’\n- **í™˜ê²½ë³„ ì˜¤ë²„ë¼ì´ë“œ**: í•´ë‹¹ í™˜ê²½ì—ë§Œ ë‹¤ë¥¸ ê°’ì„ ë®ì–´ì”Œì›€\n\n## ë””ë ‰í† ë¦¬ êµ¬ì¡°\n\n```\nk8s/manifests/my-service/helm/\nâ”œâ”€â”€ values.application.yaml    # ê³µí†µ ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •\nâ”œâ”€â”€ values.infra.yaml          # ê³µí†µ ì¸í”„ë¼ ì„¤ì •\nâ”œâ”€â”€ dev/\nâ”‚   â”œâ”€â”€ Chart.yaml\nâ”‚   â”œâ”€â”€ values.application.yaml  # dev í™˜ê²½ ì˜¤ë²„ë¼ì´ë“œ\nâ”‚   â””â”€â”€ values.infra.yaml\nâ”œâ”€â”€ int/\nâ”‚   â”œâ”€â”€ Chart.yaml\nâ”‚   â”œâ”€â”€ values.application.yaml  # int í™˜ê²½ ì˜¤ë²„ë¼ì´ë“œ\nâ”‚   â””â”€â”€ values.infra.yaml\nâ”œâ”€â”€ stage/\nâ”‚   â”œâ”€â”€ Chart.yaml\nâ”‚   â”œâ”€â”€ values.application.yaml  # stage í™˜ê²½ ì˜¤ë²„ë¼ì´ë“œ\nâ”‚   â””â”€â”€ values.infra.yaml\nâ””â”€â”€ real/\n    â”œâ”€â”€ Chart.yaml\n    â”œâ”€â”€ values.application.yaml  # real(production) í™˜ê²½ ì˜¤ë²„ë¼ì´ë“œ\n    â””â”€â”€ values.infra.yaml\n```\n\n## ê³µí†µ ì„¤ì •\n\n### values.application.yaml (ê³µí†µ)\n\nëª¨ë“  í™˜ê²½ì—ì„œ ë™ì¼í•œ ê¸°ë³¸ê°’ì„ ì •ì˜:\n\n```yaml\n# ê³µí†µ ì„¤ì • - ëª¨ë“  í™˜ê²½ì—ì„œ ë™ì¼\napp:\n  regionCode: \"apn2\"\n  enableProbe: \"true\"\n  \n  args: [\"run\", \"--config\", \"/etc/my-service/config.yaml\"]\n  \n  readinessProbe:\n    grpc:\n      port: 9090\n    initialDelaySeconds: 15\n    periodSeconds: 20\n    \n  livenessProbe:\n    grpc:\n      port: 9090\n    initialDelaySeconds: 15\n    periodSeconds: 20\n  \n  replicaCount: 1  # ê¸°ë³¸ê°’, í™˜ê²½ë³„ë¡œ ì˜¤ë²„ë¼ì´ë“œ\n  \n  service:\n    ports:\n      - name: http\n        port: 8080\n        protocol: TCP\n      - name: grpc\n        port: 9090\n        protocol: TCP\n```\n\n## í™˜ê²½ë³„ ì˜¤ë²„ë¼ì´ë“œ\n\n### int/values.application.yaml\n\nINT í™˜ê²½ì—ë§Œ ë‹¤ë¥¸ ì„¤ì •:\n\n```yaml\n# int í™˜ê²½ ì˜¤ë²„ë¼ì´ë“œ - ê³µí†µ ê°’ì„ ë®ì–´ì”€\n\napp:\n  serviceAccount:\n    annotations:\n      eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789:role/int-role\"\n    create: true\n    name: \"my-service-api\"\n  \n  # INT í™˜ê²½ ì‹œí¬ë¦¿\n  secretsStores:\n    - path: /common/int/secrets\n      type: secretsmanager\n      keys:\n        - MONGO_CONNECTION_STRING\n        - REDIS_CONNECTION_STRING\n  \n  # INT í™˜ê²½ ConfigMap\n  configmaps:\n    - name: my-service-config\n      mount:\n        enabled: true\n        path: /etc/my-service\n        filename: config.yaml\n      value: |\n        database:\n          mongo_connection_string: \"get_secret_from_secret_manager\"\n          redis_connection_strings:\n            - \"get_secret_from_secret_manager\"\n        stellar:\n          url: https://api.int.example.com\n  \n  # INT í™˜ê²½ í™˜ê²½ë³€ìˆ˜\n  env:\n    - name: ENV\n      value: \"int\"\n```\n\n### real/values.application.yaml\n\nREAL(Production) í™˜ê²½:\n\n```yaml\n# real(production) í™˜ê²½ ì˜¤ë²„ë¼ì´ë“œ\n\napp:\n  replicaCount: 3  # í”„ë¡œë•ì…˜ì€ 3ê°œ ë ˆí”Œë¦¬ì¹´\n  \n  resources:\n    requests:\n      cpu: 500m\n      memory: 2Gi\n    limits:\n      cpu: 1000m\n      memory: 2Gi\n  \n  serviceAccount:\n    annotations:\n      eks.amazonaws.com/role-arn: \"arn:aws:iam::987654321:role/real-role\"\n    create: true\n    name: \"my-service-api\"\n  \n  secretsStores:\n    - path: /common/real/secrets\n      type: secretsmanager\n      keys:\n        - MONGO_CONNECTION_STRING\n        - REDIS_CONNECTION_STRING\n  \n  configmaps:\n    - name: my-service-config\n      mount:\n        enabled: true\n        path: /etc/my-service\n        filename: config.yaml\n      value: |\n        database:\n          mongo_connection_string: \"get_secret_from_secret_manager\"\n        stellar:\n          url: https://api.example.com\n  \n  env:\n    - name: ENV\n      value: \"real\"\n```\n\n## ë¡œì»¬ í…ŒìŠ¤íŠ¸\n\në°°í¬ ì „ í…œí”Œë¦¿ ë Œë”ë§ìœ¼ë¡œ ê²€ì¦:\n\n```bash\n# ì˜ì¡´ì„± ì—…ë°ì´íŠ¸\nhelm dependency update int\n\n# í…œí”Œë¦¿ ë Œë”ë§ (ê³µí†µ + í™˜ê²½ë³„ ë³‘í•©)\nhelm template my-service \\\n  -f values.infra.yaml \\\n  -f values.application.yaml \\\n  -f int/values.infra.yaml \\\n  -f int/values.application.yaml \\\n  ./int | yq\n```\n\nìˆœì„œê°€ ì¤‘ìš”í•©ë‹ˆë‹¤:\n\n1. ê³µí†µ infra â†’ 2. ê³µí†µ application â†’ 3. í™˜ê²½ë³„ infra â†’ 4. í™˜ê²½ë³„ application\n\në’¤ì— ì˜¤ëŠ” íŒŒì¼ì´ ì•ì˜ ê°’ì„ ì˜¤ë²„ë¼ì´ë“œí•©ë‹ˆë‹¤.\n\n## ArgoCD GitOps ë°°í¬\n\nArgoCDëŠ” ê° í™˜ê²½ë³„ ë””ë ‰í† ë¦¬ë¥¼ ë³„ë„ Applicationìœ¼ë¡œ ë“±ë¡:\n\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: my-service-int\n  namespace: argocd\nspec:\n  source:\n    repoURL: https://github.com/myorg/k8s-manifests.git\n    path: k8s/manifests/my-service/helm/int\n    helm:\n      valueFiles:\n        - ../values.infra.yaml\n        - ../values.application.yaml\n        - values.infra.yaml\n        - values.application.yaml\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: my-namespace-int\n```\n\n## í•µì‹¬ ì •ë¦¬\n\n| íŒŒì¼ | ìœ„ì¹˜ | ì—­í•  |\n|------|------|------|\n| `values.application.yaml` | ë£¨íŠ¸ | ê³µí†µ ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • |\n| `values.infra.yaml` | ë£¨íŠ¸ | ê³µí†µ ì¸í”„ë¼ ì„¤ì • |\n| `{env}/values.application.yaml` | í™˜ê²½ í´ë” | í™˜ê²½ë³„ ì˜¤ë²„ë¼ì´ë“œ |\n| `{env}/Chart.yaml` | í™˜ê²½ í´ë” | ì˜ì¡´ì„± ì°¨íŠ¸ ì •ì˜ |\n\n## ëª¨ë²” ì‚¬ë¡€\n\n1. **ê³µí†µ ìµœëŒ€í™”**: ìµœëŒ€í•œ ê³µí†µ ì„¤ì •ì— ë„£ê³ , í™˜ê²½ë³„ì€ ìµœì†Œí™”\n2. **ì‹œí¬ë¦¿ ë¶„ë¦¬**: ë¯¼ê° ì •ë³´ëŠ” Secrets Manager/Vault ì‚¬ìš©\n3. **ë¦¬ì†ŒìŠ¤ ì°¨ë“±**: í”„ë¡œë•ì…˜ì€ ë” ë†’ì€ ë¦¬ì†ŒìŠ¤ ì„¤ì •\n4. **ë¡œì»¬ ê²€ì¦**: ë°°í¬ ì „ `helm template`ë¡œ ë Œë”ë§ í™•ì¸\n5. **ë ˆí”Œë¦¬ì¹´ ì°¨ë“±**: dev=1, int=1~2, real=3+\n\n## ì°¸ê³  ìë£Œ\n\n- [Helm Values Files](https://helm.sh/docs/chart_template_guide/values_files/)\n- [ArgoCD Helm](https://argo-cd.readthedocs.io/en/stable/user-guide/helm/)",
    "docType": "original",
    "category": "Backend",
    "tags": [
      "Backend",
      "DevOps",
      "Kubernetes"
    ],
    "readingTime": 3,
    "wordCount": 587,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-30"
  },
  {
    "id": "mac-os-finderì—ì„œ-ì‚¬ìš©ì-ë£¨íŠ¸-ë””ë ‰í† ë¦¬ë¡œ-ì´ë™í•˜ëŠ”-ë²•",
    "slug": "mac-os-findereseo-sayongja-ruteu-diregtoriro-idonghaneun-beob",
    "path": "misc",
    "fullPath": "misc/mac-os-findereseo-sayongja-ruteu-diregtoriro-idonghaneun-beob",
    "title": "Mac OS Finderì—ì„œ ì‚¬ìš©ì ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•˜ëŠ” ë²•",
    "excerpt": "Mac OS Finderì—ì„œ ì‚¬ìš©ì ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•˜ëŠ” ë²• ë¥¼ ëˆŒëŸ¬ë³´ë©´ ìˆœê°„ì´ë™í•œë‹¤!...",
    "content": "# Mac OS Finderì—ì„œ ì‚¬ìš©ì ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•˜ëŠ” ë²•\n\n`CMD + SHIFT + H`ë¥¼ ëˆŒëŸ¬ë³´ë©´ ìˆœê°„ì´ë™í•œë‹¤!",
    "docType": "original",
    "category": "Research",
    "tags": [],
    "readingTime": 1,
    "wordCount": 17,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "typescript-5-0-rc-ë°œí‘œì—ì„œ-ë°ì½”ë ˆì´í„°-ë¶€ë¶„-ìš”ì•½",
    "slug": "typescript-5-0-rc-balpyoeseo-dekoreiteo-bubun-yoyag",
    "path": "languages/typescript",
    "fullPath": "languages/typescript/typescript-5-0-rc-balpyoeseo-dekoreiteo-bubun-yoyag",
    "title": "Typescript 5.0 RC ë°œí‘œì—ì„œ ë°ì½”ë ˆì´í„° ë¶€ë¶„ ìš”ì•½",
    "excerpt": "Typescript 5.0 RC ë°œí‘œì—ì„œ ë°ì½”ë ˆì´í„° ë¶€ë¶„ ìš”ì•½ ì„¤ì¹˜ ë°©ë²• Decorators ë°ì½”ë ˆì´í„°ëŠ” ECMAScriptì— ê³§ ì¶”ê°€ë˜ëŠ” ê¸°ëŠ¥ìœ¼ë¡œ, í´ë˜ìŠ¤ì™€ ë©¤ë²„ë¥¼ ì¬ì‚¬ìš© ê°€ëŠ¥...",
    "content": "# Typescript 5.0 RC ë°œí‘œì—ì„œ ë°ì½”ë ˆì´í„° ë¶€ë¶„ ìš”ì•½\n\n## ì„¤ì¹˜ ë°©ë²•\n\n```shell\nnpm install typescript@rc\n```\n\n## Decorators\n\në°ì½”ë ˆì´í„°ëŠ” ECMAScriptì— ê³§ ì¶”ê°€ë˜ëŠ” ê¸°ëŠ¥ìœ¼ë¡œ, í´ë˜ìŠ¤ì™€ ë©¤ë²„ë¥¼ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ìí™” í•  ìˆ˜ ìˆë„ë¡ í•´ì¤ë‹ˆë‹¤.\n\në‹¤ìŒ ì½”ë“œë¥¼ ê³ ë ¤í•´ë´…ì‹œë‹¤.\n\n```typescript\nclass Person {\n    name: string;\n\n    constructor(name: string) {\n        this.name = name;\n    }\n\n    greet() {\n        console.log(`Hello, my name is ${this.name}`);\n    }\n}\n\nconst p = new Person(\"Ray\");\np.greet();\n```\n\n`greet`ì€ ë§¤ìš° ê°„ë‹¨í•˜ê²Œ ì‘ì„±ë˜ì—ˆìœ¼ë‚˜, ì¢€ ë” ë³µì¡í•œ ê²½ìš°ë¥¼ ìƒìƒí•´ë´…ì‹œë‹¤. ë¹„ë™ê¸° ë…¼ë¦¬ íë¦„ì´ë‚˜ ì¬ê·€í˜¸ì¶œ, ë˜ëŠ” ì˜ˆê¸°ì¹˜ ëª»í•œ ë¶€ì‘ìš© ë“± ì—¬ëŸ¬ê°€ì§€ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ê²ƒì„ ìƒìƒí•˜ë“  ê°„ì—, ìš°ë¦¬ëŠ” í•œë²ˆ\në””ë²„ê¹… ë¡œê·¸ë¥¼ ì°ì–´ë´…ì‹œë‹¤.\n\n```typescript\nclass Person {\n    name: string;\n\n    constructor(name: string) {\n        this.name = name;\n    }\n\n    greet() {\n        console.log(\"LOG: Entering method.\");\n        console.log(`Hello, my name is ${this.name}`);\n        console.log(\"LOG: Exiting method.\");\n    }\n}\n```\n\nì´ëŸ¬í•œ íŒ¨í„´ì€ ê½¤ë‚˜ ì¼ë°˜ì ì…ë‹ˆë‹¤. ì‚¬ì‹¤ ëª¨ë“  ë©”ì†Œë“œì— ì ìš©í•´ë„ ì¢‹ì„ë§Œ í•˜ì£ !\nì—¬ê¸°ì„œ ë°ì½”ë ˆì´í„°ê°€ ë“±ì¥í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” `loggedMethod`ë¼ëŠ” í•¨ìˆ˜ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•´ë´…ë‹ˆë‹¤.\n\n```typescript\nfunction loggedMethod(originalMethod: any, _context: any) {\n    function replacementMethod(this: any, ...args: any[]) {\n        console.log(\"LOG: Entering method.\");\n        const result = originalMethod.call(this, args);\n        console.log(\"LOG: Exiting method.\");\n        return result;\n    }\n\n    return replacementMethod;\n}\n```\n\n\"ëŒ€ì²´ ì™œ `any`ë¡œ ë–¡ì¹ í•œê±°ì•¼, `any`scriptì•¼?\"\n\nì¸ë‚´ì‹¬ì„ ê°€ì ¸ë³´ì„¸ìš”. ë‹¹ì¥ì€ ìš°ë¦¬ê°€ ì´ í•¨ìˆ˜ì˜ ë™ì‘ì„ ë³´ëŠ” ê²ƒì— ì§‘ì¤‘í•˜ê¸° ìœ„í•´ ë‹¤ë¥¸ ê²ƒì„ ë‹¨ìˆœí™” í–ˆìŠµë‹ˆë‹¤.\n`loggedMethod`ê°€ ì›ë³¸ ë©”ì†Œë“œë¥¼ ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ê³ , ì›ë³¸ ë©”ì†Œë“œì˜ ë™ì‘ ì• ë’¤ë¡œ ë¡œê·¸ë¥¼ ì°ì€ ë’¤, ì›ë³¸ ë©”ì†Œë“œì˜ ê²°ê³¼ê°’ì„ ë°˜í™˜í•˜ëŠ” ê²ƒì„ ëˆˆì¹˜ ì±„ì…¨ë‚˜ìš”?\n\nì´ì œ ìš°ë¦¬ëŠ” `loggedmethod`ë¡œ `greet`ë©”ì†Œë“œë¥¼ `decorate`í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```typescript\nclass Person {\n    name: string;\n\n    constructor(name: string) {\n        this.name = name;\n    }\n\n    @loggedMethod\n    greet() {\n        console.log(`Hello, my name is ${this.name}.`);\n    }\n}\n\nconst p = new Person(\"Ray\");\np.greet();\n\n// Output:\n//\n//   LOG: Entering method.\n//   Hello, my name is Ray.\n//   LOG: Exiting method.\n```\n\nìš°ë¦¬ëŠ” `loggedMethod`ë¥¼ ë‹¨ìˆœíˆ `greet`ìœ„ì— `@`ë¥¼ ë¶™ì—¬ì„œ ì˜¬ë ¤ë†“ì•˜ìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë‹ˆ ë§¤ê°œë³€ìˆ˜ë¡œëŠ” *target*ê³¼ *context* ê°œì²´ê°€ ë„˜ì–´ì˜µë‹ˆë‹¤.\n`loggedMethod`ê°€ ìƒˆë¡œìš´ í•¨ìˆ˜ë¥¼ ë°˜í™˜í•˜ê¸°ì— ì›ë˜ ì •ì˜ëœ `greet`ëŠ” ë°˜í™˜ë˜ëŠ” ìƒˆë¡œìš´ í•¨ìˆ˜ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.\n\nì–¸ê¸‰í•˜ì§€ëŠ” ì•Šì•˜ì§€ë§Œ `loggedMethod`ì—ëŠ” \"context object\"ë¼ëŠ” ë‘ë²ˆì§¸ ë§¤ê°œë³€ìˆ˜ê°€ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ decoratedëœ ë©”ì†Œë“œê°€ ì–´ë–»ê²Œ ì„ ì–¸ë˜ì—ˆëŠ”ì§€ì— ëŒ€í•œ ìœ ìš©í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì •ë³´ì—ëŠ”\nê·¸ê²ƒì´ `#private`ì´ë‚˜ `static` ë©¤ë²„ì¸ì§€, ë˜ëŠ” ë©”ì†Œë“œì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ì§€ ë“±ì´ ìˆì£ . ì´ë¥¼ í™œìš©í•´ `decorated`ëœ ë©”ì†Œë“œ ì´ë¦„ì„ ì¶œë ¥í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n\n```typescript\nfunction loggedMethod(originalMethod: any, context: ClassMethodDecoratorContext) {\n    const methodName = String(context.name);\n\n    function replacementMethod(this: any, ...args: any[]) {\n        console.log(`LOG: Entering method '${methodName}'.`)\n        const result = originalMethod.call(this, ...args);\n        console.log(`LOG: Exiting method '${methodName}'.`)\n        return result;\n    }\n\n    return replacementMethod;\n}\n```\n\në“œë””ì–´ `loggedMethod`ì—ì„œ `any`ë¥¼ í•˜ë‚˜ ì§€ì› ìŠµë‹ˆë‹¤. íƒ€ì…ìŠ¤í¬ë¦½íŠ¸ëŠ” `ClassMethodDecoratorContext`ë¼ëŠ” íƒ€ì…ì„ ì œê³µí•˜ëŠ”ë°, ì´ê²ƒì€ ë°ì½”ë ˆì´í„°ê°€ ë¶™ì€ ë©”ì†Œë“œì˜ context\nobjectë¥¼ ìœ í˜•í™”í•©ë‹ˆë‹¤.\n\në©”íƒ€ë°ì´í„°ì™€ ë³„ê°œë¡œ, ë©”ì†Œë“œë¥¼ ìœ„í•œ context objectëŠ” `addInitializer`ë¼ëŠ” ìœ ìš©í•œ í•¨ìˆ˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ëŠ” ìƒì„±ìê°€ í˜¸ì¶œë˜ê±°ë‚˜ ìŠ¤íƒœí‹± ë©”ì†Œë“œ í˜¸ì¶œì‹œ í´ë˜ìŠ¤ê°€ ì´ˆê¸°í™” ë  ë•Œ ì—°ê²°í•˜ëŠ”\në°©ë²•ì…ë‹ˆë‹¤.)\n\nì˜ˆë¥¼ ë“¤ì–´, ìë°”ìŠ¤í¬ë¦½íŠ¸ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.\n\n```typescript\nclass Person {\n    name: string;\n\n    constructor(name: string) {\n        this.name = name;\n        this.greet = this.greet.bind(this);\n    }\n\n    greet() {\n        console.log(`Hello, my name is ${this.name}`);\n    }\n}\n```\n\në˜ëŠ” `greet`ë¥¼ í™”ì‚´í‘œ í•¨ìˆ˜ë¡œ ì„ ì–¸ì´ê³  ì†ì„±ìœ¼ë¡œ ì„ ì–¸í–ˆì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n\n```typescript\nclass Person {\n    name: string;\n\n    constructor(name: string) {\n        this.name = name;\n    }\n\n    greet = () => {\n        console.log(`Hello, my name is ${this.name}.`);\n    }\n}\n```\n\nì´ ì½”ë“œëŠ” `greet`ì´ ë…ë¦½ ì‹¤í–‰í˜• í•¨ìˆ˜ë¡œ í˜¸ì¶œë˜ê±°ë‚˜ ì½œë°±ìœ¼ë¡œ ì „ë‹¬ë˜ëŠ” ê²½ìš° *this*ê°€ ë‹¤ì‹œ ë°”ì¸ë”© ë˜ì§€ ì•Šë„ë¡ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n```typescript\nconst greet = new Person(\"Ray\").greet;\n// We don't want this to fail!\ngreet();\n```\n\nìš°ë¦¬ëŠ” `addInitializer`ë¡œ ìƒì„±ìì— `bind`í•˜ë„ë¡ í•˜ëŠ” ë°ì½”ë ˆì´í„°ë¥¼ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```typescript\nfunction bound(originalMethod: any, context: ClassMethodDecoratorContext) {\n    const methodName = context.name;\n    if (context.private) {\n        throw new Error(`'bound' cannot decorate private properties like ${methodName as string}.`);\n    }\n    context.addInitializer(function () {\n        this[methodName] = this[methodName].bind(this);\n    });\n}\n```\n\n`bound`ëŠ” ì•„ë¬´ê²ƒë„ ë°˜í™˜í•˜ì§€ ì•Šê³  ì˜¤ë²„ë¼ì´ë”© í•˜ì§€ë„ ì•Šìœ¼ë¯€ë¡œ ì´ ë¡œì§ì€ ì´ˆê¸°í™”ì‹œì—ë§Œ ì‹¤í–‰ë  ê²ë‹ˆë‹¤.\n\n```typescript\nclass Person {\n    name: string;\n\n    constructor(name: string) {\n        this.name = name;\n    }\n\n    @bound\n    @loggedMethod\n    greet() {\n        console.log(`Hello, my name is ${this.name}.`);\n    }\n}\n\nconst p = new Person(\"Ray\");\nconst greet = p.greet;\n\n// Works!\ngreet();\n```\n\në°ì½”ë ˆì´í„°ê°€ ë‘ê°œ ì´ìƒì¼ ë• ì—­ìˆœìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤. ê·¸ëŸ¬ëª¨ë¥´ ìœ„ ê²½ìš°ì—ì„  `loggedMethod`ê°€ `greet`ì„ ê°ì‹¸ê³  ìƒˆ í•¨ìˆ˜ë¥¼ ë°˜í™˜í•˜ë©°, ê·¸ ìƒˆ í•¨ìˆ˜ë¥¼ boundê°€ ê°ì‹¸ê²Œ ë˜ê² ë„¤ìš”. ì§€ê¸ˆ ê²½ìš°ì—ì„  ë¬¸ì œê°€\nì•ˆë˜ì§€ë§Œ íŠ¹ì • ìˆœì„œê°€ ì¤‘ìš”í•œ êµ¬ì¡°ì—ì„œëŠ” ë¬¸ì œê°€ ë˜ë¯€ë¡œ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.\n\nì—¬ê¸°ì— ì•½ê°„ì˜ ê¸°ìˆ ì„ ë”í•˜ë©´ ë°ì½”ë ˆì´í„°ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n\n```typescript\nfunction loggedMethod(headMessage = \"LOG:\") {\n    return function actualDecorator(originalMethod: any, context: ClassMethodDecoratorContext) {\n        const methodName = String(context.name);\n\n        function replacementMethod(this: any, ...args: any[]) {\n            console.log(`${headMessage} Entering method '${methodName}'.`)\n            const result = originalMethod.call(this, ...args);\n            console.log(`${headMessage} Exiting method '${methodName}'.`)\n            return result;\n        }\n\n        return replacementMethod;\n    }\n}\n```\n\nì´ë•Œ ìš°ë¦¬ëŠ” ë°˜ë“œì‹œ `loggedMethod`ë¥¼ ë©”ì†Œë“œ ì „ì— í˜¸ì¶œí•´ì•¼ í•˜ê³ , í•„ìš”ì‹œ íŒŒë¼ë¯¸í„°ë„ ë„˜ê²¨ì¤˜ì•¼ í•©ë‹ˆë‹¤.\n\n```typescript\nclass Person {\n    name: string;\n\n    constructor(name: string) {\n        this.name = name;\n    }\n\n    @loggedMethod(\"\")\n    greet() {\n        console.log(`Hello, my name is ${this.name}.`);\n    }\n}\n\nconst p = new Person(\"Ray\");\np.greet();\n\n// Output:\n//\n//    Entering method 'greet'.\n//   Hello, my name is Ray.\n//    Exiting method 'greet'.\n```\n\në°ì½”ë ˆì´í„°ëŠ” ë©”ì†Œë“œ ë¿ë§Œ ì•„ë‹ˆë¼ í”„ë¡œí¼í‹°, í•„ë“œ, ê²Œí„°, ì„¸í„°, ìë™ì ‘ê·¼ì(`auto-accessor`)ê¹Œì§€ë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¬ì§€ì–´ëŠ” í´ë˜ìŠ¤ ìŠ¤ìŠ¤ë¡œë„ ì„œë¸Œí´ë˜ì‹±ì´ë‚˜ ë“±ë¡ìœ¼ë¡œ ë°ì½”ë ˆì´íŒ… ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\në°ì½”ë ˆì´í„°ë¥¼ ë” ê¹Šê²Œ ê³µë¶€í•˜ê³  ì‹¶ë‹¤ë©´ [Axel Rauschmayerâ€™s extensive summary.](https://2ality.com/2022/10/javascript-decorators.html)ì„ ì½ì–´\në³´ì„¸ìš”.\ní¬í•¨ëœ ë³€ê²½ì‚¬í•­ì— ëŒ€í•´ ë” ë§ì€ ì •ë³´ë¥¼ ì•Œê³  ì‹¶ë‹¤ë©´ [ì›ë³¸ í’€ë¦¬í€˜ìŠ¤íŠ¸](https://github.com/microsoft/TypeScript/pull/50820)ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”.",
    "docType": "original",
    "category": "Research",
    "tags": [
      "TypeScript"
    ],
    "readingTime": 4,
    "wordCount": 788,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "mochaì˜-node-envëŠ”",
    "slug": "mochayi-node-envneun",
    "path": "languages/typescript",
    "fullPath": "languages/typescript/mochayi-node-envneun",
    "title": "Mochaì˜ NODE_ENVëŠ”?",
    "excerpt": "Mochaì˜ NODE_ENVëŠ”? ì°¸ì¡° https://github.com/mochajs/mocha/issues/185 23ë…„ 1ì›” 24ì¼ ê¸°ì¤€ í™•ì¸í•´ë³¸ ê²°ê³¼ `\"mocha\": \"^10.2.0\" ì—ì„œ undefinedì´ë‹¤....",
    "content": "# Mochaì˜ NODE_ENVëŠ”?\n\n> ì°¸ì¡° https://github.com/mochajs/mocha/issues/185\n\n23ë…„ 1ì›” 24ì¼ ê¸°ì¤€ í™•ì¸í•´ë³¸ ê²°ê³¼ \n`\"mocha\": \"^10.2.0\" ì—ì„œ undefinedì´ë‹¤.",
    "docType": "original",
    "category": "Research",
    "tags": [
      "TypeScript"
    ],
    "readingTime": 1,
    "wordCount": 18,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "enum-ëŒ€ì‹ -ì‚¬ìš©í• -ìˆ˜-ìˆëŠ”-as-const",
    "slug": "enum-daesin-sayonghal-su-issneun-as-const",
    "path": "languages/typescript",
    "fullPath": "languages/typescript/enum-daesin-sayonghal-su-issneun-as-const",
    "title": "Enum ëŒ€ì‹  ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” `as const`",
    "excerpt": "Enum ëŒ€ì‹  ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìì„¸í•œ ë‚´ìš©ì€ ìš°ì•„í•œí˜•ì œë“¤ ê¸°ìˆ  ë¸”ë¡œê·¸ ì°¸ê³  ë¶€íƒë“œë¦½ë‹ˆë‹¤. ë§í¬ ```typescript / const NodeEnv...",
    "content": "# Enum ëŒ€ì‹  ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” `as const`\n\n> ìì„¸í•œ ë‚´ìš©ì€ ìš°ì•„í•œí˜•ì œë“¤ ê¸°ìˆ  ë¸”ë¡œê·¸ ì°¸ê³  ë¶€íƒë“œë¦½ë‹ˆë‹¤. [ë§í¬](https://techblog.woowahan.com/9804/#toc-1)\n\n```typescript\n/**\n * const NodeEnvMap: {  \n *   readonly Local: \"local\";  \n *   readonly Dev: \"dev\";  \n *   readonly Prod: \"prod\";  \n *   readonly Test: \"test\";  \n * }\n */\nexport const NodeEnvMap = {\n\tLocal: 'local',\n\tDev: 'dev',\n\tProd: 'prod',\n\tTest: 'test',\n} as const;\n  \n// type NodeEnvMapType = \"local\" | \"dev\" | \"prod\" | \"test\"\nexport type NodeEnvMapType = typeof NodeEnvMap[keyof typeof NodeEnvMap];\n```\n\n`as const`ë¥¼ ì•ˆí•´ì£¼ë©´ ì•„ë˜ì™€ ê°™ì´ `string`!\n\n```typescript\n/**\n* const NodeEnvMap: {\n*   Local: string;\n*   Dev: string;\n*   Prod: string;\n*   Test: string;\n* }\n*/\nexport const NodeEnvMap = {\n\tLocal: 'local',\n\tDev: 'dev',\n\tProd: 'prod',\n\tTest: 'test',\n} \n```",
    "docType": "original",
    "category": "Research",
    "tags": [
      "TypeScript"
    ],
    "readingTime": 1,
    "wordCount": 122,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "rustì—ì„œ-os-cpu-ê°¯ìˆ˜-ì°¾ê¸°",
    "slug": "rusteseo-os-cpu-gaessu-cajgi",
    "path": "languages/rust",
    "fullPath": "languages/rust/rusteseo-os-cpu-gaessu-cajgi",
    "title": "Rustì—ì„œ OS CPU ê°¯ìˆ˜ ì°¾ê¸°",
    "excerpt": "ì°¸ê³  Rustì—ì„œ OS CPU ê°¯ìˆ˜ ì°¾ê¸° íƒœì´ˆì˜ ë°©ì‹ (deprecated) ```rs extern crate num_cpus; let num = num_cpus::get(...",
    "content": "> ì°¸ê³ \n\n# Rustì—ì„œ OS CPU ê°¯ìˆ˜ ì°¾ê¸°\n\n## íƒœì´ˆì˜ ë°©ì‹ (deprecated)\n\n```ini\n[dependencies]\nnum_cpus = \"1.0\"\n```\n\n```rs\nextern crate num_cpus;\nlet num = num_cpus::get();\n```\n\n## ê·¸ ë‹¤ìŒ ë°©ì‹ (deprecated)\n\n```rs\nfn main() {\n    println!(\"{}\", std::os::num_cpus());\n}\n```\n\n## í˜„ì¬ ë°©ì‹\n\n```rs\n// rustc 1.67.0 (fc594f156 2023-01-24)\nstd::thread::available_parallelism().unwrap().get()\n```",
    "docType": "original",
    "category": "Research",
    "tags": [
      "Rust"
    ],
    "readingTime": 1,
    "wordCount": 52,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ë¹„íŠ¸-ì—°ì‚°-í™œìš©",
    "slug": "biteu-yeonsan-hwalyong",
    "path": "languages/others",
    "fullPath": "languages/others/biteu-yeonsan-hwalyong",
    "title": "ë¹„íŠ¸ ì—°ì‚° í™œìš©",
    "excerpt": "ë¹„íŠ¸ ì—°ì‚° í™œìš© ì¢…ë¥˜ ë¹„íŠ¸ ì—°ì‚°ì ì„¤ëª… &amp;</...",
    "content": "# ë¹„íŠ¸ ì—°ì‚° í™œìš©\n\n## ì¢…ë¥˜\n\n<table class=\"tb-2\" >\n\t<thead>\n\t\t<tr class=\"bg\">\n\t\t\t<th>ë¹„íŠ¸ ì—°ì‚°ì</th>\n\t\t\t<th>ì„¤ëª…</th>\n\t\t</tr>\n\t</thead>\n\t<tbody>\n\t\t<tr>\n\t\t\t<td>&amp;</td>\n\t\t\t<td>ëŒ€ì‘ë˜ëŠ” ë¹„íŠ¸ê°€ ëª¨ë‘ 1ì´ë©´ 1ì„ ë°˜í™˜í•¨. (ë¹„íŠ¸ AND ì—°ì‚°)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>|</td>\n\t\t\t<td>ëŒ€ì‘ë˜ëŠ” ë¹„íŠ¸ ì¤‘ì—ì„œ í•˜ë‚˜ë¼ë„ 1ì´ë©´ 1ì„ ë°˜í™˜í•¨. (ë¹„íŠ¸ OR ì—°ì‚°)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>^</td>\n\t\t\t<td>ëŒ€ì‘ë˜ëŠ” ë¹„íŠ¸ê°€ ì„œë¡œ ë‹¤ë¥´ë©´ 1ì„ ë°˜í™˜í•¨. (ë¹„íŠ¸ XOR ì—°ì‚°)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>~</td>\n\t\t\t<td>ë¹„íŠ¸ë¥¼ 1ì´ë©´ 0ìœ¼ë¡œ, 0ì´ë©´ 1ë¡œ ë°˜ì „ì‹œí‚´. (ë¹„íŠ¸ NOT ì—°ì‚°, 1ì˜ ë³´ìˆ˜)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>&lt;&lt;</td>\n\t\t\t<td>ëª…ì‹œëœ ìˆ˜ë§Œí¼ ë¹„íŠ¸ë“¤ì„ ì „ë¶€ ì™¼ìª½ìœ¼ë¡œ ì´ë™ì‹œí‚´. (left shift ì—°ì‚°)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>&gt;&gt;</td>\n\t\t\t<td>ë¶€í˜¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì§€ì •í•œ ìˆ˜ë§Œí¼ ë¹„íŠ¸ë¥¼ ì „ë¶€ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™ì‹œí‚´. (right shift ì—°ì‚°)</td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td>&gt;&gt;&gt;</td>\n\t\t\t<td>ì§€ì •í•œ ìˆ˜ë§Œí¼ ë¹„íŠ¸ë¥¼ ì „ë¶€ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™ì‹œí‚¤ë©°, ìƒˆë¡œìš´ ë¹„íŠ¸ëŠ” ì „ë¶€ 0ì´ ë¨.</td>\n\t\t</tr>\n\t</tbody>\n</table>\n\n## ì¢…ë¥˜ ë³„ ì˜ˆì‹œ\n\n![](http://www.tcpschool.com/lectures/img_php_bitwise_and.png)\n![](http://www.tcpschool.com/lectures/img_php_bitwise_or.png)\n![](http://www.tcpschool.com/lectures/img_php_bitwise_xor.png)\n![](http://www.tcpschool.com/lectures/img_php_bitwise_not.png)\n\n## ë¹„íŠ¸ë§ˆìŠ¤í¬\n\n<table>\n<thead>\n<tr>\n<td>\n    ì—°ì‚°\n</td>\n<td>\n    ì‚¬ìš© ì˜ˆì‹œ\n</td>\n</tr>\n</thead>\n<tbody>\n<tr>\n</tr>\n<tr>\n<td>\n    ië²ˆì§¸ ìš”ì†Œ ì¡°íšŒí•˜ê¸°\n</td>\n<td >\n\n```\n// 10 & (1 << 2)\n// 1010 & 0100\n// => 0000\n// ê²°ê³¼ê°’ì˜ idx=3 ìš”ì†ŒëŠ” ië²ˆì§¸ ìš”ì†Œì˜ ì¡´ì¬ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤ \nn & (1 << i);\n```\n\n</td>\n</tr>\n<tr>\n<td>\n    ë³€ê²½(ì‚½ì…)\n</td>\n<td>\n\n```\n// 10 | (1 << 2)\n// 1010 | 0100\n// => 1110\n// ê²°ê³¼ê°’ì˜ idx=2 ìš”ì†Œë¥¼ 1ë¡œ ë§Œë“¤ì—ˆë‹¤\nn | (1 << i)\n```\n\n</td>\n</tr>\n<tr>\n<td>\n    ì‚­ì œ\n</td>\n<td>\n\n```\n// 15 & ~(1 << 2) \n// 1111 & ~0100\n// 1111 & 1011\n// => 1011\n// ê²°ê³¼ê°’ì˜ idx=2 ìš”ì†Œë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì—ˆë‹¤\nn & ~(1 << i)\n```\n\n</td>\n</tr>\n<tr>\n<td>\nê³µì§‘í•©\n</td>\n<td>\n\n```\nint result = 0;\n```\n\n</td>\n</tr>\n<tr>\n<td>\nê½‰ ì°¬ ì§‘í•©\n</td>\n<td>\n\n```\n// Aê°œì˜ ì›ì†Œë¥¼ ê°€ì§„ ì§‘í•©ì˜ ì¢…ë¥˜\n// ì í™”ì‹ìœ¼ë¡œëŠ” (2**n) - 1\nint result = ((1 << A) - 1);\n```\n\n</td>\n</tr>\n<tr>\n<td>\nìµœì†Œ ì›ì†Œ ì°¾ê¸°\n</td>\n<td>\n\n```\nint firstBit = b & -b;\n```\n</td>\n</tr>\n<tr>\n<td>\nìµœì†Œ ì›ì†Œ ì§€ìš°ê¸°\n</td>\n<td>\n\n```\nint removed = origin & (origin-1);\n```\n</td>\n</tr>\n<tr>\n<td>\në¶€ë¶„ ì§‘í•© ìˆœíšŒ\n</td>\n<td>\n\n```\nì§‘í•© Aì˜ ë¶€ë¶„ì§‘í•© ìˆœíšŒ\nfor (int i = A;; i = ((i - 1) & A)) {\n    ...\n}\n```\n</td>\n</tr>\n</tbody>\n</table>\n\n## ì°¸ê³  ë¬¸ì„œ\n\n- [ë¹„íŠ¸ë§ˆìŠ¤í¬](https://hongjuzzang.github.io/bitmask/bit_mask/#-%EB%B9%84%ED%8A%B8%EB%A7%88%EC%8A%A4%ED%81%AC)",
    "docType": "original",
    "category": "Research",
    "tags": [],
    "readingTime": 2,
    "wordCount": 359,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "c-overview",
    "slug": "c-overview",
    "path": "languages/others",
    "fullPath": "languages/others/c-overview",
    "title": "C# Overview",
    "excerpt": "C Overview Summary of the data types in C with example code snippets Value types bool: represents a Boolean value (true or false) ```cs b...",
    "content": "# C# Overview\n\nSummary of the data types in C# with example code snippets\n\n## Value types\n\n- bool: represents a Boolean value (true or false)\n\n```cs\nbool isCorrect = true;\n```\n\n- char: represents a single Unicode character\n\n```cs\nchar letter = 'a';\n```\n\n- byte: represents an 8-bit unsigned integer\n\n```cs\nbyte b = 255;\n```\n\n- short: represents a 16-bit signed integer\n\n```cs\nshort s = -32768;\n```\n\n- int: represents a 32-bit signed integer\n\n```cs\nint i = 42;\n```\n\n- long: represents a 64-bit signed integer\n\n```cs\nlong l = 1000000000L;\n```\n\n- float: represents a single-precision floating-point number\n\n```cs\nfloat f = 3.1415927f;\n```\n\n- double: represents a double-precision floating-point number\n\n```cs\ndouble d = 3.141592653589793;\n```\n\n- decimal: represents a decimal number with up to 28 significant digits\n\n```cs\ndecimal price = 9.99M;\n```\n\n## Reference types\n\n- string: represents a sequence of Unicode characters\n\n```cs\nstring greeting = \"Hello, world!\";\n```\n\n- object: represents an instance of any type\n\n```cs\nobject obj = new object();\n```\n\ndynamic: represents a type that is determined at runtime\n\n```cs\ndynamic dynamicVar = \"hello\";\nConsole.WriteLine(dynamicVar.GetType()); // System.String\ndynamicVar = 42;\nConsole.WriteLine(dynamicVar.GetType()); // System.Int32\n```\n\n- array: represents a collection of elements of the same type\n\n```cs\nint[] numbers = { 1, 2, 3, 4, 5 };\n```\n\nclass: represents a blueprint for creating objects\n\n```cs\npublic class Person\n{\npublic string Name { get; set; }\npublic int Age { get; set; }\n}\n\nPerson person = new Person { Name = \"Alice\", Age = 30 };\n```\n\n- interface: represents a contract for implementing functionality\n\n```csharp\npublic interface IShape\n{\ndouble GetArea();\n}\n\npublic class Rectangle : IShape\n{\npublic double Width { get; set; }\npublic double Height { get; set; }\n\n    public double GetArea()\n    {\n        return Width * Height;\n    }\n\n}\n\nIShape shape = new Rectangle { Width = 10, Height = 20 };\ndouble area = shape.GetArea();\n```",
    "docType": "original",
    "category": "Research",
    "tags": [],
    "readingTime": 2,
    "wordCount": 327,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "dart-programming-language",
    "slug": "dart-programming-language",
    "path": "languages/dart",
    "fullPath": "languages/dart/dart-programming-language",
    "title": "Dart Programming Language",
    "excerpt": "Dart Programming Language Hello world Variables The Var keywor...",
    "content": "# Dart Programming Language\n\n## Hello world\n\n```dart\nvoid main() {\n  print('hello world');\n}\n\ndart run main.dart\n```\n\n## Variables\n\n### The Var keyword\n\n```dart\nvoid main() {\n  // Compiler automatically infers type of value\n  var name = 'seogyugim';\n\n  // Declare obviously type of value\n  String strname = 'seogyugim';\n}\n```\n\n### Dynamic Type\n\n```dart\nvoid main() {\n  dynamic name;\n  if (name is String) {\n    // Compiler already knows type of name\n    name.isEmpty;\n  }\n\n  // And dart support optional chaining\n  name?.isEmpty;\n}\n```\n\n### Null Safety\n\n```dart\nbool isEmpty(String s) => s.length == 0;\n\nvoid main() {\n  // It'll throw NoSuchMethodError\n  isEmpty(null);\n\n  // It's not okay\n  String name1 = 'seogyugim';\n  // Because name2 must be not null\n  if (name1 != null) {\n    nico.isNotEmpty;\n  }\n\n  // It's not okay\n  String? name2 = 'seogyugim';\n  name = null;\n  // Because name could be a null\n  name.isNotEmpty\n  if (name2 != null) {\n    nico.isNotEmpty;\n  }\n\n  // It's okay\n  String? name = 'seogyugim';\n  name = null;\n  name?.isNotEmpty;\n}\n```\n\n### Final\n\n```dart\nvoid main() {\n  // As same as val of kotlin, const of javascript\n  final name = 'seogyugim';\n}\n```\n\n### Late Variables\n\n```dart\nvoid main() {\n  // We can create variable without data with late keyword\n  late final String name;\n  late var name2;\n\n  // It will throw Error because it is not definitely assigned\n  print(name);\n}\n```\n\n### Constant Variables\n\n```dart\nvoid main() {\n  // compile-time constant\n  const name = 'seogyugim';\n  // Error\n  name = ''\n\n  // OK\n  const API_KEY = '123123';\n  // Error, because compiler don't know when compile-time.\n  const apiRes = fetchApi();\n  // OK\n  final apiRes = fetchApi();\n}\n```\n\n## Data Types\n\n### Basics\n\n```dart\nvoid main() {\n  String name = 'seogyugim';\n  bool isExist = true;\n  int age = 30;\n  double money = 0.01;\n\n // father class of int and double\n // abstract class int extends num { ...\n  num x = 12;\n}\n```\n\n### Lists\n\n```dart\nvoid main() {\n // Type:  List<int>\n var numbers = [1, 2, 3, 4];\n\n // abstract class List<E> implements ...\n List<int> nums = [1, 2, 3, 4];\n numbs.first;\n numbs.last;\n numbs.add(3);\n numbs.contains(9);\n\n var giveMeFive = true;\n var nums = [\n  1,\n  2,\n  3,\n  // Collection If\n  if (giveMeFive) 5,\n ];\n}\n```\n\n### String Interpolation\n\n```dart\nvoid main() {\n var age = 10;\n var hello = \"Hello everyone! my name is $name, and I\\'m ${age + 1} Nice to meet you!\";\n print(hello);\n}\n```\n\n### Collection For\n\n```dart\nvoid main() {\n var oldFriends = [\"nico\", \"lynn\"];\n var newFriends = [\n  \"seogyugim\",\n  \"kyoungseo\",\n  for (var f in oldFriends) \"Hi $f\",\n ];\n}\n```\n\n### Maps\n\n```dart\nvoid main() {\n // Object in Dart is as same as 'any' type in Typescript\n var p = {\n  \"name\": 'seogyugim',\n  \"xp\": 100,\n  \"po\": 100,\n };\n Map<int, bool> existanceTable = {\n  1: true,\n  2: false,\n  3: true,\n };\n}\n```\n\n### Sets\n\n```dart\nvoid main() {\n var numbers = {1, 2, 3, 4};\n Set<int> nums = {1, 2, 3, 4};\n}\n```\n\n## Functions\n\n### How to define\n\n```dart\nvoid sayHello(String name) {\n print(\"Hello $name, nice to meet you!\");\n}\n\nvoid retHello(String name) => \"Hello $name, nice to meet you!\";\n\nvoid main() {\n sayHello('seogyugim');\n print(rethello('seogyugim'));\n}\n```\n\n### Named Parameters\n\n```dart\nString hello(\n String name,\n int age,\n String country,\n) {\n return \"$name, $age, $country\";\n}\n\nString namedDefaultHello({\n String name = 'anonymous',\n int age = 50,\n String country = 'Korea',\n}) {\n return \"$name, $age, $country\";\n}\n\nString namedRequiredHello({\n required String name,\n required int age,\n required String country,\n}) {\n return \"$name, $age, $country\";\n}\n\nvoid main() {\n print(hello(\n  'seogyugim',\n  30,\n  'Korea',\n ));\n\n print(namedDefaultHello());\n\n print(namedRequiredHello(\n  name: 'seogyugim',\n  age: 30,\n  country: 'Korea',\n ));\n}\n```\n\n### Optional Positional Parameters\n\n```dart\nString sayHello(String name, int age, [String? country = \"Hello\"]) {\n return \"$name, $age, $country\";\n}\n\nvoid main(List<String> args) {\n print(sayHello(\"Hello\",31,));\n}\n```\n\n### Question Question Operator\n\n```dart\nString getName([String? name]) => name?.toUpperCase() ?? \"Kim Seogyu\";\n\nvoid main(List<String> args) {\n String name = getName();\n String? name2;\n name2 ??= \"Example\";\n print(name);\n print(name2);\n}\n```\n\n### Typedef\n\n```dart\ntypedef ListOfInts = List<int>;\n\nListOfInts reverseListOfNumbers(ListOfInts list) {\n var reversed = list.reversed;\n return reversed.toList();\n}\n\nvoid main() {\n reverseListOfNumbers([1,2,3]);\n}\n```\n\n## Classes\n\n### Constructors\n\n### Named Constructor Parameters\n\n### Named Constructors\n\n### Cascade Notations\n\n### Enums\n\n### Abstract Classes\n\n### Inheritance\n\n### Mixins",
    "docType": "original",
    "category": "Research",
    "tags": [],
    "readingTime": 4,
    "wordCount": 709,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "minioì˜-íë§-healing-ë©”ì»¤ë‹ˆì¦˜",
    "slug": "minioyi-hilring-healing-mekeonijeum",
    "path": "distributed-systems/minio",
    "fullPath": "distributed-systems/minio/minioyi-hilring-healing-mekeonijeum",
    "title": "MinIOì˜ íë§(Healing) ë©”ì»¤ë‹ˆì¦˜",
    "excerpt": "MinIOì˜ íë§(Healing) ë©”ì»¤ë‹ˆì¦˜ MinIOì˜ íë§ ë©”ì»¤ë‹ˆì¦˜ì€ ë¶„ì‚° ìŠ¤í† ë¦¬ì§€ í™˜ê²½ì—ì„œ ë°ì´í„° ë‚´êµ¬ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•œ í•µì‹¬ ê¸°ëŠ¥ì…ë‹ˆë‹¤. ì´ ë©”ì»¤ë‹ˆì¦˜ì€ ë””ìŠ¤í¬ ì¥ì• , ì†ìƒëœ ë©”íƒ€ë°ì´í„°, ë¶ˆì™„ì „í•œ ì“°ê¸° ë“±ì„ ê°ì§€í•˜ê³  ìë™ìœ¼ë¡œ ë³µêµ¬í•©ë‹ˆë‹¤. íë§ í”„...",
    "content": "# MinIOì˜ íë§(Healing) ë©”ì»¤ë‹ˆì¦˜\n\nMinIOì˜ íë§ ë©”ì»¤ë‹ˆì¦˜ì€ ë¶„ì‚° ìŠ¤í† ë¦¬ì§€ í™˜ê²½ì—ì„œ ë°ì´í„° ë‚´êµ¬ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•œ í•µì‹¬ ê¸°ëŠ¥ì…ë‹ˆë‹¤. ì´ ë©”ì»¤ë‹ˆì¦˜ì€ ë””ìŠ¤í¬ ì¥ì• , ì†ìƒëœ ë©”íƒ€ë°ì´í„°, ë¶ˆì™„ì „í•œ ì“°ê¸° ë“±ì„ ê°ì§€í•˜ê³  ìë™ìœ¼ë¡œ ë³µêµ¬í•©ë‹ˆë‹¤.\n\n## 1. íë§ í”„ë¡œì„¸ìŠ¤ ê°œìš”\n\níë§ì€ ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë¡œ ì§„í–‰ë©ë‹ˆë‹¤:\n\n```go\nfunc (er *erasureObjects) healObject(ctx context.Context, bucket, object, versionID string, opts madmin.HealOpts) (result madmin.HealResultItem, err error) {\n    // 1. ê°ì²´ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘\n    // 2. ê°ì²´ ì†ìƒ ì—¬ë¶€ í™•ì¸\n    // 3. í•„ìš”ì‹œ ë°ì´í„° ë³µêµ¬\n    // 4. ë³µêµ¬ëœ ë°ì´í„° ì¬ë°°í¬\n}\n```\n\n## 2. ì†ìƒ ê°ì§€ ë©”ì»¤ë‹ˆì¦˜\n\n### ë””ìŠ¤í¬ ìƒíƒœ ëª¨ë‹ˆí„°ë§\n\nMinIOëŠ” ì§€ì†ì ìœ¼ë¡œ ë””ìŠ¤í¬ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤:\n\n```go\nfunc diskErrToDriveState(err error) (state string) {\n    if err == nil {\n        return madmin.DriveStateOk\n    }\n    switch {\n    case errors.Is(err, errDiskNotFound):\n        return madmin.DriveStateOffline\n    case errors.Is(err, errCorruptedFormat):\n        return madmin.DriveStateCorrupt\n    // ... ê¸°íƒ€ ìƒíƒœ í™•ì¸\n    }\n    return madmin.DriveStateUnknown\n}\n```\n\n### ê°ì²´ ë¬´ê²°ì„± í™•ì¸\n\n`checkObjectWithAllParts` í•¨ìˆ˜ëŠ” ê°ì²´ì˜ ëª¨ë“  ë¶€ë¶„ì´ ì˜¬ë°”ë¥´ê²Œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤:\n\n```go\nfunc checkObjectWithAllParts(ctx context.Context, onlineDisks []StorageAPI, partsMetadata []FileInfo,\n    errs []error, latestMeta FileInfo, filterByETag bool, bucket, object string,\n    scanMode madmin.HealScanMode) (dataErrsByDisk map[int][]int, dataErrsByPart map[int][]int) {\n    // ê° ë””ìŠ¤í¬ì—ì„œ ê°ì²´ ë¶€ë¶„ í™•ì¸\n    // ëˆ„ë½ë˜ê±°ë‚˜ ì†ìƒëœ ë¶€ë¶„ ì‹ë³„\n    // ë””ìŠ¤í¬ë³„, ë¶€ë¶„ë³„ ì˜¤ë¥˜ ë§¤í•‘\n}\n```\n\n## 3. íë§ ê²°ì • ë¡œì§\n\nMinIOëŠ” ë‹¤ìŒ ì¡°ê±´ì— ë”°ë¼ íë§ì´ í•„ìš”í•œì§€ ê²°ì •í•©ë‹ˆë‹¤:\n\n```go\nfunc shouldHealObjectOnDisk(erErr error, partsErrs []int, meta FileInfo, latestMeta FileInfo) (bool, bool, error) {\n    switch {\n    case erErr != nil:\n        // ë””ìŠ¤í¬ ì˜¤ë¥˜ ë°œìƒ ì‹œ íë§ í•„ìš”\n        return true, false, nil\n    case !meta.IsValid():\n        // ë©”íƒ€ë°ì´í„°ê°€ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš° íë§ í•„ìš”\n        return true, false, nil\n    case meta.XLV1:\n        // êµ¬ ë²„ì „ í˜•ì‹ì˜ ê²½ìš° ì—…ê·¸ë ˆì´ë“œ í•„ìš”\n        return true, false, nil\n    case meta.ModTime.Before(latestMeta.ModTime):\n        // ë©”íƒ€ë°ì´í„°ê°€ ìµœì‹ ì´ ì•„ë‹Œ ê²½ìš° íë§ í•„ìš”\n        return true, false, nil\n    // ... ê¸°íƒ€ ì¡°ê±´\n    }\n    \n    // ë°ì´í„° ë¶€ë¶„ ì†ìƒ í™•ì¸\n    for _, err := range partsErrs {\n        if err != 0 {\n            return true, false, nil\n        }\n    }\n    \n    return false, false, nil\n}\n```\n\n## 4. ë°ì´í„° ë³µêµ¬ ê³¼ì •\n\n### ì¿¼ëŸ¼ ê¸°ë°˜ ë°ì´í„° ë³µêµ¬\n\nMinIOëŠ” ì¶©ë¶„í•œ ìˆ˜ì˜ ì •ìƒ ë””ìŠ¤í¬ê°€ ìˆì„ ë•Œ ì†ìƒëœ ë°ì´í„°ë¥¼ ë³µêµ¬í•©ë‹ˆë‹¤:\n\n```go\nfunc (e Erasure) Heal(ctx context.Context, writers []io.Writer, readers []io.ReaderAt, totalLength int64, prefer []bool) (derr error) {\n    // ë³‘ë ¬ë¡œ ë°ì´í„° ë¸”ë¡ ì½ê¸°\n    // ë¦¬ë“œ-ì†”ë¡œëª¬ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ëˆ„ë½/ì†ìƒëœ ë¸”ë¡ ë³µêµ¬\n    // ë³µêµ¬ëœ ë°ì´í„°ë¥¼ í•´ë‹¹ ë””ìŠ¤í¬ì— ì“°ê¸°\n}\n```\n\n### ë©”íƒ€ë°ì´í„° ë³µêµ¬\n\nê°ì²´ ë©”íƒ€ë°ì´í„° ë³µêµ¬ëŠ” ë³„ë„ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤:\n\n```go\nfunc writeAllMetadata(ctx context.Context, disks []StorageAPI, origbucket, bucket, prefix string, files []FileInfo, quorum int) ([]StorageAPI, error) {\n    // ëª¨ë“  ë””ìŠ¤í¬ì— ë©”íƒ€ë°ì´í„° ì“°ê¸° ì‹œë„\n    // ì¿¼ëŸ¼ ì¶©ì¡± í™•ì¸\n}\n```\n\n## 5. ìë™ íë§ ìŠ¤ìºë„ˆ\n\nMinIOëŠ” ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìŠ¤ìºë„ˆë¥¼ ì‹¤í–‰í•˜ì—¬ ì†ìƒëœ ê°ì²´ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤:\n\n```go\nfunc (er erasureObjects) nsScanner(ctx context.Context, buckets []BucketInfo, wantCycle uint32, updates chan<- dataUsageCache, healScanMode madmin.HealScanMode) error {\n    // ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìŠ¤ìº”\n    // ê°ì²´ ìƒíƒœ í™•ì¸\n    // í•„ìš”ì‹œ íë§ íì— ì¶”ê°€\n}\n```\n\n## 6. ëŒ•ê¸€ë§(Dangling) ê°ì²´ ì²˜ë¦¬\n\nëŒ•ê¸€ë§ ê°ì²´ëŠ” ë©”íƒ€ë°ì´í„°ëŠ” ìˆì§€ë§Œ ì‹¤ì œ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ë¶ˆì™„ì „í•œ ê°ì²´ì…ë‹ˆë‹¤:\n\n```go\nfunc isObjectDangling(metaArr []FileInfo, errs []error, dataErrsByPart map[int][]int) (validMeta FileInfo, ok bool) {\n    // ë©”íƒ€ë°ì´í„°ì™€ ì‹¤ì œ ë°ì´í„° ìƒíƒœ ë¹„êµ\n    // ë¶ˆì¼ì¹˜ ë°œê²¬ ì‹œ ëŒ•ê¸€ë§ ê°ì²´ë¡œ íŒë‹¨\n}\n```\n\nMinIOëŠ” ëŒ•ê¸€ë§ ê°ì²´ë¥¼ ê°ì§€í•˜ë©´ ìë™ìœ¼ë¡œ ì‚­ì œí•˜ê±°ë‚˜ ë³µêµ¬í•©ë‹ˆë‹¤:\n\n```go\nfunc (er erasureObjects) deleteIfDangling(ctx context.Context, bucket, object string, metaArr []FileInfo, errs []error, dataErrsByPart map[int][]int, opts ObjectOptions) (FileInfo, error) {\n    // ëŒ•ê¸€ë§ ê°ì²´ ê°ì§€\n    // ë³µêµ¬ ê°€ëŠ¥ì„± í‰ê°€\n    // ë³µêµ¬ ë¶ˆê°€ëŠ¥í•˜ë©´ ì•ˆì „í•˜ê²Œ ì œê±°\n}\n```\n\n## 7. ì„±ëŠ¥ ìµœì í™”\n\n### ë³‘ë ¬ íë§\n\nì—¬ëŸ¬ ê°ì²´ì™€ ë””ìŠ¤í¬ë¥¼ ë™ì‹œì— íë§í•˜ì—¬ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤:\n\n```go\nfunc (z *erasureServerPools) HealObjects(ctx context.Context, bucket, prefix string, opts madmin.HealOpts, healObjectFn HealObjectFn) error {\n    // ë³‘ë ¬ë¡œ ê°ì²´ ìŠ¤ìº”\n    // ë™ì‹œì— ì—¬ëŸ¬ ê°ì²´ íë§\n}\n```\n\n### íë§ ì¶”ì  ë° ë©”íŠ¸ë¦­\n\n```go\nfunc healTrace(funcName healingMetric, startTime time.Time, bucket, object string, opts *madmin.HealOpts, err error, result *madmin.HealResultItem) {\n    // íë§ ì‘ì—… ì¶”ì \n    // ì„±ëŠ¥ ë° ê²°ê³¼ ë©”íŠ¸ë¦­ ìˆ˜ì§‘\n}\n```\n\n## ê²°ë¡ \n\nMinIOì˜ íë§ ë©”ì»¤ë‹ˆì¦˜ì€ ë¶„ì‚° í™˜ê²½ì—ì„œ ë°ì´í„° ì¼ê´€ì„±ê³¼ ë‚´êµ¬ì„±ì„ ë³´ì¥í•˜ëŠ” í•µì‹¬ ê¸°ëŠ¥ì…ë‹ˆë‹¤. ë””ìŠ¤í¬ ì˜¤ë¥˜, ë°ì´í„° ì†ìƒ, ë©”íƒ€ë°ì´í„° ë¶ˆì¼ì¹˜ ë“± ë‹¤ì–‘í•œ ì¥ì•  ìƒí™©ì„ ê°ì§€í•˜ê³ , ë¦¬ë“œ-ì†”ë¡œëª¬ ì´ë ˆì´ì € ì½”ë”©ì„ í†µí•´ ìë™ìœ¼ë¡œ ë³µêµ¬í•¨ìœ¼ë¡œì¨ ë°ì´í„° ì†ì‹¤ ì—†ì´ ì‹œìŠ¤í…œì´ ì§€ì†ì ìœ¼ë¡œ ì‘ë™í•˜ë„ë¡ í•©ë‹ˆë‹¤.",
    "docType": "original",
    "category": "Distributed Systems",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 4,
    "wordCount": 631,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "minioì˜-ì´ë ˆì´ì €-ì½”ë”©-êµ¬í˜„",
    "slug": "minioyi-ireijeo-koding-guhyeon",
    "path": "distributed-systems/minio",
    "fullPath": "distributed-systems/minio/minioyi-ireijeo-koding-guhyeon",
    "title": "MinIOì˜ ì´ë ˆì´ì € ì½”ë”© êµ¬í˜„",
    "excerpt": "MinIOì˜ ì´ë ˆì´ì € ì½”ë”© êµ¬í˜„ MinIOëŠ” ë¶„ì‚° ê°ì²´ ìŠ¤í† ë¦¬ì§€ ì‹œìŠ¤í…œìœ¼ë¡œ, ë°ì´í„° ë‚´êµ¬ì„±ê³¼ ê°€ìš©ì„±ì„ ìœ„í•´ ì´ë ˆì´ì € ì½”ë”©(Erasure Coding)ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤. ì´ë ˆì´ì € ì½”ë”©ì€ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ê³  íŒ¨ë¦¬í‹° ì¡°ê°ì„ ì¶”ê°€í•˜ì—¬ ì¼ë¶€ ë””ìŠ¤í¬ ì†ì‹¤ì—ë„ ë°ì´...",
    "content": "# MinIOì˜ ì´ë ˆì´ì € ì½”ë”© êµ¬í˜„\n\nMinIOëŠ” ë¶„ì‚° ê°ì²´ ìŠ¤í† ë¦¬ì§€ ì‹œìŠ¤í…œìœ¼ë¡œ, ë°ì´í„° ë‚´êµ¬ì„±ê³¼ ê°€ìš©ì„±ì„ ìœ„í•´ ì´ë ˆì´ì € ì½”ë”©(Erasure Coding)ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤. ì´ë ˆì´ì € ì½”ë”©ì€ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ê³  íŒ¨ë¦¬í‹° ì¡°ê°ì„ ì¶”ê°€í•˜ì—¬ ì¼ë¶€ ë””ìŠ¤í¬ ì†ì‹¤ì—ë„ ë°ì´í„°ë¥¼ ë³µêµ¬í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\n\n## í•µì‹¬ êµ¬ì¡°\n\n### 1. êµ¬ì¡°ì²´ ê³„ì¸µ\n\n- **Erasure**: ì‹¤ì œ ì¸ì½”ë”©/ë””ì½”ë”©ì„ ìˆ˜í–‰í•˜ëŠ” ê¸°ë³¸ êµ¬ì¡°ì²´ (`erasure-coding.go`)\n  ```go\n  type Erasure struct {\n    encoder                  func() reedsolomon.Encoder\n    dataBlocks, parityBlocks int\n    blockSize                int64\n  }\n  ```\n\n- **erasureObjects**: ê°ì²´ ìŠ¤í† ë¦¬ì§€ ì‘ì—…ì„ ì²˜ë¦¬í•˜ëŠ” êµ¬ì¡°ì²´ (`erasure.go`)\n  ```go\n  type erasureObjects struct {\n    setDriveCount      int\n    defaultParityCount int\n    setIndex           int\n    poolIndex          int\n    getDisks           func() []StorageAPI\n    // ...ê¸°íƒ€ í•„ë“œ\n  }\n  ```\n\n- **erasureSets**: ì—¬ëŸ¬ erasureObjects ì„¸íŠ¸ë¥¼ ê´€ë¦¬ (`erasure-sets.go`)\n\n- **erasureServerPools**: ì—¬ëŸ¬ erasureSets í’€ì„ ê´€ë¦¬í•˜ëŠ” ìµœìƒìœ„ ê³„ì¸µ (`erasure-server-pool.go`)\n\n### 2. ì½”ë”© ë©”ì»¤ë‹ˆì¦˜\n\nMinIOëŠ” Reed-Solomon ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì´ë ˆì´ì € ì½”ë”©ì„ êµ¬í˜„í•©ë‹ˆë‹¤:\n\n```go\nfunc NewErasure(ctx context.Context, dataBlocks, parityBlocks int, blockSize int64) (e Erasure, err error) {\n  e = Erasure{\n    dataBlocks:   dataBlocks,\n    parityBlocks: parityBlocks,\n    blockSize:    blockSize,\n  }\n  e.encoder = func() reedsolomon.Encoder {\n    // Reed-Solomon ì¸ì½”ë” ì´ˆê¸°í™”\n    return encoder\n  }\n  return e, nil\n}\n```\n\n## ë°ì´í„° íë¦„\n\n### 1. ë°ì´í„° ì¸ì½”ë”© (ì“°ê¸°)\n\n`erasure-encode.go`ì˜ `Encode` ë©”ì„œë“œëŠ” ë°ì´í„°ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì²˜ë¦¬í•©ë‹ˆë‹¤:\n\n1. ê°ì²´ ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ë¶„í• \n2. ê° ì²­í¬ë¥¼ Reed-Solomon ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì¸ì½”ë”©í•˜ì—¬ ë°ì´í„° ë¸”ë¡ê³¼ íŒ¨ë¦¬í‹° ë¸”ë¡ ìƒì„±\n3. ë°ì´í„°ì™€ íŒ¨ë¦¬í‹° ë¸”ë¡ì„ ì—¬ëŸ¬ ë””ìŠ¤í¬ì— ë¶„ì‚° ì €ì¥\n\n```go\nfunc (e *Erasure) Encode(ctx context.Context, src io.Reader, writers []io.Writer, buf []byte, quorum int) (total int64, err error) {\n  // ë°ì´í„° ì½ê¸° ë° ì¸ì½”ë”©\n  blocks, err := e.EncodeData(ctx, buf[:n])\n  // ì¸ì½”ë”©ëœ ë¸”ë¡ì„ ì—¬ëŸ¬ ë””ìŠ¤í¬ì— ì“°ê¸°\n  err = writer.Write(ctx, blocks)\n}\n```\n\n### 2. ë°ì´í„° ë””ì½”ë”© (ì½ê¸°)\n\n`erasure-decode.go`ì˜ `Decode` ë©”ì„œë“œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë°ì´í„°ë¥¼ ë³µì›í•©ë‹ˆë‹¤:\n\n1. ì—¬ëŸ¬ ë””ìŠ¤í¬ì—ì„œ ë°ì´í„°ì™€ íŒ¨ë¦¬í‹° ë¸”ë¡ ì½ê¸°\n2. ì¼ë¶€ ë¸”ë¡ì´ ì†ìƒë˜ê±°ë‚˜ ëˆ„ë½ë˜ì—ˆì„ ê²½ìš° Reed-Solomon ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë³µêµ¬\n3. ì›ë³¸ ë°ì´í„° ì¬êµ¬ì„±\n\n```go\nfunc (e Erasure) Decode(ctx context.Context, writer io.Writer, readers []io.ReaderAt, offset, length, totalLength int64, prefer []bool) (written int64, err error) {\n  // ë³‘ë ¬ ì½ê¸°ë¡œ ë°ì´í„° ë¸”ë¡ ìˆ˜ì§‘\n  // í•„ìš”ì‹œ ë°ì´í„° ë³µêµ¬\n  // ì›ë³¸ ë°ì´í„° ì¬êµ¬ì„±í•˜ì—¬ writerì— ì“°ê¸°\n}\n```\n\n## ë‚´êµ¬ì„± ë©”ì»¤ë‹ˆì¦˜\n\n### 1. ì¿¼ëŸ¼ ê¸°ë°˜ ì‘ì—…\n\nMinIOëŠ” ì¿¼ëŸ¼ ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì½ê¸°/ì“°ê¸° ì‘ì—…ì˜ ë‚´êµ¬ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤:\n\n- **ì½ê¸° ì¿¼ëŸ¼(Read Quorum)**: ë°ì´í„° ë¸”ë¡ ìˆ˜ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ì€ ë””ìŠ¤í¬ì—ì„œ ì½ê¸° ì„±ê³µ í•„ìš”\n  ```go\n  func (er erasureObjects) defaultRQuorum() int {\n    return er.setDriveCount - er.defaultParityCount\n  }\n  ```\n\n- **ì“°ê¸° ì¿¼ëŸ¼(Write Quorum)**: ë°ì´í„° ë¸”ë¡ + íŒ¨ë¦¬í‹° ë¸”ë¡ ìˆ˜ì—ì„œ íŒ¨ë¦¬í‹° ë¸”ë¡ ìˆ˜ë¥¼ ëº€ ê²ƒ\n  ```go\n  func (er erasureObjects) defaultWQuorum() int {\n    return er.setDriveCount - er.defaultParityCount\n  }\n  ```\n\n### 2. íë§(Healing) ë©”ì»¤ë‹ˆì¦˜\n\nMinIOëŠ” ìë™ìœ¼ë¡œ ì†ìƒëœ ë°ì´í„°ë¥¼ ê°ì§€í•˜ê³  ë³µêµ¬í•˜ëŠ” íë§ ë©”ì»¤ë‹ˆì¦˜ì„ ì œê³µí•©ë‹ˆë‹¤:\n\n```go\nfunc (er *erasureObjects) healObject(ctx context.Context, bucket, object, versionID string, opts madmin.HealOpts) (result madmin.HealResultItem, err error) {\n  // ê°ì²´ ìƒíƒœ í™•ì¸\n  // ì†ìƒëœ ë¶€ë¶„ ê°ì§€\n  // Reed-Solomon ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©í•˜ì—¬ ë³µêµ¬\n  // ë³µêµ¬ëœ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë¶„ì‚° ì €ì¥\n}\n```\n\n## ê³ ê¸‰ ê¸°ëŠ¥\n\n### 1. ë©€í‹°íŒŒíŠ¸ ì—…ë¡œë“œ\n\nëŒ€ìš©ëŸ‰ ê°ì²´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì—…ë¡œë“œí•˜ê¸° ìœ„í•œ ë©€í‹°íŒŒíŠ¸ ì—…ë¡œë“œ ì§€ì›:\n\n```go\nfunc (er erasureObjects) PutObjectPart(ctx context.Context, bucket, object, uploadID string, partID int, r *PutObjReader, opts ObjectOptions) (pi PartInfo, err error) {\n  // íŒŒíŠ¸ ë°ì´í„°ë¥¼ ì´ë ˆì´ì € ì½”ë”©ìœ¼ë¡œ ì¸ì½”ë”©\n  // ì¸ì½”ë”©ëœ ì¡°ê°ì„ ë””ìŠ¤í¬ì— ì €ì¥\n}\n```\n\n### 2. ë””ìŠ¤í¬ í’€ ë¦¬ë°¸ëŸ°ì‹± ë° ë””ì»¤ë¯¸ì…”ë‹\n\n- **ë¦¬ë°¸ëŸ°ì‹±**: ë””ìŠ¤í¬ ê°„ ë°ì´í„° ì¬ë¶„ë°°\n- **ë””ì»¤ë¯¸ì…”ë‹**: í’€ì—ì„œ ë””ìŠ¤í¬ ì•ˆì „í•˜ê²Œ ì œê±°\n\n## ì„±ëŠ¥ ìµœì í™”\n\n1. **ë³‘ë ¬ I/O ì‘ì—…**: ë™ì‹œì— ì—¬ëŸ¬ ë””ìŠ¤í¬ì—ì„œ ì½ê¸°/ì“°ê¸° ìˆ˜í–‰\n2. **ë²„í¼ í’€ë§**: ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì í™”\n3. **ë¹„íŠ¸ë§µ ê¸°ë°˜ ë””ìŠ¤í¬ ìƒíƒœ ì¶”ì **: ë¹ ë¥¸ ë””ìŠ¤í¬ ìƒíƒœ í™•ì¸\n\n## ê²°ë¡ \n\nMinIOì˜ ì´ë ˆì´ì € ì½”ë”© êµ¬í˜„ì€ Reed-Solomon ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ê³„ì¸µì  êµ¬ì¡°(serverPools > sets > objects)ë¥¼ í†µí•´ í™•ì¥ì„±ì„ ì œê³µí•©ë‹ˆë‹¤. ë°ì´í„° ë¸”ë¡ê³¼ íŒ¨ë¦¬í‹° ë¸”ë¡ì˜ ë¶„ì‚° ì €ì¥, ì¿¼ëŸ¼ ê¸°ë°˜ ì‘ì—…, ìë™ íë§ ê¸°ëŠ¥ìœ¼ë¡œ ë†’ì€ ë‚´êµ¬ì„±ê³¼ ê°€ìš©ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.",
    "docType": "original",
    "category": "Distributed Systems",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 3,
    "wordCount": 569,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "minioì˜-ì†ìƒ-ê°ì§€-ì•Œê³ ë¦¬ì¦˜",
    "slug": "minioyi-sonsang-gamji-algorijeum",
    "path": "distributed-systems/minio",
    "fullPath": "distributed-systems/minio/minioyi-sonsang-gamji-algorijeum",
    "title": "MinIOì˜ ì†ìƒ ê°ì§€ ì•Œê³ ë¦¬ì¦˜",
    "excerpt": "MinIOì˜ ì†ìƒ ê°ì§€ ì•Œê³ ë¦¬ì¦˜ MinIOëŠ” ë¶„ì‚° ì‹œìŠ¤í…œì—ì„œ ë°ì´í„° ì†ìƒì„ ê°ì§€í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ê³„ì¸µì˜ ê²€ì¦ ë©”ì»¤ë‹ˆì¦˜ì„ êµ¬í˜„í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ì•Œê³ ë¦¬ì¦˜ë“¤ì€ ë©”íƒ€ë°ì´í„°ë¶€í„° ì‹¤ì œ ë°ì´í„° ë¸”ë¡ê¹Œì§€ ë‹¤ì–‘í•œ ìˆ˜ì¤€ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤. ë©”íƒ€ë°ì´í„° ê²€ì¦ File...",
    "content": "# MinIOì˜ ì†ìƒ ê°ì§€ ì•Œê³ ë¦¬ì¦˜\n\nMinIOëŠ” ë¶„ì‚° ì‹œìŠ¤í…œì—ì„œ ë°ì´í„° ì†ìƒì„ ê°ì§€í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ê³„ì¸µì˜ ê²€ì¦ ë©”ì»¤ë‹ˆì¦˜ì„ êµ¬í˜„í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ì•Œê³ ë¦¬ì¦˜ë“¤ì€ ë©”íƒ€ë°ì´í„°ë¶€í„° ì‹¤ì œ ë°ì´í„° ë¸”ë¡ê¹Œì§€ ë‹¤ì–‘í•œ ìˆ˜ì¤€ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤.\n\n## 1. ë©”íƒ€ë°ì´í„° ê²€ì¦\n\n### FileInfo ìœ íš¨ì„± ê²€ì¦\n\n```go\nfunc (fi FileInfo) IsValid() bool {\n    if fi.Erasure.DataBlocks == 0 || fi.Erasure.ParityBlocks == 0 {\n        return false\n    }\n    if len(fi.Erasure.Distribution) != (fi.Erasure.DataBlocks + fi.Erasure.ParityBlocks) {\n        return false\n    }\n    for _, checksum := range fi.Parts {\n        if checksum.ETag == \"\" {\n            return false\n        }\n    }\n    return true\n}\n```\n\nì´ í•¨ìˆ˜ëŠ” ê°ì²´ì˜ ë©”íƒ€ë°ì´í„°ê°€ ìœ íš¨í•œì§€ ê²€ì‚¬í•©ë‹ˆë‹¤:\n- ë°ì´í„° ë¸”ë¡ê³¼ íŒ¨ë¦¬í‹° ë¸”ë¡ ìˆ˜ê°€ ì˜¬ë°”ë¥¸ì§€\n- ë¶„ì‚° íŒ¨í„´ì´ ì „ì²´ ë¸”ë¡ ìˆ˜ì™€ ì¼ì¹˜í•˜ëŠ”ì§€\n- ëª¨ë“  ë¶€ë¶„(íŒŒíŠ¸)ì´ ì²´í¬ì„¬ì„ ê°€ì§€ê³  ìˆëŠ”ì§€\n\n### ë²„ì „ ì¼ê´€ì„± ê²€ì‚¬\n\n```go\nfunc findFileInfoInQuorum(ctx context.Context, metaArr []FileInfo, modTime time.Time, etag string, quorum int) (FileInfo, error) {\n    // ë©”íƒ€ë°ì´í„° ë°°ì—´ì—ì„œ ì¿¼ëŸ¼ì„ ë§Œì¡±í•˜ëŠ” ì¼ê´€ëœ ë²„ì „ ì°¾ê¸°\n    // ì‹œê°„ ê¸°ë°˜ ê·¸ë£¹í™” ë° ë²„ì „ ë¹„êµ\n}\n```\n\nì´ í•¨ìˆ˜ëŠ” ì—¬ëŸ¬ ë””ìŠ¤í¬ì—ì„œ ìˆ˜ì§‘í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ë¹„êµí•˜ì—¬ ì¿¼ëŸ¼ì„ ë§Œì¡±í•˜ëŠ” ì •í™•í•œ ë²„ì „ì„ ì°¾ìŠµë‹ˆë‹¤.\n\n## 2. ë°ì´í„° ë¸”ë¡ ê²€ì¦\n\n### ì²´í¬ì„¬ ê²€ì¦\n\nMinIOëŠ” ê° ë°ì´í„° ë¶€ë¶„ì— ëŒ€í•´ ETag(MD5 ì²´í¬ì„¬)ë¥¼ ì €ì¥í•˜ê³  ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë¬´ê²°ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤:\n\n```go\nfunc (e ErasureInfo) GetChecksumInfo(partNumber int) (ckSum ChecksumInfo) {\n    // ì§€ì •ëœ íŒŒíŠ¸ ë²ˆí˜¸ì— ëŒ€í•œ ì²´í¬ì„¬ ì •ë³´ ê²€ìƒ‰\n}\n```\n\në°ì´í„°ë¥¼ ì½ì„ ë•Œ, ê³„ì‚°ëœ ì²´í¬ì„¬ê³¼ ì €ì¥ëœ ì²´í¬ì„¬ì„ ë¹„êµí•˜ì—¬ ì†ìƒ ì—¬ë¶€ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.\n\n### ë¹„íŠ¸ë¡¯ ê²€ì¦\n\në¹„íŠ¸ë¡¯(Bitrot)ì€ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ë°œìƒí•˜ëŠ” ë°ì´í„° ì†ìƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. MinIOëŠ” ì´ë¥¼ ê°ì§€í•˜ê¸° ìœ„í•´ ì¶”ê°€ì ì¸ í•´ì‹œ(ì˜ˆ: SHA-256, HighwayHash)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:\n\n```go\n// BitrotVerifier ì¸í„°í˜ì´ìŠ¤ëŠ” ë¹„íŠ¸ë¡¯ ê°ì§€ë¥¼ ìœ„í•œ ê²€ì¦ ë©”ì»¤ë‹ˆì¦˜ì„ ì œê³µ\ntype BitrotVerifier interface {\n    // ë°ì´í„° ê²€ì¦ì„ ìœ„í•œ ë©”ì„œë“œ\n    Verify(buf []byte) error\n}\n```\n\n## 3. ê°ì²´ ë¶€ë¶„ ê²€ì¦\n\nì‹¤ì œ ë°ì´í„° ë¸”ë¡ì˜ ì¡´ì¬ì™€ ë¬´ê²°ì„±ì„ ê²€ì¦í•˜ëŠ” ê³¼ì •:\n\n```go\nfunc checkObjectWithAllParts(ctx context.Context, onlineDisks []StorageAPI, partsMetadata []FileInfo,\n    errs []error, latestMeta FileInfo, filterByETag bool, bucket, object string,\n    scanMode madmin.HealScanMode) (dataErrsByDisk map[int][]int, dataErrsByPart map[int][]int) {\n    \n    // ê²°ê³¼ ë§µ ì´ˆê¸°í™”\n    dataErrsByDisk = make(map[int][]int)\n    dataErrsByPart = make(map[int][]int)\n    \n    // ê° íŒŒíŠ¸ì— ëŒ€í•œ ìƒíƒœ í™•ì¸\n    for partIdx, partInfo := range latestMeta.Parts {\n        // ê° ë””ìŠ¤í¬ì—ì„œ íŒŒíŠ¸ ë°ì´í„° í™•ì¸\n        for diskIdx, disk := range onlineDisks {\n            if disk == nil {\n                // ë””ìŠ¤í¬ ì˜¤í”„ë¼ì¸\n                continue\n            }\n            \n            // íŒŒíŠ¸ ë°ì´í„° ìƒíƒœ í™•ì¸\n            partPath := filepath.Join(bucket, object, partInfo.ETag)\n            err := disk.CheckParts(ctx, partPath)\n            \n            if err != nil {\n                // ì˜¤ë¥˜ ê¸°ë¡\n                dataErrsByDisk[diskIdx] = append(dataErrsByDisk[diskIdx], partIdx)\n                dataErrsByPart[partIdx] = append(dataErrsByPart[partIdx], diskIdx)\n            }\n        }\n    }\n    \n    return dataErrsByDisk, dataErrsByPart\n}\n```\n\nì´ í•¨ìˆ˜ëŠ”:\n1. ê° ë””ìŠ¤í¬ì—ì„œ ê°ì²´ì˜ ëª¨ë“  ë¶€ë¶„ì„ í™•ì¸\n2. ëˆ„ë½ë˜ê±°ë‚˜ ì†ìƒëœ ë¶€ë¶„ì„ ì‹ë³„\n3. ë””ìŠ¤í¬ë³„, ë¶€ë¶„ë³„ë¡œ ì˜¤ë¥˜ë¥¼ ë§µí•‘í•˜ì—¬ ìƒì„¸í•œ ì†ìƒ ì •ë³´ ì œê³µ\n\n## 4. ì¿¼ëŸ¼ ê¸°ë°˜ ì†ìƒ ê°ì§€\n\nMinIOëŠ” ì¿¼ëŸ¼ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìˆ˜ê²° ì›ì¹™ìœ¼ë¡œ ì†ìƒì„ ê°ì§€í•©ë‹ˆë‹¤:\n\n```go\nfunc reduceReadQuorumErrs(ctx context.Context, errs []error, ignoredErrs []error, readQuorum int) (maxErr error) {\n    // ì˜¤ë¥˜ ìœ í˜•ë³„ ì¹´ìš´íŒ…\n    errCount := make(map[error]int)\n    for _, err := range errs {\n        if err != nil {\n            errCount[err]++\n        }\n    }\n    \n    // ì½ê¸° ì¿¼ëŸ¼ì´ ì¶©ì¡±ë˜ëŠ”ì§€ í™•ì¸\n    if len(errs) - len(errCount) >= readQuorum {\n        return nil // ì¿¼ëŸ¼ ì¶©ì¡±\n    }\n    \n    // ê°€ì¥ ë§ì´ ë°œìƒí•œ ì˜¤ë¥˜ ë°˜í™˜\n    maxCount := 0\n    for err, count := range errCount {\n        if count > maxCount {\n            maxCount = count\n            maxErr = err\n        }\n    }\n    \n    return maxErr\n}\n```\n\nì´ ì ‘ê·¼ ë°©ì‹ì€:\n1. í•„ìš”í•œ ìµœì†Œ ì¿¼ëŸ¼ ìˆ˜ì˜ ë””ìŠ¤í¬ê°€ ë™ì¼í•œ ë°ì´í„°ë¥¼ ê°€ì§ˆ ë•Œ í•´ë‹¹ ë°ì´í„°ê°€ ì •í™•í•˜ë‹¤ê³  íŒë‹¨\n2. ì¿¼ëŸ¼ì— ë¯¸ë‹¬í•˜ëŠ” ê²½ìš° ì†ìƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ë³µêµ¬ ì‹œë„\n\n## 5. ëŒ•ê¸€ë§ ê°ì²´ ê°ì§€\n\nì¼ê´€ì„± ì—†ëŠ” ìƒíƒœì˜ ê°ì²´ë¥¼ ê°ì§€í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜:\n\n```go\nfunc isObjectDangling(metaArr []FileInfo, errs []error, dataErrsByPart map[int][]int) (validMeta FileInfo, ok bool) {\n    // ìœ íš¨í•œ ë©”íƒ€ë°ì´í„° ì°¾ê¸°\n    for _, meta := range metaArr {\n        if meta.IsValid() {\n            validMeta = meta\n            break\n        }\n    }\n    \n    if !validMeta.IsValid() {\n        return FileInfo{}, false // ìœ íš¨í•œ ë©”íƒ€ë°ì´í„° ì—†ìŒ\n    }\n    \n    // ë°ì´í„° íŒŒíŠ¸ì˜ ìƒíƒœ í™•ì¸\n    for partIdx, errDisks := range dataErrsByPart {\n        // ì†ìƒëœ ë””ìŠ¤í¬ ìˆ˜ ê³„ì‚°\n        notFoundCount, nonActionableCount := danglingPartErrsCount(errDisks)\n        \n        // ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ëŒ•ê¸€ë§ìœ¼ë¡œ ê°„ì£¼\n        if notFoundCount > (len(metaArr) / 2) {\n            return validMeta, true\n        }\n    }\n    \n    return FileInfo{}, false\n}\n```\n\nì´ í•¨ìˆ˜ëŠ”:\n1. ìœ íš¨í•œ ë©”íƒ€ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n2. ì‹¤ì œ ë°ì´í„° íŒŒíŠ¸ì˜ ìƒíƒœì™€ ë©”íƒ€ë°ì´í„° ì¼ì¹˜ ì—¬ë¶€ í™•ì¸\n3. ëŒ€ë‹¤ìˆ˜ì˜ ë””ìŠ¤í¬ì—ì„œ ë°ì´í„°ê°€ ëˆ„ë½ëœ ê²½ìš° ëŒ•ê¸€ë§ ê°ì²´ë¡œ íŒë‹¨\n\n## 6. ë””ìŠ¤í¬ ìƒíƒœ ê°ì§€\n\në””ìŠ¤í¬ ìì²´ì˜ ìƒíƒœë¥¼ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜:\n\n```go\nfunc diskErrToDriveState(err error) (state string) {\n    if err == nil {\n        return madmin.DriveStateOk\n    }\n    \n    switch {\n    case errors.Is(err, errDiskNotFound):\n        return madmin.DriveStateOffline\n    case errors.Is(err, errCorruptedFormat):\n        return madmin.DriveStateCorrupt\n    case errors.Is(err, errUnformattedDisk):\n        return madmin.DriveStateUnformatted\n    case errors.Is(err, errDiskAccessDenied):\n        return madmin.DriveStatePermission\n    case errors.Is(err, errFaultyDisk):\n        return madmin.DriveStateFaulty\n    case errors.Is(err, errDiskFull):\n        return madmin.DriveStateFull\n    }\n    \n    return madmin.DriveStateUnknown\n}\n```\n\nì´ í•¨ìˆ˜ëŠ” ë””ìŠ¤í¬ ì ‘ê·¼ ì‹œ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ ìœ í˜•ì„ ë¶„ì„í•˜ì—¬ ë””ìŠ¤í¬ ìƒíƒœë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.\n\n## ê²°ë¡ \n\nMinIOì˜ ì†ìƒ ê°ì§€ ì•Œê³ ë¦¬ì¦˜ì€ ì—¬ëŸ¬ ê³„ì¸µì—ì„œ ì‘ë™í•©ë‹ˆë‹¤:\n\n1. **ë©”íƒ€ë°ì´í„° ìˆ˜ì¤€**: êµ¬ì¡° ìœ íš¨ì„±, ë²„ì „ ì¼ê´€ì„± ê²€ì¦\n2. **ë°ì´í„° ë¸”ë¡ ìˆ˜ì¤€**: ì²´í¬ì„¬, ë¹„íŠ¸ë¡¯ ê²€ì¦\n3. **ê°ì²´ ë¶€ë¶„ ìˆ˜ì¤€**: ê° ë¶€ë¶„ì˜ ì¡´ì¬ ë° ë¬´ê²°ì„± í™•ì¸\n4. **ì¿¼ëŸ¼ ê¸°ë°˜ ê²€ì¦**: ë‹¤ìˆ˜ê²° ì›ì¹™ìœ¼ë¡œ ì†ìƒ ì—¬ë¶€ íŒë‹¨\n5. **ëŒ•ê¸€ë§ ê°ì²´ ê°ì§€**: ë©”íƒ€ë°ì´í„°ì™€ ì‹¤ì œ ë°ì´í„° ê°„ ì¼ê´€ì„± í™•ì¸\n6. **ë””ìŠ¤í¬ ìƒíƒœ ëª¨ë‹ˆí„°ë§**: ë””ìŠ¤í¬ ìì²´ì˜ ê±´ê°• ìƒíƒœ í‰ê°€\n\nì´ëŸ¬í•œ ë‹¤ì¸µì  ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ MinIOëŠ” ë¶„ì‚° í™˜ê²½ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ìœ í˜•ì˜ ë°ì´í„° ì†ìƒì„ íš¨ê³¼ì ìœ¼ë¡œ ê°ì§€í•˜ê³  ë³µêµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    "docType": "original",
    "category": "Distributed Systems",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 5,
    "wordCount": 802,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ì£¼ìš”-êµ¬í˜„-ë‚´ìš©",
    "slug": "juyo-guhyeon-naeyong",
    "path": "distributed-systems/erasure-coding",
    "fullPath": "distributed-systems/erasure-coding/juyo-guhyeon-naeyong",
    "title": "ì£¼ìš” êµ¬í˜„ ë‚´ìš©",
    "excerpt": "Ceph erasure-code ì£¼ìš” êµ¬í˜„ ë‚´ìš© í•µì‹¬ ì¸í„°í˜ì´ìŠ¤ (ErasureCodeInterface) ëª©ì : ëª¨ë“  ì´ë ˆì´ì € ì½”ë“œ êµ¬í˜„ì˜ í‘œì¤€í™”ëœ API ì œê³µ ì£¼ìš” ë©”ì„œë“œ: : í”„ë¡œíŒŒì¼ì— ë”°ë¼ ì½”ë“œ ì´ˆê¸°...",
    "content": "## Ceph erasure-code ì£¼ìš” êµ¬í˜„ ë‚´ìš©\n\n### 1. í•µì‹¬ ì¸í„°í˜ì´ìŠ¤ (ErasureCodeInterface)\n- **ëª©ì **: ëª¨ë“  ì´ë ˆì´ì € ì½”ë“œ êµ¬í˜„ì˜ í‘œì¤€í™”ëœ API ì œê³µ\n- **ì£¼ìš” ë©”ì„œë“œ**:\n  - `init()`: í”„ë¡œíŒŒì¼ì— ë”°ë¼ ì½”ë“œ ì´ˆê¸°í™”\n  - `encode()`: ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ì¸ì½”ë”©\n  - `decode()`: ì²­í¬ì—ì„œ ì›ë³¸ ë°ì´í„° ë³µêµ¬\n  - `minimum_to_decode_with_cost()`: ìµœì†Œ ë¹„ìš©ìœ¼ë¡œ ë³µêµ¬ ê°€ëŠ¥í•œ ì²­í¬ ì„¸íŠ¸ ê²°ì •\n\n### 2. ê¸°ë³¸ êµ¬í˜„ (ErasureCode)\n- **ì—­í• **: ê³µí†µ ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ëŠ” ì¶”ìƒ í´ë˜ìŠ¤\n- **ê¸°ëŠ¥**:\n  - CRUSH ê·œì¹™ ìƒì„±\n  - ì²­í¬ ë§¤í•‘ ê´€ë¦¬\n  - ë””ì½”ë”©ì„ ìœ„í•œ ìµœì†Œ ì²­í¬ ì„¸íŠ¸ ê³„ì‚°\n  - ê¸°ë³¸ì ì¸ ì¸ì½”ë”©/ë””ì½”ë”© ì›Œí¬í”Œë¡œìš°\n\n### 3. í”ŒëŸ¬ê·¸ì¸ ì‹œìŠ¤í…œ (ErasureCodePlugin)\n- **êµ¬ì¡°**: ë™ì  ë¡œë”© ê°€ëŠ¥í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ êµ¬ì¡°\n- **ê´€ë¦¬**: `ErasureCodePluginRegistry`ê°€ í”ŒëŸ¬ê·¸ì¸ ë¡œë”©, ì´ˆê¸°í™”, ê´€ë¦¬\n- **í™•ì¥ì„±**: ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥\n\n### 4. ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„\n\n#### Jerasure\n- **íŠ¹ì§•**: ë‹¤ì–‘í•œ ì´ë ˆì´ì € ì½”ë“œ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„(Reed-Solomon, Cauchy ë“±)\n- **ìµœì í™”**: SSE, NEON ë“± í•˜ë“œì›¨ì–´ ê°€ì† ì§€ì›\n- **í•˜ìœ„ ë¼ì´ë¸ŒëŸ¬ë¦¬**: \n  - `jerasure`: ì½”ì–´ ì´ë ˆì´ì € ì½”ë”© ê¸°ëŠ¥\n  - `gf-complete`: ìœ í•œì²´(Galois Field) ì—°ì‚° ìµœì í™”\n\n#### LRC (Local Reconstruction Codes)\n- **ëª©ì **: ì§€ì—­ì  ë³µêµ¬ë¥¼ í†µí•œ ì„±ëŠ¥ í–¥ìƒ\n- **ë™ì‘**: \n  - ì „ì—­ íŒ¨ë¦¬í‹°ì™€ ì§€ì—­ íŒ¨ë¦¬í‹° ëª¨ë‘ ìƒì„±\n  - ë‹¨ì¼ ë””ìŠ¤í¬ ì˜¤ë¥˜ëŠ” ë¡œì»¬ íŒ¨ë¦¬í‹°ë§Œìœ¼ë¡œ ë¹ ë¥´ê²Œ ë³µêµ¬\n  - ì‹¬ê°í•œ ì˜¤ë¥˜ëŠ” ì „ì—­ íŒ¨ë¦¬í‹°ë¡œ ë³µêµ¬\n\n#### SHEC (Shingled Erasure Code)\n- **íŠ¹ì§•**: ë³µêµ¬ ëŒ€ì—­í­ ìµœì í™”\n- **êµ¬í˜„**: ì¤‘ì²©ëœ(shingled) íŒ¨ë¦¬í‹° êµ¬ì¡°ë¡œ ë” íš¨ìœ¨ì ì¸ ë³µêµ¬\n\n#### ISA (Intel Storage Acceleration)\n- **íŠ¹ì§•**: ì¸í…” ISA-L ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©\n- **ìµœì í™”**: AVX, AVX2 ëª…ë ¹ì–´ í™œìš©í•œ ê³ ì„±ëŠ¥ êµ¬í˜„\n- **ì¡°ê±´ë¶€ ì»´íŒŒì¼**: í•˜ë“œì›¨ì–´ ì§€ì› ì—¬ë¶€ì— ë”°ë¼ ì»´íŒŒì¼ íƒ€ì„ì— ê²°ì •\n\n#### CLAY (Coupled-Layer)\n- **íŠ¹ì§•**: ìµœì†Œ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©ìœ¼ë¡œ ìµœì  ë³µêµ¬\n- **êµ¬í˜„**: ê³„ì¸µì  ì¸ì½”ë”© êµ¬ì¡°\n\n### 5. êµ¬í˜„ ì„¸ë¶€ì‚¬í•­\n- **ì²­í¬ ì •ë ¬**: SIMD ì—°ì‚°ì„ ìœ„í•œ ë©”ëª¨ë¦¬ ì •ë ¬ (SIMD_ALIGN = 64ë°”ì´íŠ¸)\n- **í”„ë¡œíŒŒì¼ ê´€ë¦¬**: í‚¤-ê°’ í˜•íƒœë¡œ ì½”ë”© íŒŒë¼ë¯¸í„° ê´€ë¦¬\n- **ì‹œìŠ¤í…œ í†µí•©**: CRUSH ë§µê³¼ í†µí•©í•˜ì—¬ ë°ì´í„° ë°°ì¹˜ ê´€ë¦¬\n- **ì„±ëŠ¥ ìµœì í™”**: \n  - í•˜ìœ„ ì²­í¬(sub-chunks) ì§€ì›ìœ¼ë¡œ ì„¸ë°€í•œ ë°ì´í„° ë°°ì¹˜\n  - ë¹„ìš© ê¸°ë°˜ ì²­í¬ ì„ íƒ ì•Œê³ ë¦¬ì¦˜\n\n### 6. ë¹Œë“œ ì‹œìŠ¤í…œ (CMakeLists.txt)\n- **í”ŒëŸ¬ê·¸ì¸ êµ¬ì„±**: ê° ì•Œê³ ë¦¬ì¦˜ì€ ë³„ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë¹Œë“œ\n- **ì¡°ê±´ë¶€ ì»´íŒŒì¼**: í•˜ë“œì›¨ì–´ ê¸°ëŠ¥(SIMD, AVX ë“±)ì— ë”°ë¥¸ ìµœì í™” ë²„ì „ ì„ íƒ\n- **ì¢…ì†ì„± ê´€ë¦¬**: ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ í†µí•©",
    "docType": "original",
    "category": "Distributed Systems",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 2,
    "wordCount": 324,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ìš©ì–´-ì •ë¦¬",
    "slug": "yongeo-jeongri",
    "path": "distributed-systems/erasure-coding",
    "fullPath": "distributed-systems/erasure-coding/yongeo-jeongri",
    "title": "ìš©ì–´ ì •ë¦¬",
    "excerpt": "ì´ë ˆì´ì € ì½”ë”© ìš©ì–´ ì •ë¦¬ ê¸°ë³¸ ê°œë…: ì´ë ˆì´ì € ì½”ë”©(Erasure Coding): ë°ì´í„° ì¤‘ ì¼ë¶€ê°€ ì†ì‹¤ë˜ë”ë¼ë„ ë³µêµ¬í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ê¸°ìˆ  ì²­í¬(Chunk): ë°ì´í„°ë¥¼ ë¶„í• í•œ ë‹¨ìœ„, ê° ì²­í¬ëŠ” ë³„ë„ì˜ ì €ì¥ì†Œì— ë¶„ì‚° ì €ì¥ë¨ -...",
    "content": "## ì´ë ˆì´ì € ì½”ë”© ìš©ì–´ ì •ë¦¬\n\n**ê¸°ë³¸ ê°œë…:**\n- **ì´ë ˆì´ì € ì½”ë”©(Erasure Coding)**: ë°ì´í„° ì¤‘ ì¼ë¶€ê°€ ì†ì‹¤ë˜ë”ë¼ë„ ë³µêµ¬í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ê¸°ìˆ \n- **ì²­í¬(Chunk)**: ë°ì´í„°ë¥¼ ë¶„í• í•œ ë‹¨ìœ„, ê° ì²­í¬ëŠ” ë³„ë„ì˜ ì €ì¥ì†Œì— ë¶„ì‚° ì €ì¥ë¨\n- **ë°ì´í„° ì²­í¬(Data Chunk)**: ì›ë³¸ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ì²­í¬\n- **ì½”ë”© ì²­í¬/íŒ¨ë¦¬í‹° ì²­í¬(Coding/Parity Chunk)**: ë°ì´í„° ë³µêµ¬ì— ì‚¬ìš©ë˜ëŠ” ì¶”ê°€ ì •ë³´ë¥¼ ë‹´ì€ ì²­í¬\n\n**ì½”ë”© ë§¤ê°œë³€ìˆ˜:**\n- **K**: ë°ì´í„° ì²­í¬ì˜ ìˆ˜\n- **M**: ì½”ë”©/íŒ¨ë¦¬í‹° ì²­í¬ì˜ ìˆ˜\n- **K+M**: ì´ ì²­í¬ ìˆ˜ë¡œ, ì‹œìŠ¤í…œì´ ìµœëŒ€ Mê°œì˜ ì²­í¬ ì†ì‹¤ê¹Œì§€ ê²¬ë”œ ìˆ˜ ìˆìŒ\n\n**ì•Œê³ ë¦¬ì¦˜:**\n- **Reed-Solomon**: ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì´ë ˆì´ì € ì½”ë”© ì•Œê³ ë¦¬ì¦˜, ì–´ë–¤ Kê°œì˜ ì²­í¬ë¡œë„ ì›ë³¸ ë°ì´í„° ë³µêµ¬ ê°€ëŠ¥\n- **LRC(Local Reconstruction Codes)**: ì¼ë¶€ ë°ì´í„°ëŠ” ë¡œì»¬ íŒ¨ë¦¬í‹°ë¡œ ë¹ ë¥´ê²Œ ë³µêµ¬í•˜ë„ë¡ ìµœì í™”ëœ ì½”ë“œ\n- **SHEC(Shingled Erasure Code)**: ë³µêµ¬ ì„±ëŠ¥ì„ ê°œì„ í•œ ì½”ë“œ\n- **CLAY(Coupled-Layer)**: ë³µêµ¬ ì‹œ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©ëŸ‰ì„ ìµœì†Œí™”í•˜ëŠ” ìµœì  ë³µêµ¬ ì½”ë“œ\n\n**Ceph ê´€ë ¨ ìš©ì–´:**\n- **í”ŒëŸ¬ê·¸ì¸(Plugin)**: ë‹¤ì–‘í•œ ì´ë ˆì´ì € ì½”ë”© ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•œ ëª¨ë“ˆ\n- **CRUSH**: Cephì˜ ë°ì´í„° ë°°ì¹˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ë°ì´í„°ë¥¼ ë¬¼ë¦¬ì  ì¥ì¹˜ì— ë¶„ì‚° ë°°ì¹˜\n- **CRUSH ê·œì¹™(Rule)**: ë°ì´í„° ë°°ì¹˜ ì •ì±…ì„ ì •ì˜\n- **ì‹¤íŒ¨ ë„ë©”ì¸(Failure Domain)**: í•¨ê»˜ ì‹¤íŒ¨í•  ìˆ˜ ìˆëŠ” êµ¬ì„± ìš”ì†Œ ì§‘í•©(ì˜ˆ: ë™, í˜¸ìŠ¤íŠ¸)\n\n**ì„±ëŠ¥ ê´€ë ¨ ìš©ì–´:**\n- **ì¸ì½”ë”©(Encoding)**: ì›ë³¸ ë°ì´í„°ë¥¼ ë°ì´í„° ì²­í¬ì™€ ì½”ë”© ì²­í¬ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •\n- **ë””ì½”ë”©(Decoding)**: ì‚¬ìš© ê°€ëŠ¥í•œ ì²­í¬ì—ì„œ ì›ë³¸ ë°ì´í„°ë¥¼ ë³µêµ¬í•˜ëŠ” ê³¼ì •\n- **ìŠ¤íŠ¸ë¼ì´í”„(Stripe)**: í•¨ê»˜ ì¸ì½”ë”©ë˜ëŠ” ë°ì´í„° ì§‘í•©\n- **ìµœì†Œ ë³µêµ¬ ì§‘í•©(Minimum Recovery Set)**: ë°ì´í„° ë³µêµ¬ì— í•„ìš”í•œ ìµœì†Œ ì²­í¬ ì§‘í•©",
    "docType": "original",
    "category": "Distributed Systems",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 2,
    "wordCount": 205,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ë¦¬ë“œ-ì†”ë¡œëª¬-ì½”ë“œì˜-ìˆ˜í•™ì -ì›ë¦¬",
    "slug": "rideu-solromon-kodeuyi-suhagjeog-weonri",
    "path": "distributed-systems/erasure-coding",
    "fullPath": "distributed-systems/erasure-coding/rideu-solromon-kodeuyi-suhagjeog-weonri",
    "title": "ë¦¬ë“œ-ì†”ë¡œëª¬ ì½”ë“œì˜ ìˆ˜í•™ì  ì›ë¦¬",
    "excerpt": "ë¦¬ë“œ-ì†”ë¡œëª¬ ì½”ë“œì˜ ìˆ˜í•™ì  ì›ë¦¬ ê°œìš” ë¦¬ë“œ-ì†”ë¡œëª¬(Reed-Solomon) ì½”ë“œëŠ” ë°ì´í„° ë¬´ê²°ì„±ì„ ë³´ì¥í•˜ê³  ì˜¤ë¥˜ ì •ì •ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ê°•ë ¥í•œ ì˜¤ë¥˜ ìˆ˜ì • ì½”ë“œ(Error Correction Code, ECC)ì´ë‹¤. ë³¸ ë¬¸ì„œì—ì„œëŠ” ë¦¬ë“œ-ì†”ë¡œëª¬ ì½”ë“œì˜ í•µì‹¬...",
    "content": "# ë¦¬ë“œ-ì†”ë¡œëª¬ ì½”ë“œì˜ ìˆ˜í•™ì  ì›ë¦¬\n\n## 1. ê°œìš”\në¦¬ë“œ-ì†”ë¡œëª¬(Reed-Solomon) ì½”ë“œëŠ” ë°ì´í„° ë¬´ê²°ì„±ì„ ë³´ì¥í•˜ê³  ì˜¤ë¥˜ ì •ì •ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ê°•ë ¥í•œ ì˜¤ë¥˜ ìˆ˜ì • ì½”ë“œ(Error Correction Code, ECC)ì´ë‹¤. ë³¸ ë¬¸ì„œì—ì„œëŠ” ë¦¬ë“œ-ì†”ë¡œëª¬ ì½”ë“œì˜ í•µì‹¬ ì›ë¦¬ì¸ ë‹¤í•­ì‹ í‘œí˜„ê³¼ ê°ˆë£¨ì•„ í•„ë“œ(Galois Field, GF)ë¥¼ í™œìš©í•œ ì—°ì‚° ê³¼ì •ì„ ì„¤ëª…í•œë‹¤.\n\n---\n\n## 2. ë‹¤í•­ì‹ì„ ì´ìš©í•œ ë°ì´í„° í‘œí˜„\në¦¬ë“œ-ì†”ë¡œëª¬ ì½”ë“œëŠ” ë°ì´í„°ë¥¼ ë‹¤í•­ì‹(polynomial)ì˜ ê³„ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•œë‹¤. ì´ëŠ” ì¼ë¶€ ë°ì´í„°ê°€ ì†ì‹¤ë˜ë”ë¼ë„ ë‚¨ì•„ ìˆëŠ” ë°ì´í„°ë¡œ ì›ë³¸ ë‹¤í•­ì‹ì„ ë³µêµ¬í•  ìˆ˜ ìˆë„ë¡ í•˜ê¸° ìœ„í•¨ì´ë‹¤.\n\n### 2.1 ë‹¤í•­ì‹ í‘œí˜„\nì£¼ì–´ì§„ ë°ì´í„° $d_0, d_1, \\dots, d_{k-1}$ ë¥¼ ê³„ìˆ˜ë¡œ í•˜ëŠ” ë‹¤í•­ì‹ $P(x)$ ë¥¼ ì •ì˜í•  ìˆ˜ ìˆë‹¤.\n\n$$\nP(x) = d_0 + d_1 x + d_2 x^2 + \\dots + d_{k-1} x^{k-1}\n$$\n\nì´ ë‹¤í•­ì‹ì„ í†µí•´ íŠ¹ì •í•œ $x$ ê°’ì—ì„œ í‰ê°€(evaluation)í•œ ê°’ì´ ë°ì´í„° ì¡°ê°ì´ ëœë‹¤.\n\n### 2.2 ë°ì´í„° ìƒ˜í”Œë§\në°ì´í„° ìƒ˜í”Œë§ ê³¼ì •ì—ì„œëŠ” íŠ¹ì •í•œ $x$ ê°’ì—ì„œ ë‹¤í•­ì‹ì„ í‰ê°€í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•œë‹¤. ì¼ë°˜ì ìœ¼ë¡œ, $x$ ê°’ì€ ì„œë¡œ ë‹¤ë¥¸ ì •ìˆ˜ ë˜ëŠ” ìœ í•œì²´ ê°’ìœ¼ë¡œ ì„¤ì •ëœë‹¤.\n\nìƒ˜í”Œë§í•œ ë°ì´í„° í¬ì¸íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ë  ìˆ˜ ìˆë‹¤:\n\n$$\n(x_0, P(x_0)), (x_1, P(x_1)), \\dots, (x_{k-1}, P(x_{k-1)})\n$$\n\nì—¬ê¸°ì„œ ê° $P(x_i)$ ê°’ì´ ì›ë³¸ ë°ì´í„°ì˜ ì¡°ê°ì´ ëœë‹¤.\n\n---\n\n## 3. ê°ˆë£¨ì•„ í•„ë“œ(Galois Field, GF)\në¦¬ë“œ-ì†”ë¡œëª¬ ì½”ë“œì—ì„œëŠ” ì •ìˆ˜ ë˜ëŠ” ì‹¤ìˆ˜ ì—°ì‚°ì´ ì•„ë‹ˆë¼ **ìœ í•œì²´(Galois Field, GF)** ìœ„ì—ì„œ ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤. ìœ í•œì²´ë¥¼ ì‚¬ìš©í•˜ë©´ ë°ì´í„° í¬ê¸°ë¥¼ ì¼ì •í•˜ê²Œ ìœ ì§€í•˜ë©´ì„œë„ ì˜¤ë¥˜ ì •ì •ì„ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n\n### 3.1 $GF(2^m)$ ì—°ì‚°\në¦¬ë“œ-ì†”ë¡œëª¬ ì½”ë“œì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ìœ í•œì²´ëŠ” $GF(2^m)$ ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, $GF(2^8)$ ëŠ” 256ê°œì˜ ì›ì†Œ(0ë¶€í„° 255ê¹Œì§€ì˜ ìˆ«ì)ë¡œ êµ¬ì„±ë˜ë©°, 8ë¹„íŠ¸ ì—°ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. $GF(2^m)$ ì—ì„œëŠ” ë§ì…ˆê³¼ ê³±ì…ˆ ì—°ì‚°ì´ ëª¨ë“ˆëŸ¬ ì—°ì‚°ì„ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜í–‰ëœë‹¤.\n\n- **ë§ì…ˆ**: $GF(2^m)$ì—ì„œëŠ” ë¹„íŠ¸ ë‹¨ìœ„ XOR ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤.\n- **ê³±ì…ˆ**: ë‹¤í•­ì‹ ê³±ì…ˆì„ ìˆ˜í–‰í•œ í›„ íŠ¹ì •í•œ **ì›ì‹œ ë‹¤í•­ì‹(primitive polynomial)** ë¡œ ë‚˜ëˆˆë‹¤.\n\nì´ëŸ¬í•œ ì—°ì‚°ì„ ì‚¬ìš©í•˜ë©´, ìœ í•œì²´ ë‚´ì—ì„œ í•­ìƒ ì¼ì •í•œ í¬ê¸°ì˜ ìˆ«ìë¥¼ ìœ ì§€í•˜ë©´ì„œë„ ì˜¤ë¥˜ ì •ì •ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n\n---\n\n## 4. íŒ¨ë¦¬í‹° ë°ì´í„° ìƒì„±\në¦¬ë“œ-ì†”ë¡œëª¬ ì½”ë“œì—ì„œëŠ” ì›ë³¸ ë°ì´í„° $k$ ê°œë¥¼ ê¸°ë°˜ìœ¼ë¡œ $r$ ê°œì˜ íŒ¨ë¦¬í‹° ë°ì´í„°ë¥¼ ìƒì„±í•˜ì—¬ ì´ $n = k + r$ ê°œì˜ ë°ì´í„° ì¡°ê°ì„ ì €ì¥í•œë‹¤.\n\n### 4.1 íŒ¨ë¦¬í‹° ìƒì„± ë°©ì‹\nìƒˆë¡œìš´ íŒ¨ë¦¬í‹° ë°ì´í„°ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ê¸°ì¡´ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ $x$ ìœ„ì¹˜ì—ì„œ ë‹¤í•­ì‹ì„ í‰ê°€í•œë‹¤.\n\n1. $k$ ê°œì˜ ì›ë³¸ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ì´ìš©í•˜ì—¬ ë‹¤í•­ì‹ $P(x)$ ë¥¼ ìƒì„±í•œë‹¤.\n2. ìƒˆë¡œìš´ ìœ„ì¹˜ $x_k, x_{k+1}, \\dots, x_{k+r-1}$ ì—ì„œ ë‹¤í•­ì‹ì„ í‰ê°€í•˜ì—¬ íŒ¨ë¦¬í‹° ë°ì´í„°ë¥¼ ìƒì„±í•œë‹¤.\n3. ìƒì„±ëœ íŒ¨ë¦¬í‹° ë°ì´í„°ë¥¼ ì›ë³¸ ë°ì´í„°ì™€ í•¨ê»˜ ì €ì¥í•œë‹¤.\n\níŒ¨ë¦¬í‹° ë°ì´í„°ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤:\n\n$$\nP(x_k), P(x_{k+1}), \\dots, P(x_{k+r-1})\n$$\n\nì´ë¥¼ í†µí•´ ì¼ë¶€ ë°ì´í„°ê°€ ì†ì‹¤ë˜ë”ë¼ë„ ë‹¤í•­ì‹ ë³µì›ì„ í†µí•´ ì›ë³¸ ë°ì´í„°ë¥¼ ì¬êµ¬ì„±í•  ìˆ˜ ìˆë‹¤.\n\n---\n\n## 5. ì†ì‹¤ ë°ì´í„° ë³µêµ¬\në°ì´í„° ì¼ë¶€ê°€ ì†ì‹¤ë˜ì—ˆì„ ë•Œ, ë¦¬ë“œ-ì†”ë¡œëª¬ ì½”ë“œì—ì„œëŠ” **ë¼ê·¸ë‘ì£¼ ë³´ê°„ë²•(Lagrange Interpolation)** ì„ ì´ìš©í•˜ì—¬ ì›ë˜ ë‹¤í•­ì‹ì„ ë³µêµ¬í•  ìˆ˜ ìˆë‹¤.\n\n### 5.1 ë¼ê·¸ë‘ì£¼ ë³´ê°„ë²•ì„ ì´ìš©í•œ ë³µêµ¬\në‚¨ì•„ ìˆëŠ” $k$ ê°œ ì´ìƒì˜ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ì´ìš©í•˜ì—¬ ì›ë˜ ë‹¤í•­ì‹ì„ ì¬êµ¬ì„±í•  ìˆ˜ ìˆë‹¤. ë¼ê·¸ë‘ì£¼ ë³´ê°„ë²•ì„ ì‚¬ìš©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì›ë³¸ ë‹¤í•­ì‹ì„ ë³µêµ¬í•  ìˆ˜ ìˆë‹¤.\n\n$$\nP(x) = \\sum_{i=0}^{k-1} P(x_i) \\cdot l_i(x)\n$$\n\nì—¬ê¸°ì„œ $l_i(x)$ ëŠ” ë¼ê·¸ë‘ì£¼ ê¸°ë³¸ ë‹¤í•­ì‹ìœ¼ë¡œ ì •ì˜ëœë‹¤:\n\n$$\nl_i(x) = \\prod_{j \\neq i} \\frac{x - x_j}{x_i - x_j}\n$$\n\nì´ ë³´ê°„ë²•ì„ í†µí•´ ì†ì‹¤ëœ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ë³µêµ¬í•  ìˆ˜ ìˆë‹¤. ì¦‰, $k$ ê°œ ì´ìƒì˜ ë°ì´í„° ì¡°ê°ì´ ë‚¨ì•„ ìˆë‹¤ë©´ ì†ì‹¤ëœ ë°ì´í„°ë„ ë³µêµ¬ê°€ ê°€ëŠ¥í•˜ë‹¤.\n\n---\n\n## 6. ê²°ë¡ \në¦¬ë“œ-ì†”ë¡œëª¬ ì½”ë“œëŠ” ë‹¤í•­ì‹ í‘œí˜„ê³¼ ê°ˆë£¨ì•„ í•„ë“œ ì—°ì‚°ì„ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„° ë¬´ê²°ì„±ì„ ë³´ì¥í•˜ëŠ” ì˜¤ë¥˜ ì •ì • ê¸°ë²•ì´ë‹¤. ì´ë¥¼ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ì´ ì œê³µëœë‹¤:\n\n1. **ë°ì´í„° ì†ì‹¤ ë³µêµ¬ ê°€ëŠ¥**: ì¼ë¶€ ë°ì´í„°ê°€ ì†ì‹¤ë˜ë”ë¼ë„ ë‚¨ì€ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ì›ë˜ ë°ì´í„°ë¥¼ ë³µì›í•  ìˆ˜ ìˆìŒ.\n2. **ìœ ì—°í•œ ì €ì¥ ì‹œìŠ¤í…œ ì§€ì›**: RAID 6, í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€, ìœ„ì„± í†µì‹  ë“± ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ì‚¬ìš©ë¨.\n3. **ìˆ˜í•™ì ìœ¼ë¡œ ê°•ë ¥í•œ ë³´ì¥**: ê°ˆë£¨ì•„ í•„ë“œ ì—°ì‚°ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë³´í˜¸í•˜ê³  ì •ì • ê°€ëŠ¥.\n\në¦¬ë“œ-ì†”ë¡œëª¬ ì½”ë“œëŠ” ë‹¨ìˆœí•œ ì˜¤ë¥˜ ê²€ì¶œì„ ë„˜ì–´ **ë°ì´í„° ë³µêµ¬ê¹Œì§€ ê°€ëŠ¥í•œ ê°•ë ¥í•œ ì•Œê³ ë¦¬ì¦˜**ì´ë©°, í˜„ëŒ€ì˜ ë°ì´í„° ì €ì¥ ë° í†µì‹  ì‹œìŠ¤í…œì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ê³  ìˆë‹¤.",
    "docType": "original",
    "category": "Research",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 3,
    "wordCount": 558,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ë°ì´í„°-ì›ë³¸ì„-ì „ì²´ì ìœ¼ë¡œ-ë¦¬ë“œì†”ë¡œëª¬-ì¸ì½”ë”©-í›„-ìƒ¤ë“œ-ë¶„ë°°-vs-2-ìŠ¤íŠ¸ë¼ì´í•‘-í›„-ê°œë³„-ë¸”ë¡-ë‹¨ìœ„ë¡œ-ë¦¬ë“œì†”ë¡œëª¬-ì¸ì½”ë”©-í›„-ìƒ¤ë“œ-ë¶„ë°°-ë¹„êµ",
    "slug": "deiteo-weonboneul-jeoncejeogeuro-rideusolromon-inkoding-hu-syadeu-bunbae-vs-2-seuteuraiping-hu-gaebyeol-beulrog-danwiro-rideusolromon-inkoding-hu-syadeu-bunbae-bigyo",
    "path": "distributed-systems/erasure-coding",
    "fullPath": "distributed-systems/erasure-coding/deiteo-weonboneul-jeoncejeogeuro-rideusolromon-inkoding-hu-syadeu-bunbae-vs-2-seuteuraiping-hu-gaebyeol-beulrog-danwiro-rideusolromon-inkoding-hu-syadeu-bunbae-bigyo",
    "title": "ë°ì´í„° ì›ë³¸ì„ ì „ì²´ì ìœ¼ë¡œ ë¦¬ë“œì†”ë¡œëª¬ ì¸ì½”ë”© í›„ ìƒ¤ë“œ ë¶„ë°° vs. 2. ìŠ¤íŠ¸ë¼ì´í•‘ í›„ ê°œë³„ ë¸”ë¡ ë‹¨ìœ„ë¡œ ë¦¬ë“œì†”ë¡œëª¬ ì¸ì½”ë”© í›„ ìƒ¤ë“œ ë¶„ë°° ë¹„êµ",
    "excerpt": "--- ğŸ”¹ 1. ë°ì´í„° ì›ë³¸ì„ ë¦¬ë“œì†”ë¡œëª¬ ì¸ì½”ë”© í›„ ë…¸ë“œë§ˆë‹¤ ìƒ¤ë“œë¥¼ ë‚˜ëˆ„ëŠ” ë°©ì‹ ğŸ› ï¸ ê°œìš” â€¢ ì›ë³¸ ë°ì´í„°ë¥¼ í•œ ë²ˆì— í° ë‹¨ìœ„ë¡œ ë¦¬ë“œì†”ë¡œëª¬(Erasure Coding, EC) ì¸ì½”ë”©ì„ ìˆ˜í–‰í•œ ë’¤, ìƒì„±ëœ ë°ì´í„° ë¸”ë¡(ì›ë³¸)ê³¼ íŒ¨ë¦¬í‹°...",
    "content": "---\n\n**ğŸ”¹ 1. ë°ì´í„° ì›ë³¸ì„ ë¦¬ë“œì†”ë¡œëª¬ ì¸ì½”ë”© í›„ ë…¸ë“œë§ˆë‹¤ ìƒ¤ë“œë¥¼ ë‚˜ëˆ„ëŠ” ë°©ì‹**\n\n  \n\n**ğŸ› ï¸ ê°œìš”**\n\nâ€¢ ì›ë³¸ ë°ì´í„°ë¥¼ í•œ ë²ˆì— í° ë‹¨ìœ„ë¡œ ë¦¬ë“œì†”ë¡œëª¬(Erasure Coding, EC) ì¸ì½”ë”©ì„ ìˆ˜í–‰í•œ ë’¤, ìƒì„±ëœ **ë°ì´í„° ë¸”ë¡(ì›ë³¸)ê³¼ íŒ¨ë¦¬í‹° ë¸”ë¡(ë³µêµ¬ìš©)ì„ ì—¬ëŸ¬ ë…¸ë“œì— ë¶„ë°°**í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\n\nâ€¢ ì˜ˆë¥¼ ë“¤ì–´, **(6,3) Reed-Solomon ì½”ë“œ**ë¥¼ ì‚¬ìš©í•˜ë©´, ì›ë³¸ ë°ì´í„°ë¥¼ 6ê°œë¡œ ë‚˜ëˆ„ê³ , 3ê°œì˜ íŒ¨ë¦¬í‹° ë¸”ë¡ì„ ì¶”ê°€ ìƒì„±í•˜ì—¬ **ì´ 9ê°œ ë¸”ë¡ì„ 9ê°œ ë…¸ë“œì— ë¶„ë°°**í•©ë‹ˆë‹¤.\n\n  \n\n**âœ… ì¥ì **\n\n1. **ì“°ê¸° ì„±ëŠ¥ì´ ìƒëŒ€ì ìœ¼ë¡œ ì¢‹ìŒ**\n\nâ€¢ ì›ë³¸ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¸ì½”ë”© í›„ ë¶„ë°°í•˜ê¸° ë•Œë¬¸ì—, **ì¶”ê°€ì ì¸ ìŠ¤íŠ¸ë¼ì´í•‘ ì—°ì‚°ì´ í•„ìš” ì—†ìŒ**.\n\nâ€¢ ë„¤íŠ¸ì›Œí¬ ì „ì†¡ëŸ‰ì´ ì˜ˆì¸¡ ê°€ëŠ¥í•˜ê³  ì¼ì •í•¨.\n\n2. **ë³µêµ¬ íš¨ìœ¨ì„±ì´ ë†’ìŒ (íŠ¹íˆ ì „ì²´ ë…¸ë“œ ì¥ì•  ì‹œ)**\n\nâ€¢ ë°ì´í„° ë¸”ë¡ê³¼ íŒ¨ë¦¬í‹° ë¸”ë¡ì´ ì¼ì •í•œ ë°©ì‹ìœ¼ë¡œ ì €ì¥ë˜ë¯€ë¡œ, **íŠ¹ì • ë…¸ë“œ ì†ì‹¤ ì‹œ íŒ¨ë¦¬í‹° ë¸”ë¡ì„ ì´ìš©í•´ ë¹ ë¥´ê²Œ ì¬êµ¬ì„± ê°€ëŠ¥**.\n\n3. **CPU ì˜¤ë²„í—¤ë“œê°€ ë¹„êµì  ë‚®ìŒ**\n\nâ€¢ ë¦¬ë“œì†”ë¡œëª¬ ì¸ì½”ë”©ì„ í•œ ë²ˆë§Œ ìˆ˜í–‰í•˜ë©´ ë˜ë¯€ë¡œ, CPU ì—°ì‚°ëŸ‰ì´ ì¤„ì–´ë“¦.\n\n  \n\n**âŒ ë‹¨ì **\n\n1. **ë°ì´í„°ê°€ ë…¸ë“œì— ê· ë“±í•˜ê²Œ ì €ì¥ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ**\n\nâ€¢ í•˜ë‚˜ì˜ ì›ë³¸ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ë…¸ë“œì— ë‚˜ëˆ„ì–´ ì €ì¥í•˜ê¸° ë•Œë¬¸ì—, **íŠ¹ì • ë…¸ë“œê°€ ìì£¼ ì‚¬ìš©ë  ê°€ëŠ¥ì„±**ì´ ìˆìŒ.\n\nâ€¢ ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½ì´ íŠ¹ì • ë…¸ë“œì— ì§‘ì¤‘ë  ê°€ëŠ¥ì„±ì´ ìˆìŒ.\n\n2. **ì¡°ê°ë‚œ ë¸”ë¡ë§Œ ì½ê³  ì‹¶ì–´ë„ ì „ì²´ ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ ë³µì›í•´ì•¼ í•  ìˆ˜ë„ ìˆìŒ**\n\nâ€¢ ì‘ì€ ë‹¨ìœ„ ë°ì´í„° ì ‘ê·¼ ì‹œì—ë„, ì›ë³¸ ë°ì´í„°ê°€ ì¸ì½”ë”©ëœ ìƒíƒœì´ë¯€ë¡œ **ì›ë³¸ ë¸”ë¡ì„ ì§ì ‘ ì½ëŠ” ê²ƒì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ**.\n\n---\n\n**ğŸ”¹ 2. ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¼ì´í•‘í•œ í›„ ê°œë³„ ë¸”ë¡ ë‹¨ìœ„ë¡œ ë¦¬ë“œì†”ë¡œëª¬ ì¸ì½”ë”© í›„ ìƒ¤ë“œ ë¶„ë°°**\n\n  \n\n**ğŸ› ï¸ ê°œìš”**\n\nâ€¢ ì›ë³¸ ë°ì´í„°ë¥¼ ì¼ì • í¬ê¸°ë¡œ **ìŠ¤íŠ¸ë¼ì´í•‘(Striping)** í•œ ë’¤, ê° ë¸”ë¡ ë‹¨ìœ„ë¡œ ë¦¬ë“œì†”ë¡œëª¬ ì¸ì½”ë”©ì„ ì ìš©í•˜ì—¬ ìƒ¤ë“œë¥¼ ìƒì„±í•˜ê³  ë¶„ë°°í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\n\nâ€¢ ì˜ˆë¥¼ ë“¤ì–´, 1GB íŒŒì¼ì„ 64MB ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì €ì¥í•  ê²½ìš°, ê°ê°ì˜ 64MB ë¸”ë¡ì„ ê°œë³„ì ìœ¼ë¡œ ë¦¬ë“œì†”ë¡œëª¬ ì¸ì½”ë”©í•˜ì—¬ ë¶„ì‚° ì €ì¥.\n\n  \n\n**âœ… ì¥ì **\n\n1. **ì‘ì€ ë‹¨ìœ„ ë°ì´í„° ì ‘ê·¼ì´ íš¨ìœ¨ì **\n\nâ€¢ íŠ¹ì • ë¸”ë¡ë§Œ í•„ìš”í•  ê²½ìš°, í•´ë‹¹ ë¸”ë¡ë§Œ ì½ê³  íŒ¨ë¦¬í‹° ë¸”ë¡ì„ í™œìš©í•˜ì—¬ ë³µêµ¬ ê°€ëŠ¥.\n\nâ€¢ **Hadoop HDFSì˜ Erasure Coding ë°©ì‹**ì´ë‚˜ **MinIOì˜ Parity Striping** ë°©ì‹ì—ì„œ ì‚¬ìš©ë¨.\n\n2. **ë…¸ë“œ ê°„ ë¶€í•˜ê°€ ê· ë“±í•˜ê²Œ ë¶„ì‚°ë¨**\n\nâ€¢ ë°ì´í„°ê°€ ì‘ì€ ë‹¨ìœ„ë¡œ ìª¼ê°œì ¸ ë¶„ì‚°ë˜ë¯€ë¡œ, **íŠ¹ì • ë…¸ë“œì— íŠ¸ë˜í”½ì´ ì§‘ì¤‘ë˜ëŠ” ë¬¸ì œë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŒ**.\n\n3. **ë³‘ë ¬ I/O ì„±ëŠ¥ í–¥ìƒ**\n\nâ€¢ ì—¬ëŸ¬ ê°œì˜ ì‘ì€ ë¸”ë¡ì´ ë™ì‹œì— ë‹¤ë¥¸ ë…¸ë“œì—ì„œ ì½íˆë¯€ë¡œ, **ëŒ€ê·œëª¨ ë¶„ì‚° ìŠ¤í† ë¦¬ì§€ì—ì„œ ë†’ì€ ë³‘ë ¬ì„±ì„ ê°€ì§ˆ ìˆ˜ ìˆìŒ**.\n\n  \n\n**âŒ ë‹¨ì **\n\n1. **ì“°ê¸° ì„±ëŠ¥ì´ ë‚®ì„ ìˆ˜ ìˆìŒ**\n\nâ€¢ ë°ì´í„° ë‹¨ìœ„ë§ˆë‹¤ ê°œë³„ì ìœ¼ë¡œ ì¸ì½”ë”©ì„ ìˆ˜í–‰í•´ì•¼ í•˜ë¯€ë¡œ, CPU ì—°ì‚°ëŸ‰ ì¦ê°€ ë° ì¶”ê°€ì ì¸ ë„¤íŠ¸ì›Œí¬ ë¹„ìš© ë°œìƒ.\n\nâ€¢ ë„¤íŠ¸ì›Œí¬ì—ì„œ ë” ë§ì€ ì‘ì€ íŒ¨í‚·ì´ ì´ë™í•´ì•¼ í•˜ë¯€ë¡œ, ì§€ì—°(latency)ì´ ì¦ê°€í•  ê°€ëŠ¥ì„±ì´ ìˆìŒ.\n\n2. **ë³µêµ¬ ì‹œ ì˜¤ë²„í—¤ë“œ ì¦ê°€**\n\nâ€¢ ë¸”ë¡ ë‹¨ìœ„ë¡œ ë¶„ì‚°ë˜ì—ˆê¸° ë•Œë¬¸ì—, íŠ¹ì • ë…¸ë“œì—ì„œ ë³µêµ¬í•´ì•¼ í•  ë¸”ë¡ì´ ë§ìœ¼ë©´ ë³µêµ¬ ì‘ì—…ì´ ë³‘ëª©ì´ ë  ìˆ˜ ìˆìŒ.\n\n---\n\n**ğŸ”¹ ìµœì¢… ë¹„êµ**\n\n|**ë¹„êµ í•­ëª©**|**1. ì›ë³¸ ë°ì´í„° ë‹¨ìœ„ë¡œ ë¦¬ë“œì†”ë¡œëª¬ ì¸ì½”ë”© í›„ ìƒ¤ë“œ ë¶„ë°°**|**2. ìŠ¤íŠ¸ë¼ì´í•‘ í›„ ê°œë³„ ë¸”ë¡ ë‹¨ìœ„ë¡œ ë¦¬ë“œì†”ë¡œëª¬ ì¸ì½”ë”©**|\n|---|---|---|\n|**ì“°ê¸° ì„±ëŠ¥**|ìƒëŒ€ì ìœ¼ë¡œ ë¹ ë¦„ (í•œ ë²ˆë§Œ ì¸ì½”ë”©)|ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦¼ (ë¸”ë¡ë§ˆë‹¤ ê°œë³„ ì¸ì½”ë”© í•„ìš”)|\n|**ì½ê¸° ì„±ëŠ¥**|ì „ì²´ ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ ë³µêµ¬í•´ì•¼ í•  ìˆ˜ë„ ìˆìŒ|íŠ¹ì • ë¸”ë¡ë§Œ ì½ëŠ” ê²ƒì´ ê°€ëŠ¥|\n|**ë³µêµ¬ íš¨ìœ¨ì„±**|ì „ì²´ ì›ë³¸ì„ ìœ ì§€í•˜ê¸° ì‰¬ì›€|ê°œë³„ ë¸”ë¡ ë³µêµ¬ ì‹œ ì˜¤ë²„í—¤ë“œ ì¦ê°€ ê°€ëŠ¥|\n|**ë…¸ë“œ ë¶€í•˜**|ì¼ë¶€ ë…¸ë“œì— ì§‘ì¤‘ë  ê°€ëŠ¥ì„±ì´ ìˆìŒ|ë…¸ë“œ ê°„ ë¶€í•˜ê°€ ê· ë“±í•˜ê²Œ ë¶„ë°°ë¨|\n|**ë³‘ë ¬ ì²˜ë¦¬**|ë³‘ë ¬ì„±ì´ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ìŒ|ë†’ì€ ë³‘ë ¬ì„± ê°€ëŠ¥|\n\n  \n\n---\n\n**ğŸ”¹ ê²°ë¡ : ì–´ë–¤ ë°©ì‹ì´ ë” ì í•©í• ê¹Œ?**\n\nâ€¢ **ëŒ€ìš©ëŸ‰ ë°ì´í„° ì €ì¥ & ì½ê¸°/ì“°ê¸° ì„±ëŠ¥ì´ ì¤‘ìš”í•œ ê²½ìš°** â†’ **1ë²ˆ ë°©ì‹ì´ ìœ ë¦¬**\n\nâ€¢ Ceph RADOS Erasure Codingê³¼ ìœ ì‚¬í•œ ë°©ì‹.\n\nâ€¢ í´ëŸ¬ìŠ¤í„° ì „ì²´ì˜ ì•ˆì •ì„±ì´ í•„ìš”í•˜ê³ , íŠ¹ì • íŒŒì¼ì„ ì „ì²´ì ìœ¼ë¡œ ë‹¤ë£¨ëŠ” ê²½ìš° ì í•©.\n\nâ€¢ **ë°ì´í„° ì¡°ê° ë‹¨ìœ„ë¡œ ì ‘ê·¼ì´ ë§ê³ , ë³‘ë ¬ì„±ì´ ì¤‘ìš”í•œ ê²½ìš°** â†’ **2ë²ˆ ë°©ì‹ì´ ìœ ë¦¬**\n\nâ€¢ Hadoop HDFSì˜ EC, MinIOì˜ ìŠ¤íŠ¸ë¼ì´í•‘ ë°©ì‹ê³¼ ìœ ì‚¬.\n\nâ€¢ ê°œë³„ì ì¸ ì‘ì€ íŒŒì¼ì´ë‚˜ ë¸”ë¡ ë‹¨ìœ„ì˜ ë³‘ë ¬ ì²˜ë¦¬ê°€ ì¤‘ìš”í•œ ê²½ìš° ì í•©.\n\n  \n\n**ì¦‰, â€œ1ë²ˆì€ ì „ì²´ ë°ì´í„° ì¤‘ì‹¬ì˜ ì•ˆì •ì„±ì„ ê°•ì¡°í•˜ê³ , 2ë²ˆì€ ê°œë³„ ë¸”ë¡ ì ‘ê·¼ì„±ê³¼ ë³‘ë ¬ì„±ì„ ê°•ì¡°í•˜ëŠ” ë°©ì‹â€**ì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸš€\n\n  \n\ní˜¹ì‹œ ë” êµ¬ì²´ì ìœ¼ë¡œ ë¹„êµí•  ë¶€ë¶„ì´ ìˆë‹¤ë©´ ì•Œë ¤ì£¼ì„¸ìš”! ğŸ˜Š",
    "docType": "original",
    "category": "Distributed Systems",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 3,
    "wordCount": 543,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "succinct-proofs-of-random-access-spora-ë²ˆì—­",
    "slug": "succinct-proofs-of-random-access-spora-beonyeog",
    "path": "distributed-systems/data-availability",
    "fullPath": "distributed-systems/data-availability/succinct-proofs-of-random-access-spora-beonyeog",
    "title": "Succinct Proofs of Random Access (SPoRA) ë²ˆì—­",
    "excerpt": "--- ê°œìš” (Abstract) ì´ ë¬¸ì„œëŠ” Arweave ë„¤íŠ¸ì›Œí¬ì˜ ìƒˆë¡œìš´ í•©ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì„¤ëª…í•œë‹¤. ì´ ë©”ì»¤ë‹ˆì¦˜ì€ ìµœì‹  ë¸”ë¡ìœ„ë¸Œ(blockweave) ìƒíƒœì—ì„œ ìœ ì¶”ëœ ê³¼ê±° ë°ì´í„° ì²­í¬ë¥¼ ì°¾ëŠ” ê²½ìŸ ë°©ì‹ì— ê¸°ë°˜ì„ ë‘”ë‹¤. --- ë™ê¸°...",
    "content": "---\n\n**1. ê°œìš” (Abstract)**\n\nì´ ë¬¸ì„œëŠ” Arweave ë„¤íŠ¸ì›Œí¬ì˜ ìƒˆë¡œìš´ í•©ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì„¤ëª…í•œë‹¤. ì´ ë©”ì»¤ë‹ˆì¦˜ì€ **ìµœì‹  ë¸”ë¡ìœ„ë¸Œ(blockweave) ìƒíƒœì—ì„œ ìœ ì¶”ëœ ê³¼ê±° ë°ì´í„° ì²­í¬ë¥¼ ì°¾ëŠ” ê²½ìŸ ë°©ì‹**ì— ê¸°ë°˜ì„ ë‘”ë‹¤.\n\n---\n\n**2. ë™ê¸° (Motivation)**\n\ní˜„ì¬ Arweave ë„¤íŠ¸ì›Œí¬ì—ì„œ ì‚¬ìš©ë˜ëŠ” í•©ì˜ ì•Œê³ ë¦¬ì¦˜ì€ **ê¸°ì¡´ ì‘ì—… ì¦ëª…(Proof of Work, PoW)** ë°©ì‹ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ, ì¶”ê°€ì ìœ¼ë¡œ ìµœì‹  ë¸”ë¡ìœ„ë¸Œ ìƒíƒœì—ì„œ ê²°ì •ëœ **ê³¼ê±° ë°ì´í„° ì²­í¬(ìµœëŒ€ 256KiB)** ë¥¼ í¬í•¨í•´ì•¼ í•œë‹¤. ì´ ë°©ì‹ì€ **ë„¤íŠ¸ì›Œí¬ê°€ ê³¼ê±° ë°ì´í„°ë¥¼ ìœ ì§€í•˜ë„ë¡ ì¥ë ¤**í•˜ëŠ” íš¨ê³¼ëŠ” ìˆì§€ë§Œ, ì±„êµ´ìê°€ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆì–´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ **ì œí•œì´ ê±°ì˜ ì—†ëŠ” ë¬¸ì œ**ê°€ ìˆë‹¤.\n\nâ€¢ **ë¬¸ì œì  1:**\n\tâ€¢ ì±„êµ´ìê°€ ì›ê²© ìŠ¤í† ë¦¬ì§€ í’€(remote storage pool)ì„ í™œìš©í•˜ì—¬ ë¹ ë¥´ê²Œ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ëŠ” ë°©ì‹ì´ ê°€ëŠ¥í•˜ë‹¤.\n\tâ€¢ ì›ê²© ì €ì¥ì†Œì™€ ì»´í“¨íŒ… í’€ì„ ì¡°í•©í•˜ë©´ **1Gbps ì¸í„°ë„· ë§í¬ë¥¼ í†µí•´ ì´ˆë‹¹ ìˆ˜ë°±ë§Œ ê°œì˜ PoW ì…ë ¥ê°’ì„ ê³„ì‚°í•˜ì—¬ ì œê³µ**í•  ìˆ˜ë„ ìˆë‹¤.\n\tâ€¢ ì‹¤ì œë¡œ Arweave ë„¤íŠ¸ì›Œí¬ì—ì„œëŠ” **ê³µê°œ ë…¸ë“œ(public node) ìˆ˜ê°€ ê°ì†Œí•˜ëŠ” ë™ì‹œì—, ë„¤íŠ¸ì›Œí¬ í•´ì‹œíŒŒì›Œ(hashpower)ëŠ” ì¦ê°€**í•˜ê³  ìˆë‹¤.\n\tâ€¢ ì´ëŠ” ì¼ë¶€ ì±„êµ´ìë“¤ì´ **ê³µë™ ì €ì¥ ë° ê³„ì‚° í’€(storage & computation pool)** ì„ í˜•ì„±í•˜ê³  ìˆìŒì„ ì‹œì‚¬í•œë‹¤.\n\nâ€¢ **ë¬¸ì œì  2:**\n\tâ€¢ PoW ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬ëŠ” **ì—ë„ˆì§€ ì†Œë¹„ëŸ‰ì´ ë§¤ìš° í¬ë‹¤.**\n\tâ€¢ íƒ„ì†Œ ë°°ì¶œëŸ‰ì´ ë§ì•„ í™˜ê²½ì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆë‹¤.\n\tâ€¢ ì¹œí™˜ê²½ì ì¸ í•©ì˜ ì•Œê³ ë¦¬ì¦˜ì„ ë„ì…í•˜ëŠ” ê²ƒì€ **Arweave í”Œë«í¼ì˜ ì¥ê¸°ì ì¸ ì§€ì† ê°€ëŠ¥ì„±ì„ ìœ„í•´ í•„ìˆ˜ì **ì´ë‹¤.\n\n---\n\n**SPoRAì˜ ëª©í‘œ**\n\nìƒˆë¡œìš´ í•©ì˜ ì•Œê³ ë¦¬ì¦˜(SPoRA)ì€ ë‹¤ìŒ ë‘ ê°€ì§€ ì£¼ìš” ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê³ ì í•œë‹¤.\n\n1. **ë°ì´í„°ë¥¼ í•„ìš”í•  ë•Œë§Œ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ì‹(ì˜¨ë””ë§¨ë“œ ë°ì´í„° ê²€ìƒ‰)ì„ ì–µì œí•˜ê³ , ì±„êµ´ìë“¤ì´ ë°ì´í„°ë¥¼ ì±„êµ´ ê¸°ê³„ì— ë” ê°€ê¹ê²Œ ì €ì¥í•˜ë„ë¡ ìœ ë„í•œë‹¤.**\n\tâ€¢ ì¦‰, ë°ì´í„°ë¥¼ ë¡œì»¬ì— ì €ì¥í•˜ì§€ ì•Šê³  ë„¤íŠ¸ì›Œí¬ì—ì„œ ë¶ˆëŸ¬ì˜¤ëŠ” ì±„êµ´ìëŠ” ë¶ˆë¦¬í•´ì§€ë„ë¡ ì„¤ê³„í•œë‹¤.\n\n2. **ë„¤íŠ¸ì›Œí¬ì˜ ì—ë„ˆì§€ ì†Œë¹„ë¥¼ ì¤„ì¸ë‹¤.**\n\tâ€¢ ê¸°ì¡´ PoWì˜ ë†’ì€ ì—°ì‚° ë¹„ìš©ê³¼ ì „ë ¥ ì†Œëª¨ë¥¼ ë‚®ì¶° ë³´ë‹¤ íš¨ìœ¨ì ì¸ í•©ì˜ ê³¼ì •ì„ ë§Œë“ ë‹¤.\n\n---\n\n**3. ì°¸ì¡° êµ¬í˜„ (Reference Implementation)**\n\ní•´ë‹¹ í•©ì˜ ì•Œê³ ë¦¬ì¦˜ì˜ ì°¸ì¡° êµ¬í˜„(Reference Implementation)ì€ ì•„ë˜ ë§í¬ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n\nğŸ”— [**ArweaveTeam GitHub ë§í¬**](https://github.com/ArweaveTeam/arweave/pull/269)\n\n---\n\n**4. SPoRA ì•Œê³ ë¦¬ì¦˜ ëª…ì„¸ (Specification)**\n\n**4.1 ì‚¬ì „ ìš”êµ¬ ì‚¬í•­ (Prerequisites)**\n\n**1. ì¸ë±ì‹±ëœ ë°ì´í„°ì…‹ (Indexed Dataset)**\n\nSPoRAì˜ í•µì‹¬ì€ **ê³¼ê±° ë°ì´í„° ì²­í¬ë¥¼ ì§€ì†ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ëŠ” ê²ƒ**ì´ë‹¤. ëª¨ë“  ë°ì´í„° ì²­í¬ëŠ” **ì „ì—­ ì˜¤í”„ì…‹(Global Offset)** ìœ¼ë¡œ ì‹ë³„ë˜ë©°, ë„¤íŠ¸ì›Œí¬ ì „ì²´ì—ì„œ ëª¨ë“  ë°”ì´íŠ¸ê°€ **ë™ì¼í•œ ì¸ì„¼í‹°ë¸Œë¥¼ ë°›ë„ë¡ ì„¤ê³„**ë˜ì–´ì•¼ í•œë‹¤. ë”°ë¼ì„œ, **ë¸”ë¡ìœ„ë¸Œ ì „ì²´ë¥¼ ì¸ë±ì‹±í•˜ì—¬, íŠ¹ì • ì²­í¬ë¥¼ ë¹ ë¥´ê²Œ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ í•´ì•¼ í•œë‹¤.** **Arweave Erlang í´ë¼ì´ì–¸íŠ¸**ëŠ” **ë²„ì „ 2.1ë¶€í„°** ì´ëŸ¬í•œ ì¸ë±ìŠ¤ë¥¼ ìœ ì§€í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤.\n\n---\n\n**2. ëŠë¦° í•´ì‹œ(Slow Hash)**\n\nSPoRAëŠ” **ì±„êµ´ìê°€ ì§€ì†ì ìœ¼ë¡œ ì €ì¥ì†Œì— ì ‘ê·¼í•˜ë„ë¡ ê°•ì œ**í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘í•´ì•¼ í•œë‹¤. ì¦‰, **ì±„êµ´ìê°€ íŠ¹ì • ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ì„ íƒí•  ìˆ˜ ì—†ë„ë¡ ë§Œë“¤ì–´ì•¼ í•˜ë©°**, ì´ëŠ” ì•„ë˜ ë‘ ê°€ì§€ ì´ìœ  ë•Œë¬¸ì…ë‹ˆë‹¤.\n\nâ€¢ **ìœ„í˜‘ 1:**\n\tâ€¢ ì±„êµ´ìê°€ ë°ì´í„° ì €ì¥ ë¹„ìš©ì„ ì•„ë¼ê³ , ëŒ€ì‹  **ë¹ ë¥¸ ì—°ì‚°ìœ¼ë¡œ PoWë¥¼ ìˆ˜í–‰í•˜ë ¤ëŠ” ê°€ëŠ¥ì„±**ì´ ìˆë‹¤.\n\tâ€¢ ë°ì´í„° ì €ì¥ ì—†ì´ ë¹ ë¥¸ ê³„ì‚°ë§Œìœ¼ë¡œ ë™ì¼í•œ ë³´ìƒì„ ë°›ëŠ”ë‹¤ë©´, ì €ì¥ ì¸ì„¼í‹°ë¸Œê°€ ì‚¬ë¼ì§„ë‹¤.\nâ€¢ **ìœ„í˜‘ 2:**\n\tâ€¢ í˜„ì¬ì˜ ì»´í“¨íŒ… ê¸°ìˆ ì€ ë§¤ìš° ë°œì „í•˜ì—¬, ë°ì´í„° ê²€ìƒ‰ì„ í•˜ì§€ ì•Šë”ë¼ë„ ë†’ì€ íš¨ìœ¨ë¡œ ì—°ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n\tâ€¢ ì´ëŠ” ê¸°ì¡´ PoW ë°©ì‹ë³´ë‹¤ë„ ë” ë¹ ë¥´ê²Œ ì±„êµ´ì´ ì´ë£¨ì–´ì§ˆ ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•œë‹¤.\n\nì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ArweaveëŠ” **ë²„ì „ 1.7ë¶€í„°** [**RandomX**](https://44jxru4mdgbtd66dlzjlc3huktqmmzufomg5p24jl66zyut562yq.arweave.net/5xN404wZgzH7w15SsWz0VODGZoVzDdfriV-9nFJ99rE)**ë¥¼ ì‚¬ìš©**í•œë‹¤. RandomXëŠ” **ì¼ë°˜ CPUì— ìµœì í™”ëœ PoW ì•Œê³ ë¦¬ì¦˜**ìœ¼ë¡œ, **ì „ë¬¸ í•˜ë“œì›¨ì–´ì˜ ì±„êµ´ ìš°ìœ„ë¥¼ ì¤„ì´ëŠ” ì—­í• **ì„ í•œë‹¤.\n\n---\n\n**4.2 ì•Œê³ ë¦¬ì¦˜ ì„¤ëª… (Algorithm Description)**\n\n  **ì±„êµ´ì(Miner)ì˜ ê³¼ì •**\n1. ëœë¤ ë…¼ìŠ¤(nonce)ë¥¼ ìƒì„±í•˜ê³ , í˜„ì¬ ë¸”ë¡ ìƒíƒœ, í›„ë³´ ë¸”ë¡, nonceë¥¼ í¬í•¨í•˜ëŠ” ë¨¸í´ íŠ¸ë¦¬ í•´ì‹œë¥¼ ìƒì„±í•œë‹¤.\n2. ì´ í•´ì‹œ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ **íŠ¹ì • Recall Byte(ê²€ìƒ‰í•´ì•¼ í•  ë°”ì´íŠ¸ ìœ„ì¹˜)** ë¥¼ ê²°ì •í•œë‹¤.\n3. ë¡œì»¬ ì €ì¥ì†Œì—ì„œ í•´ë‹¹ ë°”ì´íŠ¸ê°€ í¬í•¨ëœ **ë°ì´í„° ì²­í¬ë¥¼ ê²€ìƒ‰**í•œë‹¤.\n\tâ€¢ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤ë©´ 1ë²ˆ ë‹¨ê³„ë¶€í„° ë‹¤ì‹œ ì‹œì‘í•œë‹¤.\n4. ì°¾ì€ ë°ì´í„° ì²­í¬ì™€ ì´ì „ ë¸”ë¡ í•´ì‹œë¥¼ ì¡°í•©í•˜ì—¬ **ë¹ ë¥¸ í•´ì‹œ(Fast Hash)ë¥¼ ê³„ì‚°**í•œë‹¤.\n5. ê³„ì‚°ëœ í•´ì‹œ ê°’ì´ **í˜„ì¬ ë‚œì´ë„ë³´ë‹¤ í¬ë‹¤ë©´**, í•´ë‹¹ ë¸”ë¡ì„ ë„¤íŠ¸ì›Œí¬ì— ì „íŒŒí•œë‹¤.\n\tâ€¢ ë¸”ë¡ì—ëŠ” **nonceì™€ í•´ë‹¹ ë°ì´í„° ì²­í¬**ê°€ í¬í•¨ëœë‹¤.\n\nì´ ê³¼ì •ì—ì„œ ì‚¬ìš©ëœ **í•´ë‹¹ ë°ì´í„° ì²­í¬ì™€ ë¨¸í´ ì¦ëª…(Merkle Proofs)** ë¥¼ **Succinct Proof of Random Access (SPoRA)** ë¼ê³  í•œë‹¤.\n\nì´ëŠ” ìƒˆë¡œìš´ í•©ì˜ ì•Œê³ ë¦¬ì¦˜ì˜ ì´ë¦„ì´ê¸°ë„ í•˜ë‹¤.\n\n---\n\n**ê²€ì¦ì(Verifier)ì˜ ê³¼ì •**\n\nâ€¢ ê²€ì¦ìëŠ” **ì±„êµ´ìê°€ ìˆ˜í–‰í•œ ê³¼ì •**ì„ í•œ ë²ˆ ì‹¤í–‰í•˜ì—¬ ë¸”ë¡ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•œë‹¤.\nâ€¢ ë¸”ë¡ ë‚´ì˜ **nonceì™€ ë°ì´í„° ì²­í¬ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸**í•˜ë©´ ëœë‹¤.\n\n---\n\n**4.3 ê²€ìƒ‰ ê³µê°„ ì œí•œ (Search Space Constraints)**\n\nSPoRAëŠ” **ê²€ìƒ‰ ê³µê°„(Search Space)ì„ ì ì ˆíˆ ì„¤ì •í•˜ì—¬ ë°ì´í„° ì €ì¥ì„ ì¥ë ¤**í•´ì•¼ í•œë‹¤.\n\n1. ê²€ìƒ‰ ê³µê°„ì´ **ì¶©ë¶„íˆ ì»¤ì•¼ í•˜ëŠ” ì´ìœ **:\n\tâ€¢ ì±„êµ´ìê°€ ì˜¨ë””ë§¨ë“œ ë°©ì‹ìœ¼ë¡œ **ì „ì²´ ê²€ìƒ‰ ê³µê°„ì„ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥í•˜ë„ë¡ í•˜ê¸° ìœ„í•´ì„œ**.\n\tâ€¢ ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ì´ ì¦ê°€í•¨ì— ë”°ë¼, ë°ì´í„° ìš”ì²­ë§Œìœ¼ë¡œ PoWë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ì ì  ì‰¬ì›Œì§€ë¯€ë¡œ ì´ë¥¼ ë°©ì§€í•´ì•¼ í•œë‹¤.\n\n2. ê²€ìƒ‰ ê³µê°„ì´ **ë„ˆë¬´ í¬ë©´ ì•ˆ ë˜ëŠ” ì´ìœ **:\n\tâ€¢ ë°ì´í„° ì €ì¥ëŸ‰ì´ ì ì€ ì±„êµ´ìë„ ê²½ìŸí•  ìˆ˜ ìˆë„ë¡ í•˜ë ¤ë©´ ê²€ìƒ‰ ê³µê°„ì´ ë„ˆë¬´ í¬ë©´ ì•ˆ ëœë‹¤.\n\tâ€¢ ë”°ë¼ì„œ SPoRAëŠ” **ê²€ìƒ‰ ê³µê°„ì„ ë¸”ë¡ìœ„ë¸Œì˜ 10%ë¡œ ì„¤ì •**í•œë‹¤.\n\tâ€¢ ì´ ê²½ìš°, ì¼ë¶€ ì±„êµ´ìê°€ íŠ¹ì • ë¶€ë¶„ì„ ì§‘ì¤‘ì ìœ¼ë¡œ ì €ì¥í•˜ë©´ **ì•½ 1.2ë°°ì˜ ì±„êµ´ íš¨ìœ¨ì„ ì–»ê²Œ ë¨**.\n\n---\n\n**5. ê´€ë ¨ ì—°êµ¬ (Related Work)**\n\nSPoRAëŠ” **Permacoin: Repurposing Bitcoin Work for Data Preservation** ë…¼ë¬¸ì—ì„œ ì˜ê°ì„ ë°›ì•˜ë‹¤. ArweaveëŠ” ì´ë¥¼ **ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¤‘ì•™í™”ëœ ë°ì´í„° ëŒ€ì‹ , ë¶„ì‚° ë„¤íŠ¸ì›Œí¬ ì „ì²´ì—ì„œ ë°ì´í„°ë¥¼ ìœ ì§€í•˜ëŠ” ë°©ì‹**ìœ¼ë¡œ ê°œì„ í–ˆë‹¤. SPoRAëŠ” **ì—°ì‚°ìœ¼ë¡œ ë°ì´í„° ë¶€ì¡±ì„ ë³´ì™„í•˜ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„**ë˜ì—ˆìœ¼ë©°,\në„¤íŠ¸ì›Œí¬ê°€ ë°ì´í„°ë¥¼ ê³ ë¥´ê²Œ ë³µì œí•˜ë„ë¡ ì¸ì„¼í‹°ë¸Œë¥¼ ì œê³µí•œë‹¤.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 4,
    "wordCount": 707,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "2024-data-replication-design-spectrum-ìš”ì•½",
    "slug": "2024-data-replication-design-spectrum-yoyag",
    "path": "distributed-systems/data-availability",
    "fullPath": "distributed-systems/data-availability/2024-data-replication-design-spectrum-yoyag",
    "title": "â€œ2024 Data Replication Design Spectrumâ€ ìš”ì•½",
    "excerpt": "â€œ2024 Data Replication Design Spectrumâ€ ìš”ì•½ ì´ ê¸€ì—ì„œëŠ” ë°ì´í„° ë³µì œ(Data Replication) ì•Œê³ ë¦¬ì¦˜ì˜ ë‹¤ì–‘í•œ ì„¤ê³„ ë°©ì‹ì„ ì†Œê°œí•˜ë©°, íŠ¹íˆ ë ˆí”Œë¦¬ì¹´(replica) ì¥ì• ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ì— ì´ˆì ì„ ë§ì¶”ê³  ìˆ...",
    "content": "**â€œ2024 Data Replication Design Spectrumâ€ ìš”ì•½**\n\nì´ ê¸€ì—ì„œëŠ” **ë°ì´í„° ë³µì œ(Data Replication) ì•Œê³ ë¦¬ì¦˜ì˜ ë‹¤ì–‘í•œ ì„¤ê³„ ë°©ì‹**ì„ ì†Œê°œí•˜ë©°, íŠ¹íˆ **ë ˆí”Œë¦¬ì¹´(replica) ì¥ì• ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹**ì— ì´ˆì ì„ ë§ì¶”ê³  ìˆìŒ. ë³µì œ ì•Œê³ ë¦¬ì¦˜ë“¤ì€ **ì¥ì•  ê´€ë¦¬ ë°©ì‹**ì— ë”°ë¼ ë¶„ë¥˜ë˜ë©°, **ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±, ê°€ìš©ì„±, ì§€ì—°ì‹œê°„(ë ˆì´í„´ì‹œ)** ë“±ì˜ ì¸¡ë©´ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ê°€ì§€ê²Œ ë¨.\n\n**ğŸ”¹ 1. ì¥ì•  ì²˜ë¦¬ ë°©ì‹: Failure Masking vs. Failure Detection**\n\n**âœ… Failure Masking (ì¥ì•  ë§ˆìŠ¤í‚¹)**\n\nâ€¢ ì¼ë¶€ ë ˆí”Œë¦¬ì¹´ê°€ ì¥ì• ê°€ ë‚˜ë”ë¼ë„ **ì¦‰ê°ì ì¸ ê°œì… ì—†ì´ ìš´ì˜ ê°€ëŠ¥**í•œ ë°©ì‹.\nâ€¢ **ì¿¼ëŸ¼ ê¸°ë°˜(Quorum-based) ë¦¬ë” ì—†ëŠ” ë³µì œ**ê°€ ëŒ€í‘œì ì¸ ì˜ˆì‹œ.\nâ€¢ **íŠ¹ì§•:**\n\tâ€¢ ë‹¤ìˆ˜ê²°(majority) ê¸°ë°˜ì˜ ë™ì‘ â†’ ê³¼ë°˜ìˆ˜ ì´ìƒì´ ì‚´ì•„ìˆë‹¤ë©´ ì„œë¹„ìŠ¤ ì§€ì† ê°€ëŠ¥.\n\tâ€¢ ì¥ì•  íƒì§€ë¥¼ ìœ„í•œ ë³„ë„ ì‘ì—… ì—†ì´ ê³„ì† ìš´ì˜ ê°€ëŠ¥í•˜ì§€ë§Œ, ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥ì„± ì¡´ì¬.\n\n  \n\n**âœ… Failure Detection (ì¥ì•  ê°ì§€)**\n\nâ€¢ ì¥ì•  ë°œìƒ ì‹œ, **ëª…ì‹œì ìœ¼ë¡œ ê°ì§€í•˜ê³  ì¬êµ¬ì„±(reconfiguration)ì´ í•„ìš”í•œ ë°©ì‹**.\nâ€¢ ë ˆí”Œë¦¬ì¹´ì˜ ìƒíƒœë¥¼ ì¶”ì í•˜ê³ , ì¥ì• ê°€ í™•ì¸ë˜ë©´ ìƒˆë¡œìš´ ë³µì œ êµ¬ì¡°ë¥¼ ì„¤ì •í•´ì•¼ í•¨.\nâ€¢ **íŠ¹ì§•:**\n\tâ€¢ ì¥ì•  ê°ì§€ë¥¼ ìœ„í•œ ì¶”ê°€ì ì¸ ì˜¤ë²„í—¤ë“œ ì¡´ì¬.\n\tâ€¢ ì¥ì• ê°€ ë°œìƒí•˜ë©´ ì¦‰ì‹œ ëŒ€ì‘ì´ í•„ìš”í•˜ë¯€ë¡œ, ë³µêµ¬ ê³¼ì •ì´ í•„ìš”í•¨.\n\n**ğŸ”¹ 2. í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹: ë¦¬ë” ê¸°ë°˜ ë³µì œ (Leader-Based Replication)**\n\nâ€¢ ë¦¬ë”(Leader)ë¥¼ ë‘ê³ , ë¦¬ë”ê°€ ëª¨ë“  ë³µì œë¥¼ ê´€ë¦¬í•˜ëŠ” ë°©ì‹.\nâ€¢ ëŒ€í‘œì ì¸ ì•Œê³ ë¦¬ì¦˜: **Raft, Paxos**\nâ€¢ **Failure Maskingê³¼ Failure Detectionì˜ ì¤‘ê°„ í˜•íƒœ**\nâ†’ ë¦¬ë”ê°€ ì¥ì• ê°€ ë‚˜ë©´ ìƒˆë¡œìš´ ë¦¬ë”ë¥¼ ì„ ì¶œí•´ì•¼ í•˜ì§€ë§Œ, ìš´ì˜ ì¤‘ì—ëŠ” ë³µì œë¥¼ ì‰½ê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆìŒ.\n\nâ€¢ **íŠ¹ì§•:**\n\tâ€¢ ì¥ì•  ì‹œ ë¦¬ë”ë¥¼ ì„ ì¶œí•˜ëŠ” ê³¼ì •ì—ì„œ ì§€ì—°ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ.\n\tâ€¢ íŠ¸ëœì­ì…˜ ì¼ê´€ì„±ì„ ë³´ì¥í•˜ê¸°ì— ì í•©.\n\n**ğŸ”¹ 3. ë³µì œ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ (íŠ¸ë ˆì´ë“œì˜¤í”„)**\n\n| **ë°©ì‹**                         | **ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±** | **ê°€ìš©ì„±(Availability)** | **ì§€ì—°ì‹œê°„(Latency)** |\n| ------------------------------ | ----------- | --------------------- | ----------------- |\n| **Failure Masking (ì¿¼ëŸ¼ ê¸°ë°˜)**    | ë³´í†µ          | ë†’ìŒ                    | ë‚®ìŒ                |\n| **Failure Detection (ì¬êµ¬ì„± í•„ìš”)** | ë‚®ìŒ          | ì¤‘ê°„                    | ë†’ìŒ                |\n| **ë¦¬ë” ê¸°ë°˜ ë³µì œ (Raft ë“±)**          | ë†’ìŒ          | ì¤‘ê°„                    | ì¤‘ê°„                |\n\nê° ë°©ì‹ì€ **íŠ¹ì •í•œ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ì í•©ì„±ì´ ë‹¬ë¼ì§€ë©°, ì™„ë²½í•œ ë°©ì‹ì€ ì—†ìŒ**.\n\n**ğŸ”¹ 4. ì£¼ìš” ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œì˜ ì ìš© ë°©ì‹**\n\n  ë‹¤ì–‘í•œ ë°ì´í„°ë² ì´ìŠ¤ë“¤ì´ ê°ê°ì˜ ëª©ì ì— ë§ëŠ” ë³µì œ ë°©ì‹ì„ ì„ íƒí•˜ê³  ìˆì–´.\n\nâ€¢ **ë¦¬ë” ê¸°ë°˜ ë³µì œ:** MongoDB, Redis Cluster ë“±\nâ€¢ **ì¿¼ëŸ¼ ê¸°ë°˜ ë³µì œ:** Cassandra, Riak KV ë“±\nâ€¢ **ì¬êµ¬ì„± ê¸°ë°˜ ë³µì œ:** Elasticsearch, InfluxDB ë“±\n\n**ğŸ”¹ 5. ê²°ë¡ : ì™„ë²½í•œ ë°©ì‹ì€ ì—†ìŒ**\n\nâ€¢ **ëª¨ë“  ë³µì œ ì•Œê³ ë¦¬ì¦˜ì€ íŠ¸ë ˆì´ë“œì˜¤í”„ê°€ ì¡´ì¬**í•˜ë©°, ì‹œìŠ¤í…œì´ ìš”êµ¬í•˜ëŠ” **ì¼ê´€ì„±(Consistency), ê°€ìš©ì„±(Availability), ì„±ëŠ¥(Performance)** ì„ ê³ ë ¤í•´ ì„ íƒí•´ì•¼ í•¨.\nâ€¢ ì˜ˆë¥¼ ë“¤ì–´,\n\tâ€¢ **ë†’ì€ ê°€ìš©ì„±**ì´ í•„ìš”í•˜ë©´ Failure Masking ë°©ì‹ì´ ìœ ë¦¬.\n\tâ€¢ **ë¦¬ë” ê¸°ë°˜ìœ¼ë¡œ ê°•í•œ ì¼ê´€ì„±**ì„ ì›í•˜ë©´ Raft ê°™ì€ ë¦¬ë” ê¸°ë°˜ ë³µì œê°€ ì í•©.\n\tâ€¢ **ì¬êµ¬ì„±ì´ ìš©ì´í•œ ì‹œìŠ¤í…œ**ì´ í•„ìš”í•˜ë©´ Failure Detection ê¸°ë°˜ì˜ ì ‘ê·¼ì´ íš¨ê³¼ì .\n\n  \n\nğŸ”— ì›ë¬¸: [Transactional Blog](https://transactional.blog/blog/2024-data-replication-design-spectrum?utm_source=chatgpt.com)",
    "docType": "original",
    "category": "Research",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 2,
    "wordCount": 378,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "crush-controlled-replication-under-scalable-hashing",
    "slug": "crush-controlled-replication-under-scalable-hashing",
    "path": "distributed-systems/ceph",
    "fullPath": "distributed-systems/ceph/crush-controlled-replication-under-scalable-hashing",
    "title": "CRUSH (Controlled Replication Under Scalable Hashing)",
    "excerpt": "CRUSH (Controlled Replication Under Scalable Hashing) ê¸°ë³¸ ê°œë… CRUSHëŠ” Cephì˜ í•µì‹¬ ë°ì´í„° ë°°ì¹˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ë°ì´í„° ê°ì²´ë¥¼ í´ëŸ¬ìŠ¤í„°ì˜ ë¬¼ë¦¬ì  ì €ì¥ ì¥ì¹˜ì— ë¶„ì‚° ë°°ì¹˜í•˜ëŠ” ë°©ë²•ì„ ê²°ì •í•©ë‹ˆë‹¤. ì£¼ìš” íŠ¹...",
    "content": "# CRUSH (Controlled Replication Under Scalable Hashing)\n\n## ê¸°ë³¸ ê°œë…\nCRUSHëŠ” Cephì˜ í•µì‹¬ ë°ì´í„° ë°°ì¹˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ë°ì´í„° ê°ì²´ë¥¼ í´ëŸ¬ìŠ¤í„°ì˜ ë¬¼ë¦¬ì  ì €ì¥ ì¥ì¹˜ì— ë¶„ì‚° ë°°ì¹˜í•˜ëŠ” ë°©ë²•ì„ ê²°ì •í•©ë‹ˆë‹¤.\n\n## ì£¼ìš” íŠ¹ì§•\n1. **ê²°ì •ì  ë°°ì¹˜**: ë™ì¼í•œ ì…ë ¥ì— ëŒ€í•´ í•­ìƒ ê°™ì€ ê²°ê³¼ ë°˜í™˜ (ì¤‘ì•™ ì¡°ì • ì—†ìŒ)\n2. **í™•ì¥ì„±**: ìˆ˜ì²œ~ìˆ˜ë§Œ ê°œì˜ ì¥ì¹˜ ì§€ì› ê°€ëŠ¥\n3. **ìê°€ ê´€ë¦¬**: í´ëŸ¬ìŠ¤í„° ë³€í™”ì— ìë™ ì ì‘\n4. **ì¥ì•  ë„ë©”ì¸ ì¸ì‹**: í•˜ë“œì›¨ì–´ ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤ ê³ ë ¤í•œ ë°°ì¹˜\n\n## CRUSH ì‘ë™ ë°©ì‹\n\n### 1. ê³„ì¸µ êµ¬ì¡°(CRUSH ë§µ)\n- **ì¥ì¹˜(Device)**: ì‹¤ì œ ë¬¼ë¦¬ì  OSD(Object Storage Daemon)\n- **ë²„í‚·(Bucket)**: ì¥ì¹˜ë‚˜ ë‹¤ë¥¸ ë²„í‚·ì„ í¬í•¨í•˜ëŠ” ë…¼ë¦¬ì  ê·¸ë£¹\n  - **í˜¸ìŠ¤íŠ¸(Host)**: í•œ ì„œë²„ì˜ OSD ê·¸ë£¹\n  - **ë™(Rack)**: ì—¬ëŸ¬ í˜¸ìŠ¤íŠ¸ ê·¸ë£¹\n  - **ë¡œìš°(Row)**: ì—¬ëŸ¬ ë™ ê·¸ë£¹\n  - **ë£¸(Room)**: ì—¬ëŸ¬ ë¡œìš° ê·¸ë£¹\n  - **ë°ì´í„°ì„¼í„°(DC)**: ì—¬ëŸ¬ ë£¸ ê·¸ë£¹\n  - **ë£¨íŠ¸(Root)**: ìµœìƒìœ„ ë²„í‚·\n\n### 2. ë°°ì¹˜ ê·œì¹™(CRUSH Rule)\n- **ëª©ì **: ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë°°ì¹˜í• ì§€ ì •ì˜\n- **êµ¬ì„± ìš”ì†Œ**:\n  - **ê·œì¹™ ì„¸íŠ¸(Rule Set)**: ê·œì¹™ ëª¨ìŒ\n  - **ê·œì¹™ ë‹¨ê³„(Rule Step)**: ê° ê·œì¹™ì˜ ì‘ì—… ë‹¨ìœ„\n  - **ì‹¤íŒ¨ ë„ë©”ì¸(Failure Domain)**: í•¨ê»˜ ì‹¤íŒ¨í•  ìˆ˜ ìˆëŠ” êµ¬ì„± ìš”ì†Œ ë‹¨ìœ„(ì˜ˆ: í˜¸ìŠ¤íŠ¸, ë™)\n  - **íƒ€ì… ì§€ì •ì(Type Specifier)**: ë°°ì¹˜ ì‹œ ì‚¬ìš©í•  ê³„ì¸µ ì§€ì •\n\n### 3. ì•Œê³ ë¦¬ì¦˜ í”„ë¡œì„¸ìŠ¤\n1. **í•´ì‹±**: ê°ì²´ IDë¥¼ í•´ì‹œí•˜ì—¬ ì˜ì‚¬ ë‚œìˆ˜ ì‹œë“œ ìƒì„±\n2. **ê·œì¹™ ì ìš©**: CRUSH ê·œì¹™ì— ë”°ë¼ ë°°ì¹˜ ìœ„ì¹˜ ê²°ì •\n3. **ê³„ì¸µ íƒìƒ‰**: ì§€ì •ëœ ì‹¤íŒ¨ ë„ë©”ì¸ì—ì„œ ì ì ˆí•œ ì¥ì¹˜ ì„ íƒ\n4. **ë°°ì¹˜ í™•ì •**: ì„ íƒëœ ì¥ì¹˜ì— ë°ì´í„° ë°°ì¹˜\n\n## ì´ë ˆì´ì € ì½”ë”©ê³¼ CRUSH í†µí•©\n\n### 1. ì²­í¬ ë°°ì¹˜\n- ê° ë°ì´í„° ì²­í¬(K)ì™€ ì½”ë”© ì²­í¬(M)ëŠ” CRUSH ì•Œê³ ë¦¬ì¦˜ì— ì˜í•´ ì„œë¡œ ë‹¤ë¥¸ ì¥ì¹˜ì— ë°°ì¹˜\n- ì‹¤íŒ¨ ë„ë©”ì¸ì„ ê³ ë ¤í•˜ì—¬ ê°™ì€ ì‹¤íŒ¨ ì§€ì ì— ì—¬ëŸ¬ ì²­í¬ ë°°ì¹˜ ë°©ì§€\n\n### 2. ì´ë ˆì´ì € ì½”ë“œë³„ CRUSH ê·œì¹™\n- `ErasureCode::create_rule()`: ê° ì´ë ˆì´ì € ì½”ë“œ êµ¬í˜„ì€ ìì‹ ë§Œì˜ CRUSH ê·œì¹™ ìƒì„±\n- **ê·œì¹™ ë§¤ê°œë³€ìˆ˜**:\n  - `rule_root`: ìµœìƒìœ„ ë²„í‚· ì§€ì • (ê¸°ë³¸ê°’: \"default\")\n  - `rule_failure_domain`: ì‹¤íŒ¨ ë„ë©”ì¸ ì§€ì • (ê¸°ë³¸ê°’: \"host\")\n  - `rule_device_class`: ì¥ì¹˜ í´ë˜ìŠ¤ ì œí•œ (ì˜ˆ: SSDë§Œ ì‚¬ìš©)\n\n### 3. êµ¬í˜„ ì„¸ë¶€ì‚¬í•­\n```cpp\nint ErasureCode::create_rule(const std::string &name,\n                            CrushWrapper &crush,\n                            std::ostream *ss) const {\n  // ìµœì†Œ í•„ìš” ì¥ì¹˜ ìˆ˜ ê³„ì‚°\n  int min_rep = get_chunk_count();\n  // ì§€ì •ëœ ì‹¤íŒ¨ ë„ë©”ì¸ì— í•´ë‹¹í•˜ëŠ” íƒ€ì… ID ì¡°íšŒ\n  int type = crush.get_type_id(rule_failure_domain);\n  // ë£¨íŠ¸ ë²„í‚· ID ì¡°íšŒ\n  int rootid = crush.get_item_id(rule_root);\n  // ì‹¤ì œ CRUSH ê·œì¹™ ìƒì„±\n  int rno = crush.add_simple_rule(name, rule_root, rule_failure_domain,\n                                  \"firstn\", pg_pool_t::TYPE_ERASURE,\n                                  min_rep, ss);\n  return rno;\n}\n```\n\n## ì‹¤íŒ¨ ë³µêµ¬ ì‹œë‚˜ë¦¬ì˜¤\n\n### 1. ì²­í¬ ì†ì‹¤ ìƒí™©\n- íŠ¹ì • OSD ì‹¤íŒ¨ë¡œ ì¼ë¶€ ì²­í¬ ì†ì‹¤\n- CRUSH ë§µ ì°¸ì¡°í•˜ì—¬ ì†ì‹¤ëœ ì²­í¬ì˜ ìœ„ì¹˜ íŒŒì•…\n\n### 2. ë³µêµ¬ í”„ë¡œì„¸ìŠ¤\n- `minimum_to_decode_with_cost()`: ìµœì†Œ ë¹„ìš©ìœ¼ë¡œ ë³µêµ¬ ê°€ëŠ¥í•œ ì²­í¬ ì„¸íŠ¸ ê³„ì‚°\n- ë‚¨ì•„ìˆëŠ” ì²­í¬ ìœ„ì¹˜ íŒŒì•… (CRUSH ë§µ ì´ìš©)\n- í•´ë‹¹ ì²­í¬ë“¤ë¡œë¶€í„° ì†ì‹¤ëœ ì²­í¬ ë³µêµ¬\n- ìƒˆë¡œìš´ ì¥ì¹˜ì— ë³µêµ¬ëœ ì²­í¬ ì¬ë°°ì¹˜ (CRUSH ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©)\n\n## CRUSHì˜ ì¥ì \n1. **í™•ì¥ì„±**: ì¤‘ì•™ ë©”íƒ€ë°ì´í„° í…Œì´ë¸” ì—†ì´ ë°°ì¹˜ ê³„ì‚°\n2. **ë³µì›ë ¥**: í´ëŸ¬ìŠ¤í„° ë³€í™”ì— ë™ì  ì ì‘\n3. **íŠœë‹ ê°€ëŠ¥ì„±**: ë‹¤ì–‘í•œ ì›Œí¬ë¡œë“œì— ìµœì í™” ê°€ëŠ¥\n4. **í•˜ë“œì›¨ì–´ ì¸ì‹**: ì‹¤ì œ ë¬¼ë¦¬ì  í† í´ë¡œì§€ ë°˜ì˜ ê°€ëŠ¥",
    "docType": "original",
    "category": "Distributed Systems",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 3,
    "wordCount": 441,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ceph-erasure-coding-ë©”íƒ€ë°ì´í„°-ê´€ë¦¬",
    "slug": "ceph-erasure-coding-metadeiteo-gwanri",
    "path": "distributed-systems/ceph",
    "fullPath": "distributed-systems/ceph/ceph-erasure-coding-metadeiteo-gwanri",
    "title": "Ceph Erasure Coding ë©”íƒ€ë°ì´í„° ê´€ë¦¬",
    "excerpt": "Ceph Erasure Coding ë©”íƒ€ë°ì´í„° ê´€ë¦¬ ë©”íƒ€ë°ì´í„° ì¢…ë¥˜ ì½”ë”© í”„ë¡œíŒŒì¼ ì •ë³´: K, M ê°’ (ë°ì´í„° ì²­í¬ ìˆ˜, ì½”ë”© ì²­í¬ ìˆ˜) ì‚¬ìš©ëœ ì´ë ˆì´ì € ì½”ë”© ì•Œê³ ë¦¬ì¦˜ ìœ í˜• íŠ¹ìˆ˜ íŒŒë¼ë¯¸í„° (ì˜ˆ: jerasure...",
    "content": "# Ceph Erasure Coding ë©”íƒ€ë°ì´í„° ê´€ë¦¬\n\n## 1. ë©”íƒ€ë°ì´í„° ì¢…ë¥˜\n1. **ì½”ë”© í”„ë¡œíŒŒì¼ ì •ë³´**:\n   - K, M ê°’ (ë°ì´í„° ì²­í¬ ìˆ˜, ì½”ë”© ì²­í¬ ìˆ˜)\n   - ì‚¬ìš©ëœ ì´ë ˆì´ì € ì½”ë”© ì•Œê³ ë¦¬ì¦˜ ìœ í˜•\n   - íŠ¹ìˆ˜ íŒŒë¼ë¯¸í„° (ì˜ˆ: jerasure ê¸°ë²•, LRC ë¡œì»¬ ê·¸ë£¹ í¬ê¸°)\n\n2. **ì²­í¬ ë§¤í•‘ ì •ë³´**:\n   - ê° ì²­í¬ì˜ ë…¼ë¦¬ì  ì¸ë±ìŠ¤ì™€ ë¬¼ë¦¬ì  OSD ë§¤í•‘\n   - `chunk_mapping` ë²¡í„°ì— ì €ì¥\n\n3. **ê°ì²´ ë ˆì´ì•„ì›ƒ ì •ë³´**:\n   - ì²­í¬ í¬ê¸°\n   - ìŠ¤íŠ¸ë¼ì´í”„ í¬ê¸°\n   - ê°ì²´ í¬ê¸° ë° íŒ¨ë”© ì •ë³´\n\n4. **CRUSH ê·œì¹™ ë©”íƒ€ë°ì´í„°**:\n   - ì‹¤íŒ¨ ë„ë©”ì¸ ì„¤ì •\n   - ê·œì¹™ ID ë° ì´ë¦„\n   - ë””ë°”ì´ìŠ¤ í´ë˜ìŠ¤ ì œí•œ ì •ë³´\n\n## 2. ë©”íƒ€ë°ì´í„° ì €ì¥ ë°©ì‹\n\n### í’€(Pool) ìˆ˜ì¤€ ë©”íƒ€ë°ì´í„°\n1. **í’€ ì†ì„±**:\n   - `ceph osd pool set {pool-name} erasure_code_profile {profile-name}`\n   - í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥\n   - CRUSH ê·œì¹™ê³¼ ì—°ê²°\n\n2. **í”„ë¡œíŒŒì¼ ì €ì¥**:\n   ```cpp\n   const ErasureCodeProfile &get_profile() const {\n     return _profile;\n   }\n   ```\n   - í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ì˜ í‚¤-ê°’ ì €ì¥ì†Œì— ë³´ê´€\n   - ê° í’€ë§ˆë‹¤ í•˜ë‚˜ì˜ í”„ë¡œíŒŒì¼ ì‚¬ìš©\n\n### ê°ì²´ ìˆ˜ì¤€ ë©”íƒ€ë°ì´í„°\n1. **OMAP(Object Map) í™œìš©**:\n   - ê°ì²´ í—¤ë”ì— ë©”íƒ€ë°ì´í„° ì €ì¥\n   - xattr(í™•ì¥ ì†ì„±)ìœ¼ë¡œ ì²­í¬ ì •ë³´ ì €ì¥\n\n2. **ê°ì²´ ì†ì„±**:\n   - ê°ì²´ í¬ê¸°, ìŠ¤íŠ¸ë¼ì´í”„ ì •ë³´\n   - íƒ€ì„ìŠ¤íƒ¬í”„, ì‚¬ìš©ì ì •ì˜ ë©”íƒ€ë°ì´í„°\n\n### PG(Placement Group) ìˆ˜ì¤€ ë©”íƒ€ë°ì´í„°\n1. **PG ë¡œê·¸**:\n   - ëª¨ë“  ì“°ê¸° ì‘ì—… ê¸°ë¡\n   - ë³µêµ¬ì— í•„ìš”í•œ ì‘ì—… ìˆœì„œ ë³´ì¡´\n\n2. **OSDMap**:\n   - í˜„ì¬ í´ëŸ¬ìŠ¤í„° ìƒíƒœ ë°˜ì˜\n   - ê° PGì˜ CRUSH ë§¤í•‘ ì •ë³´ í¬í•¨\n\n## 3. ì²­í¬ ë§¤í•‘ ê´€ë¦¬\n\n1. **ë…¼ë¦¬ì  ë§¤í•‘**:\n   ```cpp\n   const std::vector<int> &get_chunk_mapping() const;\n   ```\n   - ì²­í¬ ì¸ë±ìŠ¤(0~K+M-1)ì™€ ì‹¤ì œ OSD ë§¤í•‘\n   - ì¸ì½”ë”©/ë””ì½”ë”© ì‹œ ì°¸ì¡°\n\n2. **ë§¤í•‘ ì´ˆê¸°í™”**:\n   ```cpp\n   int to_mapping(const ErasureCodeProfile &profile, std::ostream *ss);\n   ```\n   - í”„ë¡œíŒŒì¼ì—ì„œ ë§¤í•‘ ì •ë³´ ì¶”ì¶œ\n   - í•„ìš”ì‹œ ê¸°ë³¸ ìˆœì°¨ ë§¤í•‘ ìƒì„±\n\n3. **ë§¤í•‘ í™œìš©**:\n   ```cpp\n   int chunk_index(unsigned int i) const;\n   ```\n   - ì²­í¬ IDë¥¼ ì‹¤ì œ ì €ì¥ ìœ„ì¹˜ë¡œ ë³€í™˜\n   - ë°ì´í„° ì¡°íšŒ ì‹œ í•„ìš”í•œ ìœ„ì¹˜ ê³„ì‚°\n\n## 4. í”„ë¡œíŒŒì¼ ê´€ë¦¬\n\n1. **í”„ë¡œíŒŒì¼ êµ¬ì„±**:\n   ```cpp\n   typedef std::map<std::string,std::string> ErasureCodeProfile;\n   ```\n   - í‚¤-ê°’ ìŒ í˜•íƒœë¡œ ì„¤ì • ì €ì¥\n   - ì˜ˆ: `{\"k\":\"4\", \"m\":\"2\", \"technique\":\"reed_sol_van\"}`\n\n2. **í”„ë¡œíŒŒì¼ íŒŒì‹±**:\n   ```cpp\n   int parse(const ErasureCodeProfile &profile, std::ostream *ss);\n   ```\n   - ë¬¸ìì—´ í˜•íƒœì˜ ì„¤ì •ì„ ë‚´ë¶€ ê°’ìœ¼ë¡œ ë³€í™˜\n   - íƒ€ì… ë³€í™˜ í—¬í¼ ë©”ì„œë“œ ì œê³µ:\n     ```cpp\n     static int to_int(const std::string &name, ErasureCodeProfile &profile, ...);\n     static int to_bool(const std::string &name, ErasureCodeProfile &profile, ...);\n     ```\n\n3. **í”„ë¡œíŒŒì¼ ê²€ì¦**:\n   ```cpp\n   int sanity_check_k_m(int k, int m, std::ostream *ss);\n   ```\n   - K, M ê°’ì˜ ìœ íš¨ì„± ê²€ì‚¬\n   - ìµœì†Œ/ìµœëŒ€ ê°’ ì œí•œ ì ìš©\n\n## 5. ë©”íƒ€ë°ì´í„° ë³µêµ¬ ì „ëµ\n\n1. **í´ëŸ¬ìŠ¤í„° ë§µ ë™ê¸°í™”**:\n   - ëª¨ë‹ˆí„° ë…¸ë“œì—ì„œ ì£¼ê¸°ì ìœ¼ë¡œ OSDMap ë™ê¸°í™”\n   - ë³€ê²½ ì‚¬í•­ì„ ëª¨ë“  ë…¸ë“œì— ì „íŒŒ\n\n2. **ë©”íƒ€ë°ì´í„° ì¤‘ë³µ ì €ì¥**:\n   - ì¤‘ìš” ë©”íƒ€ë°ì´í„°ëŠ” ì—¬ëŸ¬ ëª¨ë‹ˆí„° ë…¸ë“œì— ë³µì œ\n   - Paxos ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì¼ê´€ì„± ë³´ì¥\n\n3. **PG ë¡œê·¸ í™œìš©**:\n   - ì‘ì—… ìˆœì„œëŒ€ë¡œ ê¸°ë¡ëœ ë¡œê·¸ë¡œ ë©”íƒ€ë°ì´í„° ë³µêµ¬\n   - PG ìŠ¤í¬ëŸ¬ë¹™ìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦\n\n4. **ì˜¤ë¥˜ ë³µêµ¬ ì‹œë‚˜ë¦¬ì˜¤**:\n   - OSD ì‹¤íŒ¨: ë‹¤ë¥¸ OSDì—ì„œ ë©”íƒ€ë°ì´í„° ë³µêµ¬\n   - ëª¨ë‹ˆí„° ì‹¤íŒ¨: ë‹¤ë¥¸ ëª¨ë‹ˆí„°ì—ì„œ ë©”íƒ€ë°ì´í„° ë³µì œ\n   - ì „ì²´ ë©”íƒ€ë°ì´í„° ì†ì‹¤: ë°±ì—… ë˜ëŠ” CRUSH ì¬ê³„ì‚°\n\n## 6. ì„±ëŠ¥ ìµœì í™”\n\n1. **ë©”íƒ€ë°ì´í„° ìºì‹±**:\n   - ìì£¼ ì‚¬ìš©ë˜ëŠ” í”„ë¡œíŒŒì¼ê³¼ ë§¤í•‘ ì •ë³´ ë©”ëª¨ë¦¬ ìºì‹±\n   - OSD ë° í´ë¼ì´ì–¸íŠ¸ ì¸¡ ìºì‹œ í™œìš©\n\n2. **íš¨ìœ¨ì ì¸ ì¡°íšŒ**:\n   - ì¸ë±ìŠ¤ ê¸°ë°˜ ë¹ ë¥¸ ì²­í¬ ìœ„ì¹˜ ì¡°íšŒ\n   - ë³‘ë ¬ ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì§€ì›\n\n3. **ì••ì¶• ì €ì¥**:\n   - ë©”íƒ€ë°ì´í„° ì••ì¶•ìœ¼ë¡œ ì €ì¥ ê³µê°„ ë° ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ì ˆì•½\n   - ì‘ì€ ê°ì²´ ë©”íƒ€ë°ì´í„° í†µí•© ì €ì¥\n\n4. **ì§€ì—° ì—…ë°ì´íŠ¸**:\n   - ë¹„ì¤‘ìš” ë©”íƒ€ë°ì´í„° ë³€ê²½ì€ ì§€ì—° ê¸°ë¡ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ\n   - ì¼ê´„ ì²˜ë¦¬(batching)ë¡œ ë””ìŠ¤í¬ I/O ìµœì†Œí™”",
    "docType": "original",
    "category": "Distributed Systems",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 3,
    "wordCount": 521,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ceph-erasure-coding-ë°ì´í„°-ì¡°íšŒ-íë¦„",
    "slug": "ceph-erasure-coding-deiteo-johoe-heureum",
    "path": "distributed-systems/ceph",
    "fullPath": "distributed-systems/ceph/ceph-erasure-coding-deiteo-johoe-heureum",
    "title": "Ceph Erasure Coding ë°ì´í„° ì¡°íšŒ íë¦„",
    "excerpt": "Ceph Erasure Coding ë°ì´í„° ì¡°íšŒ íë¦„ í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ ë‹¨ê³„ ê°ì²´ ì ‘ê·¼ ìš”ì²­: í´ë¼ì´ì–¸íŠ¸ê°€ íŠ¹ì • ê°ì²´ IDë¡œ ë°ì´í„° ìš”ì²­ CRUSH ê³„ì‚°: í´ë¼ì´ì–¸íŠ¸ëŠ” CRUSH ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ ê°ì²´ì˜ ì²­í¬ë“¤ì´ ì €ì¥ëœ OSD...",
    "content": "# Ceph Erasure Coding ë°ì´í„° ì¡°íšŒ íë¦„\n\n## 1. í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ ë‹¨ê³„\n1. **ê°ì²´ ì ‘ê·¼ ìš”ì²­**: í´ë¼ì´ì–¸íŠ¸ê°€ íŠ¹ì • ê°ì²´ IDë¡œ ë°ì´í„° ìš”ì²­\n2. **CRUSH ê³„ì‚°**: í´ë¼ì´ì–¸íŠ¸ëŠ” CRUSH ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ ê°ì²´ì˜ ì²­í¬ë“¤ì´ ì €ì¥ëœ OSD ìœ„ì¹˜ ê³„ì‚°\n3. **PG(Placement Group) ê²°ì •**: ê°ì²´ê°€ ì†í•œ ë°°ì¹˜ ê·¸ë£¹ ì‹ë³„\n\n## 2. ë°ì´í„° ì²­í¬ ìˆ˜ì§‘ ë‹¨ê³„\n1. **ê°€ìš© ì²­í¬ í™•ì¸**: \n   - ëª¨ë“  ë°ì´í„° ì²­í¬(Kê°œ)ê°€ ê°€ìš©í•œ ê²½ìš°: ì§ì ‘ ë°ì´í„° ì²­í¬ë§Œ ì½ìŒ\n   - ì¼ë¶€ ë°ì´í„° ì²­í¬ ëˆ„ë½: ë³µêµ¬ë¥¼ ìœ„í•œ ìµœì†Œ ì²­í¬ ì„¸íŠ¸ ê²°ì •\n\n2. **ìµœì  ì²­í¬ ì„ íƒ**:\n   ```cpp\n   minimum_to_decode_with_cost(want_to_read, available, &minimum);\n   ```\n   - `want_to_read`: í•„ìš”í•œ ì²­í¬ ID ì§‘í•©\n   - `available`: ê°€ìš©í•œ ì²­í¬ì™€ ì ‘ê·¼ ë¹„ìš© ë§µ\n   - `minimum`: ìµœì†Œ ë¹„ìš©ìœ¼ë¡œ ë³µêµ¬ ê°€ëŠ¥í•œ ì²­í¬ ì§‘í•©\n\n3. **ì²­í¬ ë³‘ë ¬ ìš”ì²­**: í•„ìš”í•œ ëª¨ë“  ì²­í¬ë¥¼ ë³‘ë ¬ë¡œ ìš”ì²­ (ë°ì´í„° ì²­í¬ + í•„ìš”í•œ ì½”ë”© ì²­í¬)\n\n## 3. ë°ì´í„° ë³µì› ë‹¨ê³„\n1. **ëª¨ë“  ë°ì´í„° ì²­í¬ ê°€ìš© ì‹œ**: \n   - ë‹¨ìˆœíˆ í•„ìš”í•œ ë°ì´í„° ì²­í¬ë¥¼ ì—°ê²°í•˜ì—¬ ì›ë³¸ ë°ì´í„° êµ¬ì„±\n   - ì˜ˆ: K=4ì¸ ê²½ìš°, 4ê°œ ë°ì´í„° ì²­í¬ ì—°ê²°\n\n2. **ì¼ë¶€ ë°ì´í„° ì²­í¬ ëˆ„ë½ ì‹œ**:\n   - `decode()` ë©”ì„œë“œ í˜¸ì¶œë¡œ ë³µì› ì‹œì‘\n   ```cpp\n   decode(want_to_read, chunks, &decoded, chunk_size);\n   ```\n\n3. **ë³µí˜¸í™” ê³¼ì •**:\n   - ì´ë ˆì´ì € ì½”ë”© ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©í•˜ì—¬ ëˆ„ë½ëœ ì²­í¬ ë³µì›\n   - ê° êµ¬í˜„ë³„ ë³µí˜¸í™” ë°©ì‹ ì°¨ì´:\n     - **Jerasure**: Reed-Solomon ê¸°ë°˜ í–‰ë ¬ ì—°ì‚°ìœ¼ë¡œ ë³µì›\n     - **LRC**: ë¡œì»¬/ê¸€ë¡œë²Œ íŒ¨ë¦¬í‹° í™œìš© ë³µì›\n     - **ISA**: Intel ISA-L ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©\n     - **CLAY**: ê³„ì¸µì  ë³µì› ë°©ì‹\n\n4. **í•˜ìœ„ ì²­í¬(Sub-chunk) ì²˜ë¦¬**:\n   - ì¼ë¶€ êµ¬í˜„ì€ ì²­í¬ë¥¼ ë” ì‘ì€ í•˜ìœ„ ì²­í¬ë¡œ ë¶„í• \n   - í•„ìš”í•œ í•˜ìœ„ ì²­í¬ë§Œ ë³µì›í•˜ì—¬ íš¨ìœ¨ì„± ì¦ê°€\n\n## 4. ì‘ë‹µ ì²˜ë¦¬ ë‹¨ê³„\n1. **ë°ì´í„° ì¬êµ¬ì„±**:\n   - ë³µì›ëœ ì²­í¬ë“¤ì—ì„œ ìš”ì²­ëœ ë°ì´í„° ë¶€ë¶„ ì¶”ì¶œ\n   - ì²­í¬ ì˜¤í”„ì…‹ ê³„ì‚°: `byte_offset = B % chunk_size`\n   - ì²­í¬ ì¸ë±ìŠ¤ ê³„ì‚°: `chunk_index = B / chunk_size`\n\n2. **ë²„í¼ ìƒì„± ë° ë°˜í™˜**:\n   - ë³µì›ëœ ë°ì´í„°ë¥¼ ë‹¨ì¼ ë²„í¼ë¡œ ì—°ê²°\n   - íŒ¨ë”© ì œê±° (ë§ˆì§€ë§‰ ì²­í¬ì— ì¶”ê°€ëœ íŒ¨ë”©)\n   ```cpp\n   decode_concat(want_to_read, chunks, &decoded);\n   ```\n\n3. **í´ë¼ì´ì–¸íŠ¸ ì‘ë‹µ**:\n   - ì¬êµ¬ì„±ëœ ì›ë³¸ ë°ì´í„°ë¥¼ í´ë¼ì´ì–¸íŠ¸ì— ë°˜í™˜\n\n## 5. ì²­í¬ ìºì‹± ë° ìµœì í™”\n1. **ì½ê¸° ìºì‹±**: ìì£¼ ì ‘ê·¼í•˜ëŠ” ì²­í¬ëŠ” ë©”ëª¨ë¦¬ì— ìºì‹±\n2. **ë¶€ë¶„ ì½ê¸°**: í•„ìš”í•œ ë¶€ë¶„ë§Œ ë³µì› (ì „ì²´ ê°ì²´ê°€ í•„ìš” ì—†ëŠ” ê²½ìš°)\n3. **ì§€ì—° ë³µêµ¬**: ì½ê¸° ìš”ì²­ì´ ì—†ëŠ” ì†ì‹¤ ì²­í¬ëŠ” ì¦‰ì‹œ ë³µêµ¬í•˜ì§€ ì•Šê³  ì§€ì—°\n\n## ì„±ëŠ¥ ê³ ë ¤ì‚¬í•­\n1. **ë³µêµ¬ ë¹„ìš©**: M(ì½”ë”© ì²­í¬ ìˆ˜)ì´ í´ìˆ˜ë¡ ì €ì¥ íš¨ìœ¨ì„± ì¦ê°€, ë³µêµ¬ ë¹„ìš© ì¦ê°€\n2. **ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½**: ë³µêµ¬ ì‹œ í•„ìš”í•œ ë°ì´í„° ì „ì†¡ëŸ‰ (íŠ¹íˆ CLAY ì•Œê³ ë¦¬ì¦˜ì—ì„œ ìµœì í™”)\n3. **ë””ì½”ë”© ê³„ì‚° ë¹„ìš©**: ì•Œê³ ë¦¬ì¦˜ë³„ CPU ì‚¬ìš©ëŸ‰ê³¼ ì§€ì—° ì‹œê°„ ì°¨ì´",
    "docType": "original",
    "category": "Distributed Systems",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 2,
    "wordCount": 370,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ceph-erasure-coding-ë°ì´í„°-ì €ì¥-íë¦„",
    "slug": "ceph-erasure-coding-deiteo-jeojang-heureum",
    "path": "distributed-systems/ceph",
    "fullPath": "distributed-systems/ceph/ceph-erasure-coding-deiteo-jeojang-heureum",
    "title": "Ceph Erasure Coding ë°ì´í„° ì €ì¥ íë¦„",
    "excerpt": "Ceph Erasure Coding ë°ì´í„° ì €ì¥ íë¦„ í´ë¼ì´ì–¸íŠ¸ ì“°ê¸° ìš”ì²­ ë‹¨ê³„ ê°ì²´ ì“°ê¸° ìš”ì²­: í´ë¼ì´ì–¸íŠ¸ê°€ íŠ¹ì • í’€ì— ë°ì´í„° ì“°ê¸° ìš”ì²­ PG(Placement Group) ê²°ì •: ê°ì²´ IDë¥¼ í•´ì‹œí•˜ì—¬ ì†í•  PG ê³„ì‚° 3....",
    "content": "# Ceph Erasure Coding ë°ì´í„° ì €ì¥ íë¦„\n\n## 1. í´ë¼ì´ì–¸íŠ¸ ì“°ê¸° ìš”ì²­ ë‹¨ê³„\n1. **ê°ì²´ ì“°ê¸° ìš”ì²­**: í´ë¼ì´ì–¸íŠ¸ê°€ íŠ¹ì • í’€ì— ë°ì´í„° ì“°ê¸° ìš”ì²­\n2. **PG(Placement Group) ê²°ì •**: ê°ì²´ IDë¥¼ í•´ì‹œí•˜ì—¬ ì†í•  PG ê³„ì‚°\n3. **ì£¼ OSD ì„ íƒ**: í•´ë‹¹ PGì˜ ì£¼(Primary) OSDë¡œ ì“°ê¸° ìš”ì²­ ì „ë‹¬\n\n## 2. ë°ì´í„° ì¸ì½”ë”© ë‹¨ê³„\n1. **ì²­í¬ í¬ê¸° ê³„ì‚°**:\n   - ê°ì²´ í¬ê¸°ë¥¼ ë°ì´í„° ì²­í¬ ìˆ˜(K)ë¡œ ë‚˜ëˆ„ì–´ ê° ì²­í¬ í¬ê¸° ê²°ì •\n   - í•„ìš”ì‹œ ë§ˆì§€ë§‰ ì²­í¬ íŒ¨ë”© ì¶”ê°€\n\n2. **ì¸ì½”ë”© ìš”ì²­**:\n   ```cpp\n   encode(want_to_encode, in, &encoded);\n   ```\n   - `want_to_encode`: ì¸ì½”ë”©í•  ì²­í¬ ID ì§‘í•© (ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë“  K+M ì²­í¬)\n   - `in`: ì›ë³¸ ë°ì´í„° ë²„í¼\n   - `encoded`: ì¸ì½”ë”©ëœ ì²­í¬ë“¤ì„ ë‹´ì„ ë§µ\n\n3. **ë°ì´í„° ë¶„í• **:\n   - ì›ë³¸ ë°ì´í„°ë¥¼ Kê°œì˜ ë°ì´í„° ì²­í¬ë¡œ ë¶„í• \n   - ì˜ˆ: 12KB ë°ì´í„°, K=4ì¸ ê²½ìš° ê° 3KB ì²­í¬ë¡œ ë¶„í• \n\n4. **ì½”ë”© ì²­í¬ ìƒì„±**:\n   - ì„ íƒëœ ì´ë ˆì´ì € ì½”ë”© ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰\n   - ë°ì´í„° ì²­í¬ë¡œë¶€í„° Mê°œì˜ ì½”ë”© ì²­í¬ ê³„ì‚°\n   - êµ¬í˜„ë³„ ê³„ì‚° ë°©ì‹:\n     - **Jerasure**: Reed-Solomon í–‰ë ¬ ì—°ì‚°\n     - **LRC**: ë¡œì»¬/ê¸€ë¡œë²Œ íŒ¨ë¦¬í‹° ê³„ì‚°\n     - **ISA**: ì¸í…” ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©\n     - **CLAY**: ê³„ì¸µì  ì¸ì½”ë”© ë°©ì‹\n\n## 3. ì²­í¬ ë°°ì¹˜ ê³¼ì •\n1. **CRUSH ê³„ì‚°**:\n   - ê° ì²­í¬ì˜ ì €ì¥ ìœ„ì¹˜ ê²°ì •ì„ ìœ„í•´ CRUSH ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰\n   - ì‹¤íŒ¨ ë„ë©”ì¸ ê³ ë ¤ (ì˜ˆ: ë‹¤ë¥¸ í˜¸ìŠ¤íŠ¸ì— ì²­í¬ ë°°ì¹˜)\n\n2. **ë°°ì¹˜ ê·œì¹™ ì ìš©**:\n   ```cpp\n   int rule_id = crush.get_rule_id(rule_name);\n   crush.do_rule(rule_id, x, out, placement_count, weights);\n   ```\n   - ìƒì„±ëœ K+Mê°œ ì²­í¬ë¥¼ ì„œë¡œ ë‹¤ë¥¸ OSDì— ë°°ì¹˜\n   - ì²­í¬ ë§¤í•‘ ì •ë³´ ì €ì¥ (ê° ì²­í¬ê°€ ì–´ë–¤ OSDì— ìˆëŠ”ì§€)\n\n3. **ì²­í¬ ì¸ë±ìŠ¤ ê´€ë¦¬**:\n   - `chunk_mapping` ë²¡í„°ì— ì²­í¬ ì¸ë±ìŠ¤ì™€ ì‹¤ì œ OSD ë§¤í•‘ ì €ì¥\n   - ì´í›„ ë°ì´í„° ì¡°íšŒ ì‹œ ì´ ë§¤í•‘ ì •ë³´ ì‚¬ìš©\n\n## 4. ì²­í¬ ì €ì¥ ê³¼ì •\n1. **ë³‘ë ¬ ì“°ê¸° ì‘ì—…**:\n   - ê° ì²­í¬ë¥¼ í•´ë‹¹ OSDì— ë³‘ë ¬ë¡œ ì“°ê¸° ìš”ì²­\n   - ë°ì´í„° ì²­í¬(Kê°œ)ì™€ ì½”ë”© ì²­í¬(Mê°œ) ëª¨ë‘ ì €ì¥\n\n2. **ì›ìì  ì“°ê¸° ë³´ì¥**:\n   - ëª¨ë“  ì²­í¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì“°ì—¬ì§ˆ ë•Œê¹Œì§€ ëŒ€ê¸°\n   - ì¼ë¶€ ì‹¤íŒ¨ ì‹œ ë¡¤ë°± ë©”ì»¤ë‹ˆì¦˜ í™œì„±í™”\n\n3. **ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸**:\n   - ê°ì²´ ì†ì„±, í¬ê¸°, ì²­í¬ ìœ„ì¹˜ ë“± ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸\n   - OMAP(Object Map)ì— ì¶”ê°€ ì†ì„± ì €ì¥ ê°€ëŠ¥\n\n## 5. ì™„ë£Œ ë° í™•ì¸ ë‹¨ê³„\n1. **ì¿¼ëŸ¼ í™•ì¸**:\n   - í•„ìš”í•œ ìµœì†Œ ìˆ˜ì˜ OSDê°€ ì“°ê¸° ì™„ë£Œ í™•ì¸\n   - ì¼ë°˜ì ìœ¼ë¡œ (K+M)/2 + 1 ê°œì˜ í™•ì¸ í•„ìš”\n\n2. **í´ë¼ì´ì–¸íŠ¸ ì‘ë‹µ**:\n   - ì“°ê¸° ì™„ë£Œ í™•ì¸ ë©”ì‹œì§€ë¥¼ í´ë¼ì´ì–¸íŠ¸ì— ë°˜í™˜\n\n3. **ë°±ê·¸ë¼ìš´ë“œ ë³µì œ**:\n   - ì¼ë¶€ OSD ì“°ê¸° ì‹¤íŒ¨ ì‹œ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë³µì œ ì‹œë„\n   - ìê°€ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ ì‹œì‘\n\n## 6. ìµœì í™” ê¸°ë²•\n1. **ì²­í¬ ë²„í¼ë§**: ì‘ì€ ì“°ê¸° ì‘ì—…ì„ ë²„í¼ë§í•˜ì—¬ ì¼ê´„ ì²˜ë¦¬\n2. **ìŠ¤íŠ¸ë¼ì´í•‘**: ëŒ€ìš©ëŸ‰ ê°ì²´ë¥¼ ì—¬ëŸ¬ ìŠ¤íŠ¸ë¼ì´í”„ë¡œ ë‚˜ëˆ„ì–´ ì €ì¥\n3. **ë¹„ë™ê¸° ì“°ê¸°**: ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ë¹„ë™ê¸° ì“°ê¸° ì‘ì—…\n4. **ë¡œì»¬ íŒ¨ë¦¬í‹°**: LRC ì½”ë“œì—ì„œ ë¡œì»¬ íŒ¨ë¦¬í‹°ë¡œ ì“°ê¸° ì„±ëŠ¥ ìµœì í™”\n\n## 7. ì €ì¥ íš¨ìœ¨ì„±\n- **ì €ì¥ íš¨ìœ¨**: `(K/(K+M)) * 100%`\n- ì˜ˆ: K=8, M=4 êµ¬ì„±ì—ì„œ ì €ì¥ íš¨ìœ¨ì€ 66.7%\n- ì¼ë°˜ ë³µì œ(replication) ëŒ€ë¹„ ì €ì¥ ê³µê°„ ì ˆì•½ (3x ë³µì œëŠ” 33.3% íš¨ìœ¨)",
    "docType": "original",
    "category": "Distributed Systems",
    "tags": [
      "Distributed Systems"
    ],
    "readingTime": 3,
    "wordCount": 427,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "redis-ê¸°ë°˜-ë¶„ì‚°-ë½-ê°€ì´ë“œ",
    "slug": "redis-giban-bunsan-rag-gaideu",
    "path": "database/redis",
    "fullPath": "database/redis/redis-giban-bunsan-rag-gaideu",
    "title": "Redis ê¸°ë°˜ ë¶„ì‚° ë½ ê°€ì´ë“œ",
    "excerpt": "ë¶„ì‚° ì‹œìŠ¤í…œì—ì„œ Redisë¥¼ í™œìš©í•œ ë¶„ì‚° ë½ êµ¬í˜„ ë°©ë²•ê³¼ Redlock ì•Œê³ ë¦¬ì¦˜ì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# Redis ê¸°ë°˜ ë¶„ì‚° ë½ ê°€ì´ë“œ\n\n## ê°œìš”\n\në¶„ì‚° ì‹œìŠ¤í…œì—ì„œ ì—¬ëŸ¬ ë…¸ë“œë‚˜ í”„ë¡œì„¸ìŠ¤ê°€ ê³µìœ  ìì›ì— ë™ì‹œì— ì ‘ê·¼í•˜ëŠ” ê²ƒì„ ì œì–´í•˜ê¸° ìœ„í•´ **ë¶„ì‚° ë½(Distributed Lock)**ì´ í•„ìš”í•©ë‹ˆë‹¤. RedisëŠ” ë¹ ë¥´ê³  ê°„ë‹¨í•œ ë°©ì‹ìœ¼ë¡œ ë¶„ì‚° ë½ì„ êµ¬í˜„í•  ìˆ˜ ìˆëŠ” ì¸ë©”ëª¨ë¦¬ ì €ì¥ì†Œë¡œ ìì£¼ ì‚¬ìš©ë©ë‹ˆë‹¤.\n\n## ê¸°ë³¸ ê°œë…\n\n- **ë¶„ì‚° ë½**ì€ ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ ê°„ ìì› ì ‘ê·¼ì„ ì¡°ìœ¨í•˜ëŠ” ë„êµ¬\n- RedisëŠ” ë‹¨ì¼ í‚¤ì˜ ì›ìì  ì¡°ì‘ì´ ê°€ëŠ¥í•˜ë¯€ë¡œ ë¶„ì‚° ë½ì— ì í•©\n- ë½ì€ ì¼ì • ì‹œê°„ ë™ì•ˆë§Œ ìœ íš¨í•´ì•¼ í•˜ë©°, ì ì ˆí•œ TTL ì„¤ì •ì´ ì¤‘ìš”\n\n---\n\n## ê¸°ë³¸ êµ¬í˜„ ë°©ì‹ (SET NX PX)\n\n### ë½ íšë“\n\n```bash\nSET lock_key unique_value NX PX 3000\n```\n\n| ì˜µì…˜ | ì„¤ëª… |\n|-----|------|\n| `NX` | í‚¤ê°€ ì¡´ì¬í•˜ì§€ ì•Šì„ ë•Œë§Œ ì„¤ì • |\n| `PX 3000` | TTLì„ 3ì´ˆ(3000ms)ë¡œ ì„¤ì • |\n| `unique_value` | ë½ ì£¼ì²´ë¥¼ ì‹ë³„í•˜ê¸° ìœ„í•œ UUID |\n\n### ë½ í•´ì œ (Lua ìŠ¤í¬ë¦½íŠ¸)\n\n```lua\n-- ìì‹ ì´ íšë“í•œ ë½ë§Œ í•´ì œ\nif redis.call(\"GET\", KEYS[1]) == ARGV[1] then\n  return redis.call(\"DEL\", KEYS[1])\nelse\n  return 0\nend\n```\n\n> GETê³¼ DELì„ **ì›ìì **ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì˜ ë½ì„ ì‹¤ìˆ˜ë¡œ í•´ì œí•˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.\n\n### ì£¼ì˜ì‚¬í•­\n\n- ë°˜ë“œì‹œ TTL(ë§Œë£Œ ì‹œê°„)ì„ ì„¤ì •í•  ê²ƒ\n- ë½ì„ íšë“í•œ ì£¼ì²´ë§Œ í•´ì œí•  ê²ƒ\n- Redis ì¥ì•  ì‹œ ë½ ìƒíƒœ ìœ ì‹¤ ê°€ëŠ¥\n\n---\n\n## Redlock ì•Œê³ ë¦¬ì¦˜ (ê³ ê°€ìš©ì„±)\n\n**Redlock**ì€ Redis ì°½ì‹œì Salvatore Sanfilippoê°€ ì œì•ˆí•œ ê³ ê°€ìš©ì„± ë¶„ì‚° ë½ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.\n\n### ë™ì‘ ì›ë¦¬\n\n1. í˜„ì¬ ì‹œê°„ì„ ê¸°ë¡\n2. Nê°œì˜ Redis ì¸ìŠ¤í„´ìŠ¤ì— ë™ì‹œì— ë½ ìš”ì²­ (`SET NX PX`)\n3. ê³¼ë°˜ìˆ˜(N/2 + 1) ì´ìƒ ì„±ê³µ ì‹œ ë½ íšë“\n4. ì „ì²´ ì†Œìš” ì‹œê°„ < TTLì¼ ê²½ìš°ì—ë§Œ ìœ íš¨\n5. ì‹¤íŒ¨ ì‹œ ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ì— ë½ í•´ì œ\n\n### ì¥ë‹¨ì \n\n| ì¥ì  | ë‹¨ì  |\n|-----|------|\n| ë‹¨ì¼ ë…¸ë“œ ì¥ì• ì— ê°•í•¨ | êµ¬í˜„ì´ ë³µì¡í•¨ |\n| ë½ ì¼ê´€ì„± ìœ ì§€ | ë„¤íŠ¸ì›Œí¬ ì§€ì—°ì— ë¯¼ê° |\n| | ì™„ì „í•œ ì•ˆì „ì„± ë³´ì¥ ë¶ˆê°€ (ë…¼ìŸ ìˆìŒ) |\n\n---\n\n## ì‚¬ìš© ê¶Œì¥/ë¹„ê¶Œì¥\n\n### âœ… ì¶”ì²œ ì‚¬ìš©ì²˜\n\n- í¬ë¡ ì¡ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€\n- ê³µìœ  ë¦¬ì†ŒìŠ¤ ì ‘ê·¼ ì œì–´\n- ë¶„ì‚° í™˜ê²½ ì‚¬ì „ ë™ê¸°í™”\n\n### âŒ ë¹„ê¶Œì¥ ì‚¬ìš©ì²˜\n\n- ê°•ë ¥í•œ íŠ¸ëœì­ì…˜ì´ í•„ìš”í•œ ê¸ˆìœµ ê±°ë˜\n- ë„¤íŠ¸ì›Œí¬/ì‹œê³„ ì˜¤ë¥˜ì— ë¯¼ê°í•œ ì‹œìŠ¤í…œ\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [Redis ê³µì‹ ë¬¸ì„œ: Distributed Locks](https://redis.io/docs/manual/patterns/distributed-locks/)\n- [Redlock Algorithm](https://redis.io/docs/interact/locking/)\n- [Martin Kleppmannì˜ Redlock ë¹„íŒ](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)",
    "docType": "original",
    "category": "Database",
    "tags": [
      "Concurrency",
      "Database",
      "Distributed Systems",
      "Redis"
    ],
    "readingTime": 2,
    "wordCount": 329,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "aws-elasticache-for-redis-oss-ìƒ¤ë”©ê³¼-ë§ˆìŠ¤í„°-failover-ì •ë¦¬",
    "slug": "aws-elasticache-for-redis-oss-syadinggwa-maseuteo-failover-jeongri",
    "path": "database/redis",
    "fullPath": "database/redis/aws-elasticache-for-redis-oss-syadinggwa-maseuteo-failover-jeongri",
    "title": "AWS ElastiCache for Redis (OSS) - ìƒ¤ë”©ê³¼ ë§ˆìŠ¤í„° Failover ì •ë¦¬",
    "excerpt": "AWS ElastiCache for Redis (OSS) - ìƒ¤ë”©ê³¼ ë§ˆìŠ¤í„° Failover ì •ë¦¬ ğŸ“Œ í•µì‹¬ ê°œë… ìš”ì•½ Redis Cluster ëª¨ë“œì—ì„œëŠ” ë°ì´í„°ë¥¼ ìƒ¤ë”©í•˜ì—¬ ì—¬ëŸ¬ ë§ˆìŠ¤í„°...",
    "content": "# AWS ElastiCache for Redis (OSS) - ìƒ¤ë”©ê³¼ ë§ˆìŠ¤í„° Failover ì •ë¦¬\n\n![[Pasted image 20250328100819.png]]\n\n## ğŸ“Œ í•µì‹¬ ê°œë… ìš”ì•½\n\n- Redis Cluster ëª¨ë“œì—ì„œëŠ” ë°ì´í„°ë¥¼ ìƒ¤ë”©í•˜ì—¬ ì—¬ëŸ¬ ë§ˆìŠ¤í„° ë…¸ë“œì— ë¶„ì‚° ì €ì¥í•  ìˆ˜ ìˆë‹¤.\n- ê·¸ëŸ¬ë‚˜ **í•˜ë‚˜ì˜ í‚¤ëŠ” ë°˜ë“œì‹œ í•˜ë‚˜ì˜ ë§ˆìŠ¤í„° ë…¸ë“œë§Œì´ ê´€ë¦¬**í•œë‹¤.\n- ì´ êµ¬ì¡°ì—ì„œ **ë§ˆìŠ¤í„° ë…¸ë“œì˜ ì¥ì• (Failover)** ë°œìƒ ì‹œ, ë¦¬í”Œë¦¬ì¹´ë¥¼ ìë™ ìŠ¹ê²©ì‹œì¼œ ì„œë¹„ìŠ¤ë¥¼ ì§€ì†í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.\n\n---\n\n## ğŸ§± ìƒ¤ë”© êµ¬ì¡°\n\n### ğŸ”¹ í•´ì‹œ ìŠ¬ë¡¯ ê¸°ë°˜ ìƒ¤ë”©\n\n- Redis ClusterëŠ” í‚¤ë¥¼ **0 ~ 16383 í•´ì‹œ ìŠ¬ë¡¯** ì¤‘ í•˜ë‚˜ì— ë§¤í•‘í•œë‹¤.\n- ê° í•´ì‹œ ìŠ¬ë¡¯ì€ **í•˜ë‚˜ì˜ ìƒ¤ë“œ(= ë§ˆìŠ¤í„° ë…¸ë“œ)** ê°€ ë‹´ë‹¹í•œë‹¤.\n- ìƒ¤ë“œ ìˆ˜ê°€ ëŠ˜ì–´ë‚˜ë©´ ìŠ¬ë¡¯ì´ ë¶„ì‚°ë˜ë©°, ê° ìƒ¤ë“œëŠ” ìì‹ ì´ ë§¡ì€ ìŠ¬ë¡¯ ë²”ìœ„ì˜ í‚¤ë§Œ ê´€ë¦¬í•œë‹¤.\n\n### ğŸ”¹ í‚¤ ê´€ë¦¬\n\n- í•˜ë‚˜ì˜ í‚¤ëŠ” í•˜ë‚˜ì˜ í•´ì‹œ ìŠ¬ë¡¯ì— ë§¤í•‘ë˜ë¯€ë¡œ,\n- **ë™ì¼í•œ í‚¤ì— ëŒ€í•´ ë™ì‹œì— ì—¬ëŸ¬ ë§ˆìŠ¤í„°ê°€ ì ‘ê·¼í•˜ëŠ” ì¼ì€ ì—†ìŒ**.\n\n---\n\n## ğŸ” ë§ˆìŠ¤í„° ë…¸ë“œ Failover ì²˜ë¦¬\n\n### ğŸ”¹ ë¦¬í”Œë¦¬ì¹´ êµ¬ì„±\n\n- ElastiCacheëŠ” **ê° ë§ˆìŠ¤í„° ë…¸ë“œì— ëŒ€í•´ í•˜ë‚˜ ì´ìƒì˜ ë¦¬í”Œë¦¬ì¹´ ë…¸ë“œ(replica)ë¥¼ êµ¬ì„±**í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•œë‹¤.\n- ë¦¬í”Œë¦¬ì¹´ëŠ” ë§ˆìŠ¤í„°ì˜ ë°ì´í„°ë¥¼ ë¹„ë™ê¸° ë³µì œí•œë‹¤.\n\n### ğŸ”¹ Failover ì‹œë‚˜ë¦¬ì˜¤\n\n1. ë§ˆìŠ¤í„° ë…¸ë“œì— ì¥ì•  ë°œìƒ\n2. ElastiCache ê°ì‹œ ì‹œìŠ¤í…œì´ ìë™ ê°ì§€\n3. í•´ë‹¹ ìƒ¤ë“œì˜ ë¦¬í”Œë¦¬ì¹´ ë…¸ë“œ ì¤‘ í•˜ë‚˜ë¥¼ **ìë™ìœ¼ë¡œ ë§ˆìŠ¤í„°ë¡œ ìŠ¹ê²©(promote)**\n4. í´ëŸ¬ìŠ¤í„° ë©”íƒ€ë°ì´í„°ê°€ ê°±ì‹ ë¨\n5. í´ë¼ì´ì–¸íŠ¸ëŠ” ì¬ì ‘ì† ì‹œ **MOVED ë¦¬ë‹¤ì´ë ‰ì…˜ ì‘ë‹µ**ì„ í†µí•´ ìƒˆ ë§ˆìŠ¤í„°ì— ì—°ê²°ë¨\n\n### ğŸ”¹ êµ¬ì„± ì˜ˆì‹œ\n\nìƒ¤ë“œ 1: ë§ˆìŠ¤í„° A <â€“> ë¦¬í”Œë¦¬ì¹´ Aâ€™\n\nìƒ¤ë“œ 2: ë§ˆìŠ¤í„° B <â€“> ë¦¬í”Œë¦¬ì¹´ Bâ€™\n\nìƒ¤ë“œ 3: ë§ˆìŠ¤í„° C <â€“> ë¦¬í”Œë¦¬ì¹´ Câ€™\n\n- ë§ˆìŠ¤í„° Bê°€ ì£½ìœ¼ë©´ â†’ ë¦¬í”Œë¦¬ì¹´ B'ê°€ ìƒˆ ë§ˆìŠ¤í„°ë¡œ ìŠ¹ê²©\n\n---\n\n## âš ï¸ ì£¼ì˜ì‚¬í•­\n\n- Redisì˜ ë³µì œëŠ” **ë¹„ë™ê¸°(asynchronous)** ì´ë¯€ë¡œ, ì¥ì•  ë°œìƒ ì‹œ **ì¼ë¶€ ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥ì„±**ì´ ì¡´ì¬í•œë‹¤.\n- ê°•ë ¥í•œ ë°ì´í„° ì •í•©ì„±ì´ ìš”êµ¬ë˜ëŠ” ê²½ìš°, ì‚¬ìš© ì‹œì  ë° êµ¬ì¡°ë¥¼ ì‹ ì¤‘í•˜ê²Œ ì„¤ê³„í•´ì•¼ í•œë‹¤.\n- **ë¦¬í”Œë¦¬ì¹´ê°€ ì—†ìœ¼ë©´ Failoverê°€ ë¶ˆê°€ëŠ¥**í•˜ë¯€ë¡œ, ìµœì†Œí•œ ìƒ¤ë“œë‹¹ 1ê°œì˜ ë¦¬í”Œë¦¬ì¹´ êµ¬ì„±ì´ ê¶Œì¥ëœë‹¤.\n\n---\n\n## âœ… ëª¨ë²” êµ¬ì„±\n\n- **Multi-AZ ë°°í¬**ë¡œ ì¥ì•  ë„ë©”ì¸ ë¶„ë¦¬\n- ê° ìƒ¤ë“œì— **1ê°œ ì´ìƒì˜ ë¦¬í”Œë¦¬ì¹´ êµ¬ì„±**\n- í´ë¼ì´ì–¸íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” `Cluster-aware` ëª¨ë“œ ì‚¬ìš© (ì˜ˆ: `lettuce`, `Jedis`, `ioredis` ë“±)\n\n---\n\n## ğŸ“š ì°¸ê³  ìë£Œ\n\n- [AWS ê³µì‹ ë¬¸ì„œ - ElastiCacheì—ì„œ í´ëŸ¬ìŠ¤í„° ê´€ë¦¬](https://docs.aws.amazon.com/ko_kr/AmazonElastiCache/latest/dg/Clusters.html)\n-",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [
      "Database"
    ],
    "readingTime": 2,
    "wordCount": 316,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "which-is-best-isolation-level-for-a-common-situation",
    "slug": "which-is-best-isolation-level-for-a-common-situation",
    "path": "database/mysql",
    "fullPath": "database/mysql/which-is-best-isolation-level-for-a-common-situation",
    "title": "Which is best isolation level for a common situation",
    "excerpt": "Which is best isolation level for a common situation MySQLì€ ë°ì´í„°ë² ì´ìŠ¤ ì—”ì§„ì´ ë™ì‹œ íŠ¸ëœì­ì…˜ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì •ì˜í•˜ëŠ” ì—¬ëŸ¬ ë¶„ë¦¬ ìˆ˜ì¤€ì„ ì§€ì›í•©ë‹ˆë‹¤. ì¼ë°˜ì ì¸ ìƒí™©ì— ê°€ì¥ ì í•©í•œ ì˜µì…˜ì€ ì‘ìš©í”„ë¡œê·¸ë¨ì˜ íŠ¹ì • ìš”êµ¬ì‚¬í•­ì—...",
    "content": "# Which is best isolation level for a common situation\n\nMySQLì€ ë°ì´í„°ë² ì´ìŠ¤ ì—”ì§„ì´ ë™ì‹œ íŠ¸ëœì­ì…˜ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì •ì˜í•˜ëŠ” ì—¬ëŸ¬ ë¶„ë¦¬ ìˆ˜ì¤€ì„ ì§€ì›í•©ë‹ˆë‹¤. ì¼ë°˜ì ì¸ ìƒí™©ì— ê°€ì¥ ì í•©í•œ ì˜µì…˜ì€ ì‘ìš©í”„ë¡œê·¸ë¨ì˜ íŠ¹ì • ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤.\n\nì¼ë°˜ì ìœ¼ë¡œ REPEATABLE READ ë¶„ë¦¬ ìˆ˜ì¤€ì€ ì¼ê´€ì„±ê³¼ ì„±ëŠ¥ ì‚¬ì´ì—ì„œ ê· í˜•ì„ ë§ì¶”ê¸° ë•Œë¬¸ì— ëŒ€ë¶€ë¶„ì˜ ì‘ìš© í”„ë¡œê·¸ë¨ì— ì í•©í•œ ê¸°ë³¸ ì˜µì…˜ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‘ìš©í”„ë¡œê·¸ë¨ì— ë” ë†’ì€ ìˆ˜ì¤€ì˜ ì¼ê´€ì„±ì´ í•„ìš”í•œ ê²½ìš°ì—ëŠ” ì§ë ¬í™”ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ê´€ì„±ë³´ë‹¤ ì„±ëŠ¥ì„ ìš°ì„ ì‹œí•´ì•¼ í•˜ëŠ” ê²½ìš° READ COMMITED ë˜ëŠ” READ UNCOMMITEDë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•  ìˆ˜ ìˆì§€ë§Œ ì ì¬ì ì¸ ìœ„í—˜ê³¼ ì ˆì¶©ì ì„ ì•Œê³  ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n\n## ISOLATION LEVELS\n\n- READ UNCOMMITTED: ì´ ê²©ë¦¬ ìˆ˜ì¤€ì€ ë”í‹° ì½ê¸°ë¥¼ í—ˆìš©í•˜ë©°, ì´ëŠ” íŠ¸ëœì­ì…˜ì´ ë‹¤ë¥¸ íŠ¸ëœì­ì…˜ì— ì˜í•´ ì»¤ë°‹ë˜ì§€ ì•Šì€ ë³€ê²½ ì‚¬í•­ì„ ì½ì„ ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë°ì´í„°ê°€ ì¼ê´€ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëŒ€ë¶€ë¶„ì˜ ì‘ìš© í”„ë¡œê·¸ë¨ì—ëŠ” ê¶Œì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n- READ COMMITTED: ì´ ê²©ë¦¬ ìˆ˜ì¤€ì„ í†µí•´ íŠ¸ëœì­ì…˜ì€ ì»¤ë°‹ëœ ë°ì´í„°ë§Œ ì½ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë”í‹° ì½ê¸°ë¥¼ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë‹¤ë¥¸ íŠ¸ëœì­ì…˜ì´ ì½ê¸° ê°„ì— ë³€ê²½ ì‚¬í•­ì„ ì»¤ë°‹í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê³„ì†í•´ì„œ ë°˜ë³µí•  ìˆ˜ ì—†ëŠ” ì½ê¸° ë° íŒ¬í…€ ì½ê¸°ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n- REPEATABLE READ: ì´ ê²©ë¦¬ ìˆ˜ì¤€ì„ ì‚¬ìš©í•˜ë©´ ë‹¤ë¥¸ íŠ¸ëœì­ì…˜ì´ ë³€ê²½ë˜ë”ë¼ë„ íŠ¸ëœì­ì…˜ì´ ë°ì´í„°ë² ì´ìŠ¤ì˜ ì¼ê´€ëœ ìŠ¤ëƒ…ìƒ·ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ë³µí•  ìˆ˜ ì—†ëŠ” ì½ê¸° ë° íŒ¬í…€ ì½ê¸°ë¥¼ ë°©ì§€í•˜ì§€ë§Œ ìƒˆ ë°ì´í„°ë¥¼ íŠ¸ëœì­ì…˜ì— ì‚½ì…í•  ìˆ˜ ìˆëŠ” ì‘ì€ ì°½ì„ í—ˆìš©í•©ë‹ˆë‹¤.\n\n- SERIALIZABLE: ì´ ë¶„ë¦¬ ìˆ˜ì¤€ì€ íŠ¸ëœì­ì…˜ì´ ì™„ë£Œë  ë•Œê¹Œì§€ íŠ¸ëœì­ì…˜ì´ ì•¡ì„¸ìŠ¤í•˜ëŠ” ëª¨ë“  í–‰ì„ ì ê¶ˆ ìµœê³  ìˆ˜ì¤€ì˜ ë¶„ë¦¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë“  í˜•íƒœì˜ ë™ì‹œì„± ê´€ë ¨ ì´ìƒ ì§•í›„ë¥¼ ë°©ì§€í•  ìˆ˜ ìˆì§€ë§Œ ë™ì‹œì„± ë° ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n\n## REPEATABLE READì˜ í•œê³„ì \n\nMySQLì˜ REPEATABLE READ ê²©ë¦¬ ìˆ˜ì¤€ì€ ê°•ë ¥í•œ ìˆ˜ì¤€ì˜ ì¼ê´€ì„±ì„ ì œê³µí•˜ì§€ë§Œ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì„±ëŠ¥ê³¼ ê¸°ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆëŠ” ëª‡ ê°€ì§€ ì œí•œ ì‚¬í•­ë„ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒì€ ë°˜ë³µ ê°€ëŠ¥í•œ ì½ê¸° ë¶„ë¦¬ ìˆ˜ì¤€ì˜ ëª‡ ê°€ì§€ ì œí•œ ì‚¬í•­ì…ë‹ˆë‹¤:\n\n1. ë™ì‹œì„±ì´ ê°ì†Œ: ë°˜ë³µ ê°€ëŠ¥ ì½ê¸° ë¶„ë¦¬ ìˆ˜ì¤€ì€ íŠ¸ëœì­ì…˜ì´ ì™„ë£Œë  ë•Œê¹Œì§€ íŠ¸ëœì­ì…˜ì´ ì•¡ì„¸ìŠ¤í•˜ëŠ” ëª¨ë“  í–‰ì„ ì ê¸‰ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ë‹¤ë¥¸ íŠ¸ëœì­ì…˜ì€ ì ê¸´ í–‰ì´ í•´ì œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì•¼ í•˜ë¯€ë¡œ ë™ì‹œì„±ì´ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n2. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¦ê°€: ë” ë§ì€ ì ê¸ˆì„ íšë“í•˜ê³  í•´ì œí•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— REPEATABLE READ ê²©ë¦¬ ìˆ˜ì¤€ì˜ ì ê¸ˆ ë™ì‘ì€ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ì¦ê°€ë¡œ ì´ì–´ì§ˆ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n\n3. ì¼ê´€ë˜ì§€ ì•Šì€ ì½ê¸°: REPEATABLE READ ë¶„ë¦¬ ìˆ˜ì¤€ì€ íŠ¸ëœì­ì…˜ì´ ë°ì´í„°ë² ì´ìŠ¤ì˜ ì¼ê´€ëœ ìŠ¤ëƒ…ìƒ·ì„ ë³¼ ìˆ˜ ìˆë„ë¡ ë³´ì¥í•˜ì§€ë§Œ, ê²©ë¦¬ ë ˆë²¨ì—ì„œëŠ” ì—¬ì „íˆ Phantom Readë¼ê³  ì•Œë ¤ì§„ ì‘ì€ ë¶ˆì¼ì¹˜ ì°½ì´ í—ˆìš©ë©ë‹ˆë‹¤. ì´ ë¬¸ì œëŠ” ë‹¤ë¥¸ íŠ¸ëœì­ì…˜ì´ ë™ì¼í•œ íŠ¸ëœì­ì…˜ ë‚´ì˜ ì²« ë²ˆì§¸ ì¿¼ë¦¬ ì‹¤í–‰ê³¼ ë‘ ë²ˆì§¸ ì¿¼ë¦¬ ì‹¤í–‰ ì‚¬ì´ì— ì²« ë²ˆì§¸ íŠ¸ëœì­ì…˜ì— ì˜í•´ ì‹¤í–‰ëœ ì¿¼ë¦¬ì˜ ê¸°ì¤€ê³¼ ì¼ì¹˜í•˜ëŠ” ìƒˆ ë°ì´í„°ë¥¼ ì‚½ì…í•  ë•Œ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n   ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒ ë‘ ê°€ì§€ ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•˜ëŠ” íŠ¸ëœì­ì…˜ì„ ìƒê°í•´ ë³´ì‹­ì‹œì˜¤:\n\n   ```sql\n   SELECT * FROM orders WHERE status = 'processing';\n   -- Some time passes, during which another transaction inserts a new order with status 'processing'\n   SELECT * FROM orders WHERE status = 'processing';\n   ```\n\n   íŠ¸ëœì­ì…˜ì´ REPEATABLE READì—ì„œ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°, ìƒˆ ì£¼ë¬¸ì´ ê·¸ ì‚¬ì´ì— ì‚½ì…ëœ ê²½ìš°ì—ë„ ë‘ ë²ˆ ëª¨ë‘ ë™ì¼í•œ ì£¼ë¬¸ ì§‘í•©ì„ ì½ìŠµë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ì˜ ì‹¤ì œ ìƒíƒœë¥¼ ë°˜ì˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— íŠ¸ëœì­ì…˜ì—ì„œ ì½ì€ ë°ì´í„°ì— ë¶ˆì¼ì¹˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n   ì´ ë¬¸ì œë¥¼ ì™„í™”í•˜ë ¤ë©´ ì‘ìš©í”„ë¡œê·¸ë¨ì˜ ì¼ê´€ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ ì ê¸ˆ ë˜ëŠ” ë‚™ê´€ì  ë™ì‹œì„± ì œì–´ì™€ ê°™ì€ ì¶”ê°€ ë©”ì»¤ë‹ˆì¦˜ì„ êµ¬í˜„í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ íŠ¹ì • ì‹¤í–‰ ìˆœì„œì— ì˜ì¡´í•˜ì§€ ì•Šë„ë¡ ì¿¼ë¦¬ ë° ì‘ìš© í”„ë¡œê·¸ë¨ ë¡œì§ì„ ì‹ ì¤‘í•˜ê²Œ ì„¤ê³„í•˜ê±°ë‚˜, ê°•ë ¥í•œ ì¼ê´€ì„±ì´ í•„ìš”í•œ ê²½ìš° SERIALIZABLEì™€ ê°™ì€ ë‹¤ë¥¸ ë¶„ë¦¬ ìˆ˜ì¤€ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•´ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n\n4. êµì°© ìƒíƒœ(Deadlocks): ë°˜ë³µ ê°€ëŠ¥í•œ ì½ê¸° ê²©ë¦¬ ìˆ˜ì¤€ì€ ë‘˜ ì´ìƒì˜ íŠ¸ëœì­ì…˜ì´ ì ê¸´ ë¦¬ì†ŒìŠ¤ë¥¼ í•´ì œí•˜ê¸° ìœ„í•´ ì„œë¡œ ëŒ€ê¸°í•˜ëŠ” êµì°© ìƒíƒœë¡œ ì´ì–´ì§ˆ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n\n   ì´ëŸ¬í•œ ì œí•œì„ ì™„í™”í•˜ë ¤ë©´, êµì°© ìƒíƒœì˜ ê°€ëŠ¥ì„±ì„ ìµœì†Œí™”í•˜ê³  í•„ìš”í•œ ì ê¸ˆ ì–‘ì„ ì¤„ì´ê¸° ìœ„í•´ ì‘ìš©í”„ë¡œê·¸ë¨ ë° ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆë¥¼ ì‹ ì¤‘í•˜ê²Œ ì„¤ê³„í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ì‘ìš©í”„ë¡œê·¸ë¨ì˜ íŠ¹ì • ìš”êµ¬ì‚¬í•­ ë° ì„±ëŠ¥ íŠ¹ì„±ì— ë”°ë¼ READ COMMITTED ë˜ëŠ” SERIALIZABLEì™€ ê°™ì€ ë‹¤ë¥¸ ë¶„ë¦¬ ìˆ˜ì¤€ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•´ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.",
    "docType": "original",
    "category": "Database",
    "tags": [
      "Database"
    ],
    "readingTime": 3,
    "wordCount": 562,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "mysql-íŠ¸ëœì­ì…˜-ì¿¼ë¦¬-ì˜ˆì‹œ",
    "slug": "mysql-teuraenjaegsyeon-kweori-yesi",
    "path": "database/mysql",
    "fullPath": "database/mysql/mysql-teuraenjaegsyeon-kweori-yesi",
    "title": "MySQL íŠ¸ëœì­ì…˜ ì¿¼ë¦¬ ì˜ˆì‹œ ",
    "excerpt": "MySQL íŠ¸ëœì­ì…˜ ì¿¼ë¦¬ ì˜ˆì‹œ ```mysql START TRANSACTION; SAVEPOINT A; INSERT INTO user(username, password, salt) VALUES ('testuser', 'testpassword',...",
    "content": "# MySQL íŠ¸ëœì­ì…˜ ì¿¼ë¦¬ ì˜ˆì‹œ \n\n```mysql\nSTART TRANSACTION;  \n  \nSAVEPOINT A;  \nINSERT INTO user(username, password, salt)  \nVALUES ('testuser', 'testpassword', 'testsalt');  \n  \nSAVEPOINT B;  \nINSERT INTO user(username, password, salt)  \nVALUES ('testuser1', 'testpassword1', 'testsalt1');  \n  \n# B ì¿¼ë¦¬ ì´ì „, ì¦‰ A - B ì‚¬ì´ì˜ WRITEë§Œ ìœ íš¨  \nROLLBACK TO SAVEPOINT B;  \n  \n# ì „ì²´ ì·¨ì†Œ  \n# ROLLBACK;  \n  \n# ì „ì²´ ì ìš©  \n# COMMIT;\n```",
    "docType": "original",
    "category": "Database",
    "tags": [
      "Database"
    ],
    "readingTime": 1,
    "wordCount": 58,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "upsert-vs-unique-key-constraint",
    "slug": "upsert-vs-unique-key-constraint",
    "path": "database/concepts",
    "fullPath": "database/concepts/upsert-vs-unique-key-constraint",
    "title": "ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œ Upsert íŒ¨í„´ í™œìš©ë²•",
    "excerpt": "ëŒ€ëŸ‰ ë°ì´í„° ì¸ë±ì‹± ì‘ì—…ì—ì„œ INSERTì™€ Upsert íŒ¨í„´ì„ ë¹„êµí•˜ê³ , íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬ ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œ Upsert íŒ¨í„´ í™œìš©ë²•\n\n## Upsertë€?\n\nUpsertëŠ” \"Update + Insert\"ì˜ í•©ì„±ì–´ë¡œ, ë ˆì½”ë“œê°€ ì¡´ì¬í•˜ë©´ ì—…ë°ì´íŠ¸í•˜ê³  ì—†ìœ¼ë©´ ì‚½ì…í•˜ëŠ” ì›ìì  ì—°ì‚°ì…ë‹ˆë‹¤.\n\n### DBë³„ Upsert ë¬¸ë²•\n\n**PostgreSQL**\n\n```sql\nINSERT INTO users (id, name, email)\nVALUES (1, 'Kim', 'kim@example.com')\nON CONFLICT (id) DO UPDATE SET\n  name = EXCLUDED.name,\n  email = EXCLUDED.email;\n```\n\n**MySQL**\n\n```sql\nINSERT INTO users (id, name, email)\nVALUES (1, 'Kim', 'kim@example.com')\nON DUPLICATE KEY UPDATE\n  name = VALUES(name),\n  email = VALUES(email);\n```\n\n## ì™œ Upsertê°€ ë” íš¨ìœ¨ì ì¸ê°€?\n\n### ê¸°ì¡´ ë°©ì‹ì˜ ë¬¸ì œì \n\n```sql\n-- ë¹„íš¨ìœ¨ì ì¸ ë°©ì‹: SELECT í›„ INSERT/UPDATE\nSELECT * FROM users WHERE id = 1;\n-- ê²°ê³¼ì— ë”°ë¼\nINSERT INTO users ... -- ë˜ëŠ”\nUPDATE users SET ... WHERE id = 1;\n```\n\nì´ ë°©ì‹ì€:\n\n1. **2ë²ˆì˜ ì¿¼ë¦¬ ì‹¤í–‰** (SELECT + INSERT/UPDATE)\n2. **Race Condition ìœ„í—˜** (SELECTì™€ INSERT ì‚¬ì´ì— ë‹¤ë¥¸ íŠ¸ëœì­ì…˜ì´ ì‚½ì…í•  ìˆ˜ ìˆìŒ)\n3. **íŠ¸ëœì­ì…˜ ì˜¤ë²„í—¤ë“œ** ì¦ê°€\n\n### Upsertì˜ ì¥ì \n\n1. **ë‹¨ì¼ ì›ìì  ì—°ì‚°**: DB ì—”ì§„ì´ ìµœì í™”ëœ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬\n2. **Race Condition ë°©ì§€**: ë™ì‹œì„± ë¬¸ì œ í•´ê²°\n3. **ë„¤íŠ¸ì›Œí¬ ë¼ìš´ë“œíŠ¸ë¦½ ê°ì†Œ**: í•œ ë²ˆì˜ ì¿¼ë¦¬ë¡œ ì²˜ë¦¬\n\n## ë°°ì¹˜ Upsert íŒ¨í„´\n\nëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œ ë°°ì¹˜ Upsertê°€ ê°€ì¥ íš¨ìœ¨ì ì…ë‹ˆë‹¤:\n\n**PostgreSQL ë°°ì¹˜ Upsert**\n\n```sql\nINSERT INTO transactions (tx_id, amount, status)\nVALUES \n  ('tx_001', 100, 'pending'),\n  ('tx_002', 200, 'confirmed'),\n  ('tx_003', 150, 'pending')\nON CONFLICT (tx_id) DO UPDATE SET\n  amount = EXCLUDED.amount,\n  status = EXCLUDED.status,\n  updated_at = NOW();\n```\n\n## ì£¼ì˜ì‚¬í•­\n\n- **ì¸ë±ìŠ¤ í•„ìˆ˜**: UpsertëŠ” ê³ ìœ  ì œì•½ ì¡°ê±´(UNIQUE) ë˜ëŠ” PKê°€ ìˆì–´ì•¼ ë™ì‘\n- **ëŒ€ìš©ëŸ‰ ì»¬ëŸ¼**: TEXT/BLOB ë“± í° ì»¬ëŸ¼ì´ ë§ìœ¼ë©´ UPDATE ë¹„ìš©ì´ ë†’ì•„ì§ˆ ìˆ˜ ìˆìŒ\n- **íŠ¸ë¦¬ê±° ì£¼ì˜**: INSERT/UPDATE íŠ¸ë¦¬ê±°ê°€ ëª¨ë‘ ì‹¤í–‰ë  ìˆ˜ ìˆìŒ\n\n## ê²°ë¡ \n\nëŒ€ëŸ‰ ë°ì´í„° ì¸ë±ì‹± ì‘ì—…ì—ì„œëŠ” **ë°°ì¹˜ Upsert íŒ¨í„´**ì„ ì‚¬ìš©í•˜ë©´:\n\n- ì¿¼ë¦¬ ìˆ˜ ê°ì†Œ\n- íŠ¸ëœì­ì…˜ ì˜¤ë²„í—¤ë“œ ê°ì†Œ\n- ë™ì‹œì„± ë¬¸ì œ í•´ê²°\n\në¡œ ì¸í•´ ì „ì²´ì ì¸ ë°ì´í„°ë² ì´ìŠ¤ ë¶€í•˜ë¥¼ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    "docType": "original",
    "category": "Database",
    "tags": [
      "Database",
      "MySQL",
      "Performance",
      "PostgreSQL"
    ],
    "readingTime": 2,
    "wordCount": 283,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "acid",
    "slug": "acid",
    "path": "database/concepts",
    "fullPath": "database/concepts/acid",
    "title": "ACID",
    "excerpt": "ACID ì •ì˜ Atomic íŠ¸ëœì­ì…˜ì˜ ì—°ì‚°ì€ ëª¨ë‘ ì •ìƒì ìœ¼ë¡œ ì‹¤íŒ¨í•˜ê±°ë‚˜ ëª¨ë‘ ì‹¤íŒ¨í•´ì•¼ í•œë‹¤. Consistent íŠ¸ëœì­ì…˜ì´ ì„±ê³µí•œ í›„ì— ë°ì´í„°ë² ì´ìŠ¤ì˜ ì œì•½ì¡°ê±´ì„ í¬í•¨í•œ ì¼ê´€ì„±ì´ ì§€ì¼œì ¸ì•¼ í•œë‹¤. Isolation í˜„ì¬ ì§„í–‰ì¤‘ì¸...",
    "content": "# ACID\n\n## ì •ì˜\n\n- Atomic\n\n  íŠ¸ëœì­ì…˜ì˜ ì—°ì‚°ì€ ëª¨ë‘ ì •ìƒì ìœ¼ë¡œ ì‹¤íŒ¨í•˜ê±°ë‚˜ ëª¨ë‘ ì‹¤íŒ¨í•´ì•¼ í•œë‹¤.\n\n- Consistent\n\n  íŠ¸ëœì­ì…˜ì´ ì„±ê³µí•œ í›„ì— ë°ì´í„°ë² ì´ìŠ¤ì˜ ì œì•½ì¡°ê±´ì„ í¬í•¨í•œ ì¼ê´€ì„±ì´ ì§€ì¼œì ¸ì•¼ í•œë‹¤.\n\n- Isolation\n\n  í˜„ì¬ ì§„í–‰ì¤‘ì¸ íŠ¸ëœì­ì…˜ì´ ìˆë‹¤ë©´ ë‹¤ë¥¸ íŠ¸ëœì­ì…˜ì´ ì´ íŠ¸ëœì­ì…˜ì— ì ‘ê·¼í•  ìˆ˜ ì—†ë‹¤. ê° íŠ¸ëœì­ì…˜ì€ ë…ë¦½ì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì–´ì•¼ í•œë‹¤.\n\n- Durability\n\n  íŠ¸ëœì­ì…˜ì´ ì„±ê³µë˜ì—ˆë‹¤ë©´ ë°ì´í„°ë² ì´ìŠ¤ëŠ” ì´ ê²°ê³¼ë¥¼ ì˜êµ¬ì ìœ¼ë¡œ ë°˜ì˜í•´ì•¼ í•œë‹¤. í•˜ë‚˜ë¼ë„ ì†ì‹¤ì´ ìˆìœ¼ë©´ ì•ˆëœë‹¤.\n\n## ë” ë‚˜ì•„ê°€ì„œ\n\nRedisì™€ í•¨ê»˜ ë°ì´í„°ë² ì´ìŠ¤ íŠ¸ëœì­ì…˜ ë° ë¶„ì‚° ì ê¸ˆì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë°±ì—”ë“œì—ì„œ ë™ì‹œì„± ì²˜ë¦¬ì˜ ì›ìì„±ê³¼ ì¼ê´€ì„±ì„ ë³´ì¥í•˜ëŠ” ì¢‹ì€ ë°©ë²•ì…ë‹ˆë‹¤. ì‘ìš©í”„ë¡œê·¸ë¨ì˜ íŠ¹ì • ìš”êµ¬ì— ë”°ë¼ ì´ëŸ¬í•œ ê¸°ìˆ ì„ ì ìš©í•  ìˆ˜ ìˆëŠ” ì¶”ê°€ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.\n\në¹…í…Œí¬ì—ì„œ ì›ìì„±ê³¼ ì¼ê´€ì„±ì„ ë³´ì¥í•˜ëŠ” í•œ ê°€ì§€ ì¶”ê°€ì ì¸ ë°©ë²•ì€ ì—¬ëŸ¬ ë°ì´í„°ë² ì´ìŠ¤ ë˜ëŠ” ì„œë¹„ìŠ¤ì— ë¶„ì‚° íŠ¸ëœì­ì…˜ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ê° ì„œë¹„ìŠ¤ì— ìì²´ ë°ì´í„°ë² ì´ìŠ¤ê°€ ìˆëŠ” ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ê°€ ìˆëŠ” ê²½ìš° ë¶„ì‚° íŠ¸ëœì­ì…˜ì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • íŠ¸ëœì­ì…˜ê³¼ ê´€ë ¨ëœ ëª¨ë“  ì„œë¹„ìŠ¤ê°€ í•¨ê»˜ ì»¤ë°‹ë˜ê±°ë‚˜ ë¡¤ë°±ë˜ë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\në¶„ì‚° íŠ¸ëœì­ì…˜ì„ ì§€ì›í•˜ê¸° ìœ„í•´ 2PC(Two-Phase Commit) í”„ë¡œí† ì½œ, X/Open XA í”„ë¡œí† ì½œ, Saga íŒ¨í„´ ë“± ë‹¤ì–‘í•œ ë„êµ¬ì™€ ê¸°ìˆ ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ìˆ ì„ í†µí•´ íŠ¸ëœì­ì…˜ì´ ì™„ì „íˆ ì»¤ë°‹ë˜ê±°ë‚˜ ê´€ë ¨ëœ ëª¨ë“  ì„œë¹„ìŠ¤ì— ê±¸ì³ ì™„ì „íˆ ë¡¤ë°±ë˜ì–´ ì‹œìŠ¤í…œì´ ì¼ê´€ë˜ê²Œ ìœ ì§€ë©ë‹ˆë‹¤.\n\nê·¸ëŸ¬ë‚˜ ë¶„ì‚° íŠ¸ëœì­ì…˜ì€ êµ¬í˜„í•˜ê¸°ê°€ ë³µì¡í•˜ê³  ì„±ëŠ¥ ì˜¤ë²„í—¤ë“œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì ˆì¶©ì•ˆì„ ì‹ ì¤‘í•˜ê²Œ ê³ ë ¤í•˜ê³  ì‚¬ìš© ì‚¬ë¡€ì— í•„ìš”í•œì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n\në¶„ì‚° íŠ¸ëœì­ì…˜ ì™¸ì—ë„ Big Techì—ì„œ ì›ìì„±ê³¼ ì¼ê´€ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•œ ë‹¤ë¥¸ ëª¨ë²” ì‚¬ë¡€ë¡œëŠ” ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ êµ¬í˜„, ìµœì ì˜ ì ê¸ˆ ê¸°ìˆ  ì‚¬ìš©, ì •ê¸°ì ì¸ ë°ì´í„° ë°±ì—… ë° ì¬í•´ ë³µêµ¬ ê³„íš ìˆ˜í–‰ ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë²” ì‚¬ë¡€ëŠ” ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš°ì—ë„ ë°ì´í„°ë¥¼ ì¼ê´€ì„± ìˆê²Œ ìœ ì§€í•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë³´ì¥í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.\n\nUsing database transactions and distributed locks with Redis is a good way to ensure atomicity and consistency in concurrency processing at the backend. There are additional ways to apply these techniques depending on the specific needs of your application.\n\nOne additional way to ensure atomicity and consistency in Big Tech is to use distributed transactions across multiple databases or services. For example, if you have a microservices architecture where each service has its own database, you can use distributed transactions to ensure that all the services involved in a particular transaction either commit or rollback together.\n\nThere are various tools and technologies available to support distributed transactions, such as Two-Phase Commit (2PC) protocol, X/Open XA protocol, and the Saga pattern. These technologies ensure that transactions are either fully committed or fully rolled back across all services involved, ensuring that the system remains consistent.\n\nHowever, it's important to note that distributed transactions can be complex to implement and can introduce performance overhead. Therefore, it's important to carefully consider the trade-offs and determine if they are necessary for your use case.\n\nIn addition to distributed transactions, other best practices to ensure atomicity and consistency in Big Tech include implementing data validation checks, using optimistic locking techniques, and performing regular data backups and disaster recovery planning. These best practices help to ensure that data remains consistent and available even in the face of unexpected failures or errors.\n\n## Ref\n\n- [ACID](https://ko.wikipedia.org/wiki/ACID)",
    "docType": "original",
    "category": "Database",
    "tags": [
      "Database"
    ],
    "readingTime": 3,
    "wordCount": 474,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "solana-sol",
    "slug": "solana-sol",
    "path": "blockchain/solana",
    "fullPath": "blockchain/solana/solana-sol",
    "title": "Solana (SOL)",
    "excerpt": "Solana (SOL) ğŸ’¡ Cheatsheet of SOL for web3 developers. ìš”ì•½ ì¤‘ì•™í™”ë¥¼ ë²—ì–´ë‚˜ì§€ ëª»í•¨. ì¬ë‹¨ì—ì„œ ë„¤íŠ¸ì›Œí¬ë¥¼ ê»ë‹¤ ì¼°ë‹¤ í•  ìˆ˜ ìˆìŒ ë…¸ë“œê°€ ì—„ì²­ ì˜ ì£½ìŒ (ì˜¬í•´ë§Œ ëª‡ ë²ˆ ì¤‘ì§€...",
    "content": "# Solana (SOL)\n\n<aside>\nğŸ’¡ Cheatsheet of SOL for web3 developers.\n\n</aside>\n\n# ìš”ì•½\n\n1. ì¤‘ì•™í™”ë¥¼ ë²—ì–´ë‚˜ì§€ ëª»í•¨. ì¬ë‹¨ì—ì„œ ë„¤íŠ¸ì›Œí¬ë¥¼ ê»ë‹¤ ì¼°ë‹¤ í•  ìˆ˜ ìˆìŒ\n2. ë…¸ë“œê°€ ì—„ì²­ ì˜ ì£½ìŒ (ì˜¬í•´ë§Œ ëª‡ ë²ˆ ì¤‘ì§€ë˜ì—ˆëŠ”ì§€..)\n3. ì†”ë¼ë‚˜ì™€ ì†”ë¼ë‚˜ í”„ë¡œê·¸ë¨(ì»¨íŠ¸ë™íŠ¸) ê°œë°œì–¸ì–´ê°€ í†µì¼ë˜ì–´ ìˆë‹¤ëŠ”ê²Œ ë§¤ìš° ë§¤ë ¥í¬ì¸íŠ¸\n4. ì¬ë‹¨ì—ì„œ ì œê³µí•˜ëŠ” íˆ´ë“¤ì´ ë„ˆë¬´ í™˜ìƒì ì„. ì•„ì´ë””ì–´ë§Œ ìˆë‹¤ë©´ ëŒ€ì¶© ì‹œì‘í•  ìˆ˜ ìˆì„ ì •ë„\n5. ì´ê²ƒë„ mempoolì´ ì—†ìŒ\n\n# How to create and manage address or account?\n\n### Using Private Key\n\n```jsx\nconst account = Keypair.generate()\nconsole.log(JSON.stringify(account.publicKey.toBase58()))\n// output: \"gVazpxjimX3EP4mto53pEi4YSE36KP2nDwyEvLcKjmR\"\n```\n\n### Using Mnemonic\n\n```jsx\nconst mnemonic =\n\t'pill tomorrow foster begin walnut borrow virtual kick shift mutual shoe scatter'\nconst seed = bip39.mnemonicToSeedSync(mnemonic, '') // (mnemonic, password)\n\n// BIP39\nconst keypair = Keypair.fromSeed(seed.slice(0, 32))\n\n// BIP44\nfor (let i = 0; i < 10; i++) {\n\tconst path = `m/44'/501'/${i}'/0'`\n\tconst keypair = Keypair.fromSeed(derivePath(path, seed.toString('hex')).key)\n\tconsole.log(`${path} => ${keypair.publicKey.toBase58()}`)\n}\n\n// Check if a given public key has an associated private key\nconst key = new PublicKey('5oNDL3swdJJF1g9DzJiZ4ynHXgszjAEpUkxVYejchzrY')\nconsole.log(PublicKey.isOnCurve(key.toBytes()))\n```\n\n# How to sign and verify messages with wallets\n\n```jsx\nconst message = \"The quick brown fox jumps over the lazy dog\";\nconst messageBytes = decodeUTF8(message);\n\nconst signature = nacl.sign.detached(messageBytes, keypair.secretKey);\nconst result = nacl.sign.detached.verify(\n  messageBytes,\n  signature,\n  keypair.publicKey.toBytes()\n);\n\nconsole.log(result);\n\n-------------------------------------------------------------------------------\n\n{\n  let recoverTx = Transaction.populate(Message.from(realDataNeedToSign));\n  recoverTx.addSignature(feePayer.publicKey, Buffer.from(feePayerSignature));\n  recoverTx.addSignature(alice.publicKey, Buffer.from(aliceSignature));\n\n  console.log(\n    `txhash: ${await connection.sendRawTransaction(recoverTx.serialize())}`\n  );\n}\n\n-------------------------------------------------------------------------------\n\n{\n  let recoverTx = Transaction.populate(Message.from(realDataNeedToSign), [\n    bs58.encode(feePayerSignature),\n    bs58.encode(aliceSignature),\n  ]);\n  console.log(\n    `txhash: ${await connection.sendRawTransaction(recoverTx.serialize())}`\n  );\n}\n```\n\n# How to query balance of address on blockchain?\n\n# How to query transaction?\n\n# How to add a memo to a tx?\n\n```jsx\nconst transferTransaction = new Transaction().add(\n\tSystemProgram.transfer({\n\t\tfromPubkey: fromKeypair.publicKey,\n\t\ttoPubkey: toKeypair.publicKey,\n\t\tlamports: lamportsToSend,\n\t}),\n)\n\nawait transferTransaction.add(\n\tnew TransactionInstruction({\n\t\tkeys: [\n\t\t\t{ pubkey: fromKeypair.publicKey, isSigner: true, isWritable: true },\n\t\t],\n\t\tdata: Buffer.from('Data to send in transaction', 'utf-8'),\n\t\tprogramId: new PublicKey('MemoSq4gqABAXKb96qnH8TysNcWxMyWCqXgDLGmfcHr'),\n\t}),\n)\n\nawait sendAndConfirmTransaction(connection, transferTransaction, [fromKeypair])\n```\n\n# How to estimate fee?\n\n```jsx\ngetEstimatedFee\nconst recentBlockhash = await connection.getLatestBlockhash()\nconst transaction = new Transaction({\n\trecentBlockhash: recentBlockhash.blockhash,\n}).add(\n\tSystemProgram.transfer({\n\t\tfromPubkey: payer.publicKey,\n\t\ttoPubkey: payee.publicKey,\n\t\tlamports: 10,\n\t}),\n)\n\nconst fees = await transaction.getEstimatedFee(connection)\nconsole.log(`Estimated SOL transfer cost: ${fees} lamports`)\n// Estimated SOL transfer cost: 5000 lamports\n\n// getFeeForMessage\nconst message = new Message(messageParams)\n\nconst fees = await connection.getFeeForMessage(message)\nconsole.log(`Estimated SOL transfer cost: ${fees.value} lamports`)\n// Estimated SOL transfer cost: 5000 lamports\n```\n\n# How to Transfer tokens one to another?\n\n```jsx\n// Send SOL\nconst transferTransaction = new Transaction().add(\n\tSystemProgram.transfer({\n\t\tfromPubkey: fromKeypair.publicKey,\n\t\ttoPubkey: toKeypair.publicKey,\n\t\tlamports: lamportsToSend,\n\t}),\n)\n\nawait sendAndConfirmTransaction(connection, transferTransaction, [fromKeypair])\n\n// Send SPL Token\n// Add token transfer instructions to transaction\nconst transaction = new web3.Transaction().add(\n\tsplToken.Token.createTransferInstruction(\n\t\tsplToken.TOKEN_PROGRAM_ID,\n\t\tfromTokenAccount.address,\n\t\ttoTokenAccount.address,\n\t\tfromWallet.publicKey,\n\t\t[],\n\t\t1,\n\t),\n)\n\n// Sign transaction, broadcast, and confirm\nawait web3.sendAndConfirmTransaction(connection, transaction, [fromWallet])\n```\n\n---\n\n## Build Frontend and deploy contract to Solana devnet\n\nI using rust, anchor to develop smart contract. All I have to do is initializing some struct, and add methods in main program module. Itâ€™s quite easy to understand, and I think itâ€™s simillar with creating and using gRPC.\n\n![](/images/solanademo.png)\nScreenshot of Demo project (thx to https://buildspace.so)",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 3,
    "wordCount": 501,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "tezos-xtz",
    "slug": "tezos-xtz",
    "path": "blockchain/others",
    "fullPath": "blockchain/others/tezos-xtz",
    "title": "Tezos (XTZ)",
    "excerpt": "Tezos (XTZ) ğŸ’¡ Cheatsheet of XTZ for web3 developers. ìš”ì•½ ë…¸ë“œì— ì¢€ë§Œ ìš”ì²­ë³´ë‚´ë„ í—ë–¡ëŒ€ë‹¤ê°€ ì£½ìŒ ìµœì†Œ ì”ê³  ë§ì¶°ì¤˜ì•¼ í•¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ í¸í•˜ê²Œ ì˜ë˜ì–´ ìˆê¸´ í•œë° ë…¸ë“œ ê°œ...",
    "content": "# Tezos (XTZ)\n\n<aside>\nğŸ’¡ Cheatsheet of XTZ for web3 developers.\n\n</aside>\n\n# ìš”ì•½\n\n1. ë…¸ë“œì— ì¢€ë§Œ ìš”ì²­ë³´ë‚´ë„ í—ë–¡ëŒ€ë‹¤ê°€ ì£½ìŒ\n2. ìµœì†Œ ì”ê³  ë§ì¶°ì¤˜ì•¼ í•¨\n3. ë¼ì´ë¸ŒëŸ¬ë¦¬ í¸í•˜ê²Œ ì˜ë˜ì–´ ìˆê¸´ í•œë° ë…¸ë“œ ê°œë°œì„ Ocamlìœ¼ë¡œ í•´ë†”ì„œ ê¹Šì€ ë””ë²„ê¹…ì‹œì— ì œì•½ì´ ìˆìŒ\n4. ì»¤ë®¤ë‹ˆí‹°ê°€ ì‘ì•„ì„œ ì†”ì§íˆ ìë£Œ ë‚˜ì˜¤ëŠ”ê²Œ ë­ ì—†ìŒ",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 48,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "stellar-lumens-xlm",
    "slug": "stellar-lumens-xlm",
    "path": "blockchain/others",
    "fullPath": "blockchain/others/stellar-lumens-xlm",
    "title": "Stellar Lumens (XLM)",
    "excerpt": "Stellar Lumens (XLM) ğŸ’¡ Cheatsheet of XLM for web3 developers. ìš”ì•½ ë¦¬í”Œ v2 protobufê°™ì€ í”„ë¡œí† ì½œì¸ xdrì„ ì‚¬ìš©í•˜ëŠ” íŠ¹ì´ì‚¬í•­ì´ ìˆìŒ íŠ¸ëœì­ì…˜...",
    "content": "# Stellar Lumens (XLM)\n\n<aside> ğŸ’¡ Cheatsheet of XLM for web3 developers.\n\n</aside>\n\n# ìš”ì•½\n\n1.  ë¦¬í”Œ v2\n2.  protobufê°™ì€ í”„ë¡œí† ì½œì¸ xdrì„ ì‚¬ìš©í•˜ëŠ” íŠ¹ì´ì‚¬í•­ì´ ìˆìŒ\n3.  íŠ¸ëœì­ì…˜ ì „íŒŒì‹œì— ê°€ìŠ¤ ë¶€ì¡±í•˜ë©´ íƒ€ì„ì•„ì›ƒ ë‚¨\n4.  íŠ¸ëœì­ì…˜ ì „íŒŒì‹œì— mempool ì—†ì–´ì„œ ì¬ì‹œë„ í•˜ê¸° ê·€ì°®ìŒ\n5.  SDK í†µí•´ì„œ txHash í¸í•˜ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì°¾ì•„ë³´ë©´ ì¢‹ìŒ\n6.  ì£¼ì†Œ ë¡œì»¬ì—ì„œ ë§Œë“ ë‹¤ê³  ë„¤íŠ¸ì›ì—ì„œ ì¡°íšŒ ì•ˆë¨. í•œë²ˆì€ tx ìƒì„±ì„ í†µí•´ì„œ ë…¸ì¶œí•´ì¤˜ì•¼í•¨",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 65,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ripple-xrp",
    "slug": "ripple-xrp",
    "path": "blockchain/others",
    "fullPath": "blockchain/others/ripple-xrp",
    "title": "Ripple (XRP)",
    "excerpt": "Ripple (XRP) ğŸ’¡ Cheatsheet of XRP for web3 developers. ìš”ì•½ êµ­ì œ ì†¡ê¸ˆë§ì„ íƒ€ê²Ÿìœ¼ë¡œ í•œ ë¸”ë¡ì²´ì¸ ì†ë„ë¥¼ ìœ„í•´ì„œ íƒˆì¤‘ì•™ì„ í¬ê¸°í•œ ì‚¬ë¡€ ê°œë°œì ë„êµ¬ë‚˜ ì»¤ë®¤ë‹ˆí‹°ê°€ í›Œ...",
    "content": "# Ripple (XRP)\n\n<aside> ğŸ’¡ Cheatsheet of XRP for web3 developers.\n\n</aside>\n\n# ìš”ì•½\n\n1.  êµ­ì œ ì†¡ê¸ˆë§ì„ íƒ€ê²Ÿìœ¼ë¡œ í•œ ë¸”ë¡ì²´ì¸\n2.  ì†ë„ë¥¼ ìœ„í•´ì„œ íƒˆì¤‘ì•™ì„ í¬ê¸°í•œ ì‚¬ë¡€\n3.  ê°œë°œì ë„êµ¬ë‚˜ ì»¤ë®¤ë‹ˆí‹°ê°€ í›Œë¥­í•¨\n4.  ë¦¬í”Œê³¼ Issuedtoken ê°¯ìˆ˜ì— ë§ì¶°ì„œ ìµœì†Œ ì”ê³  ë“¤ê³  ìˆì–´ì•¼í•¨",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 41,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "istanbul-byzantine-fault",
    "slug": "istanbul-byzantine-fault",
    "path": "blockchain/others",
    "fullPath": "blockchain/others/istanbul-byzantine-fault",
    "title": "***Istanbul Byzantine Fault***",
    "excerpt": "Istanbul Byzantine Fault This article refers to link below! [Link](https://steemit.com/kr/@kanghamin/istanbul-byzantine-fault-toleranc...",
    "content": "# ***Istanbul Byzantine Fault***\n\n> This article refers to link below!\n> \n> [Link](https://steemit.com/kr/@kanghamin/istanbul-byzantine-fault-tolerance)\n\n## Consensus process\n\n1.  Pre-Prepare\n2.  Prepare\n3.  Commit\n\n-   Assuming that the bad node is F, System can run if the total number of nodes is 3F+1 or more.\n-   Validator is that derive consensus, and they choose a proposer before generating blocks(before every round).\n\n### Protocol: Propose\n\nFirst, the proposer choose and propose a new block, then announce it with message.\n\n![https://velog.velcdn.com/images%2Fasap0208%2Fpost%2F0a8048d2-f260-4e4c-be50-a88e40aca8a9%2Fimage.png](https://velog.velcdn.com/images%2Fasap0208%2Fpost%2F0a8048d2-f260-4e4c-be50-a88e40aca8a9%2Fimage.png)\n\n### Protocol: Pre-Prepare\n\nSecond, protocol is 'pre-prepare' state with sending 'prepare' message when validators receive a message from proposer. This process makes every validators notice that they're in same state.\n\n![https://velog.velcdn.com/images%2Fasap0208%2Fpost%2Fc3524567-17ce-4cc7-ad10-72bb78e13f9a%2Fimage.png](https://velog.velcdn.com/images%2Fasap0208%2Fpost%2Fc3524567-17ce-4cc7-ad10-72bb78e13f9a%2Fimage.png)\n\n### Protocol: Prepare\n\nThird, when validators get more than 2F+1 of messages, protocol is 'prepare' state and send 'commit' message.\n\n![https://velog.velcdn.com/images%2Fasap0208%2Fpost%2F7973d4cc-6dd8-43d7-9349-db904db584f3%2Fimage.png](https://velog.velcdn.com/images%2Fasap0208%2Fpost%2F7973d4cc-6dd8-43d7-9349-db904db584f3%2Fimage.png)\n\n### Protocol: Commit\n\nThis protocol makes peers notice that proposed blocks are permitted, and connects those to the chain. When they receive more than 2F+1 of messages, protocol should be 'commit' state, then blocks are connected to chain.\n\n![https://velog.velcdn.com/images%2Fasap0208%2Fpost%2Faf462703-0be6-439d-925f-2c14c0dc9412%2Fimage.png](https://velog.velcdn.com/images%2Fasap0208%2Fpost%2Faf462703-0be6-439d-925f-2c14c0dc9412%2Fimage.png)\n\n### Overall process\n\n![https://velog.velcdn.com/images%2Fasap0208%2Fpost%2F7939f561-1b3d-4801-8bda-e3df4b3482cc%2Fimage.png](https://velog.velcdn.com/images%2Fasap0208%2Fpost%2F7939f561-1b3d-4801-8bda-e3df4b3482cc%2Fimage.png)\n\nImage above shows the whole algorithm, when consensus is failed, go to 'Round change' and then restart over again.\n\n## Wrap up\n\nConsensus process is composed of three state, Success insertion of block means finality. Simply put, there's no way to change it at all.Therefore, it cannot be forked and valid block must be on the main blockchain.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 2,
    "wordCount": 232,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "hd-hierachy-deterministic-wallet",
    "slug": "hd-hierachy-deterministic-wallet",
    "path": "blockchain/others",
    "fullPath": "blockchain/others/hd-hierachy-deterministic-wallet",
    "title": "HD(Hierarchical Deterministic) Wallet",
    "excerpt": "HD ì§€ê°‘ì˜ ê°œë…ê³¼ BIP-32/39/44 í‘œì¤€, í‚¤ íŒŒìƒ ê²½ë¡œ êµ¬ì¡°ë¥¼ ì•Œì•„ë´…ë‹ˆë‹¤.",
    "content": "# HD(Hierarchical Deterministic) Wallet\n\n## ê°œìš”\n\nHD ì§€ê°‘ì€ í•˜ë‚˜ì˜ **ì‹œë“œ(Seed)**ë¡œë¶€í„° ë¬´í•œí•œ ê°œìˆ˜ì˜ ê°œì¸í‚¤/ê³µê°œí‚¤ ìŒì„ ê³„ì¸µì ìœ¼ë¡œ íŒŒìƒí•  ìˆ˜ ìˆëŠ” ì§€ê°‘ êµ¬ì¡°ì…ë‹ˆë‹¤. BIP-32 í‘œì¤€ì— ì •ì˜ë˜ì–´ ìˆìœ¼ë©°, í˜„ëŒ€ ëŒ€ë¶€ë¶„ì˜ ì•”í˜¸í™”í ì§€ê°‘ì´ ì´ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\n## ì™œ HD ì§€ê°‘ì´ í•„ìš”í•œê°€?\n\n### ê¸°ì¡´ ì§€ê°‘ì˜ ë¬¸ì œì \n\n- ê° ì£¼ì†Œë§ˆë‹¤ ë³„ë„ì˜ ê°œì¸í‚¤ë¥¼ ìƒì„±í•˜ê³  ë°±ì—…í•´ì•¼ í•¨\n- ìƒˆ ì£¼ì†Œë¥¼ ë§Œë“¤ ë•Œë§ˆë‹¤ ë°±ì—… ê°±ì‹  í•„ìš”\n- ê°œì¸í‚¤ ê´€ë¦¬ê°€ ë³µì¡í•˜ê³  ë¶„ì‹¤ ìœ„í—˜ ì¦ê°€\n\n### HD ì§€ê°‘ì˜ ì¥ì \n\n- **ë‹¨ì¼ ì‹œë“œ ë°±ì—…**: 12~24ê°œ ë‹ˆëª¨ë‹‰ ë‹¨ì–´ë§Œ ë°±ì—…í•˜ë©´ ëª¨ë“  í‚¤ ë³µêµ¬ ê°€ëŠ¥\n- **ê²°ì •ë¡ ì  íŒŒìƒ**: ë™ì¼í•œ ì‹œë“œì—ì„œ í•­ìƒ ë™ì¼í•œ í‚¤ ì‹œí€€ìŠ¤ ìƒì„±\n- **ê³„ì¸µì  êµ¬ì¡°**: ìš©ë„ë³„ë¡œ ê³„ì •ì„ ë¶„ë¦¬ ê´€ë¦¬ ê°€ëŠ¥\n\n---\n\n## í•µì‹¬ BIP í‘œì¤€\n\n### BIP-39: ë‹ˆëª¨ë‹‰ ì‹œë“œ êµ¬ë¬¸\n\nì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ë‹¨ì–´ ëª©ë¡ìœ¼ë¡œ ì‹œë“œë¥¼ í‘œí˜„í•©ë‹ˆë‹¤.\n\n```\nabandon ability able about above absent absorb abstract absurd abuse access accident\n```\n\n- 12, 15, 18, 21, 24ê°œ ë‹¨ì–´ ì§€ì›\n- 2048ê°œ ë‹¨ì–´ ëª©ë¡ì—ì„œ ì„ íƒ\n- ë§ˆì§€ë§‰ ë‹¨ì–´ì— ì²´í¬ì„¬ í¬í•¨\n\n### BIP-32: í‚¤ íŒŒìƒ êµ¬ì¡°\n\nì‹œë“œë¡œë¶€í„° ë§ˆìŠ¤í„° í‚¤ë¥¼ ìƒì„±í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìì‹ í‚¤ë¥¼ íŒŒìƒí•©ë‹ˆë‹¤.\n\n```\nSeed â†’ Master Key â†’ Child Key â†’ Grandchild Key â†’ ...\n```\n\n**í‚¤ íŒŒìƒ ë°©ì‹:**\n\n- **ì¼ë°˜ íŒŒìƒ (Normal)**: í™•ì¥ ê³µê°œí‚¤ë¡œ ìì‹ ê³µê°œí‚¤ íŒŒìƒ ê°€ëŠ¥\n- **ê°•í™” íŒŒìƒ (Hardened)**: ê°œì¸í‚¤ê°€ ìˆì–´ì•¼ë§Œ ìì‹ í‚¤ íŒŒìƒ ê°€ëŠ¥ (ë³´ì•ˆ ê°•í™”)\n\n### BIP-44: ê²½ë¡œ ê·œì•½\n\në‹¤ì¤‘ ì½”ì¸, ë‹¤ì¤‘ ê³„ì •ì„ ì§€ì›í•˜ê¸° ìœ„í•œ í‘œì¤€í™”ëœ ê²½ë¡œ êµ¬ì¡°ì…ë‹ˆë‹¤.\n\n```\nm / purpose' / coin_type' / account' / change / address_index\n```\n\n| ë ˆë²¨ | ì„¤ëª… | ì˜ˆì‹œ |\n|-----|------|------|\n| `purpose'` | BIP ë²ˆí˜¸ (44 = BIP-44) | 44' |\n| `coin_type'` | ì½”ì¸ ì¢…ë¥˜ | 0' (BTC), 60' (ETH) |\n| `account'` | ê³„ì • ë²ˆí˜¸ | 0', 1', 2' |\n| `change` | ì™¸ë¶€(0) / ë‚´ë¶€(1) | 0 |\n| `address_index` | ì£¼ì†Œ ì¸ë±ìŠ¤ | 0, 1, 2... |\n\n**ê²½ë¡œ ì˜ˆì‹œ:**\n\n```\nm/44'/0'/0'/0/0   â†’ ì²« ë²ˆì§¸ ë¹„íŠ¸ì½”ì¸ ì£¼ì†Œ\nm/44'/60'/0'/0/0  â†’ ì²« ë²ˆì§¸ ì´ë”ë¦¬ì›€ ì£¼ì†Œ\nm/44'/501'/0'/0'  â†’ ì²« ë²ˆì§¸ ì†”ë¼ë‚˜ ì£¼ì†Œ\n```\n\n---\n\n## ë³´ì•ˆ ê³ ë ¤ì‚¬í•­\n\n### ê°•í™” íŒŒìƒì„ ì‚¬ìš©í•´ì•¼ í•˜ëŠ” ê³³\n\n- `purpose`, `coin_type`, `account` ë ˆë²¨ì€ ë°˜ë“œì‹œ ê°•í™” íŒŒìƒ(`'`) ì‚¬ìš©\n- ì¼ë°˜ íŒŒìƒ í‚¤ê°€ ë…¸ì¶œë˜ë©´ í˜•ì œ ê°œì¸í‚¤ ì¶”ë¡  ê°€ëŠ¥\n\n### ë‹ˆëª¨ë‹‰ ë³´ê´€\n\n- ì˜¤í”„ë¼ì¸ í™˜ê²½ì—ì„œ ìƒì„±\n- ë¬¼ë¦¬ì  ë§¤ì²´(ì¢…ì´, ê¸ˆì†íŒ)ì— ë°±ì—…\n- ë””ì§€í„¸ ì €ì¥ ì ˆëŒ€ ê¸ˆì§€\n\n---\n\n## ì£¼ìš” ì½”ì¸ë³„ ê²½ë¡œ\n\n| ì½”ì¸ | BIP-44 ê²½ë¡œ | coin_type |\n|-----|------------|-----------|\n| Bitcoin | m/44'/0'/... | 0 |\n| Ethereum | m/44'/60'/... | 60 |\n| Solana | m/44'/501'/... | 501 |\n| Ripple | m/44'/144'/... | 144 |\n| Aptos | m/44'/637'/... | 637 |\n\n---\n\n## ì°¸ê³  ìë£Œ\n\n- [BIP-32: Hierarchical Deterministic Wallets](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki)\n- [BIP-39: Mnemonic code for generating deterministic keys](https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki)\n- [BIP-44: Multi-Account Hierarchy](https://github.com/bitcoin/bips/blob/master/bip-0044.mediawiki)\n- [SLIP-44: Registered coin types](https://github.com/satoshilabs/slips/blob/master/slip-0044.md)",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 3,
    "wordCount": 428,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "harmony-one",
    "slug": "harmony-one",
    "path": "blockchain/others",
    "fullPath": "blockchain/others/harmony-one",
    "title": "Harmony (ONE)",
    "excerpt": "Harmony (ONE) ğŸ’¡ Cheatsheet of ONE for web3 developers. ìš”ì•½ ì´ë”ë¦¬ì›€ ë³µì œí’ˆ ë‹¤ë§Œ ìƒ¤ë”© ì ìš©ë˜ì–´ ìˆëŠ”ë° í˜„ì¬ëŠ” 4ê°œ ì²´ì¸ ì´ì™¸ íŠ¹ì´ì‚¬í•­ ë”±íˆ ì—†ë‹¤...",
    "content": "# Harmony (ONE)\n\n<aside>\nğŸ’¡ Cheatsheet of ONE for web3 developers.\n\n</aside>\n\n# ìš”ì•½\n\n1. ì´ë”ë¦¬ì›€ ë³µì œí’ˆ\n2. ë‹¤ë§Œ ìƒ¤ë”© ì ìš©ë˜ì–´ ìˆëŠ”ë° í˜„ì¬ëŠ” 4ê°œ ì²´ì¸\n3. ì´ì™¸ íŠ¹ì´ì‚¬í•­ ë”±íˆ ì—†ë‹¤",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 31,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "filecoin-fil",
    "slug": "filecoin-fil",
    "path": "blockchain/others",
    "fullPath": "blockchain/others/filecoin-fil",
    "title": "Filecoin (FIL)",
    "excerpt": "Filecoin (FIL) ğŸ’¡ ì• ì¦ì˜ íŒŒì¼ì½”ì¸ ìš”ì•½ íŠ¸ëœì­ì…˜ ì „íŒŒì‹œì— íƒ€ì„ì•„ì›ƒ ë‚˜ëŠ”ê±° ê³ ì§ˆë³‘ì´ë¼ê³  í•¨ ë¯¸ë¦¬ txHash ë§Œë“œëŠ” ë°©ë²•ìˆìŒ. ì•„ë˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜ ì°¾ìœ¼ë©´ ë‚˜ì˜´ [https://docs.zondax.c...",
    "content": "# Filecoin (FIL)\n\n<aside>\nğŸ’¡ ì• ì¦ì˜ íŒŒì¼ì½”ì¸\n\n</aside>\n\n# ìš”ì•½\n\n1. íŠ¸ëœì­ì…˜ ì „íŒŒì‹œì— íƒ€ì„ì•„ì›ƒ ë‚˜ëŠ”ê±° ê³ ì§ˆë³‘ì´ë¼ê³  í•¨\n2. ë¯¸ë¦¬ txHash ë§Œë“œëŠ” ë°©ë²•ìˆìŒ. ì•„ë˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜ ì°¾ìœ¼ë©´ ë‚˜ì˜´\n\n[https://docs.zondax.ch/filecoin-signing-tools](https://docs.zondax.ch/filecoin-signing-tools)\n\n1. filecoin vs arweave ê²€ìƒ‰í•˜ë©´ IPFS ê´€ë ¨í•´ì„œ ì¬ë°ŒëŠ” ì£¼ì œ ë§ìŒ",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 39,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "biance-smart-chain-bnb",
    "slug": "biance-smart-chain-bnb",
    "path": "blockchain/others",
    "fullPath": "blockchain/others/biance-smart-chain-bnb",
    "title": "Biance Smart Chain (BNB)",
    "excerpt": "Biance Smart Chain (BNB) ğŸ’¡ Cheatsheet of BSC/BNB for web3 developers. ì„¤ëª… ì´ë”ë¦¬ì›€ ë³µì‚¬ë³¸. BNBì²´ì¸ê³¼ BSCì²´ì¸ì´ ë”°ë¡œ ìˆì–´ì„œ ìœ ì˜ í•„ìš”...",
    "content": "# Biance Smart Chain (BNB)\n\n<aside>\nğŸ’¡ Cheatsheet of BSC/BNB for web3 developers.\n\n</aside>\n\n# ì„¤ëª…\n\n1. ì´ë”ë¦¬ì›€ ë³µì‚¬ë³¸.\n2. BNBì²´ì¸ê³¼ BSCì²´ì¸ì´ ë”°ë¡œ ìˆì–´ì„œ ìœ ì˜ í•„ìš”",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 27,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ë¸”ë¡ì²´ì¸ì˜-ì´í•´",
    "slug": "beulrogceinyi-ihae",
    "path": "blockchain/fundamentals",
    "fullPath": "blockchain/fundamentals/beulrogceinyi-ihae",
    "title": "ë¸”ë¡ì²´ì¸ì˜ ì´í•´",
    "excerpt": "ë¸”ë¡ì²´ì¸ì˜ ì´í•´ _ë³´í—˜ì—°êµ¬ì›ì˜ ì—°êµ¬ë³´ê³ ì„œ '[ê¶Œí˜¸ : 18-24] ë³´í—˜ ì‚°ì—…ì˜ ë¸”ë¡ì²´ì¸ í™œìš©'ì„ ì½ê³  ì‘ì„±í•œ ë‚´ìš©ì…ë‹ˆë‹¤. ê°œì¸ ê³µë¶€ ëª©ì ì´ë¼ ì •ë¦¬ê°€ ë¯¸í¡í•œ ì  ì–‘í•´ ë¶€íƒë“œë¦½ë‹ˆë‹¤._ ë¸”ë¡ì²´ì¸ì˜ ì˜ë¯¸ ë¸”ë¡ì²´ì¸ì´ë€ P2P(Peer to Peer) ë„¤íŠ¸...",
    "content": "# ë¸”ë¡ì²´ì¸ì˜ ì´í•´\n\n> _ë³´í—˜ì—°êµ¬ì›ì˜ ì—°êµ¬ë³´ê³ ì„œ '[ê¶Œí˜¸ : 18-24] ë³´í—˜ ì‚°ì—…ì˜ ë¸”ë¡ì²´ì¸ í™œìš©'ì„ ì½ê³  ì‘ì„±í•œ ë‚´ìš©ì…ë‹ˆë‹¤. ê°œì¸ ê³µë¶€ ëª©ì ì´ë¼ ì •ë¦¬ê°€ ë¯¸í¡í•œ ì  ì–‘í•´ ë¶€íƒë“œë¦½ë‹ˆë‹¤._\n\n## 1. ë¸”ë¡ì²´ì¸ì˜ ì˜ë¯¸\n\në¸”ë¡ì²´ì¸ì´ë€ P2P(Peer to Peer) ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ê´€ë¦¬ë˜ëŠ” ë¶„ì‚° ë°ì´í„°ë² ì´ìŠ¤ì˜ í•œ í˜•íƒœë¡œ, ê±°ë˜ ì •ë³´ë¥¼ ë‹´ì€ ì¥ë¶€ë¥¼ ì¤‘ì•™ì„œë²„ í•œ ê³³ì— ì €ì¥í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë¸”ë¡ì²´ì¸ ë„¤íŠ¸ì›Œí¬ì— ì—°ê²°ëœ ì—¬ëŸ¬ ì»´í“¨í„°ì— ì €ì¥ ë° ë³´ê´€í•˜ëŠ” ê¸°ìˆ ë¡œ ë‹¤ì–‘í•œ ë¶„ì•¼ì— í™œìš©ì´ ê°€ëŠ¥í•œ ê¸°ìˆ ì´ë‹¤.\n\në¸”ë¡ì²´ì¸ì€ ë¶„ì‚°ì›ì¥ ê¸°ìˆ ì´ë¼ê³ ë„ ë¶ˆë¦¬ë©°, ì´ëŠ” ê±°ë˜ ì •ë³´ë¥¼ ê¸°ë¡í•œ ì›ì¥ ë°ì´í„°ë¥¼ ì¤‘ì•™ì„œë²„ê°€ ì•„ë‹Œ ì°¸ê°€ìë“¤ì´ ê³µë™ìœ¼ë¡œ ê¸°ë¡ ë° ê´€ë¦¬í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ë¸”ë¡ì²´ì¸ì€ ë¶„ì‚°ì²˜ë¦¬ì™€ ì•”í˜¸í™” ê¸°ìˆ ì„ ë™ì‹œì— ì ìš©í•˜ë©° ë†’ì€ ë³´ì•ˆì„±ì„ í™•ë³´í•˜ëŠ” í•œí¸ ê±°ë˜ê³¼ì •ì˜ ì‹ ì†ì„±ê³¼ íˆ¬ëª…ì„±ì„ íŠ¹ì§•ìœ¼ë¡œ í•œë‹¤.\n\në³´ì•ˆì„±ì˜ ê°•í™”ë¡œ í•´ì»¤ì˜ ê³µê²©ê³¼ ë°ì´í„°ì˜ ì™œê³¡ ê·¸ë¦¬ê³  ê¸°ì¡´ ì¤‘ì•™ì§‘ì¤‘ ì„œë²„ ë°©ì‹ì—ì„œ ê°€ì¥ í° ë¬¸ì œì¸ ë””ë„ìŠ¤ ê³µê²©ì„ ì›ì²œì ìœ¼ë¡œ ë°©ì–´í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  ë¸”ë¡ì²´ì¸ í”Œë«í¼ì„ ì´ìš©í•˜ë©´ ì œ 3ìì˜ ê±°ë˜ì— ì˜ì¡´í•˜ë˜ ì—¬ëŸ¬ ê³¼ì •ë“¤ì„ ìƒëµí•  ìˆ˜ ìˆì–´, ê·¸ì— ë”°ë¥¸ ë¹„ìš©ì„ íšê¸°ì ìœ¼ë¡œ ì ˆì•½í•  ìˆ˜ ìˆë‹¤. ì œ 3ìê°€ ê±°ë˜ ì¤‘ì‹¬ì˜ ë³´ì¥ ë° ì¦ëª… ì„œë¹„ìŠ¤ì˜ í•­ëª©ë“¤ì„ ë¸”ë¡ì²´ì¸ ì‹œìŠ¤í…œì— ìˆ˜ë ´í•  ìˆ˜ ìˆë‹¤.\n\në³´ì•ˆì„±ì´ ë†’ê³  ìœ„ë³€ì¡°ê°€ ì–´ë µë‹¤ëŠ” íŠ¹ì„± ë•Œë¬¸ì— ë°ì´í„° ì›ë³¸ì˜ ë¬´ê²°ì„±ì´ ìš”êµ¬ë˜ëŠ” ë‹¤ì–‘í•œ ê³µê³µ-ë¯¼ê°„ ì˜ì—­ì— ì ìš©ë˜ê³  ìˆìœ¼ë©°, ìƒˆë¡œìš´ ì‹ ë¢°ì‚¬íšŒ êµ¬í˜„ì˜ ê¸°ë°˜ ê¸°ìˆ ë¡œ ì£¼ëª©ë°›ê³  ìˆëŠ” ì¤‘ì´ë‹¤. ë˜í•œ, ë¸”ë¡ì²´ì¸ ê¸°ìˆ ì€ ê±°ë˜ ì¥ë¶€ì¸ ë°ì´í„°ë¿ ì•„ë‹ˆë¼ ê±°ë˜ ê³„ì•½ë„ ì¤‘ê°„ ì‹ ë¢° ë‹´ë‹¹ìì—†ì´ ê±°ë˜ë¥¼ í•  ìˆ˜ ìˆëŠ”ë° ì´ê²ƒì´ ë°”ë¡œ ì•ì„œ ì–¸ê¸‰í•œ ìŠ¤ë§ˆíŠ¸ê³„ì•½ì´ë‹¤.\n\n## 2. ë¸”ë¡ì²´ì¸ì˜ ì›ë¦¬\n\në¸”ë¡ì²´ì¸ ê¸°ìˆ ì€ ê±°ë˜ì •ë³´ë¥¼ ê¸°ë¡í•œ ì›ì¥ë°ì´í„°ë¥¼ ì¤‘ì•™ ì„œë²„ê°€ ì•„ë‹Œ ë„¤íŠ¸ì›Œí¬ì— ì°¸ê°€í•˜ëŠ” ëª¨ë“  ê³µë™ì²´ê°€ ê±°ë˜ë¥¼ ê¸°ë¡í•˜ê³  ê´€ë¦¬í•˜ëŠ” P2Pê±°ë˜ë¥¼ ì§€í–¥í•˜ëŠ” íƒˆì¤‘ì•™í™”ë¥¼ í•µì‹¬ ê°œë…ìœ¼ë¡œ í•˜ëŠ” ê¸°ìˆ ì´ë‹¤. ê¸°ì¡´ ê¸ˆìœµ ì‹œìŠ¤í…œì—ì„œëŠ” ê¸ˆìœµíšŒì‚¬ë“¤ì´ ì¤‘ì•™ ì„œë²„ì— ê±°ë˜ê¸°ë¡ì„ ë³´ê´€í•´ì˜¨ ë°˜ë©´, P2P ë°©ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë¸”ë¡ì²´ì¸ì—ì„œëŠ” ê±°ë˜ ì •ë³´ë¥¼ ë¸”ë¡ì— ë‹´ì•„ ì°¨ë¡€ëŒ€ë¡œ ì—°ê²°í•˜ê³  ì´ë¥¼ ëª¨ë“  ì°¸ì—¬ìê°€ ê³µìœ í•œë‹¤.\n\n-   ê±°ë˜ê³¼ì •1) Aê°€ Bì—ê²Œ ì†¡ê¸ˆí¬ë§ ë“±ì˜ ê±°ë˜ ìš”ì²­ì„ í•œë‹¤.2) í•»ë‹¹ ê±°ë˜ ì •ë³´ê°€ ë‹´ê¸´ ë¸”ë¡ì´ ìƒì„±ëœë‹¤.3) ë¸”ë¡ì´ ë„¤íŠ¸ì›Œí¬ ìƒì˜ ëª¨ë“  ì°¸ì—¬ìì—ê²Œ ì „ì†¡ë˜ë©´4) ì°¸ì—¬ìë“¤ì€ ê±°ë˜ ì •ë³´ì˜ ìœ íš¨ì„±ì„ ìƒí˜¸ ê²€ì¦í•œë‹¤.5) ì°¸ì—¬ì ê³¼ë°˜ìˆ˜ì˜ ë°ì´í„°ì™€ ì¼ì¹˜í•˜ëŠ” ê±°ë˜ë‚´ì—­ì€ ì •ìƒ ì¥ë¶€ë¡œ í™•ì¸í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê²€ì¦ì´ ì™„ë£Œëœ ë¸”ë¡ì€ ì´ì „ ë¸”ë¡ì— ì—°ê²°ë˜ê³ , ê·¸ ì‚¬ë³¸ì´ ë§Œë“¤ì–´ì ¸ ê° ì‚¬ìš©ìì˜ ì»´í“¨í„°ì— ë¶„ì‚° ì €ì¥ëœë‹¤.6) Aê°€ Bì—ê²Œ ì†¡ê¸ˆí•˜ì—¬ ê±°ë˜ê°€ ì™„ë£Œëœë‹¤.\n\nì´ë ‡ê²Œ ê±°ë˜í•  ë•Œë§ˆë‹¤ ê±°ë˜ ì •ë³´ê°€ ë‹´ê¸´ ë¸”ë¡ì´ ìƒì„±ë˜ì–´ ê³„ì† ì—°ê²°ë˜ë©´ì„œ ëª¨ë“  ì°¸ì—¬ìì˜ ì»´í“¨í„°ì— ë¶„ì‚° ì €ì¥ë˜ëŠ”ë°, ì´ë¥¼ í•´í‚¹í•˜ì—¬ ì„ì˜ë¡œ ìˆ˜ì •í•˜ê±°ë‚˜ ìœ„ì¡° ë˜ëŠ” ë³€ì¡°í•˜ë ¤ë©´ ì „ì²´ ì°¸ì—¬ìì˜ ê³¼ë°˜ìˆ˜ ì´ìƒì˜ ê±°ë˜ ì •ë³´ë¥¼ ë™ì‹œì— ìˆ˜ì •í•˜ì—¬ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì‚¬ì‹¤ìƒ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ë”°ë¼ì„œ ì ‘ê·¼ì„ ì°¨ë‹¨í•¨ìœ¼ë¡œì¨ ê±°ë˜ ì •ë³´ë¥¼ ë³´í˜¸ ë° ê´€ë¦¬í•˜ëŠ” ê¸°ì¡´ì˜ ê¸ˆìœµì‹œìŠ¤í…œê³¼ëŠ” ì „í˜€ ë‹¬ë¦¬, ë¸”ë¡ì²´ì¸ì—ì„œëŠ” ëª¨ë“  ê±°ë˜ ì •ë³´ë¥¼ ëˆ„êµ¬ë‚˜ ì—´ëŒí•  ìˆ˜ ìˆë„ë¡ ê³µê°œí•œ ìƒíƒœì—ì„œ ì€í–‰ ê°™ì€ ê³µì‹ ë ¥ìˆëŠ” ì œ 3ìì˜ ë³´ì¦ ì—†ì´ ë‹¹ì‚¬ì ê°„ì— ì•ˆì „í•˜ê²Œ ê±°ë˜ê°€ ì´ë£¨ì–´ ì§„ë‹¤.\n\n## 3. ë¸”ë¡ì²´ì¸ì˜ ê¸°ìˆ ì  ê°œë…\n\n### ê°€. í•´ì‹œí•¨ìˆ˜\n\në¸”ë¡ì²´ì¸, ì•”í˜¸í™”í ê¸°ìˆ ì˜ ë‚´ìš©ì— ë§¤ë²ˆ ë“±ì¥í•˜ëŠ” ê²ƒ ì¤‘ í•˜ë‚˜ê°€ í•´ì‹œí•¨ìˆ˜ì´ë‹¤. í•´ì‹œí•¨ìˆ˜ì˜ í•´ì‹œëŠ” \"ì–´ë–¤ ë°ì´í„°ë¥¼ ê³ ì •ëœ ê¸¸ì´ì˜ ë°ì´í„°ë¡œ ë³€í™˜\"í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. í•´ì‹œí•¨ìˆ˜ë¥¼ ê±°ì¹˜ë©´ ì›ë³¸ ë°ì´í„°ë¥¼ ì•Œì•„ë³¼ ìˆ˜ ì—†ë„ë¡ íŠ¹ìˆ˜í•œ ë¬¸ìì—´ë¡œ ë³€í™˜ì´ ë˜ëŠ”ë°, í•´ì‹œí•¨ìˆ˜ëŠ” ì••ì¶•ì´ ì•„ë‹ˆë¼ ë‹¨ë°©í–¥ ë³€í™˜ì´ê¸° ë•Œë¬¸ì— í•´ì‹œê°’ì„ ì´ìš©í•´ì„œ ì›ë³¸ë°ì´í„°ë¥¼ ë³µì›í•  ìˆ˜ ì—†ë‹¤ëŠ” íŠ¹ì§•ì„ ê°€ì§€ê³  ìˆë‹¤.\n\n### 1) í•´ì‹œí•¨ìˆ˜ì˜ ìœ ìš©ì„±\n\ní•´ì‹œí•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì„±ê²©ì´ ìˆê¸° ë•Œë¬¸ì— ë³´ì•ˆì—ì„œ ìœ ìš©í•˜ê²Œ ì“°ì¸ë‹¤. ì›ë³¸ë°ì´í„°ì— ì•„ì£¼ ì‘ì€ ë³€í™”ë§Œ ìˆì–´ë„ ì™„ì „ì´ ë‹¤ë¥¸ í•´ì‹œê°’ì´ ë§Œë“¤ì–´ì§€ê²Œ ëœë‹¤.ì¦‰ í•´ì‹œí•¨ìˆ˜ë¥¼ ì´ìš©í•˜ê²Œ ë˜ë©´, ì›ë³¸ë°ì´í„°ì˜ ì‚¬ì†Œí•œ ë³€í™”ë„ ì‰½ê²Œ í™•ì¸í•  ìˆ˜ ìˆê²Œ ëœë‹¤. ë˜í•œ í•´ì‹œí•¨ìˆ˜ëŠ” ëˆˆì‚¬íƒœ íš¨ê³¼ë•Œë¬¸ì— ì „ìì„œëª…, ì¦ëª…ì„œ ë“±ì—ì„œ í•´ì‹œê°’ì„ ë§ì´ í™œìš©í•˜ê³  ìˆë‹¤. ë³¸ë¬¸ì— ì•½ê°„ì˜ ìˆ˜ì •ë§Œ ê°€í•´ì ¸ë„ í•´ì‹œê°’ì´ ì™„ì „íˆ ë‹¬ë¼ì ¸ ìœ„ë³€ì¡° íŒë³„ì´ ìš©ì´í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë¸”ë¡ì²´ì¸ì˜ í•´ì‹œí•¨ìˆ˜ê°€ ì–‘ë°©í–¥ ë³€í™˜ì´ ê°€ëŠ¥í–ˆë”ë¼ë©´ ì•”í˜¸í™”ì— ì“°ì¼ ìˆ˜ê°€ ì—†ì—ˆì„ ê²ƒì´ë‹¤.\n\ní•˜ì§€ë§Œ í•´ì‹œí•¨ìˆ˜ëŠ” ë‹¨ë°©í–¥ ë³€í™˜ì´ë©°, ë³µì›ì´ ë¶ˆê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ë¸”ë¡ì²´ì¸ ê¸°ìˆ  ë° ì „ìì„œëª… ë“± ì•”í˜¸í™”ì— ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤. ë¸”ë¡ì²´ì¸ì—ì„œëŠ” ì´ í•´ì‹œê°’ì„ ì´ìš©í•´ í•´ë‹¹ ë¸”ë¡ì— ì„œëª…í•˜ê³  ì´ì „ ë¸”ë¡ì˜ í•´ì‹œê°’ì„ ë‹¤ìŒ ë¸”ë¡ì— ê¸°ë¡í•¨ìœ¼ë¡œì¨ ì²´ì¸ í˜•íƒœì˜ ì—°ê²° ë¦¬ìŠ¤íŠ¸ë¥¼ í˜•ì„±í•˜ê²Œ ëœë‹¤. ë”°ë¼ì„œ íŠ¹ì • ë¸”ë¡ì„ í•´í‚¹í•˜ë ¤ë©´ ê·¸ ë¸”ë¡ì— ì—°ê²°ëœ ë‹¤ë¥¸ ë¸”ë¡ë“¤ë„ ìˆ˜ì •ì„ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ë°ì´í„°ì˜ ìœ„ë³€ì¡°ê°€ ì•„ì£¼ ì–´ë ¤ìš´ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆë‹¤.\n\n### 2) í•´ì‹œí•¨ìˆ˜ì˜ íŠ¹ì„±\n\n(1) ì–´ë–¤ ê¸¸ì´ì˜ ë°ì´í„°ë„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤.\n\n(2) ê²°ê³¼ëŠ” ì •í•´ì§„ ê¸¸ì´ë¡œ ë‚˜ì˜¨ë‹¤.\n\n(3) ê³„ì‚° ì‹œê°„ì´ í•©ë¦¬ì ìœ¼ë¡œ ì¶”ì • ê°€ëŠ¥í•´ì•¼ í•œë‹¤.\n\n(4) ê²°ê³¼ ê°’ì´ ì¤‘ë³µë  ê°€ëŠ¥ì„±ì´ ê±°ì˜ ì—†ë‹¤.\n\n(5) \\*\\*\\*\\*ì…ë ¥ ê°’ì„ ì•Œ ìˆ˜ ì—†ë‹¤.\n\n(6) ê²°ê³¼ ê°’ì„ ì•Œë ¤ì£¼ê³  ì…ë ¥ ê°’ì„ ì°¾ì„ ìˆ˜ ìˆëŠ” íŠ¹ë³„í•œ ê³µì‹ì´ ì—†ë‹¤.\n\n![https://velog.velcdn.com/images%2Fasap0208%2Fpost%2F0a7c3c00-b692-429b-be26-d1e25b481912%2Fimage.png](https://velog.velcdn.com/images%2Fasap0208%2Fpost%2F0a7c3c00-b692-429b-be26-d1e25b481912%2Fimage.png)\n\n### 3) í•´ì‹œí•¨ìˆ˜ì— ê´€í•œ ì¶”ê°€ ì„¤ëª…\n\në¸”ë¡ì²´ì¸ì„ í™œìš©í•œ ì•”í˜¸í™”íì—ì„œ ì‚¬ìš©ë˜ëŠ” ì•”í˜¸ê¸°ìˆ ì€ í•´ì‹œí•¨ìˆ˜, ì „ìì„œëª…, ê³µê°œí‚¤ ì•”í˜¸í™” ì•Œê³ ë¦¬ì¦˜ì´ë¼ ë§í•  ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ í•´ì‹œí•¨ìˆ˜ëŠ” ì„ì˜ ë°ì´í„°ë¥¼ íŠ¹ì • ê¸¸ì´ì˜ ë¬¸ì, ìˆ«ìë¡œ ì¡°í•©ëœ í•´ì‹œê°’ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì•”í˜¸ ì•Œê³ ë¦¬ì¦˜ì˜ ì¼ì¢…ì´ë‹¤. í•´ì‹œí•¨ìˆ˜ì—ì„œ ì‚°ì¶œë˜ëŠ” í•´ì‹œê°’ì€ ì§€ë¬¸ì´ë¼ê³ ë„ í•˜ëŠ”ë°, ì•”í˜¸í™”íì—ì„œ í•´ì‹œê°’ ë¹„êµë¥¼ í†µí•˜ì—¬ ì›ë³¸ì˜ ìœ„ë³€ì¡° ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ë¬´ê²°ì„± ê²€ì¦ì— ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤.\n\në¸”ë¡ì²´ì¸ì—ì„œ í•´ì‹œí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” 3ê°€ì§€ ëª©ì ì„ ì‚´í´ë³´ë©´ ì²«ì§¸, ê³µê°œí‚¤ì˜ í•´ì‹œê°’ì„ ì§€ê°‘ì£¼ì†Œë¡œ í™œìš©í•˜ì—¬ ìµëª…í™”ëœ ê±°ë˜ë¥¼ ìˆ˜í–‰í•˜ê³ , ê°€ìƒí™”íì˜ ì „ìì§€ê°‘ ì£¼ì†ŒëŠ” ê³µê°œí‚¤ ê¸°ë°˜ ì•”í˜¸í™” ì•Œê³ ë¦¬ì¦˜ì—ì„œ ìƒì„±ëœ ê³µê°œí‚¤ì˜ í•´ì‹œê°’ì„ ì‚¬ìš©í•œë‹¤. ê°œì¸ì •ë³´(ì •í™•íˆëŠ” ì†¡ì‹ ìì˜ ê³„ì¢Œì •ë³´) ì—†ì´ ìµëª…í™”ëœ ê±°ë˜ë¥¼ í†µí•´ ì†¡ê¸ˆìì˜ ì‹ ì›ì„ ê°ì¶”ê³ , ì†¡ê¸ˆí•  ìˆ˜ ìˆë‹¤.\n\në‘˜ì§¸, í•´ì‹œí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ 2ê°€ì§€ì˜ ë¬´ê²°ì„± ê²€ì¦ì— ì‚¬ìš©í•˜ê²Œ ëœë‹¤. ì²´ì¸ìœ¼ë¡œ ì—°ê²°ëœ ë¸”ë¡í—¤ë”ì˜ í•´ì‹œê°’ì„ í™œìš©í•˜ì—¬ í•´ì‹œê°’ ì²´ì¸ìœ¼ë¡œ ì—°ê²°ëœ ë¸”ë¡ì˜ ë¬´ê²°ì„± ê²€ì¦ì— ì‚¬ìš©ëœë‹¤. ë˜ ë‹¤ë¥¸ ë¬´ê²°ì„± ê²€ì¦ì€ ê° ë¸”ë¡ì˜ ì „ì²´ê±°ë˜ë¥¼ í•˜ë‚˜ì˜ í•´ì‹œê°’(ë¨¸í´ë£¨íŠ¸)ìœ¼ë¡œ ì €ì¥í•˜ê³ , í•„ìš”í•  ê²½ìš°ì—ëŠ” ì–¸ì œë“ , í•´ë‹¹ ë¸”ë¡ì˜ ë¨¸í´ë£¨íŠ¸ ê°’ìœ¼ë¡œ ë¸”ë¡ ë‚´ì— í¬í•¨ëœ ê°œë³„ê±°ë˜ì˜ ìœ„ë³€ì¡° ì—¬ë¶€ë¥¼ ê²€ì¦í•  ìˆ˜ ìˆë‹¤. ëª¨ë“  ê±°ë˜ ë°ì´í„°ì˜ í•´ì‹œê°’ì„ ë¨¸í´ íŠ¸ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ ë§Œë“¤ì–´ì§€ëŠ” ë¨¸í´ ë£¨íŠ¸ì— ì €ì¥í•˜ê³  í–¥í›„ ê±°ë˜ë‚´ì—­ì˜ ìœ„ë³€ì¡° ì—¬ë¶€ë¥¼ ê²€ì¦í•  ë•Œ, ì›ë³¸ í•´ì‹œê°’ê³¼ ë¹„êµë¥¼ í†µí•˜ì—¬, ê° ê±°ë˜ì˜ ë¬´ê²°ì„±ì„ ê²€ì¦í•  ìˆ˜ ìˆë‹¤.ë˜í•œ ë¨¸í´ ë£¨íŠ¸ëŠ” 1MBë¡œ í¬ê¸°ê°€ ì œí•œë˜ì–´ ìˆëŠ” ë¹„íŠ¸ì½”ì¸ì˜ ê° ë¸”ë¡ì˜ í¬ê¸°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•œë‹¤. ì „ì²´ ê±°ë˜ë‚´ì—­ì„ ë‹¤ ì €ì¥í•  í•„ìš” ì—†ì´, ë¨¸í´ë£¨íŠ¸ë¼ëŠ” í•œ ê°œì˜ í•´ì‹œê°’ë§Œ ì €ì¥í•˜ë©´, í•´ë‹¹ ë¸”ë¸”ë¡ ë‚´ì˜ ëª¨ë“  ê±°ë˜ë‚´ì—­ì˜ ì§„ìœ„ë¥¼ í•„ìš”í•  ë•Œ ë¹„êµí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.\n\në§ˆì§€ë§‰ìœ¼ë¡œ í•©ì˜ ì•Œê³ ë¦¬ì¦˜ì—ì„œ PoWë°©ì‹ì„ ì‚¬ìš©í•  ê²½ìš°, í•´ì‹œê°’ì„ í™œìš©í•œ ì±„êµ´ë¬¸ì œì— í™œìš©í•œë‹¤. í•´ì‹œê°’ì„ í™œìš©í•œ ì±„êµ´ë¬¸ì œì— ë¨¼ì € ë§ì¶”ëŠ” ì±„êµ´ìì—ê²Œ ì±„êµ´ê¶Œí•œê³¼ ë³´ìƒì„ ì œê³µí•œë‹¤. í•´ì‹œìºì‹œ ë¬¸ì œí’€ì´ë¥¼ í†µí•œ ì‘ì—…ì¦ëª…ì€ ì±„êµ´ì´ë¼ê³ ë„ í•˜ëŠ”ë°, ì±„êµ´ìì— ëŒ€í•œ ë³´ìƒì„ í†µí•´, ì±„êµ´ì„ ê²½ì¥í•´ê³ , ì±„êµ´ìê°€ ììœ¨ì ìœ¼ë¡œ ìƒˆë¡œìš´ ë¸”ë¡ì„ ìƒì„±í•˜ë„ë¡ ìœ ë„í•  ìˆ˜ ìˆëŠ” ì›ë¦¬ë¥¼ ê°€ì§€ê³  ìˆë‹¤.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 5,
    "wordCount": 836,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ë¡¤ì—…ì´ë€",
    "slug": "roleobiran",
    "path": "blockchain/fundamentals",
    "fullPath": "blockchain/fundamentals/roleobiran",
    "title": "ë¡¤ì—…ì´ë€?",
    "excerpt": "ë¡¤ì—…ì´ë€? ë¡¤ì—…ì€ L2ì—ì„œ íŠ¸ëœì­ì…˜ì„ ì‹¤í–‰í•œ ë’¤, ì‹¤í–‰í•œ íŠ¸ëœì­ì…˜ ë°ì´í„°ë“¤ê³¼ ë³€ê²½ëœ ìƒíƒœì˜ ìš”ì•½ë³¸ì„ L1ì— ì €ì¥í•˜ëŠ” ì†”ë£¨ì…˜ ì¼ë°˜ì ìœ¼ë¡œ L1ì— ë°°íŒ¨ëœ ë¡¤ì—… ì»¨íŠ¸ë™íŠ¸ì— ìŠ¤í…Œì´íŠ¸ ë£¨íŠ¸(ìƒíƒœìš”ì•½ë³¸)ê°€ ì €ì¥ë˜ì–´ ìˆìŒ ë¡¤ì—…ì˜ ê²½ìš° L1ì˜ ìƒíƒœ ë³€ê²½ì‹œ TXì‹¤í–‰ ê²°ê³¼...",
    "content": "# ë¡¤ì—…ì´ë€?\n\n- ë¡¤ì—…ì€ L2ì—ì„œ íŠ¸ëœì­ì…˜ì„ ì‹¤í–‰í•œ ë’¤, ì‹¤í–‰í•œ íŠ¸ëœì­ì…˜ ë°ì´í„°ë“¤ê³¼ ë³€ê²½ëœ ìƒíƒœì˜ ìš”ì•½ë³¸ì„ L1ì— ì €ì¥í•˜ëŠ” ì†”ë£¨ì…˜\n- ì¼ë°˜ì ìœ¼ë¡œ L1ì— ë°°íŒ¨ëœ ë¡¤ì—… ì»¨íŠ¸ë™íŠ¸ì— ìŠ¤í…Œì´íŠ¸ ë£¨íŠ¸(ìƒíƒœìš”ì•½ë³¸)ê°€ ì €ì¥ë˜ì–´ ìˆìŒ\n- ë¡¤ì—…ì˜ ê²½ìš° L1ì˜ ìƒíƒœ ë³€ê²½ì‹œ TXì‹¤í–‰ ê²°ê³¼ ë¿ë§Œ ì•„ë‹ˆë¼ ë°ì´í„° ë˜í•œ ë³´ê´€í•˜ëŠ” ë°©ì‹ì„ í†µí•´ì„œ ëˆ„êµ¬ë“  ë°ì´í„° ìœ íš¨ì„±ì„ ê²€ì¦í•  ìˆ˜ ìˆê²Œ í–ˆìŒ\n<img src=\"https://cdn.coindeskkorea.com//news/photo/202207/80513_23505_1657.png\" />\n\n\n\nhttps://www.coindeskkorea.com/news/articleView.html?idxno=80513",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 54,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ë‹¨ìˆœ-í•´ì‹œ-ê²€ì¦-ì˜-í•œê³„ì™€-ë°ì´í„°-ê°€ìš©ì„±-ë¬¸ì œ",
    "slug": "dansun-haesi-geomjeung-yi-hangyewa-deiteo-gayongseong-munje",
    "path": "blockchain/fundamentals",
    "fullPath": "blockchain/fundamentals/dansun-haesi-geomjeung-yi-hangyewa-deiteo-gayongseong-munje",
    "title": "\"ë‹¨ìˆœ í•´ì‹œ ê²€ì¦\"ì˜ í•œê³„ì™€ ë°ì´í„° ê°€ìš©ì„± ë¬¸ì œ",
    "excerpt": "\"ë‹¨ìˆœ í•´ì‹œ ê²€ì¦\"ì˜ í•œê³„ì™€ ë°ì´í„° ê°€ìš©ì„± ë¬¸ì œ ì´ê²ƒì€ ë§¤ìš° í•µì‹¬ì ì¸ ì§ˆë¬¸ì…ë‹ˆë‹¤! ë‹¨ìˆœíˆ ODSì˜ í•´ì‹œë§Œ ê²€ì¦í•˜ëŠ” ë°©ì‹ì´ ì™œ ë¶€ì¡±í•œì§€, Celestiaì˜ ë°ì´í„° ê°€ìš©ì„± ìƒ˜í”Œë§ì´ ì™œ í•„ìš”í•œì§€ ëª…í™•íˆ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë‹¨ìˆœ í•´ì‹œ ê²€ì¦ì˜ ê·¼ë³¸ì  ë¬¸ì œ 1...",
    "content": "# \"ë‹¨ìˆœ í•´ì‹œ ê²€ì¦\"ì˜ í•œê³„ì™€ ë°ì´í„° ê°€ìš©ì„± ë¬¸ì œ\n\nì´ê²ƒì€ ë§¤ìš° í•µì‹¬ì ì¸ ì§ˆë¬¸ì…ë‹ˆë‹¤! ë‹¨ìˆœíˆ ODSì˜ í•´ì‹œë§Œ ê²€ì¦í•˜ëŠ” ë°©ì‹ì´ ì™œ ë¶€ì¡±í•œì§€, Celestiaì˜ ë°ì´í„° ê°€ìš©ì„± ìƒ˜í”Œë§ì´ ì™œ í•„ìš”í•œì§€ ëª…í™•íˆ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\n## ë‹¨ìˆœ í•´ì‹œ ê²€ì¦ì˜ ê·¼ë³¸ì  ë¬¸ì œ\n\n### 1. \"ë°ì´í„° ê°€ìš©ì„± ë¬¸ì œ\"ì˜ ë³¸ì§ˆ\në§Œì•½ ìš°ë¦¬ê°€ í•´ì‹œë§Œ ê²€ì¦í•œë‹¤ë©´:\n\n- ë¸”ë¡ ìƒì„±ìëŠ” ë°ì´í„°ì˜ í•´ì‹œë§Œ ì œì¶œí•˜ê³  **ì‹¤ì œ ë°ì´í„°ë¥¼ ê³µê°œí•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤**.\n- ê²€ì¦ìë“¤ì€ í•´ì‹œê°€ ë§ë‹¤ëŠ” ê²ƒì€ í™•ì¸í•  ìˆ˜ ìˆì§€ë§Œ, **ë°ì´í„°ê°€ ì‹¤ì œë¡œ ë„¤íŠ¸ì›Œí¬ì— ê²Œì‹œë˜ì—ˆëŠ”ì§€ëŠ” í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤**.\n\nì´ê²ƒì€ \"**ë°ì´í„° ê°€ìš©ì„± ë¬¸ì œ**\"ë¼ê³  ë¶€ë¥´ë©°, íŠ¹íˆ ë¡¤ì—…ì´ë‚˜ í™•ì¥ì„± ì†”ë£¨ì…˜ì—ì„œ ì‹¬ê°í•œ ë³´ì•ˆ ìœ„í—˜ì„ ì´ˆë˜í•©ë‹ˆë‹¤.\n\n### 2. \"ë¬´ë°ì´í„° ê³µê²©\" (Data Withholding Attack)\nì´ë¡œ ì¸í•´ ê°€ëŠ¥í•œ ê³µê²© ì‹œë‚˜ë¦¬ì˜¤:\n\n1. ì•…ì˜ì ì¸ ë¸”ë¡ ìƒì„±ìê°€ ìœ íš¨í•œ ë°ì´í„°ì— ëŒ€í•œ í•´ì‹œë§Œ í¬í•¨í•œ ë¸”ë¡ í—¤ë”ë¥¼ ì œì¶œ\n2. ê·¸ëŸ¬ë‚˜ ì‹¤ì œ ë°ì´í„°ëŠ” ë„¤íŠ¸ì›Œí¬ì— ê³µê°œí•˜ì§€ ì•ŠìŒ\n3. ë„¤íŠ¸ì›Œí¬ëŠ” í•´ì‹œë§Œ ë³´ê³  ë¸”ë¡ì´ ìœ íš¨í•˜ë‹¤ê³  íŒë‹¨\n4. ê·¸ëŸ¬ë‚˜ ì‹¤ì œ ë°ì´í„°ê°€ ì—†ì–´ íŠ¸ëœì­ì…˜ ê²€ì¦ì´ë‚˜ ë¡¤ì—… ìƒíƒœ ì—…ë°ì´íŠ¸ ë¶ˆê°€ëŠ¥\n5. ë¸”ë¡ì²´ì¸ì´ ë©ˆì¶”ê±°ë‚˜ ì‹¬ê°í•œ ì†ìƒ ë°œìƒ\n\n## ë°ì´í„° ê°€ìš©ì„± ìƒ˜í”Œë§(DAS)ì˜ í•„ìš”ì„±\n\n### 1. ìƒ˜í”Œë§ì„ í†µí•œ í™•ë¥ ì  ê²€ì¦\nCelestiaì˜ DASëŠ” ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´:\n\n- ë…¸ë“œë“¤ì´ ë¸”ë¡ ë°ì´í„°ì˜ **ë¬´ì‘ìœ„ ìƒ˜í”Œì„ ì§ì ‘ ìš”ì²­**í•´ì„œ í™•ì¸\n- ë§Œì•½ ëª¨ë“  ìƒ˜í”Œì´ ì œê³µë  ìˆ˜ ìˆë‹¤ë©´, ë†’ì€ í™•ë¥ ë¡œ ì „ì²´ ë°ì´í„°ê°€ ê°€ìš©í•¨ì„ ì˜ë¯¸\n- ë‹¨ í•˜ë‚˜ì˜ ìƒ˜í”Œì´ë¼ë„ ì œê³µë˜ì§€ ì•Šìœ¼ë©´, ë°ì´í„° ê°€ìš©ì„± ë¬¸ì œ ê°ì§€\n\n### 2. Reed-Solomon ì¸ì½”ë”©ì˜ ì—­í• \nEDS êµ¬ì¡°ì™€ Reed-Solomon ì¸ì½”ë”©ì€:\n\n- ë°ì´í„° ì¼ë¶€ê°€ ì†ì‹¤ë˜ë”ë¼ë„ ì „ì²´ ë³µêµ¬ ê°€ëŠ¥ì„± ì œê³µ\n- ë™ì‹œì— ë¬´ì‘ìœ„ ìƒ˜í”Œë§ì˜ íš¨ê³¼ë¥¼ ê·¹ëŒ€í™” (ë°ì´í„° ëˆ„ë½ íƒì§€ í™•ë¥  í–¥ìƒ)\n\n## ë‹¤ë¥¸ ë¸”ë¡ì²´ì¸ê³¼ì˜ ì°¨ì´ì \n\n### 1. ì¼ë°˜ ë¸”ë¡ì²´ì¸ì˜ ì ‘ê·¼ë²•\nBitcoinì´ë‚˜ Ethereum ê°™ì€ ì „í†µì ì¸ ë¸”ë¡ì²´ì¸ì—ì„œëŠ”:\n\n- **ëª¨ë“  í’€ë…¸ë“œê°€ ëª¨ë“  ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ê²€ì¦**\n- ë”°ë¼ì„œ ë°ì´í„° ê°€ìš©ì„± ë¬¸ì œê°€ ëœ ì‹¬ê°í•¨ (í’€ë…¸ë“œëŠ” ëª¨ë“  ë°ì´í„°ë¥¼ ì§ì ‘ í™•ì¸)\n\n### 2. Celestiaì˜ í˜ì‹ ì  ì ‘ê·¼ë²•\nCelestiaëŠ” í™•ì¥ì„±ì„ ìœ„í•´:\n\n- ëª¨ë“  ë…¸ë“œê°€ ëª¨ë“  ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•  í•„ìš” ì—†ìŒ\n- ëŒ€ì‹  ìƒ˜í”Œë§ì„ í†µí•´ ë°ì´í„° ê°€ìš©ì„±ì„ í™•ë¥ ì ìœ¼ë¡œ ê²€ì¦\n- ì´ëŠ” ë¼ì´íŠ¸ ë…¸ë“œì—ê²Œë„ ê°•ë ¥í•œ ë³´ì•ˆ ë³´ì¥ ì œê³µ\n\n## ê²°ë¡ : ì™œ ë‹¨ìˆœ í•´ì‹œ ê²€ì¦ìœ¼ë¡œëŠ” ë¶€ì¡±í•œê°€\n\në‹¨ìˆœ í•´ì‹œ ê²€ì¦ì€:\n\n1. **ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ”ì§€** ê²€ì¦í•˜ì§€ ëª»í•¨ (ë‹¨ì§€ í•´ì‹œê°€ ë§ë‹¤ëŠ” ê²ƒë§Œ ê²€ì¦)\n2. **ë°ì´í„° ê°€ìš©ì„± ê³µê²©**ì— ì·¨ì•½í•¨\n3. **í™•ì¥ì„± ì†”ë£¨ì…˜**ì—ì„œ ì¹˜ëª…ì ì¸ ë³´ì•ˆ ìœ„í—˜ ì´ˆë˜\n\nCelestiaì˜ DASëŠ” ì´ ê·¼ë³¸ì ì¸ ë¬¸ì œë¥¼ í•´ê²°í•˜ì—¬, í™•ì¥ì„±ê³¼ ë³´ì•ˆì„±ì„ ë™ì‹œì— ë‹¬ì„±í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. ìƒ˜í”Œë§ì„ í†µí•´ ì ì€ ì–‘ì˜ ë°ì´í„°ë§Œìœ¼ë¡œë„ ë†’ì€ í™•ë¥ ë¡œ ë°ì´í„° ê°€ìš©ì„±ì„ ê²€ì¦í•˜ê³ , ì´ë¥¼ í†µí•´ \"ë¬´ë°ì´í„° ê³µê²©\"ì„ ë°©ì§€í•©ë‹ˆë‹¤.\n\nì´ê²ƒì´ Celestiaê°€ ODS/ODSQ4 ì €ì¥ ì „ëµê³¼ ë°ì´í„° ê°€ìš©ì„± ìƒ˜í”Œë§ì„ ì‚¬ìš©í•˜ëŠ” ê·¼ë³¸ì ì¸ ì´ìœ ì…ë‹ˆë‹¤.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 2,
    "wordCount": 352,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "checksum",
    "slug": "checksum",
    "path": "blockchain/fundamentals",
    "fullPath": "blockchain/fundamentals/checksum",
    "title": "Checksum",
    "excerpt": "Checksum What is checksum ? A checksum is small-sized block of data derived from block of digital data for the purpose of deecting errors tah m...",
    "content": "# Checksum\n\n## What is checksum ?\n\nA checksum is small-sized block of data derived from block of digital data for the purpose of deecting errors tah may have been introduced during its transmission or storage. By themselves, checksums are often used to verify data integrity but are not relied upon to verify data authenticity.\n\n> The procedure which generates this checksum is called a checksum function or checksum algorithm. - Wikipedia\n\n![https://velog.velcdn.com/images%2Fasap0208%2Fpost%2F9c511980-dada-46e7-b0cc-f97143bcc6da%2Fimage.png](https://velog.velcdn.com/images%2Fasap0208%2Fpost%2F9c511980-dada-46e7-b0cc-f97143bcc6da%2Fimage.png)\n\n## Example\n\n0x25 = 370x62 = 980x3F = 630x52 = 82\n\n1.  0x25 + 0x62 + 0x3F + 0x52 = 0x118(280)-> 0001 0001 10002) drop carry nibble-> 0001 10003) 2's complements-> 1110 0111(1's complements) + 1-> 1110 10004) to Hex-> 0xE8 (=Checksum byte)\n\n### Test\n\n1.  0x118(origin) + 0xE8(checksum) = 0x200(5122) to binary-> 0010 0000 00003) drop carry nibble-> 0000 0000",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 135,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "how-base-gas-works",
    "slug": "how-base-gas-works",
    "path": "blockchain/ethereum",
    "fullPath": "blockchain/ethereum/how-base-gas-works",
    "title": "How Base Gas Works",
    "excerpt": "How Base Gas Works Aptos transactions by default charge a base gas fee, regardless of market conditions. For each transaction, this \"base gas\" amou...",
    "content": "# How Base Gas Works\n\nAptos transactions by default charge a base gas fee, regardless of market conditions.\nFor each transaction, this \"base gas\" amount is based on three conditions:\n\n1. Instructions.\n2. Storage.\n3. Payload.\n\nThe more function calls, branching conditional statements, etc. that a transaction requires, the more instruction gas it will cost.\nLikewise, the more reads from and writes into global storage that a transaction requires, the more storage gas it will cost.\nFinally, the more bytes in a transaction payload, the more it will cost.\n\nAs explained in the [optimization principles](#optimization-principles) section, storage gas has by far the largest effect on base gas. For background on the Aptos gas model, see [The Making of the Aptos Gas Schedule](https://aptoslabs.medium.com/the-making-of-the-aptos-gas-schedule-508d5686a350).\n\n## Instruction gas\n\nBasic instruction gas parameters are defined at [`instr.rs`] and include the following instruction types:\n\n### No-operation\n\n| Parameter | Meaning        |\n| --------- | -------------- |\n| `nop`     | A no-operation |\n\n### Control flow\n\n| Parameter  | Meaning                          |\n| ---------- | -------------------------------- |\n| `ret`      | Return                           |\n| `abort`    | Abort                            |\n| `br_true`  | Execute conditional true branch  |\n| `br_false` | Execute conditional false branch |\n| `branch`   | Branch                           |\n\n### Stack\n\n| Parameter           | Meaning                          |\n| ------------------- | -------------------------------- |\n| `pop`               | Pop from stack                   |\n| `ld_u8`             | Load a `u8`                      |\n| `ld_u64`            | Load a `u64`                     |\n| `ld_u128`           | Load a `u128`                    |\n| `ld_true`           | Load a `true`                    |\n| `ld_false`          | Load a `false`                   |\n| `ld_const_base`     | Base cost to load a constant     |\n| `ld_const_per_byte` | Per-byte cost to load a constant |\n\n### Local scope\n\n| Parameter                   | Meaning                  |\n| --------------------------- | ------------------------ |\n| `imm_borrow_loc`            | Immutably borrow         |\n| `mut_borrow_loc`            | Mutably borrow           |\n| `imm_borrow_field`          | Immutably borrow a field |\n| `mut_borrow_field`          | Mutably borrow a field   |\n| `imm_borrow_field_generic`  |                          |\n| `mut_borrow_field_generic`  |                          |\n| `copy_loc_base`             | Base cost to copy        |\n| `copy_loc_per_abs_val_unit` |                          |\n| `move_loc_base`             | Move                     |\n| `st_loc_base`               |                          |\n\n### Calling\n\n| Parameter                 | Meaning                       |\n| ------------------------- | ----------------------------- |\n| `call_base`               | Base cost for a function call |\n| `call_per_arg`            | Cost per function argument    |\n| `call_generic_base`       |                               |\n| `call_generic_per_ty_arg` | Cost per type argument        |\n| `call_generic_per_arg`    |                               |\n\n### Structs\n\n| Parameter                  | Meaning                              |\n| -------------------------- | ------------------------------------ |\n| `pack_base`                | Base cost to pack a `struct`         |\n| `pack_per_field`           | Cost to pack a `struct`, per field   |\n| `pack_generic_base`        |                                      |\n| `pack_generic_per_field`   |                                      |\n| `unpack_base`              | Base cost to unpack a `struct`       |\n| `unpack_per_field`         | Cost to unpack a `struct`, per field |\n| `unpack_generic_base`      |                                      |\n| `unpack_generic_per_field` |                                      |\n\n### References\n\n| Parameter                   | Meaning                            |\n| --------------------------- | ---------------------------------- |\n| `read_ref_base`             | Base cost to read from a reference |\n| `read_ref_per_abs_val_unit` |                                    |\n| `write_ref_base`            | Base cost to write to a reference  |\n| `freeze_ref`                | Freeze a reference                 |\n\n### Casting\n\n| Parameter   | Meaning          |\n| ----------- | ---------------- |\n| `cast_u8`   | Cast to a `u8`   |\n| `cast_u64`  | Cast to a `u64`  |\n| `cast_u128` | Cast to a `u128` |\n\n### Arithmetic\n\n| Parameter | Meaning  |\n| --------- | -------- |\n| `add`     | Add      |\n| `sub`     | Subtract |\n| `mul`     | Multiply |\n| `mod_`    | Modulo   |\n| `div`     | Divide   |\n\n### Bitwise\n\n| Parameter | Meaning                   |\n| --------- | ------------------------- |\n| `bit_or`  | `OR`: <code>&#124;</code> |\n| `bit_and` | `AND`: `&`                |\n| `xor`     | `XOR`: `^`                |\n| `shl`     | Shift left: `<<`          |\n| `shr`     | Shift right: `>>`         |\n\n### Boolean\n\n| Parameter | Meaning                         |\n| --------- | ------------------------------- |\n| `or`      | `OR`: <code>&#124;&#124;</code> |\n| `and`     | `AND`: `&&`                     |\n| `not`     | `NOT`: `!`                      |\n\n### Comparison\n\n| Parameter              | Meaning                        |\n| ---------------------- | ------------------------------ |\n| `lt`                   | Less than: `<`                 |\n| `gt`                   | Greater than: `>`              |\n| `le`                   | Less than or equal to: `<=`    |\n| `ge`                   | Greater than or equal to: `>=` |\n| `eq_base`              | Base equality cost: `==`       |\n| `eq_per_abs_val_unit`  |                                |\n| `neq_base`             | Base not equal cost: `!=`      |\n| `neq_per_abs_val_unit` |                                |\n\n### Global storage\n\n| Parameter                        | Meaning                                               |\n| -------------------------------- | ----------------------------------------------------- |\n| `imm_borrow_global_base`         | Base cost to immutably borrow: `borrow_global<T>()`   |\n| `imm_borrow_global_generic_base` |                                                       |\n| `mut_borrow_global_base`         | Base cost to mutably borrow: `borrow_global_mut<T>()` |\n| `mut_borrow_global_generic_base` |                                                       |\n| `exists_base`                    | Base cost to check existence: `exists<T>()`           |\n| `exists_generic_base`            |                                                       |\n| `move_from_base`                 | Base cost to move from: `move_from<T>()`              |\n| `move_from_generic_base`         |                                                       |\n| `move_to_base`                   | Base cost to move to: `move_to<T>()`                  |\n| `move_to_generic_base`           |                                                       |\n\n### Vectors\n\n| Parameter                      | Meaning                                  |\n| ------------------------------ | ---------------------------------------- |\n| `vec_len_base`                 | Length of a vector                       |\n| `vec_imm_borrow_base`          | Immutably borrow an element              |\n| `vec_mut_borrow_base`          | Mutably borrow an element                |\n| `vec_push_back_base`           | Push back                                |\n| `vec_pop_back_base`            | Pop from the back                        |\n| `vec_swap_base`                | Swap elements                            |\n| `vec_pack_base`                | Base cost to pack a vector               |\n| `vec_pack_per_elem`            | Cost to pack a vector per element        |\n| `vec_unpack_base`              | Base cost to unpack a vector             |\n| `vec_unpack_per_expected_elem` | Base cost to unpack a vector per element |\n\nAdditional storage gas parameters are defined in [`table.rs`], [`move_stdlib.rs`], and other assorted source files in [`aptos-gas/src/`].\n\n## Storage gas\n\nStorage gas is defined in [`storage_gas.move`], which is accompanied by a comprehensive and internally-linked DocGen file at [`storage_gas.md`].\n\nIn short:\n\n1. In [`initialize()`], [`base_8192_exponential_curve()`] is used to generate an exponential curve whereby per-item and per-byte costs increase rapidly as utilization approaches an upper bound.\n2. Parameters are reconfigured each epoch via [`on_reconfig()`], based on item-wise and byte-wise utilization ratios.\n3. Reconfigured parameters are stored in [`StorageGas`], which contains the following fields:\n\n| Field             | Meaning                                     |\n| ----------------- | ------------------------------------------- |\n| `per_item_read`   | Cost to read an item from global storage    |\n| `per_item_create` | Cost to create an item in global storage    |\n| `per_item_write`  | Cost to overwrite an item in global storage |\n| `per_byte_read`   | Cost to read a byte from global storage     |\n| `per_byte_create` | Cost to create a byte in global storage     |\n| `per_byte_write`  | Cost to overwrite a byte in global storage  |\n\nHere, an _item_ is either a resource having the `key` attribute, or an entry in a table, and notably, per-byte costs are assessed on the _entire_ size of an item.\nAs stated in [`storage_gas.md`], for example, if an operation mutates a `u8` field in a resource that has five other `u128` fields, the per-byte gas write cost will account for $(5 * 128) / 8 + 1 = 81$ bytes.\n\n### Vectors\n\nByte-wise fees are similarly assessed on vectors, which consume $\\sum_{i = 0}^{n - 1} e_i + b(n)$ bytes, where:\n\n- $n$ is the number of elements in the vector\n- $e_i$ is the size of element $i$\n- $b(n)$ is a \"base size\" which is a function of $n$\n\nSee the [BCS sequence specification] for more information on vector base size (technically a `ULEB128`), which typically occupies just one byte in practice, such that a vector of 100 `u8` elements accounts for $100 + 1 = 101$ bytes.\nHence per the item-wise read methodology described above, reading the last element of such a vector is treated as a 101-byte read.\n\n## Payload gas\n\nPayload gas is defined in [`transaction/mod.rs`](https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/aptos-gas/src/transaction/mod.rs), which incorporates storage gas with several payload- and pricing-associated parameters:\n\n| Parameter                       | Meaning                                                                                |\n| ------------------------------- | -------------------------------------------------------------------------------------- |\n| `min_transaction_gas_units`     | Minimum internal gas units for a transaction, charged at the start of execution        |\n| `large_transaction_cutoff`      | Size, in bytes, above which transactions will be charged an additional amount per byte |\n| `intrinsic_gas_per_byte`        | Internal gas units charged per byte for payloads above `large_transaction_cutoff`      |\n| `maximum_number_of_gas_units`   | Upper limit on external gas units for a transaction                                    |\n| `min_price_per_gas_unit`        | Minimum gas price allowed for a transaction                                            |\n| `max_price_per_gas_unit`        | Maximum gas price allowed for a transaction                                            |\n| `max_transaction_size_in_bytes` | Maximum transaction payload size in bytes                                              |\n| `gas_unit_scaling_factor`       | Conversion factor between internal gas units and external gas units                    |\n\nHere, \"internal gas units\" are defined as constants in source files like [`instr.rs`] and [`storage_gas.move`], which are more granular than \"external gas units\" by a factor of `gas_unit_scaling_factor`:\nto convert from internal gas units to external gas units, divide by `gas_unit_scaling_factor`.\nThen, to convert from external gas units to octas, multiply by the \"gas price\", which denotes the number of octas per unit of external gas.\n\n## Optimization principles\n\n### Unit and pricing constants\n\nAs of the time of this writing, `min_price_per_gas_unit` in [`transaction/mod.rs`](https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/aptos-gas/src/transaction/mod.rs) is defined as [`aptos_global_constants`]`::GAS_UNIT_PRICE` (which is itself defined as 100), with other noteworthy [`transaction/mod.rs`](https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/aptos-gas/src/transaction/mod.rs) constants as follows:\n\n| Constant                  | Value  |\n| ------------------------- | ------ |\n| `min_price_per_gas_unit`  | 100    |\n| `max_price_per_gas_unit`  | 10,000 |\n| `gas_unit_scaling_factor` | 10,000 |\n\nSee [Payload gas](#payload-gas) for the meaning of these constants.\n\n### Storage gas\n\nAs of the time of this writing, [`initialize()`] sets the following minimum storage gas amounts:\n\n| Data style | Operation | Symbol | Minimum internal gas |\n| ---------- | --------- | ------ | -------------------- |\n| Per item   | Read      | $r_i$  | 300,000              |\n| Per item   | Create    | $c_i$  | 5,000,000            |\n| Per item   | Write     | $w_i$  | 300,000              |\n| Per byte   | Read      | $r_b$  | 300                  |\n| Per byte   | Create    | $c_b$  | 5,000                |\n| Per byte   | Write     | $w_b$  | 5,000                |\n\nMaximum amounts are 100 times the minimum amounts, which means that for a utilization ratio of 40% or less, total gas costs will be on the order of 1 to 1.5 times the minimum amounts (see [`base_8192_exponential_curve()`] for supporting calculations).\nHence, in terms of octas, initial mainnet gas costs can be estimated as follows (divide internal gas by scaling factor, then multiply by minimum gas price):\n\n| Operation       | Operation | Minimum octas |\n| --------------- | --------- | ------------- |\n| Per-item read   | $r_i$     | 3000          |\n| Per-item create | $c_i$     | 50,000        |\n| Per-item write  | $w_i$     | 3000          |\n| Per-byte read   | $r_b$     | 3             |\n| Per-byte create | $c_b$     | 50            |\n| Per-byte write  | $w_b$     | 50            |\n\nHere, the most expensive per-item operation by far is creating a new item (via either `move_to<T>()` or adding to a table), which costs nearly 17 times as much as reading or overwriting an old item: $c_i = 16.\\overline{6} r_i = 16.\\overline{6} w_i$. Additionally:\n\n- Writes cost the same as reads on a per-item basis: $w_i = r_i$\n- On a per-byte basis, however, writes cost the same as creates: $w_b = c_b$\n- Per-byte writes and creates cost nearly 17 times as much as per-byte reads: $w_b = c_b = 16.\\overline{6} r_b$\n- Per-item reads cost 1000 times as much as per-byte reads: $r_i = 1000 r_b$\n- Per-item creates cost 1000 times as much as per-byte creates: $c_i = 1000 c_b$\n- Per-item writes cost 60 times as much as per-byte writes: $w_i = 60 w_b$\n\nHence per-item operations cost 1000 times more than per-byte operations for both reads and creates, but only 60 times more for writes.\n\nThus, in the absence of a legitimate economic incentive to deallocate from global storage (via either `move_from<T>()` or by removing from a table), the most effective storage gas optimization strategy is as follows:\n\n1. Minimize per-item creations\n2. Track unused items and overwrite them, rather than creating new items, when possible\n3. Contain per-item writes to as few items as possible\n4. Read, rather than write, whenever possible\n5. Minimize the number of bytes in all operations, especially writes\n\n### Instruction gas\n\nAs of the time of this writing, all instruction gas operations are multiplied by the `EXECUTION_GAS_MULTIPLIER` defined in [`gas_meter.rs`], which is set to 20.\nHence the following representative operations assume gas costs as follows (divide internal gas by scaling factor, then multiply by minimum gas price):\n\n| Operation                    | Minimum octas |\n| ---------------------------- | ------------- |\n| Table add/borrow/remove box  | 240           |\n| Function call                | 200           |\n| Load constant                | 130           |\n| Globally borrow              | 100           |\n| Read/write reference         | 40            |\n| Load `u128` on stack         | 16            |\n| Table box operation per byte | 2             |\n\n(Note that per-byte table box operation instruction gas does not account for storage gas, which is assessed separately).\n\nFor comparison, reading a 100-byte item costs $r_i + 100 * r_b = 3000 + 100 * 3 = 3300$ octas at minimum, some 16.5 times as much as a function call, and in general, instruction gas costs are largely dominated by storage gas costs.\n\nNotably, however, there is still technically an incentive to reduce the number of function calls in a program, but engineering efforts are more effectively dedicated to writing modular, decomposed code that is geared toward reducing storage gas costs, rather than attempting to write repetitive code blocks with fewer nested functions (in nearly all cases).\n\nIn extreme cases it is possible for instruction gas to far outweigh storage gas, for example if a loopwise mathematical function takes 10,000 iterations to converge; but again this is an extreme case and for most applications storage gas has a larger impact on base gas than does instruction gas.\n\n### Payload gas\n\nAs of the time of this writing, [`transaction/mod.rs`](https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/aptos-gas/src/transaction/mod.rs) defines the minimum amount of internal gas per transaction as 1,500,000 internal units (15,000 octas at minimum), an amount that increases by 2,000 internal gas units (20 octas minimum) per byte for payloads larger than 600 bytes, with the maximum number of bytes permitted in a transaction set at 65536.\nHence in practice, payload gas is unlikely to be a concern.\n\n<!--- Alphabetized reference links -->\n\n[#4540]: https://github.com/aptos-labs/aptos-core/pull/4540/files\n[`aptos-gas/src/`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/aptos-gas/src/\n[`aptos_global_constants`]: https://github.com/aptos-labs/aptos-core/blob/main/config/global-constants/src/lib.rs\n[`base_8192_exponential_curve()`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/framework/aptos-framework/doc/storage_gas.md#0x1_storage_gas_base_8192_exponential_curve\n[bcs sequence specification]: https://github.com/diem/bcs#fixed-and-variable-length-sequences\n[`gas_meter.rs`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/aptos-gas/src/gas_meter.rs\n[`initialize()`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/framework/aptos-framework/doc/storage_gas.md#0x1_storage_gas_initialize\n[`instr.rs`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/aptos-gas/src/instr.rs\n[`move_stdlib.rs`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/aptos-gas/src/move_stdlib.rs\n[`on_reconfig()`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/framework/aptos-framework/doc/storage_gas.md#@Specification_16_on_reconfig\n[`storage_gas.md`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/framework/aptos-framework/doc/storage_gas.md\n[`storage_gas.move`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/framework/aptos-framework/sources/storage_gas.move\n[`storagegas`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/framework/aptos-framework/doc/storage_gas.md#resource-storagegas\n[`table.rs`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/aptos-gas/src/table.rs\n[`transaction.rs`]: https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/aptos-gas/src/transaction.rs",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 12,
    "wordCount": 2383,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ethereum-eth",
    "slug": "ethereum-eth",
    "path": "blockchain/ethereum",
    "fullPath": "blockchain/ethereum/ethereum-eth",
    "title": "Ethereum (ETH)",
    "excerpt": "Ethereum (ETH) ğŸ’¡ ì´ë”ë¦¬ì›€ì€ ì›”ë“œ ì™€ì´ë“œ ì»´í“¨í„°ë¥¼ ê¿ˆê¾¼ë‹¤ ì¥ì  ê°œë°œì ì»¤ë®¤ë‹ˆí‹° 1ë“±ì¸ë“¯, ì–´ì§€ê°„í•œ ì—ëŸ¬ë‚˜ ì´ìŠˆëŠ” ì´ë¯¸ ë ˆí¼ëŸ°ìŠ¤ê°€ ëª¨ë‘ ìˆìŒ. ë””ë²„ê¹… ë§¤ìš° í¸í•¨ ê·¸ëŸ¬ë‹¤ë³´ë‹ˆ web3, ethers.j...",
    "content": "# Ethereum (ETH)\n\n<aside> ğŸ’¡ ì´ë”ë¦¬ì›€ì€ ì›”ë“œ ì™€ì´ë“œ ì»´í“¨í„°ë¥¼ ê¿ˆê¾¼ë‹¤\n\n</aside>\n\n# ì¥ì \n\n1.  ê°œë°œì ì»¤ë®¤ë‹ˆí‹° 1ë“±ì¸ë“¯, ì–´ì§€ê°„í•œ ì—ëŸ¬ë‚˜ ì´ìŠˆëŠ” ì´ë¯¸ ë ˆí¼ëŸ°ìŠ¤ê°€ ëª¨ë‘ ìˆìŒ. ë””ë²„ê¹… ë§¤ìš° í¸í•¨\n2.  ê·¸ëŸ¬ë‹¤ë³´ë‹ˆ web3, ethers.js ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë§¤ìš° ì˜ ê°œë°œë˜ì–´ ìˆê³  ê°œë°œë¬¸ì„œë„ ì˜ë˜ì–´ ìˆìŒ\n3.  ë…¸ë“œë„ ì•ˆì •ì ì„\n4.  ì´ê±° í•˜ë‚˜ë§Œ ì•Œì•„ë„ ì‹œì¥ì— ë‚˜ì˜¨ ë¸”ë¡ì²´ì¸ 5í• ì€ ë‹¤ë£° ìˆ˜ ìˆìŒâ€¦ (ê·¸ë§Œí¼ ë³µë¶™ì²´ì¸ì´ ë§ìŒ)\n\n# ë‹¨ì \n\n1.  ë¸”ë¡íƒ€ì„ì´ ì•½ 12 ~ 13ì´ˆë¡œ ì°¨ì„¸ëŒ€ ë¸”ë¡ì²´ì¸ì— ë¹„í•´ ëŠë¦¼\n2.  EVMì€ ìì‚°ì„ ì¼ê¸‰ê°ì²´ë¡œ ë‹¤ë£¨ì§€ ì•ŠìŒ, ì¼ì¢…ì˜ ì§€ì—­ ë³€ìˆ˜ë¥¼ ì‚¬ì¹™ì—°ì‚°í•œ ê°’ì„. ê·¸ëŸ¬ë‹¤ë³´ë‹ˆ EVMì„ ì±„íƒí•œ ë¸”ë¡ì²´ì¸ë“¤ì—ì„œ ê³ ì§ˆì ì¸ í•´í‚¹ë¬¸ì œê°€ ë°œìƒí•´ì˜´\n3.  ë¶ˆì¥ì¼ë•Œ ìˆ˜ìˆ˜ë£Œê°€ 10ë§Œì›ì„ ë„˜ìŒ.\n4.  ë¹„íƒˆë¦­ ë¶€í…Œë¦°ì˜ ì••ë„ì  ë§ˆì¼€íŒ… íš¨ê³¼ê°€ ì—†ì–´ë„ ì¸ê¸°ê°€ ì§€ì†ë ê¹Œì— ëŒ€í•œ ì˜ë¬¸",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 104,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "namespaced-merkle-tree-nmt-ë€",
    "slug": "namespaced-merkle-tree-nmt-ran",
    "path": "blockchain/celestia",
    "fullPath": "blockchain/celestia/namespaced-merkle-tree-nmt-ran",
    "title": "Namespaced Merkle Tree (NMT)ë€?",
    "excerpt": "Namespaced Merkle Tree (NMT)ë€? Namespaced Merkle Tree (NMT)ëŠ” Merkle Treeì˜ ë³€í˜•ëœ êµ¬ì¡°ë¡œ, ë„¤ì„ìŠ¤í˜ì´ìŠ¤(namespace)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì¸ì¦ ê°€ëŠ¥í•œ ë°ì´í„° êµ¬ì¡°ë‹¤. Cel...",
    "content": "**Namespaced Merkle Tree (NMT)ë€?**\n\n  \n\n**Namespaced Merkle Tree (NMT)**ëŠ” **Merkle Treeì˜ ë³€í˜•ëœ êµ¬ì¡°**ë¡œ, **ë„¤ì„ìŠ¤í˜ì´ìŠ¤(namespace)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì¸ì¦ ê°€ëŠ¥í•œ ë°ì´í„° êµ¬ì¡°**ë‹¤. Celestiaì—ì„œëŠ” **íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ì†í•˜ëŠ” ë°ì´í„°ë§Œ ê²€ì¦ ê°€ëŠ¥**í•˜ë„ë¡ ì„¤ê³„ëœ ì´ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•œë‹¤.\n\n---\n\n**1. Merkle Treeì™€ì˜ ì°¨ì´ì **\n\n  \n\nMerkle TreeëŠ” ê¸°ë³¸ì ìœ¼ë¡œ **ì „ì²´ ë°ì´í„° ë¸”ë¡ì„ ê²€ì¦í•˜ê¸° ìœ„í•œ í•´ì‹œ íŠ¸ë¦¬ êµ¬ì¡°**ë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ì¸ Merkle TreeëŠ” íŠ¹ì • ë°ì´í„°ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ë¥¼ **ë¹ ë¥´ê²Œ ì¦ëª…í•˜ëŠ” ê¸°ëŠ¥ì€ ì œê³µí•˜ì§€ë§Œ, ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë‹¨ìœ„ë¡œ ê²€ìƒ‰í•˜ê±°ë‚˜ ê²€ì¦í•˜ëŠ” ê¸°ëŠ¥ì€ ì œê³µí•˜ì§€ ì•ŠëŠ”ë‹¤**.\n\n  \n\në°˜ë©´ **NMTëŠ” ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë‹¨ìœ„ë¡œ ì¦ëª…(Proof)ì„ ì§€ì›**í•˜ê¸° ë•Œë¬¸ì— íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ì†í•˜ëŠ” ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ê²€ì¦í•  ìˆ˜ ìˆë‹¤.\n\n---\n\n**2. NMTì˜ íŠ¹ì§•**\n\n1. **ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê¸°ë°˜ Merkle í•´ì‹±**\n\nâ€¢ ê° ë…¸ë“œê°€ íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë²”ìœ„ë¥¼ í¬í•¨í•˜ê³  ìˆìœ¼ë©°, ë¶€ëª¨ ë…¸ë“œëŠ” ìì‹ ë…¸ë“œë“¤ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë²”ìœ„ë¥¼ ìœ ì§€í•œë‹¤.\n\nâ€¢ ì¦‰, **Merkle ì¦ëª…ì—ì„œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ë¡œ ë°ì´í„° í¬í•¨ ì—¬ë¶€ë¥¼ ì‰½ê²Œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.**\n\n1. **Merkle Proofì™€ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í•„í„°ë§ ì§€ì›**\n\nâ€¢ ì¼ë°˜ Merkle Treeì˜ ê²½ìš° íŠ¹ì • ë°ì´í„°ê°€ íŠ¸ë¦¬ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ë§Œ ì¦ëª…í•  ìˆ˜ ìˆì§€ë§Œ,\n\n**NMTëŠ” íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ì†í•˜ëŠ” ë°ì´í„°ê°€ ìˆëŠ”ì§€ ì—†ëŠ”ì§€ ì¦ëª…í•  ìˆ˜ ìˆë‹¤.**\n\n1. **ë¶€ë¶„ ë°ì´í„° ì ‘ê·¼ì„± í–¥ìƒ**\n\nâ€¢ íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ ë°ì´í„°ë§Œ ë¹ ë¥´ê²Œ ê²€ìƒ‰í•˜ê³ , ë‹¤ìš´ë¡œë“œí•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.\n\n---\n\n**3. Celestiaì—ì„œ NMTê°€ ì‚¬ìš©ë˜ëŠ” ì´ìœ **\n\n  \n\nCelestiaì—ì„œëŠ” **ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„° ê°€ìš©ì„±ì„ ê²€ì¦**í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, ê¸°ì¡´ Merkle Treeë³´ë‹¤ **NMTê°€ ë” ì í•©**í•˜ë‹¤. Celestiaì—ì„œ NMTê°€ ì‚¬ìš©ë˜ëŠ” ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n1. **Blob íŠ¸ëœì­ì…˜ ê²€ì¦**\n\nâ€¢ Celestiaì—ì„œëŠ” íŠ¸ëœì­ì…˜ê³¼ ë¸”ë¡ ë°ì´í„°ë¥¼ Blob í˜•íƒœë¡œ ì €ì¥í•˜ë©°, ê° Blobì€ íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ì†í•œë‹¤.\n\nâ€¢ NMTë¥¼ í™œìš©í•˜ë©´ íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ Blobì´ ì¡´ì¬í•˜ëŠ”ì§€ **íš¨ìœ¨ì ìœ¼ë¡œ ì¦ëª…í•  ìˆ˜ ìˆë‹¤**.\n\n1. **Data Availability Sampling (DAS) ìµœì í™”**\n\nâ€¢ ë¼ì´íŠ¸ ë…¸ë“œëŠ” **ë°ì´í„° ê°€ìš©ì„±ì„ ê²€ì¦í•˜ê¸° ìœ„í•´ ì¼ë¶€ ìƒ˜í”Œì„ ìš”ì²­**í•´ì•¼ í•œë‹¤.\n\nâ€¢ NMTë¥¼ í™œìš©í•˜ë©´ íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” ìƒ˜í”Œì„ ë³´ë‹¤ **íš¨ìœ¨ì ìœ¼ë¡œ ê²€ì¦ ê°€ëŠ¥**í•˜ë‹¤.\n\n1. **ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê¸°ë°˜ ë°ì´í„° ê²€ìƒ‰ ìµœì í™”**\n\nâ€¢ Celestiaì˜ ëª¨ë“ˆí˜• ë¸”ë¡ì²´ì¸ êµ¬ì¡°ì—ì„œëŠ” **ë‹¤ì–‘í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì„œë¡œ ë‹¤ë¥¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ì‚¬ìš©**í•œë‹¤.\n\nâ€¢ íŠ¹ì • ì• í”Œë¦¬ì¼€ì´ì…˜ì´ í•„ìš”í•œ ë°ì´í„°ë§Œ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡, NMTê°€ íš¨ê³¼ì ìœ¼ë¡œ ë™ì‘í•œë‹¤.\n\n---\n\n**4. NMTì˜ ë™ì‘ ë°©ì‹**\n\n  \n\n**ğŸ“Œ ê¸°ë³¸ì ì¸ Merkle Treeì™€ ë¹„êµ**\n\n  \n\nì¼ë°˜ì ì¸ Merkle Treeì˜ ê²½ìš°, ê° ë¦¬í”„ ë…¸ë“œ(ë°ì´í„° ë¸”ë¡)ëŠ” í•´ì‹œë¡œ ë³€í™˜ë˜ë©°, ë¶€ëª¨ ë…¸ë“œëŠ” ìì‹ë“¤ì˜ í•´ì‹œë¥¼ ì¡°í•©í•˜ì—¬ ìƒì„±ëœë‹¤.\n\n  \n\ní•˜ì§€ë§Œ **Namespaced Merkle Tree (NMT)ëŠ” ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ë³´ë¥¼ ì¶”ê°€í•˜ì—¬ í•´ì‹±**í•œë‹¤.\n\n  \n\n**ğŸ“Œ NMTì˜ í•´ì‹± ê·œì¹™**\n\n1. **ê° ë¦¬í”„ ë…¸ë“œ**:\n\nâ€¢ (namespace, data_hash) í˜•íƒœë¡œ ì €ì¥ë¨.\n\n1. **ê° ë‚´ë¶€ ë…¸ë“œ**:\n\nâ€¢ namespace_min = min(left.namespace_min, right.namespace_min)\n\nâ€¢ namespace_max = max(left.namespace_max, right.namespace_max)\n\nâ€¢ ë¶€ëª¨ ë…¸ë“œëŠ” ìì‹ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë²”ìœ„ë¥¼ ìœ ì§€í•˜ë©´ì„œ í•´ì‹±ë¨.\n\n  \n\n**ğŸ“Œ NMT Merkle Proof ìƒì„±**\n\nâ€¢ íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ì†í•˜ëŠ” ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ”ì§€ ê²€ì¦í•  ë•Œ, **ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë²”ìœ„ ì •ë³´ë¥¼ ê°€ì§„ Merkle Proof**ë¥¼ ì‚¬ìš©í•˜ë©´ íš¨ìœ¨ì ìœ¼ë¡œ ì¦ëª…í•  ìˆ˜ ìˆë‹¤.\n\nâ€¢ íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ ë°ì´í„°ë¥¼ í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´, **í•´ë‹¹ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë²”ìœ„ê°€ ì—†ìŒ(Empty Proof)** ì„ ì¦ëª…í•  ìˆ˜ ìˆë‹¤.\n\n---\n\n**5. ê²°ë¡ **\n\nâ€¢ Namespaced Merkle Tree(NMT)ëŠ” **Merkle Treeì˜ í™•ì¥ ë²„ì „**ìœ¼ë¡œ, **ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ ë°ì´í„° ê²€ì¦ì´ ê°€ëŠ¥**í•œ êµ¬ì¡°ë‹¤.\n\nâ€¢ Celestiaì—ì„œëŠ” **ëª¨ë“ˆí˜• ë¸”ë¡ì²´ì¸ì—ì„œ ë°ì´í„° ê°€ìš©ì„±ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê²€ì¦í•˜ê³ , íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ ë°ì´í„°ë§Œ ë¹ ë¥´ê²Œ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ ì§€ì›**í•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœë‹¤.\n\nâ€¢ **DAS (Data Availability Sampling)ì™€ Blob íŠ¸ëœì­ì…˜ ê²€ì¦ì—ë„ ì¤‘ìš”í•œ ì—­í• **ì„ í•œë‹¤.\n\n  \n\nğŸ‘‰ **ì¦‰, Celestiaì˜ í™•ì¥ì„±ê³¼ ë°ì´í„° ê°€ìš©ì„±ì„ ë†’ì´ê¸° ìœ„í•´ NMTëŠ” í•„ìˆ˜ì ì¸ ìš”ì†Œë‹¤.** ğŸš€",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 3,
    "wordCount": 437,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "celestiaì˜-ë°ì´í„°-ì €ì¥-ê³µê°„-ì‚¬ìš©ëŸ‰",
    "slug": "celestiayi-deiteo-jeojang-gonggan-sayongryang",
    "path": "blockchain/celestia",
    "fullPath": "blockchain/celestia/celestiayi-deiteo-jeojang-gonggan-sayongryang",
    "title": "Celestiaì˜ ë°ì´í„° ì €ì¥ ê³µê°„ ì‚¬ìš©ëŸ‰",
    "excerpt": "Celestiaì˜ ë°ì´í„° ì €ì¥ ê³µê°„ ì‚¬ìš©ëŸ‰ Celestiaì˜ EDS(Extended Data Square) êµ¬ì¡°ì—ì„œ ì €ì¥ ê³µê°„ ì‚¬ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: ì „ì²´ EDS ìš©ëŸ‰ (ì´ë¡ ì ì¸ ê°’) ì „ì²´ EDSëŠ” ì›ë³¸ ë°ì´í„°ì˜ 4ë°° í¬ê¸°ë¥¼ ì°¨ì§€í•©ë‹ˆë‹¤: -...",
    "content": "# Celestiaì˜ ë°ì´í„° ì €ì¥ ê³µê°„ ì‚¬ìš©ëŸ‰\n\nCelestiaì˜ EDS(Extended Data Square) êµ¬ì¡°ì—ì„œ ì €ì¥ ê³µê°„ ì‚¬ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n## ì „ì²´ EDS ìš©ëŸ‰ (ì´ë¡ ì ì¸ ê°’)\n\nì „ì²´ EDSëŠ” ì›ë³¸ ë°ì´í„°ì˜ **4ë°°** í¬ê¸°ë¥¼ ì°¨ì§€í•©ë‹ˆë‹¤:\n\n- **Q1 (ì›ë³¸ ë°ì´í„°)**: ì›ë³¸ ë°ì´í„°ì˜ 100%\n- **Q2 (í–‰ íŒ¨ë¦¬í‹°)**: ì›ë³¸ ë°ì´í„°ì˜ 100%\n- **Q3 (ì—´ íŒ¨ë¦¬í‹°)**: ì›ë³¸ ë°ì´í„°ì˜ 100%\n- **Q4 (í–‰+ì—´ íŒ¨ë¦¬í‹°)**: ì›ë³¸ ë°ì´í„°ì˜ 100%\n\nì¦‰, ëª¨ë“  ì‚¬ë¶„ë©´ì„ ëª¨ë‘ ì €ì¥í•œë‹¤ë©´ ì›ë³¸ ë°ì´í„° í¬ê¸°ì˜ 4ë°°ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n\n## ì‹¤ì œ Celestiaì˜ ì €ì¥ ì „ëµ\n\ní•˜ì§€ë§Œ Celestiaì—ì„œëŠ” ì‹¤ì œë¡œ ëª¨ë“  ì‚¬ë¶„ë©´ì„ ì €ì¥í•˜ì§€ ì•Šê³ , í•„ìš”ì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì€ ì €ì¥ ì „ëµì„ ì‚¬ìš©í•©ë‹ˆë‹¤:\n\n1. **ODSë§Œ ì €ì¥** (ì›ë³¸ ë°ì´í„°ë§Œ):\n   - ì›ë³¸ ë°ì´í„°ì˜ **1ë°°** ìš©ëŸ‰ ì‚¬ìš©\n   - ì˜¤ë˜ëœ ë¸”ë¡ì´ë‚˜ ì•„ì¹´ì´ë¸Œ ëª©ì ì˜ ì €ì¥ì— ì‚¬ìš©\n   - í•„ìš”ì‹œ ë‚˜ë¨¸ì§€ ì‚¬ë¶„ë©´(Q2, Q3, Q4)ì€ ê³„ì‚°ìœ¼ë¡œ ë³µêµ¬ ê°€ëŠ¥\n\n2. **ODSQ4 ì €ì¥** (ì›ë³¸ + Q4 ì‚¬ë¶„ë©´):\n   - ì›ë³¸ ë°ì´í„°ì˜ **2ë°°** ìš©ëŸ‰ ì‚¬ìš©\n   - ê°€ìš©ì„± ìœˆë„ìš° ë‚´ì˜ ìµœê·¼ ë¸”ë¡ì— ì‚¬ìš©\n   - íŠ¹ì • ì ‘ê·¼ íŒ¨í„´ì—ì„œ ê³„ì‚° íš¨ìœ¨ì„± í–¥ìƒ\n\n## ì‹¤ì œ ì½”ë“œì—ì„œì˜ ì €ì¥ ì „ëµ\n\nì½”ë“œì—ì„œ ì´ëŸ° ì €ì¥ ì „ëµì€ ë‹¤ìŒê³¼ ê°™ì´ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤:\n\n```go\n// ê°€ìš©ì„± ìœˆë„ìš° ë‚´ì˜ ë¸”ë¡ì€ ODSì™€ Q4 ëª¨ë‘ ì €ì¥ (ì›ë³¸ì˜ 2ë°°)\nif availability.IsWithinWindow(eh.Time(), availability.StorageWindow) {\n    err = store.PutODSQ4(ctx, eh.DAH, eh.Height(), eds)\n} else {\n    // ê·¸ ì™¸ ë¸”ë¡ì€ ODSë§Œ ì €ì¥ (ì›ë³¸ì˜ 1ë°°)\n    err = store.PutODS(ctx, eh.DAH, eh.Height(), eds)\n}\n```\n\n## ì €ì¥ ê³µê°„ ìµœì í™”ì˜ ì´ìœ \n\nì´ë ‡ê²Œ ì €ì¥ ê³µê°„ì„ ìµœì í™”í•˜ëŠ” ì´ìœ ëŠ”:\n\n1. **ë””ìŠ¤í¬ ê³µê°„ íš¨ìœ¨ì„±**:\n   - ëª¨ë“  ë¸”ë¡ì˜ ëª¨ë“  ì‚¬ë¶„ë©´ì„ ì €ì¥í•˜ëŠ” ê²ƒì€ ë¹„íš¨ìœ¨ì \n   - ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì›ë³¸ ë°ì´í„°ë§Œìœ¼ë¡œë„ ì¶©ë¶„\n\n2. **ê³„ì‚° ë¹„ìš©ê³¼ ì €ì¥ ë¹„ìš©ì˜ ê· í˜•**:\n   - Q2ì™€ Q3ëŠ” Q1ê³¼ Q4ë¡œë¶€í„° ê³„ì‚° ê°€ëŠ¥ (í•„ìš”í•  ë•Œë§Œ ê³„ì‚°)\n   - ê°€ì¥ ìì£¼ ì ‘ê·¼í•˜ëŠ” ìµœê·¼ ë¸”ë¡ì—ë§Œ Q4ë„ í•¨ê»˜ ì €ì¥í•˜ì—¬ ê³„ì‚° ë¹„ìš© ì ˆì•½\n\n3. **ê°€ìš©ì„±ê³¼ ê²€ì¦ì˜ ê· í˜•**:\n   - ë°ì´í„° ê°€ìš©ì„± ê²€ì¦ì€ ìµœê·¼ ë¸”ë¡ì— ëŒ€í•´ ë” ì¤‘ìš”\n   - ë”°ë¼ì„œ ìµœê·¼ ë¸”ë¡ì€ ë” ë§ì€ ê³µê°„(ODS+Q4)ì„ ì‚¬ìš©í•˜ê³ , ì˜¤ë˜ëœ ë¸”ë¡ì€ ê³µê°„ íš¨ìœ¨ì„±(ODSë§Œ)ì„ ìš°ì„ ì‹œ\n\nê²°ë¡ ì ìœ¼ë¡œ, CelestiaëŠ” ì›ë³¸ ë°ì´í„°ì˜ ì•½ 1~2ë°° ì •ë„ì˜ ì €ì¥ ê³µê°„ì„ ì‚¬ìš©í•˜ë©´ì„œë„, ë°ì´í„° ê°€ìš©ì„± ê²€ì¦ê³¼ ë³µêµ¬ ê¸°ëŠ¥ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 2,
    "wordCount": 294,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "celestiaì˜-ë°ì´í„°-ê°€ìš©ì„±-ì¦ëª…-data-availability-proof-ë©”ì»¤ë‹ˆì¦˜",
    "slug": "celestiayi-deiteo-gayongseong-jeungmyeong-data-availability-proof-mekeonijeum",
    "path": "blockchain/celestia",
    "fullPath": "blockchain/celestia/celestiayi-deiteo-gayongseong-jeungmyeong-data-availability-proof-mekeonijeum",
    "title": "Celestiaì˜ ë°ì´í„° ê°€ìš©ì„± ì¦ëª…(Data Availability Proof) ë©”ì»¤ë‹ˆì¦˜",
    "excerpt": "Celestiaì˜ ë°ì´í„° ê°€ìš©ì„± ì¦ëª…(Data Availability Proof) ë©”ì»¤ë‹ˆì¦˜ CelestiaëŠ” ë¸”ë¡ì²´ì¸ í™•ì¥ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë°ì´í„° ê°€ìš©ì„± ì¦ëª…(Data Availability Proof) ë©”ì»¤ë‹ˆì¦˜ì„ í•µì‹¬ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ë©”ì»¤ë‹ˆì¦˜ì€ ëª¨ë“ ...",
    "content": "# Celestiaì˜ ë°ì´í„° ê°€ìš©ì„± ì¦ëª…(Data Availability Proof) ë©”ì»¤ë‹ˆì¦˜\n\nCelestiaëŠ” ë¸”ë¡ì²´ì¸ í™•ì¥ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë°ì´í„° ê°€ìš©ì„± ì¦ëª…(Data Availability Proof) ë©”ì»¤ë‹ˆì¦˜ì„ í•µì‹¬ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ë©”ì»¤ë‹ˆì¦˜ì€ ëª¨ë“  ë…¸ë“œê°€ ì „ì²´ ë¸”ë¡ì²´ì¸ ë°ì´í„°ë¥¼ ì €ì¥í•˜ì§€ ì•Šê³ ë„ ë°ì´í„°ê°€ ë„¤íŠ¸ì›Œí¬ì— ì‹¤ì œë¡œ ê²Œì‹œë˜ì—ˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\n\n## 1. ë°ì´í„° ê°€ìš©ì„± ë¬¸ì œ\n\nê¸°ì¡´ ë¸”ë¡ì²´ì¸ì—ì„œëŠ” ëª¨ë“  ë…¸ë“œê°€ ëª¨ë“  íŠ¸ëœì­ì…˜ì„ ê²€ì¦í•˜ê³  ì €ì¥í•´ì•¼ í•˜ë¯€ë¡œ í™•ì¥ì„±ì— í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. CelestiaëŠ” \"ë°ì´í„° ê°€ìš©ì„± ìƒ˜í”Œë§(Data Availability Sampling, DAS)\"ì´ë¼ëŠ” ê¸°ìˆ ì„ í†µí•´ ì´ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.\n\n## 2. í•µì‹¬ ê¸°ìˆ  êµ¬ì„±ìš”ì†Œ\n\n### 2.1 Extended Data Square (EDS)\n\në¸”ë¡ ë°ì´í„°ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤:\n\n1. **ë°ì´í„° ì •ë ¬**: íŠ¸ëœì­ì…˜ ë°ì´í„°ë¥¼ ì •ì‚¬ê°í˜• í˜•íƒœ(Original Data Square, ODS)ë¡œ ë°°ì—´í•©ë‹ˆë‹¤.\n2. **2D Reed-Solomon ì¸ì½”ë”©**: ODSì— Reed-Solomon ì˜¤ë¥˜ ì •ì • ì½”ë“œë¥¼ ì ìš©í•˜ì—¬ í™•ì¥ëœ ë°ì´í„° ì‚¬ê°í˜•(EDS)ì„ ìƒì„±í•©ë‹ˆë‹¤.\n3. **ì‚¬ë¶„ë©´ êµ¬ì¡°**: EDSëŠ” 4ê°œì˜ ì‚¬ë¶„ë©´ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:\n   - Q1(ì¢Œìƒë‹¨): ì›ë³¸ ë°ì´í„°(ODS)\n   - Q2(ìš°ìƒë‹¨): í–‰ íŒ¨ë¦¬í‹° ë°ì´í„°\n   - Q3(ì¢Œí•˜ë‹¨): ì—´ íŒ¨ë¦¬í‹° ë°ì´í„°\n   - Q4(ìš°í•˜ë‹¨): í–‰+ì—´ íŒ¨ë¦¬í‹° ë°ì´í„°\n\n```go\n// core/eds.goì—ì„œ\nfunc extendBlock(data *types.Data, appVersion uint64, options ...nmt.Option) (*rsmt2d.ExtendedDataSquare, error) {\n    // ë¸”ë¡ ë°ì´í„°ë¥¼ EDSë¡œ í™•ì¥í•˜ëŠ” ê³¼ì •\n}\n```\n\n### 2.2 ë°ì´í„° ê°€ìš©ì„± ìƒ˜í”Œë§(DAS)\n\nê²½ëŸ‰ ë…¸ë“œëŠ” ì „ì²´ ë¸”ë¡ì„ ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•Šê³  EDSì˜ ì„ì˜ì˜ ìœ„ì¹˜ì—ì„œ ì†Œìˆ˜ì˜ ìƒ˜í”Œë§Œ ìš”ì²­í•©ë‹ˆë‹¤:\n\n1. **ë¬´ì‘ìœ„ ìƒ˜í”Œë§**: ë…¸ë“œëŠ” EDSì˜ ë¬´ì‘ìœ„ ìœ„ì¹˜ì—ì„œ ì—¬ëŸ¬ ê°œì˜ ìƒ˜í”Œ(share)ì„ ìš”ì²­í•©ë‹ˆë‹¤.\n2. **í†µê³„ì  ê²€ì¦**: ì¶©ë¶„í•œ ìˆ˜ì˜ ìƒ˜í”Œì´ ì„±ê³µì ìœ¼ë¡œ ê²€ìƒ‰ë˜ë©´, ë†’ì€ í™•ë¥ ë¡œ ì „ì²´ ë°ì´í„°ê°€ ê°€ìš©í•˜ë‹¤ê³  ê²°ë¡ ì§€ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n3. **ì´ë¡ ì  ê¸°ë°˜**: ë°ì´í„°ì˜ ì¼ë¶€ê°€ ëˆ„ë½ëœ ê²½ìš°, ë¬´ì‘ìœ„ ìƒ˜í”Œë§ì„ í†µí•´ ë†’ì€ í™•ë¥ ë¡œ ëˆ„ë½ëœ ë¶€ë¶„ì„ ê°ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n### 2.3 Namespaced Merkle Tree (NMT)\n\nCelestiaëŠ” ë°ì´í„°ë¥¼ ë„¤ì„ìŠ¤í˜ì´ìŠ¤(namespace)ë¡œ êµ¬ë¶„í•˜ì—¬ êµ¬ì¡°í™”í•©ë‹ˆë‹¤:\n\n1. **ë°ì´í„° ë¶„ë¥˜**: íŠ¸ëœì­ì…˜ ë°ì´í„°ë¥¼ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.\n2. **íš¨ìœ¨ì ì¸ ê²€ì¦**: ë…¸ë“œëŠ” íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì˜ ë°ì´í„°ë§Œ ì„ íƒì ìœ¼ë¡œ ê²€ì¦í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n3. **ì¦ëª… ìµœì í™”**: NMTëŠ” íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë°ì´í„°ì— ëŒ€í•œ ì¦ëª… í¬ê¸°ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.\n\n## 3. ë°ì´í„° ê°€ìš©ì„± ì¦ëª… í”„ë¡œì„¸ìŠ¤\n\n### 3.1 ë¸”ë¡ ìƒì„± ë° ì œì•ˆ\n\n1. ë¸”ë¡ ì œì•ˆìëŠ” íŠ¸ëœì­ì…˜ì„ ìˆ˜ì§‘í•˜ì—¬ ë¸”ë¡ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n2. íŠ¸ëœì­ì…˜ ë°ì´í„°ë¥¼ ODS í˜•íƒœë¡œ ì •ë ¬í•©ë‹ˆë‹¤.\n3. 2D Reed-Solomon ì¸ì½”ë”©ì„ ì ìš©í•˜ì—¬ EDSë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n4. ë¸”ë¡ í—¤ë”ì— ë°ì´í„° í•´ì‹œ(Merkle root)ë¥¼ í¬í•¨ì‹œì¼œ ë„¤íŠ¸ì›Œí¬ì— ì œì•ˆí•©ë‹ˆë‹¤.\n\n### 3.2 ê²½ëŸ‰ ë…¸ë“œì˜ ê²€ì¦ ê³¼ì •\n\n1. ê²½ëŸ‰ ë…¸ë“œëŠ” ë¸”ë¡ í—¤ë”ë¥¼ ë°›ìŠµë‹ˆë‹¤.\n2. EDSì˜ ë¬´ì‘ìœ„ ìœ„ì¹˜ì—ì„œ ë‹¤ìˆ˜ì˜ ìƒ˜í”Œì„ ìš”ì²­í•©ë‹ˆë‹¤.\n3. ìƒ˜í”Œì´ Merkle rootì™€ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.\n4. ì¶©ë¶„í•œ ìƒ˜í”Œ(ì¼ë°˜ì ìœ¼ë¡œ ìˆ˜ë°± ê°œ)ì´ ì„±ê³µì ìœ¼ë¡œ ê²€ì¦ë˜ë©´, ì „ì²´ ë°ì´í„°ê°€ ê°€ìš©í•˜ë‹¤ê³  íŒë‹¨í•©ë‹ˆë‹¤.\n\n```go\n// ê²½ëŸ‰ ë…¸ë“œì˜ ìƒ˜í”Œë§ ìš”ì²­ ì²˜ë¦¬ ê³¼ì • (ê°œë…ì  ì½”ë“œ)\nfunc (odsq4 *ODSQ4) Sample(ctx context.Context, coords shwap.SampleCoords) (shwap.Sample, error) {\n    // ìš”ì²­ëœ ì¢Œí‘œì—ì„œ ìƒ˜í”Œ ë°ì´í„° ê²€ìƒ‰\n}\n```\n\n### 3.3 ë°ì´í„° ë³µêµ¬ ëŠ¥ë ¥\n\nEDSì˜ Reed-Solomon ì†ì„± ë•ë¶„ì—:\n\n1. ì „ì²´ EDSì˜ ì•½ 25%ë§Œ ìˆìœ¼ë©´ ì „ì²´ ë°ì´í„°ë¥¼ ë³µêµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n2. ì¶©ë¶„í•œ ìˆ˜ì˜ ë…¸ë“œê°€ ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•˜ë©´, ë„¤íŠ¸ì›Œí¬ ì „ì²´ì ìœ¼ë¡œ ë°ì´í„° ë³µêµ¬ê°€ ê°€ëŠ¥í•œ ìˆ˜ì¤€ì˜ ìƒ˜í”Œì´ ì¡´ì¬í•©ë‹ˆë‹¤.\n\n```go\n// store/file/square.goì—ì„œ\nfunc (s square) computeAxisHalf(axisType rsmt2d.Axis, axisIdx int) (eds.AxisHalf, error) {\n    // Reed-Solomon ì¸ì½”ë”©ì„ ì‚¬ìš©í•˜ì—¬ ëˆ„ë½ëœ ë°ì´í„° ë³µêµ¬\n}\n```\n\n## 4. ì €ì¥ ìµœì í™”\n\nCelestiaëŠ” ëª¨ë“  ë…¸ë“œê°€ ëª¨ë“  ë°ì´í„°ë¥¼ ì €ì¥í•  í•„ìš”ê°€ ì—†ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤:\n\n1. **ì„ íƒì  ì €ì¥**: í’€ ë…¸ë“œëŠ” ë¸”ë¡ ë°ì´í„°ì˜ ODS ë¶€ë¶„ë§Œ ì €ì¥í•©ë‹ˆë‹¤.\n2. **ì‹œê°„ ê¸°ë°˜ ì „ëµ**: ê°€ìš©ì„± ìœˆë„ìš° ë‚´ì˜ ìµœê·¼ ë¸”ë¡ì€ ODSQ4 í˜•íƒœ(ODS+Q4)ë¡œ ì €ì¥í•˜ê³ , ì˜¤ë˜ëœ ë¸”ë¡ì€ ODS í˜•íƒœë¡œë§Œ ì €ì¥í•©ë‹ˆë‹¤.\n\n```go\n// core/eds.goì—ì„œ\nfunc storeEDS(ctx context.Context, eh *header.ExtendedHeader, eds *rsmt2d.ExtendedDataSquare, store *store.Store, window time.Duration, archival bool) error {\n    if availability.IsWithinWindow(eh.Time(), window) {\n        // ê°€ìš©ì„± ìœˆë„ìš° ë‚´ì˜ ë¸”ë¡ì€ ODSì™€ Q4 ëª¨ë‘ ì €ì¥\n        err = store.PutODSQ4(ctx, eh.DAH, eh.Height(), eds)\n    } else {\n        // ì˜¤ë˜ëœ ë¸”ë¡ì€ ODSë§Œ ì €ì¥\n        err = store.PutODS(ctx, eh.DAH, eh.Height(), eds)\n    }\n}\n```\n\n## 5. ì¥ì  ë° ì˜ì˜\n\n1. **í™•ì¥ì„± í–¥ìƒ**: ê²½ëŸ‰ ë…¸ë“œëŠ” ì „ì²´ ë¸”ë¡ì„ ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•Šê³ ë„ ë°ì´í„° ê°€ìš©ì„±ì„ ê²€ì¦í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n2. **ë³´ì•ˆ ìœ ì§€**: ì¶©ë¶„í•œ ìƒ˜í”Œë§ì„ í†µí•´ ë†’ì€ í™•ë¥ ë¡œ ë°ì´í„° ê°€ìš©ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.\n3. **ëª¨ë“ˆì‹ ì„¤ê³„**: ë°ì´í„° ê°€ìš©ì„± ë ˆì´ì–´ë¥¼ ì‹¤í–‰ ë ˆì´ì–´(execution layer)ì™€ ë¶„ë¦¬í•˜ì—¬ ëª¨ë“ˆì‹ ë¸”ë¡ì²´ì¸ ì„¤ê³„ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.\n4. **ë¡¤ì—… í™•ì¥**: Celestiaì˜ ë°ì´í„° ê°€ìš©ì„± ë ˆì´ì–´ëŠ” ë‹¤ì–‘í•œ ë¡¤ì—…(rollup)ì˜ ê¸°ë°˜ ë ˆì´ì–´ë¡œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nCelestiaì˜ ë°ì´í„° ê°€ìš©ì„± ì¦ëª… ë©”ì»¤ë‹ˆì¦˜ì€ ë¸”ë¡ì²´ì¸ì˜ \"ë°ì´í„° ê°€ìš©ì„± ë¬¸ì œ\"ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•´ê²°í•˜ì—¬, í™•ì¥ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©´ì„œë„ ë³´ì•ˆì„ ìœ ì§€í•˜ëŠ” í˜ì‹ ì ì¸ ì ‘ê·¼ ë°©ì‹ì…ë‹ˆë‹¤.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 3,
    "wordCount": 592,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "celestiaì˜-eds-extended-data-square",
    "slug": "celestiayi-eds-extended-data-square",
    "path": "blockchain/celestia",
    "fullPath": "blockchain/celestia/celestiayi-eds-extended-data-square",
    "title": "Celestiaì˜ EDS(Extended Data Square)",
    "excerpt": "EDS(Extended Data Square)ë€? EDSëŠ” Celestiaì˜ ë°ì´í„° ê°€ìš©ì„±(Data Availability) ë³´ì¥ ë©”ì»¤ë‹ˆì¦˜ì˜ í•µì‹¬ êµ¬ì¡°ì…ë‹ˆë‹¤. EDSëŠ” ì›ë³¸ ë°ì´í„°ë¥¼ 2D ì‚¬ê°í˜•ìœ¼ë¡œ êµ¬ì„±í•œ í›„, Reed-Solomon ì˜¤ë¥˜ ì •ì • ì½”ë”©ì„ ì ìš©í•˜ì—¬...",
    "content": "## EDS(Extended Data Square)ë€?\n\nEDSëŠ” Celestiaì˜ ë°ì´í„° ê°€ìš©ì„±(Data Availability) ë³´ì¥ ë©”ì»¤ë‹ˆì¦˜ì˜ í•µì‹¬ êµ¬ì¡°ì…ë‹ˆë‹¤. EDSëŠ” ì›ë³¸ ë°ì´í„°ë¥¼ 2D ì‚¬ê°í˜•ìœ¼ë¡œ êµ¬ì„±í•œ í›„, Reed-Solomon ì˜¤ë¥˜ ì •ì • ì½”ë”©ì„ ì ìš©í•˜ì—¬ í™•ì¥ëœ 2ì°¨ì› ë°ì´í„° êµ¬ì¡°ë¥¼ ë§í•©ë‹ˆë‹¤.\n\n```mermaid\ngraph LR\n    Q1[\"ğŸŸ¦ Q1: ODS<br/>ì›ë³¸ ë°ì´í„°\"] --- Q2[\"ğŸŸ© Q2: Row Parity<br/>í–‰ íŒ¨ë¦¬í‹°\"] --- Q3[\"ğŸŸ¨ Q3: Col Parity<br/>ì—´ íŒ¨ë¦¬í‹°\"] --- Q4[\"ğŸŸª Q4: Row+Col Parity<br/>í–‰+ì—´ íŒ¨ë¦¬í‹°\"]\n```\n\n> **Extended Data Square (EDS)**: ì›ë³¸ ë°ì´í„°(Q1)ë¥¼ Reed-Solomon ì¸ì½”ë”©í•˜ì—¬ í–‰/ì—´/ëŒ€ê°ì„  íŒ¨ë¦¬í‹°(Q2, Q3, Q4)ë¥¼ ìƒì„±í•œ 2D êµ¬ì¡°\n\n## EDSì˜ êµ¬ì¡°ì™€ íŠ¹ì§•\n\n1. **ì‚¬ë¶„ë©´ êµ¬ì¡°**:\n   - **Q1 (ì¢Œìƒë‹¨)**: ì›ë³¸ ë°ì´í„° ì‚¬ê°í˜•(ODS - Original Data Square)\n   - **Q2 (ìš°ìƒë‹¨)**: í–‰ íŒ¨ë¦¬í‹° ë°ì´í„°(Row Parity) - í–‰ ë‹¨ìœ„ ë³µêµ¬ ê°€ëŠ¥\n   - **Q3 (ì¢Œí•˜ë‹¨)**: ì—´ íŒ¨ë¦¬í‹° ë°ì´í„°(Column Parity) - ì—´ ë‹¨ìœ„ ë³µêµ¬ ê°€ëŠ¥\n   - **Q4 (ìš°í•˜ë‹¨)**: í–‰ê³¼ ì—´ íŒ¨ë¦¬í‹° ë°ì´í„°(Row+Column Parity) - íŠ¹ì • ë³µêµ¬ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ í•„ìš”\n\n2. **2D Reed-Solomon ì¸ì½”ë”©**:\n   - í–‰ê³¼ ì—´ ëª¨ë‘ì— ëŒ€í•´ Reed-Solomon ì¸ì½”ë”© ì ìš©\n   - 25% ì´ìƒì˜ ë°ì´í„°ê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì „ì²´ ë°ì´í„° ë³µêµ¬ ê°€ëŠ¥\n\n3. **ë°ì´í„° ë‹¨ìœ„**:\n   - **Share**: ê¸°ë³¸ ë°ì´í„° ë‹¨ìœ„. ê³ ì • í¬ê¸°ì˜ ë°”ì´íŠ¸ ë°°ì—´\n   - **Square**: nÃ—n í¬ê¸°ì˜ Share ë°°ì—´\n\n## EDS ê´€ë ¨ ì£¼ìš” ê¸°ëŠ¥\n\nì²¨ë¶€ëœ ì½”ë“œ ìŠ¤ë‹ˆí«ì—ì„œ ë³¼ ìˆ˜ ìˆëŠ” ì£¼ìš” ê¸°ëŠ¥ë“¤:\n\n### 1. EDS ìƒì„±\n\n```go\nfunc extendBlock(data *types.Data, appVersion uint64, options ...nmt.Option) (*rsmt2d.ExtendedDataSquare, error)\n```\n\n- ë¸”ë¡ ë°ì´í„°ë¥¼ EDSë¡œ í™•ì¥í•©ë‹ˆë‹¤.\n- íŠ¸ëœì­ì…˜ ë°ì´í„°ë¥¼ 2D ì‚¬ê°í˜•ìœ¼ë¡œ êµ¬ì„±í•œ í›„ Reed-Solomon ì¸ì½”ë”©ì„ ì ìš©í•©ë‹ˆë‹¤.\n\n```go\nfunc extendShares(s [][]byte, options ...nmt.Option) (*rsmt2d.ExtendedDataSquare, error)\n```\n\n- Shareì˜ ë°°ì—´ì„ EDSë¡œ í™•ì¥í•©ë‹ˆë‹¤.\n\n### 2. EDS ì €ì¥\n\n```go\nfunc storeEDS(\n    ctx context.Context,\n    eh *header.ExtendedHeader,\n    eds *rsmt2d.ExtendedDataSquare,\n    store *store.Store,\n    window time.Duration,\n    archival bool,\n) error\n```\n\n- EDSë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n- ê°€ìš©ì„± ìœˆë„ìš° ë‚´ì— ìˆìœ¼ë©´ ODSì™€ Q4 ëª¨ë‘ ì €ì¥(ODSQ4)\n- ê°€ìš©ì„± ìœˆë„ìš° ë°–ì— ìˆìœ¼ë©´ ODSë§Œ ì €ì¥\n\n## file íŒ¨í‚¤ì§€ì™€ EDSì˜ ê´€ê³„\n\nì•ì„œ ì„¤ëª…í•œ file íŒ¨í‚¤ì§€ëŠ” EDSì˜ ì €ì¥ ë° ì ‘ê·¼ ë©”ì»¤ë‹ˆì¦˜ì„ ì œê³µí•©ë‹ˆë‹¤:\n\n1. **ODS**: EDSì˜ Q1 ì‚¬ë¶„ë©´(ì›ë³¸ ë°ì´í„°)ë§Œ ì €ì¥í•˜ëŠ” ë°©ì‹\n2. **ODSQ4**: ODSì™€ Q4 ì‚¬ë¶„ë©´ì„ í•¨ê»˜ ì €ì¥í•˜ëŠ” ë°©ì‹\n   - ì¼ë¶€ ì¿¼ë¦¬ ë° ë°ì´í„° ë³µêµ¬ì— ë” íš¨ìœ¨ì \n   - ì €ì¥ ê³µê°„ì„ ë” í•„ìš”ë¡œ í•¨\n\n## EDSì˜ Celestiaì—ì„œì˜ ì—­í• \n\n1. **ë°ì´í„° ê°€ìš©ì„± ê²€ì¦**:\n   - ë…¸ë“œë“¤ì€ EDSì˜ ì„ì˜ ìƒ˜í”Œì„ ìš”ì²­í•˜ì—¬ ë°ì´í„° ê°€ìš©ì„± ê²€ì¦\n   - ì¶©ë¶„í•œ ìƒ˜í”Œì´ í™•ì¸ë˜ë©´ ì „ì²´ ë¸”ë¡ì´ ê°€ìš©í•˜ë‹¤ê³  íŒë‹¨\n\n2. **ë°ì´í„° ë³µêµ¬**:\n   - ì¼ë¶€ ë°ì´í„°ë§Œìœ¼ë¡œ ì „ì²´ ë°ì´í„° ë³µêµ¬ ê°€ëŠ¥\n   - ë„¤íŠ¸ì›Œí¬ íš¨ìœ¨ì„± í–¥ìƒ: ì „ì²´ ë¸”ë¡ì´ ì•„ë‹Œ ì¼ë¶€ë§Œ ë‹¤ìš´ë¡œë“œí•´ë„ ë¨\n\n3. **ì €ì¥ ìµœì í™”**:\n   - ëª¨ë“  ë…¸ë“œê°€ ì „ì²´ EDSë¥¼ ì €ì¥í•  í•„ìš” ì—†ìŒ\n   - ì¼ë°˜ ë…¸ë“œëŠ” ODSë§Œ ì €ì¥í•´ë„ ì¶©ë¶„\n   - ê°€ìš©ì„± ë³´ì¥ì´ í•„ìš”í•œ ìµœê·¼ ë¸”ë¡ë§Œ ODSQ4 í˜•íƒœë¡œ ì €ì¥\n\n## ì‹¤ì œ êµ¬í˜„ì—ì„œì˜ ê³ ë ¤ì‚¬í•­\n\n1. **ê°€ìš©ì„± ìœˆë„ìš°**:\n\n   ```go\n   if availability.IsWithinWindow(eh.Time(), availability.StorageWindow) {\n       err = store.PutODSQ4(ctx, eh.DAH, eh.Height(), eds)\n   } else {\n       err = store.PutODS(ctx, eh.DAH, eh.Height(), eds)\n   }\n   ```\n\n   - ìµœê·¼ ë¸”ë¡(ê°€ìš©ì„± ìœˆë„ìš° ë‚´)ì— ëŒ€í•´ì„œë§Œ ODSQ4 ë°©ì‹ìœ¼ë¡œ ì €ì¥\n   - ì˜¤ë˜ëœ ë¸”ë¡ì€ ODS ë°©ì‹ìœ¼ë¡œë§Œ ì €ì¥í•˜ì—¬ ë””ìŠ¤í¬ ê³µê°„ ì ˆì•½\n\n2. **ì„±ëŠ¥ ìµœì í™”**:\n   - ìºì‹± ë° ë²„í¼ë§ì„ í†µí•œ I/O ìµœì í™”\n   - Q4 ì§€ì—° ë¡œë”©(í•„ìš”í•  ë•Œë§Œ ë¡œë“œ)\n   - Reed-Solomon ì¸ì½”ë” ìºì‹±\n\nEDSëŠ” Celestiaì˜ ë°ì´í„° ê°€ìš©ì„± ë ˆì´ì–´ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œë¡œ, ë¸”ë¡ì²´ì¸ì˜ ë°ì´í„° í™•ì¥ì„±ê³¼ ê°€ìš©ì„±ì„ ë™ì‹œì— í•´ê²°í•˜ê¸° ìœ„í•œ ì¤‘ìš”í•œ êµ¬ì¡°ì…ë‹ˆë‹¤.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 3,
    "wordCount": 465,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "celestiaê°€-íŠ¸ëœì­ì…˜-ë°°ì—´ì„-odsë¡œ-ë³€í™˜í•˜ëŠ”-ë°©ë²•",
    "slug": "celestiaga-teuraenjaegsyeon-baeyeoleul-odsro-byeonhwanhaneun-bangbeob",
    "path": "blockchain/celestia",
    "fullPath": "blockchain/celestia/celestiaga-teuraenjaegsyeon-baeyeoleul-odsro-byeonhwanhaneun-bangbeob",
    "title": "Celestiaê°€ íŠ¸ëœì­ì…˜ ë°°ì—´ì„ ODSë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•",
    "excerpt": "í•µì‹¬ ê°œë… Share (ê³µê°„ë‹¨ìœ„) Celestiaì˜ ê¸°ë³¸ ë°ì´í„° ë‹¨ìœ„(512ë°”ì´íŠ¸) ë„¤ì„ìŠ¤í˜ì´ìŠ¤ IDë¥¼ í¬í•¨í•œ ì›ì‹œ ë°ì´í„° ì €ì¥ ê° ShareëŠ” íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ì†í•˜ë©°, ì •ë³´ ë°”ì´íŠ¸ì™€ ë²„ì „ì„ í¬í•¨ Square (ì‚¬ê°í˜•) ë°...",
    "content": "## 1. í•µì‹¬ ê°œë…\n\n### Share (ê³µê°„ë‹¨ìœ„)\n- Celestiaì˜ ê¸°ë³¸ ë°ì´í„° ë‹¨ìœ„(512ë°”ì´íŠ¸)\n- ë„¤ì„ìŠ¤í˜ì´ìŠ¤ IDë¥¼ í¬í•¨í•œ ì›ì‹œ ë°ì´í„° ì €ì¥\n- ê° ShareëŠ” íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ì†í•˜ë©°, ì •ë³´ ë°”ì´íŠ¸ì™€ ë²„ì „ì„ í¬í•¨\n\n### Square (ì‚¬ê°í˜•)\n- ë°ì´í„°ì˜ 2D ì •ì‚¬ê°í˜• ë°°ì—´ë¡œ, í•­ìƒ ë³€ì˜ ê¸¸ì´ê°€ 2ì˜ ì œê³±ìˆ˜\n- Shareë“¤ì˜ ì§‘í•©ìœ¼ë¡œ êµ¬ì„±\n- ìµœì¢…ì ì¸ ë°ì´í„° ê°€ìš©ì„± ê³„ì¸µì˜ êµ¬ì¡°\n\n### íŠ¸ëœì­ì…˜ ìœ í˜•\n- ì¼ë°˜ íŠ¸ëœì­ì…˜: ê¸°ë³¸ íŠ¸ëœì­ì…˜\n- PFB(Pay-for-Blob) íŠ¸ëœì­ì…˜: ë¸”ë¡­ ë°ì´í„°ë¥¼ í¬í•¨í•˜ê±°ë‚˜ ì°¸ì¡°í•˜ëŠ” íŠ¹ìˆ˜ íŠ¸ëœì­ì…˜\n\n## 2. ë³€í™˜ í”„ë¡œì„¸ìŠ¤\n\n### ì´ˆê¸°í™” (Builder ìƒì„±)\n```go\nbuilder, err := NewBuilder(maxSquareSize, subtreeRootThreshold)\n```\n- `maxSquareSize`: ìµœëŒ€ ì‚¬ê°í˜• í¬ê¸°(2ì˜ ì œê³±ìˆ˜)\n- `subtreeRootThreshold`: ì„œë¸ŒíŠ¸ë¦¬ ë£¨íŠ¸ ì„ê³„ê°’ ì„¤ì •\n\n### íŠ¸ëœì­ì…˜ ì²˜ë¦¬ ìˆœì„œ\n1. **íŠ¸ëœì­ì…˜ ë¶„ë¥˜**: ì…ë ¥ëœ íŠ¸ëœì­ì…˜ì„ ì¼ë°˜ íŠ¸ëœì­ì…˜ê³¼ blob íŠ¸ëœì­ì…˜ìœ¼ë¡œ ë¶„ë¥˜\n2. **íŠ¸ëœì­ì…˜ ë°°ì¹˜**: ì¼ë°˜ íŠ¸ëœì­ì…˜ì„ ë¨¼ì € ë°°ì¹˜í•œ í›„ PFB íŠ¸ëœì­ì…˜ ë°°ì¹˜\n\n### Share ë¶„í•  í”„ë¡œì„¸ìŠ¤\n1. **Compact Share ì‚¬ìš©**: ì¼ë°˜ íŠ¸ëœì­ì…˜ê³¼ PFB íŠ¸ëœì­ì…˜ì„ Compact Shareë¡œ ë³€í™˜\n   ```go\n   txWriter := share.NewCompactShareSplitter(share.TxNamespace, share.ShareVersionZero)\n   pfbWriter := share.NewCompactShareSplitter(share.PayForBlobNamespace, share.ShareVersionZero)\n   ```\n\n2. **Sparse Share ì‚¬ìš©**: Blob ë°ì´í„°ëŠ” Sparse Shareë¡œ ë³€í™˜\n   ```go\n   blobWriter := share.NewSparseShareSplitter()\n   ```\n\n3. **Blob ì •ë ¬ ë° ë°°ì¹˜**:\n   - ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ Blob ì •ë ¬\n   - Share commitment ê·œì¹™ì— ë§ê²Œ íŒ¨ë”© ì¶”ê°€\n   - ì²« Blobì˜ ì‹œì‘ ìœ„ì¹˜ê°€ nonReservedStart ê²°ì •\n\n4. **íŒ¨ë”© ì¶”ê°€**:\n   - ë„¤ì„ìŠ¤í˜ì´ìŠ¤ íŒ¨ë”©: íŠ¸ëœì­ì…˜ê³¼ PFB ì‚¬ì´\n   - í…Œì¼ íŒ¨ë”©: ì‚¬ê°í˜• í¬ê¸°ë¥¼ ë§ì¶”ê¸° ìœ„í•´ ì¶”ê°€\n\n### ì‚¬ê°í˜• ìƒì„±\n```go\nsquare, err := WriteSquare(txWriter, pfbWriter, blobWriter, nonReservedStart, squareSize)\n```\n\n1. **ì‚¬ê°í˜• í¬ê¸° ê²°ì •**:\n   - í•„ìš”í•œ ìµœì†Œ í¬ê¸° ê³„ì‚° (`inclusion.BlobMinSquareSize`)\n   - íŠ¸ëœì­ì…˜, PFB, ë¸”ë¡­ ë°ì´í„°ë¥¼ ëª¨ë‘ ìˆ˜ìš©í•  ìˆ˜ ìˆëŠ” 2ì˜ ì œê³±ìˆ˜ë¡œ í¬ê¸° ì„¤ì •\n\n2. **ë°ì´í„° ë°°ì¹˜**:\n   - ì¼ë°˜ íŠ¸ëœì­ì…˜ â†’ PFB íŠ¸ëœì­ì…˜ â†’ íŒ¨ë”© â†’ Blob ë°ì´í„° â†’ í…Œì¼ íŒ¨ë”© ìˆœìœ¼ë¡œ ë°°ì¹˜\n   - ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ë¡œ ë°ì´í„° êµ¬ë¶„ (TxNamespace, PayForBlobNamespace ë“±)\n\n## 3. ì£¼ìš” ìµœì í™” ë° íŠ¹ì§•\n\n### ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±\n- Share Counterë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ í¬ê¸° ê³„ì‚°\n- í•„ìš”í•œ ë§Œí¼ë§Œ íŒ¨ë”© ì¶”ê°€ë¡œ ê³µê°„ ìµœì í™”\n\n### ë„¤ì„ìŠ¤í˜ì´ìŠ¤ êµ¬ë¶„\n- ê° ë°ì´í„° ìœ í˜•ì— ë§ëŠ” ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í• ë‹¹\n- ì´ë¥¼ í†µí•´ ë°ì´í„° ìœ í˜•ë³„ ê²€ìƒ‰ ë° ì ‘ê·¼ ìš©ì´\n\n### ë¨¸í´ íŠ¸ë¦¬ ìµœì í™”\n- ì„œë¸ŒíŠ¸ë¦¬ ì„ê³„ê°’ ì„¤ì •ìœ¼ë¡œ ë¨¸í´ íŠ¸ë¦¬ êµ¬ì¡° ìµœì í™”\n- ë¸”ë¡­ ë°ì´í„° ìœ„ì¹˜ ì¡°ì •ìœ¼ë¡œ íš¨ìœ¨ì ì¸ ì¦ëª… ìƒì„±\n\n### Share commitment ê·œì¹™ \n- ê° Blobì˜ ì‹œì‘ ìœ„ì¹˜ëŠ” Share commitment ê·œì¹™ì— ë”°ë¼ ê²°ì •\n- ì´ëŠ” ë°ì´í„° ê°€ìš©ì„± ê²€ì¦ì„ ìœ„í•œ íš¨ìœ¨ì ì¸ ì¦ëª…ì„ ê°€ëŠ¥í•˜ê²Œ í•¨\n\n## ì„¸ë¶€ ìŠ¤í™ì— ëŒ€í•œ Note\n\n## 1. maxSquareSizeì™€ ì›ë³¸/íŒ¨ë¦¬í‹° share ê´€ê³„\n\n- **ODS(Original Data Square)**: ëª¨ë“  shareê°€ ì›ë³¸ ë°ì´í„°ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” íŠ¸ëœì­ì…˜, PFB, ë¸”ë¡­ ë°ì´í„°, ê·¸ë¦¬ê³  í•„ìš”í•œ íŒ¨ë”©ë§Œ í¬í•¨ë©ë‹ˆë‹¤. ì´ ë‹¨ê³„ì—ì„œëŠ” íŒ¨ë¦¬í‹° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n- **EDS(Extended Data Square)**: ODSë¥¼ 2D ë¦¬ë“œ-ì†”ë¡œëª¬ ì¸ì½”ë”©ìœ¼ë¡œ í™•ì¥í•œ ê²ƒìœ¼ë¡œ, ì´ ë•Œ íŒ¨ë¦¬í‹° ë°ì´í„°ê°€ ì¶”ê°€ë©ë‹ˆë‹¤. ì´ëŠ” ì½”ë“œë² ì´ìŠ¤ì˜ ë‹¤ë¥¸ ë¶€ë¶„ì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤.\n\nODSì˜ í¬ê¸°ê°€ 2^n x 2^nì¸ ì´ìœ ëŠ”:\n1. ë¨¸í´ íŠ¸ë¦¬ êµ¬ì„±ì˜ íš¨ìœ¨ì„± \n2. ë°ì´í„° ê°€ìš©ì„± ìƒ˜í”Œë§(DAS)ì˜ íš¨ìœ¨ì„±\n3. 2D ë¦¬ë“œ-ì†”ë¡œëª¬ ì¸ì½”ë”©ì˜ ìš”êµ¬ì‚¬í•­\n\n## 2. share í¬ê¸°ì™€ square í¬ê¸° ì œí•œ\n\n- **share í¬ê¸°**: ê° shareëŠ” 512ë°”ì´íŠ¸ë¡œ ê³ ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì½”ë“œì—ì„œ `ShareSize` ìƒìˆ˜ë¡œ ì •ì˜ë©ë‹ˆë‹¤.\n\n- **square í¬ê¸° ì œí•œ**: ì´ë¡ ì ìœ¼ë¡œëŠ” 2ì˜ ì œê³±ìˆ˜ë¡œ ë¬´í•œíˆ ì»¤ì§ˆ ìˆ˜ ìˆì§€ë§Œ, ì‹¤ì œë¡œëŠ” ì œí•œì´ ìˆìŠµë‹ˆë‹¤. ì½”ë“œë¥¼ ë³´ë©´:\n\n  ```go\n  // worstCaseShareIndexes í•¨ìˆ˜ì—ì„œ\n  squareSizeUpperBound := 128\n  worstCaseShareIndex := squareSizeUpperBound * squareSizeUpperBound\n  ```\n\n  ì´ ë¶€ë¶„ì—ì„œ ìµœëŒ€ square í¬ê¸°ë¥¼ 128x128(16,384 shares)ë¡œ ì œí•œí•˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” Celestia-app v1.xì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ê²ƒì…ë‹ˆë‹¤.\n\n  ë˜í•œ ê° ë…¸ë“œì˜ ë©”ëª¨ë¦¬ ì œí•œ, ë„¤íŠ¸ì›Œí¬ ì²˜ë¦¬ëŸ‰, ë¸”ë¡ ì‹œê°„ ë“± ì‹¤ìš©ì ì¸ ì œì•½ ìš”ì†Œë“¤ë„ ìµœëŒ€ square í¬ê¸°ë¥¼ ì œí•œí•©ë‹ˆë‹¤. 128x128 squareëŠ” ì•½ **8MB**(512ë°”ì´íŠ¸ * 16,384)ì˜ ë°ì´í„°ë¥¼ ë‹´ì„ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” í˜„ì¬ êµ¬í˜„ì˜ ì‹¤ìš©ì ì¸ ìƒí•œì„ ì…ë‹ˆë‹¤.\n\nê²°ë¡ ì ìœ¼ë¡œ, share í¬ê¸°ëŠ” ê³ ì •ë˜ì–´ ìˆê³  square í¬ê¸°ëŠ” ì´ë¡ ì ìœ¼ë¡œëŠ” 2ì˜ ì œê³±ìˆ˜ë¡œ í™•ì¥ ê°€ëŠ¥í•˜ì§€ë§Œ ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì œí•œì´ ìˆìŠµë‹ˆë‹¤.\n\nRef: https://github.com/celestiaorg/go-square",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 3,
    "wordCount": 541,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "celestia-ë°ì´í„°-ê°€ìš©ì„±-ìƒ˜í”Œë§-das-ë¶„ì„",
    "slug": "celestia-deiteo-gayongseong-saempeulring-das-bunseog",
    "path": "blockchain/celestia",
    "fullPath": "blockchain/celestia/celestia-deiteo-gayongseong-saempeulring-das-bunseog",
    "title": "Celestia ë°ì´í„° ê°€ìš©ì„± ìƒ˜í”Œë§(DAS) ë¶„ì„",
    "excerpt": "ë°ì´í„° ê°€ìš©ì„± ìƒ˜í”Œë§(DAS)ì—ì„œ ìƒ˜í”Œ ìˆ˜ëŸ‰ ê²°ì • ë°©ì‹ ë¼ì´íŠ¸ ë…¸ë“œì˜ ìƒ˜í”Œë§ ì„¤ì • share/availability/light/options.go íŒŒì¼ì—ì„œ, ë¼ì´íŠ¸ ë…¸ë“œì˜ ê¸°ë³¸ ìƒ˜í”Œ ìˆ˜ëŸ‰ì´ 16ê°œë¡œ ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ``` // D...",
    "content": "**ë°ì´í„° ê°€ìš©ì„± ìƒ˜í”Œë§(DAS)ì—ì„œ ìƒ˜í”Œ ìˆ˜ëŸ‰ ê²°ì • ë°©ì‹**\n\n1. **ë¼ì´íŠ¸ ë…¸ë“œì˜ ìƒ˜í”Œë§ ì„¤ì •**\n\n- share/availability/light/options.go íŒŒì¼ì—ì„œ, ë¼ì´íŠ¸ ë…¸ë“œì˜ ê¸°ë³¸ ìƒ˜í”Œ ìˆ˜ëŸ‰ì´ **16ê°œ**ë¡œ ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n\n```\n// DefaultSampleAmountëŠ” ë¼ì´íŠ¸ ë…¸ë“œê°€ ë¸”ë¡ì´ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤ê³  ì„ ì–¸í•˜ê¸° ì „ì— ìˆ˜í–‰í•´ì•¼ í•˜ëŠ” ìµœì†Œ ìƒ˜í”Œ ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\nvar (\n    DefaultSampleAmount uint = 16\n)\n```\n\n1. **DAS ê´€ë ¨ ì£¼ìš” ë§¤ê°œë³€ìˆ˜**\n\n- das/options.go íŒŒì¼ì˜ DefaultParameters() í•¨ìˆ˜ì—ì„œ DASì˜ ê¸°ë³¸ ì„¤ì •ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```\n// DefaultParametersëŠ” daser(ìƒ˜í”Œë§ ëª¨ë“ˆ)ì˜ ê¸°ë³¸ ì„¤ì •ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\nfunc DefaultParameters() Parameters {\n    concurrencyLimit := 16\n    return Parameters{\n        SamplingRange: 100,\n        ConcurrencyLimit: concurrencyLimit,\n        BackgroundStoreInterval: 10 * time.Minute,\n        SampleFrom: 1,\n        SampleTimeout: 15 * time.Second * time.Duration(concurrencyLimit),\n    }\n}\n```\n\nì—¬ê¸°ì„œ SamplingRange: 100ì€ í•œ ë²ˆì˜ ìƒ˜í”Œë§ ì‘ì—…ì—ì„œ **ìµœëŒ€ 100ê°œì˜ í—¤ë”ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒì„ ì˜ë¯¸**í•˜ì§€ë§Œ, ë¸”ë¡ë‹¹ ìƒ˜í”Œ ìˆ˜ëŸ‰ì„ ì§ì ‘ì ìœ¼ë¡œ ì •ì˜í•˜ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤.\n\n1. **ë¬´ì‘ìœ„ ìƒ˜í”Œë§ êµ¬í˜„ ë°©ì‹**\n\n- share/availability/light/sample.go íŒŒì¼ì˜ selectRandomSamples í•¨ìˆ˜ëŠ” ë°ì´í„° ì •ì‚¬ê°í˜•(squares)ì—ì„œ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œì„ ì„ íƒí•˜ëŠ” ë°©ì‹ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n\n```\n// selectRandomSamplesëŠ” ì£¼ì–´ì§„ í¬ê¸°ì˜ ì •ì‚¬ê°í˜•ì—ì„œ ë¬´ì‘ìœ„ë¡œ ê³ ìœ í•œ ì¢Œí‘œë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\nfunc selectRandomSamples(squareSize, sampleCount int) []shwap.SampleCoords {\n    total := squareSize * squareSize\n    if sampleCount > total {\n        sampleCount = total\n    }\n    \n    samples := make(map[shwap.SampleCoords]struct{}, sampleCount)\n    for len(samples) < sampleCount {\n        s := shwap.SampleCoords{\n            Row: randInt(squareSize),\n            Col: randInt(squareSize),\n        }\n        samples[s] = struct{}{}\n    }\n    return slices.Collect(maps.Keys(samples))\n}\n```\n\nìœ„ ì½”ë“œì—ì„œ **ìƒ˜í”Œë§ ê°œìˆ˜ëŠ” ë¸”ë¡ ë°ì´í„°ì˜ ì •ì‚¬ê°í˜• í¬ê¸°ì— ë”°ë¼ ë‹¤ë¥´ë©°**, ì£¼ì–´ì§„ ê°œìˆ˜ë§Œí¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒë©ë‹ˆë‹¤.\n\n1. **ë¼ì´íŠ¸ ë…¸ë“œì˜ ë°ì´í„° ê°€ìš©ì„± í™•ì¸ ê³¼ì •**\n\n- share/availability/light/availability.go íŒŒì¼ì˜ SharesAvailable ë©”ì„œë“œë¥¼ ì‚´í´ë³´ë©´, íŠ¹ì • ë¸”ë¡ í—¤ë”ì— ëŒ€í•œ ê¸°ì¡´ ìƒ˜í”Œë§ ê²°ê³¼ê°€ ì—†ì„ ê²½ìš° ìƒˆë¡œìš´ ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```\nsamples = NewSamplingResult(len(dah.RowRoots), int(la.params.SampleAmount))\n```\n\nì—¬ê¸°ì„œ dah.RowRootsì˜ ê¸¸ì´(ì¦‰, ë°ì´í„° ì •ì‚¬ê°í˜•ì˜ í¬ê¸°)ì™€ **ì„¤ì •ëœ ìƒ˜í”Œ ìˆ˜ëŸ‰(ê¸°ë³¸ê°’: 16ê°œ)** ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒ˜í”Œë§ì´ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.\n\n---\n\n**16KB ë¸”ë¡ì˜ ìƒ˜í”Œ ìˆ˜ëŸ‰ ë¶„ì„**\n\nCelestiaì—ì„œ **16KB ë¸”ë¡ì˜ ìƒ˜í”Œë§ ë°©ì‹**ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n1. **ë¼ì´íŠ¸ ë…¸ë“œì˜ ê¸°ë³¸ ìƒ˜í”Œ ìˆ˜ëŸ‰**\n- ê¸°ë³¸ì ìœ¼ë¡œ ë¼ì´íŠ¸ ë…¸ë“œëŠ” **ë¸”ë¡ í¬ê¸°ì— ìƒê´€ì—†ì´ 16ê°œì˜ ìƒ˜í”Œì„ ì‚¬ìš©**í•©ë‹ˆë‹¤.\n\n1. **16KB ë¸”ë¡ì˜ ê²½ìš° ìƒ˜í”Œë§ ê³¼ì •**\n\n- 16KB ë¸”ë¡ì€ ì£¼ì–´ì§„ ë¸”ë¡ í¬ê¸°ì— ë”°ë¼ ì •ì‚¬ê°í˜• í˜•íƒœë¡œ ë³€í™˜ë©ë‹ˆë‹¤.\n- ë³´í†µ **4Ã—4(16ê°œ)ì˜ ê³µìœ  ë°ì´í„°(share)ë¡œ ë‚˜ë‰˜ì–´ ì €ì¥**ë©ë‹ˆë‹¤.\n- ë¼ì´íŠ¸ ë…¸ë“œëŠ” ì´ **16ê°œì˜ ê³µìœ  ë°ì´í„° ì¤‘ 16ê°œë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ì—¬ ìƒ˜í”Œë§**í•©ë‹ˆë‹¤.\n- ì´ë ‡ê²Œ ì„ íƒëœ ìƒ˜í”Œì„ í†µí•´ ë¸”ë¡ì´ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì œëŒ€ë¡œ ì „íŒŒë˜ì—ˆëŠ”ì§€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n\n1. **í’€ ë…¸ë“œ(Full Node) vs. ë¼ì´íŠ¸ ë…¸ë“œ(Light Node)**\n\n- share/doc.go íŒŒì¼ì— ë”°ë¥´ë©´, ë¼ì´íŠ¸ ë…¸ë“œëŠ” **16ê°œì˜ ìƒ˜í”Œë§Œ ì‚¬ìš©í•˜ì—¬ ê°€ìš©ì„±ì„ í™•ì¸**í•˜ì§€ë§Œ, í’€ ë…¸ë“œëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ê°€ìš©ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.\n\n```\n// Light Availability êµ¬í˜„ì€ ë¸”ë¡ ë°ì´í„°ì˜ 16ê°œ ìƒ˜í”Œì„ í™•ì¸í•˜ì—¬ ê°€ìš©ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤.\n// Full Availability êµ¬í˜„ì€ ë°ì´í„°ë¥¼ ì™„ì „íˆ ë³µêµ¬í•  ìˆ˜ ìˆì„ ë§Œí¼ì˜ ìƒ˜í”Œì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\n```\n\n---\n\n**ê²°ë¡ : ìƒ˜í”Œ ìˆ˜ëŸ‰ ë¶„ì„ ìš”ì•½**\n\n1. **ë¼ì´íŠ¸ ë…¸ë“œì˜ ê¸°ë³¸ ìƒ˜í”Œ ìˆ˜ëŸ‰ì€ 16ê°œ**\n\n- share/availability/light/options.goì—ì„œ ê¸°ë³¸ê°’ìœ¼ë¡œ 16ê°œ ì„¤ì •\n- ë¸”ë¡ í¬ê¸°ì™€ ìƒê´€ì—†ì´ ë™ì¼í•œ ìˆ˜ëŸ‰ìœ¼ë¡œ ìƒ˜í”Œë§ ì§„í–‰\n\n1. **16KB ë¸”ë¡ì—ì„œëŠ” ì „ì²´ ë°ì´í„° ê³µìœ  ê°œìˆ˜ê°€ 16ê°œì´ë¯€ë¡œ, ì „ì²´ë¥¼ ìƒ˜í”Œë§**\n\n- ë¸”ë¡ì´ 4Ã—4 ì •ì‚¬ê°í˜•ìœ¼ë¡œ ë³€í™˜ë˜ë©°, 16ê°œ ì¤‘ 16ê°œë¥¼ ìƒ˜í”Œë§\n- 16ê°œë§Œ ìƒ˜í”Œë§í•´ë„ ì¶©ë¶„í•œ í™•ë¥ ë¡œ ê°€ìš©ì„±ì„ ê²€ì¦í•  ìˆ˜ ìˆìŒ\n\nğŸ‘‰ **ê²°ë¡ ì ìœ¼ë¡œ, Celestiaì˜ ë¼ì´íŠ¸ ë…¸ë“œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë¸”ë¡ë‹¹ 16ê°œì˜ ìƒ˜í”Œì„ ì¶”ì¶œí•˜ì—¬ ë°ì´í„° ê°€ìš©ì„±ì„ ê²€ì¦í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.** ğŸš€\n\n### ë°ì´í„° ê°€ìš©ì„± ìƒ˜í”Œë§ì˜ í™•ë¥ ì  ë³´ì¥\n\nCelestiaì˜ ë°ì´í„° ê°€ìš©ì„± ìƒ˜í”Œë§(DAS)ì—ì„œ 16ê°œì˜ ìƒ˜í”Œì€ ë‹¤ìŒê³¼ ê°™ì€ í™•ë¥ ì  ë³´ì¥ì„ ì œê³µí•©ë‹ˆë‹¤:\n\n#### ì´ë¡ ì  ë°°ê²½\n\në°ì´í„° ê°€ìš©ì„± ìƒ˜í”Œë§ì€ ì •ë³´ ì´ë¡ ê³¼ í™•ë¥ ë¡ ì— ê¸°ë°˜í•©ë‹ˆë‹¤. íŠ¹íˆ \"ìƒ˜í”Œë§ ê²Œì„\" ì´ë¡ ì— ë”°ë¥´ë©´:\n\n- ë¸”ë¡ ìƒì„±ìê°€ ë°ì´í„°ì˜ ì¼ë¶€(ì˜ˆ: 50%)ë¥¼ ìˆ¨ê¸°ë©´, ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ ê° ìƒ˜í”Œì´ ê·¸ ìˆ¨ê²¨ì§„ ë¶€ë¶„ì„ ë°œê²¬í•  í™•ë¥ ì€ 50%ì…ë‹ˆë‹¤.\n    \n- 16ê°œì˜ ë…ë¦½ì ì¸ ìƒ˜í”Œì„ ì„ íƒí•  ê²½ìš°, ìˆ¨ê²¨ì§„ ë°ì´í„°ë¥¼ í•˜ë‚˜ë„ ë°œê²¬í•˜ì§€ ëª»í•  í™•ë¥ ì€ $(0.5)^{16} = 0.0000152587890625$, ì¦‰ ì•½ 0.0015%ì…ë‹ˆë‹¤.\n    \n- ì´ëŠ” ë°˜ëŒ€ë¡œ ë§í•˜ë©´, ë°ì´í„°ì˜ 50%ê°€ ëˆ„ë½ëœ ê²½ìš° 16ê°œì˜ ìƒ˜í”Œë¡œ 99.9985%ì˜ í™•ë¥ ë¡œ ìµœì†Œ í•˜ë‚˜ ì´ìƒì˜ ë¬¸ì œë¥¼ ë°œê²¬í•  ìˆ˜ ìˆë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.\n    \n\n#### ìƒ˜í”Œ ìˆ˜ì™€ ë³´ì•ˆì„± ì‚¬ì´ì˜ ê´€ê³„\n\nì‹¤ì œ ì½”ë“œì—ì„œ DefaultSampleAmountê°€ 16ìœ¼ë¡œ ì„¤ì •ëœ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n1. **ê³µê²©ìì˜ ë°ì´í„° ì€ë‹‰ ë¹„ìœ¨ì— ë”°ë¥¸ íƒì§€ í™•ë¥ **:\n    \n    - 25% ë°ì´í„° ì€ë‹‰: 16ê°œ ìƒ˜í”Œë¡œ ì•½ 99.9999% íƒì§€ í™•ë¥ \n        \n    - 10% ë°ì´í„° ì€ë‹‰: 16ê°œ ìƒ˜í”Œë¡œ ì•½ 82% íƒì§€ í™•ë¥ \n        \n2. **íš¨ìœ¨ì„±ê³¼ ë³´ì•ˆì„±ì˜ ê· í˜•**:  \n    ìƒ˜í”Œ ìˆ˜ê°€ ì¦ê°€í• ìˆ˜ë¡ ë³´ì•ˆì„±ì€ í–¥ìƒë˜ì§€ë§Œ, ë„¤íŠ¸ì›Œí¬ ë¶€í•˜ì™€ ì²˜ë¦¬ ì‹œê°„ë„ ì¦ê°€í•©ë‹ˆë‹¤. 16ê°œì˜ ìƒ˜í”Œì€ ë‹¤ìŒì„ ê³ ë ¤í•œ ê· í˜•ì ì…ë‹ˆë‹¤:\n    \n    - ë„¤íŠ¸ì›Œí¬ ë¶€í•˜ ìµœì†Œí™”\n        \n    - ì¶©ë¶„í•œ ë³´ì•ˆ ë ˆë²¨ ì œê³µ\n        \n    - ê²½ëŸ‰ ë…¸ë“œì— ì í•©í•œ ê³„ì‚° ë¶€ë‹´\n        \n3. **ë„¤íŠ¸ì›Œí¬ ì „ì²´ì˜ ì§‘ë‹¨ì  ê²€ì¦**:  \n    ê°œë³„ ë…¸ë“œëŠ” 16ê°œ ìƒ˜í”Œë§Œ ê²€ì‚¬í•˜ì§€ë§Œ, ìˆ˜ì²œ ê°œì˜ ë…¸ë“œê°€ ê°ê° ë‹¤ë¥¸ ë¬´ì‘ìœ„ ìƒ˜í”Œì„ ê²€ì‚¬í•¨ìœ¼ë¡œì¨ ë„¤íŠ¸ì›Œí¬ ì „ì²´ì ìœ¼ë¡œëŠ” ëª¨ë“  ë°ì´í„°ê°€ ê²€ì¦ë©ë‹ˆë‹¤.\n    \n\n#### ì‹¤ì œ ë³´ì•ˆì„±\n\nì‹¤ì œ Celestia ë„¤íŠ¸ì›Œí¬ì—ì„œëŠ” ê³µê²©ìê°€ ë°ì´í„°ì˜ ì‘ì€ ë¶€ë¶„ë§Œ ìˆ¨ê¸°ë”ë¼ë„, ì¶©ë¶„í•œ ìˆ˜ì˜ ë…¸ë“œê°€ ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•˜ë©´ ë†’ì€ í™•ë¥ ë¡œ íƒì§€ë©ë‹ˆë‹¤. ì—¬ëŸ¬ ë…¸ë“œì—ì„œ ê°ê° 16ê°œì˜ ìƒ˜í”Œì„ ì·¨í•˜ë©´, ë„¤íŠ¸ì›Œí¬ ì „ì²´ì ìœ¼ë¡œëŠ” í›¨ì”¬ ë” ë†’ì€ ë³´ì•ˆì„±ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.\n\në”°ë¼ì„œ 16ê°œì˜ ìƒ˜í”Œì€ ë‹¨ì¼ ë¼ì´íŠ¸ ë…¸ë“œì—ì„œ ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±ê³¼ ì ì ˆí•œ ë³´ì•ˆì„±ì„ ê· í˜• ìˆê²Œ ì œê³µí•˜ëŠ” ìˆ˜ì¹˜ì´ë©°, ë„¤íŠ¸ì›Œí¬ ì „ì²´ì ìœ¼ë¡œëŠ” ë§¤ìš° ë†’ì€ ìˆ˜ì¤€ì˜ ë°ì´í„° ê°€ìš©ì„± ë³´ì¥ì„ ì œê³µí•©ë‹ˆë‹¤.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 4,
    "wordCount": 692,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "celestia-node-file-íŒ¨í‚¤ì§€-ë¶„ì„",
    "slug": "celestia-node-file-paekiji-bunseog",
    "path": "blockchain/celestia",
    "fullPath": "blockchain/celestia/celestia-node-file-paekiji-bunseog",
    "title": "celestia-node 'file' íŒ¨í‚¤ì§€ ë¶„ì„",
    "excerpt": "celestia-node 'file' íŒ¨í‚¤ì§€ ë¶„ì„ Celestia-node repositoryì˜ íŒ¨í‚¤ì§€ëŠ” Exten Data Square(EDS)ë¼ëŠ” ë°ì´í„° êµ¬ì¡°ë¥¼ íŒŒì¼ ì‹œìŠ¤í…œì— íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ì ‘ê·¼í•˜ê¸° ìœ„í•œ êµ¬í˜„ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ íŒ¨í‚¤...",
    "content": "# celestia-node 'file' íŒ¨í‚¤ì§€ ë¶„ì„\n\nCelestia-node repositoryì˜ `store/file` íŒ¨í‚¤ì§€ëŠ” Exten Data Square(EDS)ë¼ëŠ” ë°ì´í„° êµ¬ì¡°ë¥¼ íŒŒì¼ ì‹œìŠ¤í…œì— íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ì ‘ê·¼í•˜ê¸° ìœ„í•œ êµ¬í˜„ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ íŒ¨í‚¤ì§€ëŠ” ë°ì´í„°ì˜ ë¬´ê²°ì„±ê³¼ ë³µêµ¬ ê°€ëŠ¥ì„±ì„ ë³´ì¥í•˜ë©´ì„œë„ ë””ìŠ¤í¬ ê³µê°„ì„ ìµœì í™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n## í•µì‹¬ ê°œë…\n\n1. **Original Data Square (ODS)**: ë¸”ë¡ì²´ì¸ì˜ íŠ¸ëœì­ì…˜ ë°ì´í„°ë¥¼ ì •ì‚¬ê°í˜• í˜•íƒœë¡œ ë°°ì—´í•œ ê²ƒìœ¼ë¡œ, EDSì˜ ì²« ë²ˆì§¸ ì‚¬ë¶„ë©´(Q1)ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n2. **Extended Data Square (EDS)**: ODSì— Reed-Solomon ì¸ì½”ë”©ì„ ì ìš©í•˜ì—¬ í™•ì¥ëœ 2D ë°ì´í„° êµ¬ì¡°ì…ë‹ˆë‹¤. ì´ 4ê°œì˜ ì‚¬ë¶„ë©´ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.\n3. **Q4**: EDSì˜ ë„¤ ë²ˆì§¸ ì‚¬ë¶„ë©´ìœ¼ë¡œ, í–‰ê³¼ ì—´ ëª¨ë‘ì— íŒ¨ë¦¬í‹°ê°€ ì ìš©ëœ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n4. **Share**: ë°ì´í„°ì˜ ê¸°ë³¸ ë‹¨ìœ„ë¡œ, ê³ ì • í¬ê¸°ì˜ ë°”ì´íŠ¸ ë°°ì—´ì…ë‹ˆë‹¤.\n\n## ì €ì¥ ì „ëµê³¼ ê³µê°„ íš¨ìœ¨ì„±\n\nCelestiaëŠ” ì €ì¥ ê³µê°„ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë‘ ê°€ì§€ ì €ì¥ ì „ëµì„ ì‚¬ìš©í•©ë‹ˆë‹¤:\n\n1. **ODSë§Œ ì €ì¥**: íŠ¸ëœì­ì…˜ ë°ì´í„°(ODS)ë§Œ ì €ì¥í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, EDS ì „ì²´ í¬ê¸°ì˜ ì•½ 1/4ì— í•´ë‹¹í•˜ëŠ” ì €ì¥ ê³µê°„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì˜¤ë˜ëœ ë¸”ë¡ì´ë‚˜ ì•„ì¹´ì´ë¸Œ ëª©ì ì˜ ì €ì¥ì— ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n2. **ODSQ4 ì €ì¥**: ODSì™€ Q4 ì‚¬ë¶„ë©´ì„ í•¨ê»˜ ì €ì¥í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, EDS ì „ì²´ í¬ê¸°ì˜ ì•½ 1/2ì— í•´ë‹¹í•˜ëŠ” ì €ì¥ ê³µê°„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê°€ìš©ì„± ìœˆë„ìš°(availability window) ë‚´ì˜ ìµœê·¼ ë¸”ë¡ì— ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ë°ì´í„° ì ‘ê·¼ íŒ¨í„´ì—ì„œ ê³„ì‚° íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.\n\nê°€ìš©ì„± ìœˆë„ìš°ëŠ” ë°ì´í„° ê°€ìš©ì„± ê²€ì¦ì´ ë” ì¤‘ìš”í•œ ìµœê·¼ ë¸”ë¡ë“¤ì„ êµ¬ë¶„í•˜ëŠ” ì‹œê°„ì  ê²½ê³„ë¡œ, ì´ ê¸°ê°„ ë‚´ì˜ ë¸”ë¡ì€ ODSQ4 ë°©ì‹ìœ¼ë¡œ, ê·¸ ì™¸ì˜ ë¸”ë¡ì€ ODS ë°©ì‹ìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤.\n\n## í•µì‹¬ êµ¬ì„±ìš”ì†Œì™€ ê¸°ëŠ¥\n\në‹¤ìŒì€ íŒ¨í‚¤ì§€ì˜ ì£¼ìš” êµ¬ì¡°ì²´ì™€ ê·¸ ê´€ê³„ë¥¼ mermaid ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤:\n\n```mermaid\nclassDiagram\n    class ODS {\n        -headerV0 hdr\n        -os.File fl\n        -sync.RWMutex lock\n        -square ods\n        -bool disableCache\n        +Size(context.Context) int\n        +DataHash(context.Context) (share.DataHash, error)\n        +AxisRoots(context.Context) (*share.AxisRoots, error)\n        +Close() error\n        +Sample(context.Context, shwap.SampleCoords) (shwap.Sample, error)\n        +AxisHalf(context.Context, rsmt2d.Axis, int) (eds.AxisHalf, error)\n        +RowNamespaceData(context.Context, libshare.Namespace, int) (shwap.RowNamespaceData, error)\n        +Shares(context.Context) ([]libshare.Share, error)\n        +Reader() (io.Reader, error)\n    }\n\n    class q4 {\n        -headerV0 hdr\n        -os.File file\n        +close() error\n        +axisHalf(rsmt2d.Axis, int) (eds.AxisHalf, error)\n    }\n\n    class ODSQ4 {\n        -ODS ods\n        -string pathQ4\n        -sync.Mutex q4Mu\n        -atomic.Bool q4OpenAttempted\n        -q4 q4\n        +Size(context.Context) int\n        +DataHash(context.Context) (share.DataHash, error)\n        +AxisRoots(context.Context) (*share.AxisRoots, error)\n        +Sample(context.Context, shwap.SampleCoords) (shwap.Sample, error)\n        +AxisHalf(context.Context, rsmt2d.Axis, int) (eds.AxisHalf, error)\n        +RowNamespaceData(context.Context, libshare.Namespace, int) (shwap.RowNamespaceData, error)\n        +Shares(context.Context) ([]libshare.Share, error)\n        +Reader() (io.Reader, error)\n        +Close() error\n    }\n\n    class square {\n        +reader() (io.Reader, error)\n        +size() int\n        +shares() ([]libshare.Share, error)\n        +axisHalf(rsmt2d.Axis, int) (eds.AxisHalf, error)\n        +computeAxisHalf(rsmt2d.Axis, int) (eds.AxisHalf, error)\n    }\n\n    class headerV0 {\n        -fileVersion fileVersion\n        -uint16 shareSize\n        -uint16 squareSize\n        -share.DataHash datahash\n        +SquareSize() int\n        +ShareSize() int\n        +Size() int\n        +RootsSize() int\n        +OffsetWithRoots() int\n        +WriteTo(io.Writer) (int64, error)\n        +ReadFrom(io.Reader) (int64, error)\n    }\n\n    class Codec {\n        <<interface>>\n        +Encoder(int) (reedsolomon.Encoder, error)\n    }\n\n    class codecCache {\n        -sync.Map cache\n        +Encoder(int) (reedsolomon.Encoder, error)\n    }\n\n    ODS --* headerV0 : contains\n    ODS --* square : caches\n    q4 --* headerV0 : contains\n    ODSQ4 --* ODS : contains\n    ODSQ4 --* q4 : contains\n    codecCache ..|> Codec : implements\n```\n\n## ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ìƒì„¸ ì„¤ëª…\n\n### 1. headerV0 (header.go)\n- íŒŒì¼ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” êµ¬ì¡°ì²´ì…ë‹ˆë‹¤.\n- íŒŒì¼ ë²„ì „, share í¬ê¸°, square í¬ê¸°, ë°ì´í„° í•´ì‹œ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤.\n- íŒŒì¼ ì½ê¸°/ì“°ê¸° ì‘ì—…ì— í•„ìš”í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ì œê³µí•˜ë©°, íŒŒì¼ í˜•ì‹ì˜ ë²„ì „ ê´€ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.\n\n### 2. ODS (ods.go)\n- Original Data Square(ODS)ë¥¼ íŒŒì¼ ì‹œìŠ¤í…œì— ì €ì¥í•˜ê³  ì ‘ê·¼í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n- EDSì˜ ì²« ë²ˆì§¸ ì‚¬ë¶„ë©´(Q1)ì„ ì €ì¥í•˜ë©°, íŒŒì¼ í—¤ë”ì— ë©”íƒ€ë°ì´í„°ë¥¼ í•¨ê»˜ ì €ì¥í•©ë‹ˆë‹¤.\n- ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë©”ëª¨ë¦¬ ìºì‹± ê¸°ëŠ¥ì„ ì œê³µí•˜ì—¬ ë°˜ë³µì ì¸ íŒŒì¼ I/Oë¥¼ ì¤„ì…ë‹ˆë‹¤.\n- `eds.AccessorStreamer` ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•˜ì—¬ í‘œì¤€í™”ëœ ë°ì´í„° ì ‘ê·¼ ë°©ì‹ì„ ì œê³µí•©ë‹ˆë‹¤.\n\n### 3. q4 (q4.go)\n- EDSì˜ ë„¤ ë²ˆì§¸ ì‚¬ë¶„ë©´(Q4)ì„ íŒŒì¼ ì‹œìŠ¤í…œì— ì €ì¥í•˜ê³  ì ‘ê·¼í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n- Q4ëŠ” ë°ì´í„° ë³µêµ¬ë‚˜ íŠ¹ì • ìœ í˜•ì˜ ì¿¼ë¦¬(íŠ¹íˆ Q2, Q4 ì‚¬ë¶„ë©´ ìƒ˜í”Œë§)ì— íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©ë©ë‹ˆë‹¤.\n- íŒŒì¼ì˜ í¬ê¸° ê²€ì¦ ë° ì˜¤ë¥˜ ì²˜ë¦¬ ê¸°ëŠ¥ì„ í¬í•¨í•©ë‹ˆë‹¤.\n\n### 4. ODSQ4 (ods_q4.go)\n- ODSì™€ Q4ë¥¼ ê²°í•©í•˜ì—¬ íš¨ìœ¨ì ì¸ ë°ì´í„° ì ‘ê·¼ì„ ì œê³µí•©ë‹ˆë‹¤.\n- ì²« ë²ˆì§¸ ìš”ì²­ ì‹œ Q4 íŒŒì¼ì„ ì§€ì—° ë¡œë”©(lazy loading)í•˜ì—¬ ì´ˆê¸° ë¡œë”© ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœì í™”í•©ë‹ˆë‹¤.\n- ìƒ˜í”Œë§ ì‘ì—…ì´ë‚˜ íŠ¹ì • í–‰/ì—´ ì ‘ê·¼ ì‹œ ê³„ì‚° íš¨ìœ¨ì„±ì„ ë†’ì—¬ì¤ë‹ˆë‹¤.\n- `eds.AccessorStreamer` ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•˜ì—¬ ODSì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¡œ ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\n### 5. square (square.go)\n- ë°ì´í„° squareë¥¼ í‘œí˜„í•˜ëŠ” 2ì°¨ì› ë°°ì—´ êµ¬ì¡°ì²´ì…ë‹ˆë‹¤.\n- shareì˜ í–‰ê³¼ ì—´ì— ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ë©”ì†Œë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n- í•„ìš”í•œ ë°ì´í„°ë¥¼ ê³„ì‚°í•˜ê±°ë‚˜ ë³µêµ¬í•˜ëŠ” ê¸°ëŠ¥ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n- ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•´ ë°ì´í„° ë³µêµ¬ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.\n\n### 6. Codec/codecCache (codec.go)\n- Reed-Solomon ì¸ì½”ë”©ì„ ìœ„í•œ ì¸í„°í˜ì´ìŠ¤ì™€ êµ¬í˜„ì„ ì œê³µí•©ë‹ˆë‹¤.\n- `reedsolomon.New(ln/2, ln/2, reedsolomon.WithLeopardGF(true))` í˜•íƒœë¡œ ì¸ì½”ë”ë¥¼ ìƒì„±í•˜ì—¬ ë°ì´í„°ì˜ ì•½ 25%ë§Œìœ¼ë¡œë„ ì „ì²´ ë³µêµ¬ê°€ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.\n- ì¸ì½”ë”ë¥¼ ìºì‹±í•˜ì—¬ ë°˜ë³µì ì¸ ìƒì„± ë¹„ìš©ì„ ì¤„ì´ê³  ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.\n\n## í•µì‹¬ ê¸°ëŠ¥\n\n1. **ë°ì´í„° ì €ì¥**: EDSì˜ ì¼ë¶€(ODSì™€ ì„ íƒì ìœ¼ë¡œ Q4)ë¥¼ íŒŒì¼ ì‹œìŠ¤í…œì— íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n2. **ë°ì´í„° ì ‘ê·¼**: ì €ì¥ëœ ë°ì´í„°ì— ë‹¤ì–‘í•œ ë°©ì‹(í–‰, ì—´, ìƒ˜í”Œ ë“±)ìœ¼ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” APIë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n3. **ë°ì´í„° ë³µêµ¬**: Reed-Solomon ì¸ì½”ë”©ì„ ì‚¬ìš©í•˜ì—¬ í•„ìš”í•œ ê²½ìš° ëˆ„ë½ëœ ë°ì´í„°ë¥¼ ë³µêµ¬í•  ìˆ˜ ìˆìœ¼ë©°, EDS ì „ì²´ì˜ ì•½ 25%ë§Œìœ¼ë¡œë„ ë³µêµ¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n4. **ìµœì í™”ëœ I/O**: ë²„í¼ë§, ìºì‹±, ì§€ì—° ë¡œë”© ë“±ì„ í†µí•´ íŒŒì¼ I/O ì‘ì—…ì„ ìµœì í™”í•©ë‹ˆë‹¤.\n5. **ë¬´ê²°ì„± ê²€ì¦**: ë°ì´í„° í•´ì‹œì™€ í¬ê¸° ê²€ì¦ì„ í†µí•´ íŒŒì¼ì˜ ë¬´ê²°ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.\n\n## ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ\n\nCelestia ë„¤íŠ¸ì›Œí¬ì—ì„œ ì´ íŒ¨í‚¤ì§€ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìƒí™©ì—ì„œ í™œìš©ë©ë‹ˆë‹¤:\n\n1. **ë¸”ë¡ ë°ì´í„° ì €ì¥**: ìƒˆë¡œìš´ ë¸”ë¡ì´ ê²€ì¦ë˜ë©´, ë…¸ë“œëŠ” ë¸”ë¡ ë°ì´í„°ë¥¼ EDSë¡œ í™•ì¥í•œ í›„ ì €ì¥ ì „ëµì— ë”°ë¼ ODS ë˜ëŠ” ODSQ4 í˜•íƒœë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n\n   ```go\n   // ê°€ìš©ì„± ìœˆë„ìš° ë‚´ì˜ ë¸”ë¡ì€ ODSì™€ Q4 ëª¨ë‘ ì €ì¥\n   if availability.IsWithinWindow(eh.Time(), availability.StorageWindow) {\n       err = store.PutODSQ4(ctx, eh.DAH, eh.Height(), eds)\n   } else {\n       // ê·¸ ì™¸ ë¸”ë¡ì€ ODSë§Œ ì €ì¥í•˜ì—¬ ê³µê°„ ì ˆì•½\n       err = store.PutODS(ctx, eh.DAH, eh.Height(), eds)\n   }\n   ```\n\n2. **ë°ì´í„° ê°€ìš©ì„± ìƒ˜í”Œë§(DAS)**: ë‹¤ë¥¸ ë…¸ë“œë¡œë¶€í„° ë¸”ë¡ ë°ì´í„°ì˜ íŠ¹ì • ìƒ˜í”Œì„ ìš”ì²­ë°›ìœ¼ë©´, ì €ì¥ëœ ODS ë˜ëŠ” ODSQ4ë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì²­ëœ ìƒ˜í”Œì„ íš¨ìœ¨ì ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.\n\n3. **ë°ì´í„° ë³µêµ¬**: ëˆ„ë½ëœ ë¸”ë¡ ë°ì´í„°ê°€ ìˆì„ ê²½ìš°, ë„¤íŠ¸ì›Œí¬ì˜ ë‹¤ë¥¸ ë…¸ë“œë¡œë¶€í„° ì¶©ë¶„í•œ ìƒ˜í”Œì„ ìˆ˜ì§‘í•˜ì—¬ Reed-Solomon ì¸ì½”ë”©ì„ í†µí•´ ì „ì²´ EDSë¥¼ ë³µêµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì´ íŒ¨í‚¤ì§€ëŠ” Celestiaì˜ ë°ì´í„° ê°€ìš©ì„± ë ˆì´ì–´ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œë¡œ, ë¸”ë¡ì²´ì¸ì˜ ë°ì´í„° í™•ì¥ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ë©´ì„œë„ ë°ì´í„°ì˜ ê°€ìš©ì„±ê³¼ ë¬´ê²°ì„±ì„ ë³´ì¥í•˜ëŠ” ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 5,
    "wordCount": 847,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ë¹„íŠ¸ì½”ì¸-nftì™€-brc-20-ft",
    "slug": "biteukoin-nftwa-brc-20-ft",
    "path": "blockchain/bitcoin",
    "fullPath": "blockchain/bitcoin/biteukoin-nftwa-brc-20-ft",
    "title": "ë¹„íŠ¸ì½”ì¸ NFTì™€ BRC-20(FT)",
    "excerpt": "ë¹„íŠ¸ì½”ì¸ NFTì™€ BRC-20(FT) ì—°ì´ˆì— ë¹„íŠ¸ì½”ì¸ì—ì„œ ë°œí–‰ì´ ê°€ëŠ¥í•´ì¡Œë‹¤ëŠ” ë‰´ìŠ¤ê°€ ë‚˜ì™”ë‹¤. ìµœê·¼ì—ëŠ” íŠ¸ëœì­ì…˜ ê¸‰ì¦ìœ¼ë¡œ ë¹„íŠ¸ì½”ì¸ ë„¤íŠ¸ì›Œí¬ê°€ í˜¼ì¡í•´ì§€ê³  ìˆë‹¤. ë³¸ ê¸€ì—ì„œëŠ” ë‹¨ìˆœí–ˆë˜ ë¹„íŠ¸ì½”ì¸ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì–´ë–»ê²Œ ì´ë”ë¦¬ì›€ê°™ì€ , `NF...",
    "content": "# ë¹„íŠ¸ì½”ì¸ NFTì™€ BRC-20(FT)\n\nì—°ì´ˆì— ë¹„íŠ¸ì½”ì¸ì—ì„œ `NFT` ë°œí–‰ì´ ê°€ëŠ¥í•´ì¡Œë‹¤ëŠ” ë‰´ìŠ¤ê°€ ë‚˜ì™”ë‹¤. ìµœê·¼ì—ëŠ” `brc-20` íŠ¸ëœì­ì…˜ ê¸‰ì¦ìœ¼ë¡œ ë¹„íŠ¸ì½”ì¸ ë„¤íŠ¸ì›Œí¬ê°€ í˜¼ì¡í•´ì§€ê³  ìˆë‹¤.\në³¸ ê¸€ì—ì„œëŠ” ë‹¨ìˆœí–ˆë˜ ë¹„íŠ¸ì½”ì¸ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì–´ë–»ê²Œ ì´ë”ë¦¬ì›€ê°™ì€ `FT`, `NFT`ë¥¼ ë°œí–‰í•  ìˆ˜ ìˆì—ˆëŠ”ì§€ ì •ë¦¬í•œë‹¤.\n\n## ë¹„íŠ¸ì½”ì¸ì—ì„œì˜ Ordinals Protocal (ì„œìˆ˜ì²´ê³„) ë„ì…ê³¼ ë¹„íŠ¸ì½”ì¸ NFTì˜ ì¶œí˜„\n\n[Ordinal Numbers](https://github.com/casey/ord/blob/master/bip.mediawiki)ì˜ ì œì•ˆ ë™ê¸°ëŠ” ë¹„íŠ¸ì½”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ìš©ë  ìˆ˜ë„ ìˆëŠ” ì•ˆì •ì ì¸ ì‹ë³„ìë¥¼ ì œê³µí•˜ê¸° ìœ„í•¨ì´ë¼ê³  í•˜ëŠ”ë°, ì´ëŠ” ë¹„íŠ¸ì½”ì¸ ì£¼ì†Œ ì²´ê³„ ìƒ ì•ˆì •ì ìœ¼ë¡œ ì‚¬ìš©í•  ë§Œí•œ ê³µê°œ ì‹ ì› ì •ë³´ê°€ ì—†ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n\nì„œìˆ˜ì²´ê³„ ë„ì…ì„ ìœ„í•´ ëª¨ë“  ì‚¬í† ì‹œë¥¼ ì±„êµ´ëœ ìˆœì„œë¡œ ì¼ë ¨ë²ˆí˜¸ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ì´ ìˆ«ìë¥¼ ì„œìˆ˜, `ordinal number`ë¡œ ë¶€ë¦…ë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì´ ì½”ë“œëŠ” ë¸”ë¡ì²´ì¸ì—ì„œ ìƒˆ ë¸”ë¡ì˜ ì±„êµ´ ë³´ìƒì„ ê³„ì‚°í•˜ê³ , í•´ë‹¹ ë¸”ë¡ì— ì†í•œ íŠ¸ëœì­ì…˜ì˜ ì¶œë ¥ê°’ì— ì¼ë ¨ë²ˆí˜¸(ordinal)ë¥¼ ë¶€ì—¬í•˜ëŠ” ê¸°ëŠ¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n\n```python\n# subsidy í•¨ìˆ˜ëŠ” ë¸”ë¡ì˜ ë†’ì´(height)ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ê·¸ ë†’ì´ì— í•´ë‹¹í•˜ëŠ” ë¸”ë¡ì˜ ì±„êµ´ ë³´ìƒì„ ê³„ì‚°í•©ë‹ˆë‹¤. \n# ë¹„íŠ¸ì½”ì¸ì˜ ê²½ìš°, ì±„êµ´ ë³´ìƒì€ 50 BTCë¡œ ì‹œì‘í•˜ì—¬ 210,000 ë¸”ë¡ë§ˆë‹¤ ì ˆë°˜ìœ¼ë¡œ ì¤„ì–´ë“¤ê¸° ë•Œë¬¸ì—, ì´ í•¨ìˆ˜ëŠ” ë†’ì´ì— ë”°ë¥¸ ë³´ìƒì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì´ëŸ¬í•œ ìˆ˜ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\ndef subsidy(height):\n  return 50 * 100_000_000 >> height // 210_000\n\n# first_ordinal í•¨ìˆ˜ëŠ” ë¸”ë¡ì˜ ë†’ì´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ í•´ë‹¹ ë†’ì´ì˜ ì²« ë²ˆì§¸ íŠ¸ëœì­ì…˜ ì¶œë ¥ê°’ì— í• ë‹¹ë  ì¼ë ¨ë²ˆí˜¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. \n# ì´ í•¨ìˆ˜ëŠ” ì´ì „ ë¸”ë¡ì˜ ëª¨ë“  ì¶œë ¥ê°’ì— ëŒ€í•œ ì¼ë ¨ë²ˆí˜¸ë¥¼ ë”í•˜ê³  ë¸”ë¡ì˜ ì±„êµ´ ë³´ìƒì„ ë”í•˜ì—¬ êµ¬í•©ë‹ˆë‹¤.\ndef first_ordinal(height):\n  start = 0\n  for height in range(height):\n    start += subsidy(height)\n  return start\n\n# assign_ordinals í•¨ìˆ˜ëŠ” ë¸”ë¡ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„, ê° íŠ¸ëœì­ì…˜ ì¶œë ¥ê°’ì— ì¼ë ¨ë²ˆí˜¸ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤. \n# ì´ í•¨ìˆ˜ëŠ” ë¨¼ì € ë¸”ë¡ ì±„êµ´ ë³´ìƒì— ëŒ€í•œ ì¼ë ¨ë²ˆí˜¸ë¥¼ ê³„ì‚°í•˜ê³ , ì´ì–´ì„œ ë¸”ë¡ ë‚´ì˜ ëª¨ë“  íŠ¸ëœì­ì…˜ì— ëŒ€í•´ ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. \n# ê° íŠ¸ëœì­ì…˜ì˜ ì…ë ¥ê°’ì—ì„œ ì¼ë ¨ë²ˆí˜¸ë¥¼ ê°€ì ¸ì™€ ì¶œë ¥ê°’ì— í• ë‹¹í•˜ê³ , ì¶œë ¥ê°’ì— ì¼ë ¨ë²ˆí˜¸ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤. \n# ë§ˆì§€ë§‰ìœ¼ë¡œ, ì±„êµ´ ë³´ìƒ ì¶œë ¥ê°’ì— ì¼ë ¨ë²ˆí˜¸ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤. \n# ì´ëŸ¬í•œ ê³¼ì •ì„ í†µí•´ ë¸”ë¡ ë‚´ì˜ ëª¨ë“  ì¶œë ¥ê°’ì— ì¼ë ¨ë²ˆí˜¸ê°€ ë¶€ì—¬ë©ë‹ˆë‹¤.\ndef assign_ordinals(block):\n  first = first_ordinal(block.height)\n  last = first + subsidy(block.height)\n  coinbase_ordinals = list(range(first, last))\n\n  for transaction in block.transactions[1:]:\n    ordinals = []\n    for input in transaction.inputs:\n      ordinals.extend(input.ordinals)\n\n    for output in transaction.outputs:\n      output.ordinals = ordinals[:output.value]\n      del ordinals[:output.value]\n\n    coinbase_ordinals.extend(ordinals)\n\n  for output in block.transaction[0].outputs:\n    output.ordinals = coinbase_ordinals[:output.value]\n    del coinbase_ordinals[:output.value]\n```\n\n<br/>\n\nì—¬ê¸°ì— ë”í•´ inscriptionì´ ì ìš©ë˜ë©´ì„œ ordinal inscriptionì€ NFTì™€ ìœ ì‚¬í•œ ë””ì§€í„¸ ìì‚°ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ NFTëŠ” ìŠ¤ë§ˆíŠ¸ì²´ì¸ ë˜ëŠ” ì‚¬ì´ë“œì²´ì¸ì˜ ì»¨íŠ¸ë™íŠ¸ë¥¼ í†µí•´ í˜¸ìŠ¤íŒ… ë˜ì—ˆì§€ë§Œ, ë¹„íŠ¸ì½”ì¸ NFTëŠ” ì‚¬í† ì‹œì— ë°”ì¸ë”© ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ì‚¬ì´ë“œ ì²´ì¸ì´ë‚˜ ë³„ë„ì˜ í† í°ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\nOrdinalsëŠ” ë‹¨ìˆœíˆ ê°€ì¹˜ ì´ì „ë§Œ ì œê³µí•˜ë˜ ë¹„íŠ¸ì½”ì¸ ë„¤íŠ¸ì›Œí¬ì˜ ìƒˆë¡œìš´ ì‚¬ìš© ì‚¬ë¡€ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ìµœê·¼ íŠ¸ëœì­ì…˜ì´ ëŠ˜ì–´ë‚˜ë©´ì„œ ë„¤íŠ¸ì›Œí¬ ë¹„ìš©ì´ ì¦ê°€í•˜ê³  ê¸°ì¡´ ë¹„íŠ¸ì½”ì¸ì˜ ë‹¨ìˆœì„±ì„ í›¼ì†í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆëƒëŠ” ì§€ì ì´ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ Ordinalsì˜ ì§€ì§€ìë“¤ì€ ë¯¸ë˜ì— ë¸”ë¡ ë³´ìƒì´ ê°ì†Œí•˜ë”ë¼ë„ ë„¤íŠ¸ì›Œí¬ ìˆ˜ìˆ˜ë£Œê°€ ë¹„íŠ¸ì½”ì¸ì— ëŒ€í•œ í•´ì‹œ íŒŒì›Œë¥¼ ì•½ì†í•˜ëŠ” ì£¼ìš” ì¸ì„¼í‹°ë¸Œê°€ ë  ìˆ˜ ìˆë‹¤ê³  ì£¼ì¥í•©ë‹ˆë‹¤.\n\n## BRC-20ì˜ ë“±ì¥\n\nì•ì„œ ì„¤ëª…í•œ ì„œìˆ˜ì˜ ë“±ì¥ê³¼ íƒ­ë£¨íŠ¸ ì—…ê·¸ë ˆì´ë“œë¥¼ í†µí•´ì„œ ë¹„íŠ¸ì½”ì¸ ë¸”ë¡ì²´ì¸ì—ì„œ NFT ë°œí–‰ì´ ê°€ëŠ¥í•´ì¡Œê³ , ìµœê·¼ ì´ë¥¼ ì´ìš©í•œ BRC-20ì´ ë“±ì¥í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. BRC-20ì„ ì œì•ˆí•œ `domo`ê°€ ë°í˜”ë“¯ì´, BRC-20ì€ inscriptionsë¥¼ í™œìš©í•œ ì‹¤í—˜ì´ë©° ì•„ì§ ì§„ì§€í•˜ê²Œ ë°›ì•„ë“¤ì¼ ë§Œí•œ ê³µì‹ í‘œì¤€ì´ ì•„ë‹™ë‹ˆë‹¤. BRC-20ì˜ ê¸°ë³¸ì ì¸ ëª©ì ì€ ì„œìˆ˜ ì´ë¡ ì„ í†µí•´ ë¹„íŠ¸ì½”ì¸ì— fungibilityë¥¼ ë„ì…í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ë‹¤ìŒ ë¬¸ì„œ](https://domo-2.gitbook.io/brc-20-experiment/)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## ì°¸ê³ \n\n1. [ë¹„íŠ¸ì½”ì¸ ì„œìˆ˜ì™€ NFT - ë°”ì´ë‚¸ìŠ¤ ì•„ì¹´ë°ë¯¸](https://academy.binance.com/en/articles/what-are-ordinals-an-overview-of-bitcoin-nfts)\n2. [BRC-20 - domo](https://domo-2.gitbook.io/brc-20-experiment/)\n3. [BRC-20 í† í° - ë°”ì´ë‚¸ìŠ¤ ì•„ì¹´ë°ë¯¸](https://academy.binance.com/cs/glossary/brc-20-tokens)\n4. [Ordinal Numbers bip.memiawiki](https://github.com/casey/ord/blob/master/bip.mediawiki)\n5. [Ordinal Theory document](https://docs.ordinals.com/overview.html)\n6. [Bitcoin Taproot - ë°”ì´ë‚¸ìŠ¤ ì•„ì¹´ë°ë¯¸](https://academy.binance.com/en/articles/what-is-taproot-and-how-it-will-benefit-bitcoin)",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 3,
    "wordCount": 472,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "spv-simplified-payment-verification",
    "slug": "spv-simplified-payment-verification",
    "path": "blockchain/bitcoin",
    "fullPath": "blockchain/bitcoin/spv-simplified-payment-verification",
    "title": "SPV (Simplified Payment Verification)",
    "excerpt": "SPV (Simplified Payment Verification) SPVë€? ê±°ë˜ì— ëŒ€í•œ ëª¨ë“  ë¸”ë¡ì²´ì¸ì„ ì €ì¥í•˜ì§€ ì•Šê³ ë„ íŠ¸ëœì­ì…˜ì„ ê²€ì¦í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë¼ì´íŠ¸ ì›¨ì´íŠ¸ ë…¸ë“œ ë˜ëŠ” ê²½ëŸ‰ë…¸ë“œë¼ê³ ë„ ë¶ˆë¦½ë‹ˆë‹¤. íŠ¹ì§• ë¸”ë¡ì²´ì¸ì˜ ì‚¬ë³¸ì„ ë³´ê´€í•˜ì§€ ì•Š...",
    "content": "# SPV (Simplified Payment Verification)\n\n## SPVë€?\n\nê±°ë˜ì— ëŒ€í•œ ëª¨ë“  ë¸”ë¡ì²´ì¸ì„ ì €ì¥í•˜ì§€ ì•Šê³ ë„ íŠ¸ëœì­ì…˜ì„ ê²€ì¦í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë¼ì´íŠ¸ ì›¨ì´íŠ¸ ë…¸ë“œ ë˜ëŠ” ê²½ëŸ‰ë…¸ë“œë¼ê³ ë„ ë¶ˆë¦½ë‹ˆë‹¤.\n\n## íŠ¹ì§•\n\n-   ë¸”ë¡ì²´ì¸ì˜ ì‚¬ë³¸ì„ ë³´ê´€í•˜ì§€ ì•Šê³  íŠ¸ëœì­ì…˜ ê²€ì¦ê³¼ì •ì—ë„ ì°¸ì—¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆì— ê¸°ì—¬í•˜ì§€ ì•ŠëŠ”ë‹¤\n-   ê·¸ëŸ¬ë¯€ë¡œ ë‹¤ë¥¸ í’€ë…¸ë“œ ì •ë³´ì— ì˜ì¡´í•˜ì—¬ ê±°ë˜ë¥¼ ì§„í–‰í•œë‹¤\n-   ë¸”ë¡ í—¤ë” êµ¬ì„±\n    -   ë²„ì „: 4ë°”ì´íŠ¸\n    -   ì´ì „ ë¸”ë¡í•´ì‹œ : 32ë°”ì´íŠ¸\n    -   ë¨¸í´ë£¨íŠ¸ í•´ì‹œ : 32ë°”ì´íŠ¸\n    -   ë¸”ë¡ ì‹œê°„ : 4ë°”ì´íŠ¸\n    -   ë¹„ì¸  : 4ë°”ì´íŠ¸\n    -   ë…¼ìŠ¤ê°’ : 4ë°”ì´íŠ¸ë¡œ êµ¬ì„±ë˜ì–´ìˆìœ¼ë©°, ì´ 80ë°”ì´íŠ¸ë¡œ, 1ë…„ ë™ì•ˆ ë°œìƒí•˜ëŠ” 52,560ê°œì˜ ë¸”ë¡ í—¤ë” ìš©ëŸ‰ì´ 4MB ì •ë„ì´ë‹ˆ í˜„ì¬ 150GBë¥¼ ë„˜ê¸´ í’€ ë…¸ë“œì— ë¹„í•´ ë§¤ìš° ê°€ë³ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.\n\n## ì›ë¦¬\n\n![merkle-root](https://miro.medium.com/v2/resize:fit:720/format:webp/0*ZLrIO_B67108JStC.jpg)\n\nê±°ë˜3ê°€ ë¸”ë¡ì— ì‹¤ë ¸ëŠ”ì§€ í™•ì¸í•˜ê³  ì‹¶ë‹¤ë©´, ë¸”ë¡ ìƒì„± ì‹œë§ˆë‹¤ í•´ë‹¹ ë¸”ë¡ì˜ ë¨¸í´ë£¨íŠ¸ íšë“ì— í•„ìš”í•œ í•´ì‹œë“¤ë§Œ ê°€ì ¸ì˜¤ë©´ ëœë‹¤.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 118,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "bitcoin-segwit",
    "slug": "bitcoin-segwit",
    "path": "blockchain/bitcoin",
    "fullPath": "blockchain/bitcoin/bitcoin-segwit",
    "title": "Bitcoin Segwit",
    "excerpt": "Bitcoin Segwit ë¹„íŠ¸ì½”ì¸ì—ì„œ, ê±°ë˜ëŠ” ì…ë ¥(inputs)ê³¼ ì¶œë ¥(outputs)ì„ í¬í•¨í•©ë‹ˆë‹¤. íŠ¸ëœì­ì…˜ì´ ë„¤íŠ¸ì›Œí¬ë¡œ ì „ì†¡ë  ë•Œ ë…¸ë“œì—ì„œ íŠ¸ëœì­ì…˜ì˜ ìœ íš¨ì„±ì„ ê²€ì‚¬í•˜ì—¬ íŠ¸ëœì­ì…˜ì´ í•©ë²•ì ì´ê³  ì…ë ¥ì´ ì´ì¤‘ìœ¼ë¡œ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. ì›ë˜ ë¹„íŠ¸ì½”ì¸ í”„...",
    "content": "# Bitcoin Segwit\n\në¹„íŠ¸ì½”ì¸ì—ì„œ, ê±°ë˜ëŠ” ì…ë ¥(inputs)ê³¼ ì¶œë ¥(outputs)ì„ í¬í•¨í•©ë‹ˆë‹¤. íŠ¸ëœì­ì…˜ì´ ë„¤íŠ¸ì›Œí¬ë¡œ ì „ì†¡ë  ë•Œ ë…¸ë“œì—ì„œ íŠ¸ëœì­ì…˜ì˜ ìœ íš¨ì„±ì„ ê²€ì‚¬í•˜ì—¬ íŠ¸ëœì­ì…˜ì´ í•©ë²•ì ì´ê³  ì…ë ¥ì´ ì´ì¤‘ìœ¼ë¡œ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. ì›ë˜ ë¹„íŠ¸ì½”ì¸ í”„ë¡œí† ì½œì—ì„œëŠ” ë””ì§€í„¸ ì„œëª…(ì¦ì¸ ë°ì´í„°)ì´ íŠ¸ëœì­ì…˜ ì…ë ¥ì— í¬í•¨ë˜ì–´ íŠ¸ëœì­ì…˜ì´ ì»¤ì§€ê³  ë„¤íŠ¸ì›Œí¬ ì²˜ë¦¬ëŸ‰ì´ ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 2017ë…„ì— ë¶„ë¦¬ ì¦ì¸(SegWit)ì´ë¼ëŠ” ìƒˆë¡œìš´ ê±°ë˜ í˜•ì‹ì´ ë„ì…ë˜ì—ˆìŠµë‹ˆë‹¤. SegWitëŠ” ê±°ë˜ì˜ ì…ë ¥ì—ì„œ ì¦ì¸ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ì—¬ ì¦ì¸ì´ë¼ëŠ” ê±°ë˜ì˜ ë³„ë„ ì„¹ì…˜ì— ë°°ì¹˜í•©ë‹ˆë‹¤. SegWit ì£¼ì†ŒëŠ” SegWit íŠ¸ëœì­ì…˜ í˜•ì‹ê³¼ í˜¸í™˜ë˜ëŠ” ë¹„íŠ¸ì½”ì¸ ì£¼ì†Œì…ë‹ˆë‹¤. SegWit ì£¼ì†Œë¡œ ìê¸ˆì„ ë³´ë‚¼ ë•Œ ìê¸ˆì˜ ì†Œìœ ê¶Œì„ ì¦ëª…í•˜ëŠ” ì¦ì¸ì„ ê±°ë˜ì— í¬í•¨ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°ì‹œìì™€ íŠ¸ëœì­ì…˜ì„ ë§Œë“¤ë ¤ë©´ SegWit ì£¼ì†Œë¥¼ íŠ¸ëœì­ì…˜ì˜ ì¶œë ¥ ì£¼ì†Œë¡œ ì‚¬ìš©í•´ì•¼ í•˜ë©°, íŠ¸ëœì­ì…˜ì— ì„œëª…í•  ë•Œ ê°ì‹œì ë°ì´í„°ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. SegWit íŠ¸ëœì­ì…˜ì˜ script_pubkeyì—ëŠ” ìê¸ˆì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì¡°ê±´ì„ ì§€ì •í•˜ëŠ” ê°ì‹œ í”„ë¡œê·¸ë¨ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©°, SegWit ì£¼ì†Œë¥¼ ë§Œë“œëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n\n```rust\nuse bitcoin::{\n    blockdata::transaction::{Transaction, TxIn, TxOut},\n    network::constants::Network,\n    util::bip143::{SigHashCache, SigHashType},\n    PublicKey,\n    Script,\n    TxBuilder,\n    TxInWitness,\n};\n\nfn create_transaction() -> Transaction {\n    // Create inputs\n    let txin = TxIn {\n        previous_output: bitcoin::OutPoint::new(\n            bitcoin::hash::Sha256dHash::default(),\n            0,\n        ),\n        script_sig: Script::new(),\n        sequence: 0xFFFFFFFF,\n        witness: TxInWitness::default(),\n    };\n\n    // Create outputs\n    let txout = TxOut {\n        value: 1000000, // Satoshis\n        script_pubkey: Script::new(),\n    };\n\n    // Create transaction\n    let mut tx_builder = TxBuilder::new();\n    let tx = tx_builder\n        .add_input(txin)\n        .add_output(txout)\n        .build()\n        .unwrap();\n\n    // Sign transaction\n    let key = bitcoin::PrivateKey::from_wif(\"cTJtmP6oZmnBp9jWsnJzBvTuwT54qfN3rq8WxjZgCCzU6DDHMC6N\").unwrap();\n    let public_key = PublicKey::from_private_key(&key, true).unwrap();\n    let sig_hash = SigHashCache::new(&tx).signature_hash(0, &Script::new(), 1000000, SigHashType::All);\n    let signature = key.sign(&sig_hash);\n    let mut witness = TxInWitness::default();\n    witness.push(signature.serialize_der().to_vec());\n    witness.push(public_key.to_bytes());\n    tx_builder.set_witness(0, witness);\n\n    tx\n}\n\nfn main() {\n    let tx = create_transaction();\n    println!(\"{}\", tx);\n}\n```\n\n```toml\n[dependencies]\nbitcoin = \"0.28.1\"\nsecp256k1 = \"0.17.0\"\nhex = \"0.4.2\"\n```\n\nì´ ì˜ˆì—ì„œëŠ” ë¨¼ì € í•˜ë‚˜ì˜ ì…ë ¥ê³¼ í•˜ë‚˜ì˜ ì¶œë ¥ìœ¼ë¡œ íŠ¸ëœì­ì…˜ì„ ë§Œë“­ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ê°œì¸ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¸ëœì­ì…˜ì— ì„œëª…í•˜ê³  ë””ì§€í„¸ ì„œëª…ê³¼ ê³µê°œ í‚¤ë¥¼ í¬í•¨í•˜ëŠ” ì¦ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ TxBuilderì˜ set_witness ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¸ëœì­ì…˜ì— ëŒ€í•œ ê°ì‹œë¥¼ ì„¤ì •í•˜ê³  íŠ¸ëœì­ì…˜ì„ ì½˜ì†”ì— ì¸ì‡„í•©ë‹ˆë‹¤.\n\nì´ ì˜ˆì—ì„œëŠ” íŠ¸ëœì­ì…˜ì— ë”ë¯¸ ì…ë ¥ ë° ì¶œë ¥ì„ ì‚¬ìš©í•˜ê³  ì„œëª…ì— ì„ì˜ë¡œ ìƒì„±ëœ ê°œì¸ í‚¤ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì‹¤ì œ ì‘ìš© í”„ë¡œê·¸ë¨ì—ì„œëŠ” ì´ ê°’ì„ ì‚¬ìš© ì‚¬ë¡€ì˜ ì‹¤ì œ ê°’ìœ¼ë¡œ ëŒ€ì²´í•´ì•¼ í•©ë‹ˆë‹¤.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 2,
    "wordCount": 303,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "bitcoin-script",
    "slug": "bitcoin-script",
    "path": "blockchain/bitcoin",
    "fullPath": "blockchain/bitcoin/bitcoin-script",
    "title": "Bitcoin Script",
    "excerpt": "Bitcoin Script ë¹„íŠ¸ì½”ì¸ì€ íŠ¸ëœì­ì…˜ì— ìŠ¤í¬ë¦½íŒ… ì‹œìŠ¤í…œì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¨ìˆœí•˜ê³ , ìŠ¤íƒ ê¸°ë°˜ì´ë©°, ì¢Œì—ì„œ ìš°ë¡œ ì§„í–‰í•©ë‹ˆë‹¤. ì´ê²ƒì€ ì˜ë„ì ìœ¼ë¡œ íŠœë§ì™„ì „í•˜ì§€ ì•Šìœ¼ë©°, ë£¨í”„ê°€ í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ëŠ” ë³¸ì§ˆì ìœ¼ë¡œ ê° íŠ¸ëœì­ì…˜ì— ê¸°ë¡ëœ ì§€ì‹œì–´ë“¤ì˜...",
    "content": "# Bitcoin Script\n\në¹„íŠ¸ì½”ì¸ì€ íŠ¸ëœì­ì…˜ì— ìŠ¤í¬ë¦½íŒ… ì‹œìŠ¤í…œì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¨ìˆœí•˜ê³ , ìŠ¤íƒ ê¸°ë°˜ì´ë©°, ì¢Œì—ì„œ ìš°ë¡œ ì§„í–‰í•©ë‹ˆë‹¤. ì´ê²ƒì€ ì˜ë„ì ìœ¼ë¡œ íŠœë§ì™„ì „í•˜ì§€ ì•Šìœ¼ë©°, ë£¨í”„ê°€ í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\nìŠ¤í¬ë¦½íŠ¸ëŠ” ë³¸ì§ˆì ìœ¼ë¡œ ê° íŠ¸ëœì­ì…˜ì— ê¸°ë¡ëœ ì§€ì‹œì–´ë“¤ì˜ ë°°ì—´ì´ê³ , ì§€ì‹œì–´ë“¤ì€ ì „ì†¡ë˜ëŠ” ë¹„íŠ¸ì½”ì¸ì„ ì‚¬ìš©í•˜ë ¤ëŠ” ë‹¤ìŒ ì‚¬ëŒì´ ì ‘ê·¼ í•  ìˆ˜ ìˆëŠ” ì§€ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.\n\nì¼ë°˜ì ìœ¼ë¡œ ë¹„íŠ¸ì½”ì¸ì„ ìˆ˜ì‹ ì ì£¼ì†Œë¡œ ì „ì†¡í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‘ê°€ì§€ì´ê³  ì‚¬ìš©ìê°€ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. ì²«ë²ˆì§¸ë¡œ íŠ¸ëœì­ì…˜ì´ í•´ì‹œë  ë•Œ ìŠ¤í¬ë¦½íŠ¸ì— í¬í•¨ëœ ìˆ˜ì‹ ì ì£¼ì†Œë¥¼ ìƒì„±í•˜ëŠ” Public Key, ë‘ë²ˆì§¸ë¡œ ì²«ë²ˆì§¸ë¡œ ì œê³µí•œ Public Keyì— ëŒ€ì‘í•˜ëŠ” Private Key ì†Œìœ ê¶Œì„ ì¦ëª…í•˜ëŠ” ì„œëª…ì…ë‹ˆë‹¤.\n\nìŠ¤í¬ë¦½íŒ…ì€ ì „ì†¡ëœ ë¹„íŠ¸ì½”ì¸ì„ ì‚¬ìš©í•˜ëŠ”ë° í•„ìš”í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ ë³€ê²½í•  ìˆ˜ ìˆëŠ” ìœ ì—°ì„±ì„ ì œê³µí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìŠ¤í¬ë¦½íŒ… ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ì—¬ ë‘ ê°œì˜ ê°œì¸ í‚¤ë¥¼ ìš”êµ¬í•˜ê±°ë‚˜ ì—¬ëŸ¬ê°œì˜ í‚¤ë¥¼ ì¡°í•©í•˜ê±°ë‚˜ ì•„ì˜ˆ í‚¤ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\níŠ¸ëœì­ì…˜ì€ ìŠ¤í¬ë¦½íŠ¸ ì¡°í•©ì—ì„œ ì—ëŸ¬ë¥¼ ë°œìƒí•˜ì§€ ì•Šê³ , ìµœìƒìœ„ ìŠ¤íƒ í•­ëª©ì´ True(non-zero)ì¸ ê²½ìš°ì— ìœ íš¨í•©ë‹ˆë‹¤.\n\n## ì˜ˆì‹œ\n\n### p2pkh(pay-to-pubkey-hash)\n\n```\nscriptPubKey: OP_DUP OP_HASH160 <pubKeyHash> OP_EQUALVERIFY OP_CHECKSIG\nscriptSig: <sig> <pubKey>\n```\n\n| Stack                                             | Script                                                                         | Description                              |\n| ------------------------------------------------- | ------------------------------------------------------------------------------ | ---------------------------------------- |\n| Empty                                             | `<sig>` `<pubKey>` OP_DUP OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG | scriptSigì™€ scriptPubKeyê°€ ê²°í•©ëœë‹¤      |\n| `<sig>` `<pubKey>`                                | OP_DUP OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                    | ìƒìˆ˜ë¥¼ ìŠ¤íƒì— ì¶”ê°€í•œë‹¤                   |\n| `<sig>` `<pubKey>` `<pubKey>`                     | OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                           | ìµœìƒìœ„ ìŠ¤íƒ ì•„ì´í…œì´ ë³µì‚¬ë˜ì—ˆë‹¤          |\n| `<sig>` `<pubKey>` `<pubKeyHashA>`                | `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                                      | ìµœìƒìœ„ ìŠ¤íƒ ì•„ì´í…œì´ í•´ì‹±ë˜ì—ˆë‹¤          |\n| `<sig>` `<pubKey>` `<pubKeyHashA>` `<pubkeyHash>` | OP_EQUALVERIFY OP_CHECKSIG                                                     | ìƒìˆ˜ë¥¼ ìŠ¤íƒì— ì¶”ê°€í•œë‹¤                   |\n| `<sig>` `<pubKey>`                                | OP_CHECKSIG                                                                    | ìµœìƒìœ„ ë‘ê°œ ì•„ì´í…œì´ ë™ì¼í•¨ì„ í™•ì¸í•˜ì˜€ë‹¤ |\n| true                                              | Empty                                                                          | ìµœìƒìœ„ ë‘ê°œ ì•„ì´í…œì˜ ì„œëª…ì´ í™•ì¸ë˜ì—ˆë‹¤   |\n\n### p2wpkh(pay-to-witness-pubkey-hash)\n\n```\nscriptPubKey: OP_0 <pubkey.hash:20>\nscriptSig: Empty.\nwitness: <sig> <pubkey>\n```\n\n| Stack                                             | Script                                                                         | Description                                                                        |\n| ------------------------------------------------- | ------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------- |\n| Empty                                             | OP_0 <pubkey.hash:20>                                                          | scriptSigì™€ scriptPubKeyê°€ ê²°í•©ëœë‹¤                                                |\n| Empty                                             | `<sig>` `<pubKey>` OP_DUP OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG | p2wpkhíŒ¨í„´, witnessì •ë³´ì˜ `<sig>` `<pubkey>`ì„ í†µí•´ `<pubkey.hash:20>`ì„ ì¹˜í™˜í•œë‹¤. |\n| `<sig>` `<pubKey>`                                | OP_DUP OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                    | ìƒìˆ˜ë¥¼ ìŠ¤íƒì— ì¶”ê°€í•œë‹¤                                                             |\n| `<sig>` `<pubKey>` `<pubKey>`                     | OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                           | ìµœìƒìœ„ ìŠ¤íƒ ì•„ì´í…œì´ ë³µì‚¬ë˜ì—ˆë‹¤                                                    |\n| `<sig>` `<pubKey>` `<pubKeyHashA>`                | `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                                      | ìµœìƒìœ„ ìŠ¤íƒ ì•„ì´í…œì´ í•´ì‹±ë˜ì—ˆë‹¤                                                    |\n| `<sig>` `<pubKey>` `<pubKeyHashA>` `<pubkeyHash>` | OP_EQUALVERIFY OP_CHECKSIG                                                     | ìƒìˆ˜ë¥¼ ìŠ¤íƒì— ì¶”ê°€í•œë‹¤                                                             |\n| `<sig>` `<pubKey>`                                | OP_CHECKSIG                                                                    | ìµœìƒìœ„ ë‘ê°œ ì•„ì´í…œì´ ë™ì¼í•¨ì„ í™•ì¸í•˜ì˜€ë‹¤                                           |\n| true                                              | Empty                                                                          | ìµœìƒìœ„ ë‘ê°œ ì•„ì´í…œì˜ ì„œëª…ì´ í™•ì¸ë˜ì—ˆë‹¤                                             |\n\n### p2sh-p2wpkh\n\n```\nscriptPubKey: <OP_HASH160> <script-hash> <OP_EQUAL>\nscriptSig(redeemScript): hash160(<OP_2> <pubkey-a> <pubkey-b> <pubkey-c> <OP_3> <OP_CHECKMULTISIG>)\nwitness: <sig> <pubkey>\n```\n\n| Stack                                             | Script                                                                         | Description                                                                        |\n| ------------------------------------------------- | ------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------- |\n| Empty                                             | `<redeem_script>` OP_HASH160 `<script-hash>` `OP_EQUAL`                        | scriptSigì™€ scriptPubKeyê°€ ê²°í•©ëœë‹¤                                                |\n| Empty                                             | `<sig>` `<pubKey>` OP_DUP OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG | p2wpkhíŒ¨í„´, witnessì •ë³´ì˜ `<sig>` `<pubkey>`ì„ í†µí•´ `<pubkey.hash:20>`ì„ ì¹˜í™˜í•œë‹¤. |\n| `<sig>` `<pubKey>`                                | OP_DUP OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                    | ìƒìˆ˜ë¥¼ ìŠ¤íƒì— ì¶”ê°€í•œë‹¤                                                             |\n| `<sig>` `<pubKey>` `<pubKey>`                     | OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                           | ìµœìƒìœ„ ìŠ¤íƒ ì•„ì´í…œì´ ë³µì‚¬ë˜ì—ˆë‹¤                                                    |\n| `<sig>` `<pubKey>` `<pubKeyHashA>`                | `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                                      | ìµœìƒìœ„ ìŠ¤íƒ ì•„ì´í…œì´ í•´ì‹±ë˜ì—ˆë‹¤                                                    |\n| `<sig>` `<pubKey>` `<pubKeyHashA>` `<pubkeyHash>` | OP_EQUALVERIFY OP_CHECKSIG                                                     | ìƒìˆ˜ë¥¼ ìŠ¤íƒì— ì¶”ê°€í•œë‹¤                                                             |\n| `<sig>` `<pubKey>`                                | OP_CHECKSIG                                                                    | ìµœìƒìœ„ ë‘ê°œ ì•„ì´í…œì´ ë™ì¼í•¨ì„ í™•ì¸í•˜ì˜€ë‹¤                                           |\n| true                                              | Empty                                                                          | ìµœìƒìœ„ ë‘ê°œ ì•„ì´í…œì˜ ì„œëª…ì´ í™•ì¸ë˜ì—ˆë‹¤                                             |\n\n### ë¯¸ë˜ íŠ¹ì • ì‹œì ê¹Œì§€ ìê¸ˆì„ ë™ê²°í•˜ê³  ì‹¶ì€ ê²½ìš°\n\n```\nscriptPubKey: <expiry-time> OP_CHECKLOCKTIMEVERIFY OP_DROP OP_DUP OP_HASH160 <pubKeyHash> OP_EQUALVERIFY OP_CHECKSIG\nscriptSig: <sig> <pubKey>\n```\n\n| Stack                                             | Script                                                                                                                | Description                                                   |\n| ------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------- |\n| Empty                                             | `<sig>` `<pubKey>` `<expiry-time>` OP_CHECKLOCKTIMEVERIFY OP_DUP OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG | scriptSigì™€ scriptPubKeyê°€ ê²°í•©ëœë‹¤                           |\n| `<sig>` `<pubKey>` `<expiry-time>`                | OP_CHECKLOCKTIMEVERIFY OP_DROP OP_DUP OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                            | ìƒìˆ˜ë¥¼ ìŠ¤íƒì— ì¶”ê°€í•œë‹¤                                        |\n| `<sig>` `<pubKey>` `<expiry-time>`                | OP_DROP OP_DUP OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                                                   | ìµœìƒìœ„ ìŠ¤íƒ ì•„ì´í…œë¥¼ í˜„ì¬ ì‹œê°„ ë˜ëŠ” ë¸”ë¡ ë†’ì´ì™€ ë¹„êµ í™•ì¸í•œë‹¤ |\n| `<sig>` `<pubKey>`                                | OP_DUP OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                                                           | ìµœìƒìœ„ ìŠ¤íƒ ì•„ì´í…œì„ ì œê±°í•œë‹¤                                 |\n| `<sig>` `<pubKey>` `<pubKey>`                     | OP_HASH160 `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                                                                  | ìµœìƒìœ„ ìŠ¤íƒ ì•„ì´í…œì´ ë³µì‚¬ë˜ì—ˆë‹¤                               |\n| `<sig>` `<pubKey>` `<pubKeyHashA>`                | `<pubkeyHash>` OP_EQUALVERIFY OP_CHECKSIG                                                                             | ìµœìƒìœ„ ìŠ¤íƒ ì•„ì´í…œì´ í•´ì‹±ë˜ì—ˆë‹¤                               |\n| `<sig>` `<pubKey>` `<pubKeyHashA>` `<pubkeyHash>` | OP_EQUALVERIFY OP_CHECKSIG                                                                                            | ìƒìˆ˜ë¥¼ ìŠ¤íƒì— ì¶”ê°€í•œë‹¤                                        |\n| `<sig>` `<pubKey>`                                | OP_CHECKSIG                                                                                                           | ìµœìƒìœ„ ë‘ê°œ ì•„ì´í…œì´ ë™ì¼í•¨ì„ í™•ì¸í•˜ì˜€ë‹¤                      |\n| true                                              | Empty                                                                                                                 | ìµœìƒìœ„ ë‘ê°œ ì•„ì´í…œì˜ ì„œëª…ì´ í™•ì¸ë˜ì—ˆë‹¤                        |\n\n### pay-to-multi-signature (2-out-of-3)\n\n```\nscriptPubKey: <OP_2> <pubkey-a> <pubkey-b> <pubkey-c> <OP_3> <OP_CHECKMULTISIG>\nscriptSig: <OP_0> <sig> <sig> <OP_2>\n```\n\n### pay-to-script\n\n```\nscriptPubKey: <OP_HASH160> <script-hash> <OP_EQUAL>\nscriptSig(redeemScript): hash160(<OP_2> <pubkey-a> <pubkey-b> <pubkey-c> <OP_3> <OP_CHECKMULTISIG>)\n```\n\n![](https://learnmeabitcoin.com/technical/images/address/encode-p2sh.png)\n\nìœ„ ê·¸ë¦¼ì„ ë³´ë©´ ì¤‘ê°„ì— hash160(script)ê°€ ìˆëŠ”ë° ì´ê²Œ `redeem-script`ì˜ í•´ì‹œê°’ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ì´ê²ƒì„ base58ë¡œ ë³€í™˜í–ˆì„ë•Œ aë¡œ ì‹œì‘í•˜ëŠ” P2SHì£¼ì†Œ í˜•íƒœë¡œ ìƒëŒ€ë°©ì—ê²Œ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## Ref\n\n- https://learnmeabitcoin.com/technical/p2sh\n- https://medium.com/programming-bitcoin/chapter-13-ì„¸ê·¸ìœ—-865a0c3f6414\n- https://dev-notes.eu/2020/11/Bitcoin-Pay-To-Script-Hash/\n- https://bitcoin.design/guide/glossary/address/\n- https://developer.bitcoin.org/reference/rpc/\n- https://en.bitcoin.it/wiki/Category:Technical",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 4,
    "wordCount": 758,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "address",
    "slug": "address",
    "path": "blockchain/bitcoin",
    "fullPath": "blockchain/bitcoin/address",
    "title": "Address",
    "excerpt": "Address ë¹„íŠ¸ì½”ì¸ ì£¼ì†ŒëŠ” ë¹„íŠ¸ì½”ì¸ì„ ë°›ëŠ” ë° ì‚¬ìš©ë˜ëŠ” 26-35ê°œì˜ ì˜ìˆ«ì ì‹ë³„ìì…ë‹ˆë‹¤. ì„œë¡œ ë‹¤ë¥¸ ì‚¬ì–‘ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì—¬ëŸ¬ ì£¼ì†Œ í˜•ì‹ì´ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì£¼ì†Œë¥¼ ì…ë ¥í•  ë•Œ ì´ëŸ¬í•œ í˜•ì‹ì—ëŠ” íŠ¹ì • ì ‘ë‘ì‚¬ê°€ ìˆìœ¼ë¯€ë¡œ ì‚¬ìš© ì¤‘ì¸ í˜•ì‹ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤....",
    "content": "# Address\n\në¹„íŠ¸ì½”ì¸ ì£¼ì†ŒëŠ” ë¹„íŠ¸ì½”ì¸ì„ ë°›ëŠ” ë° ì‚¬ìš©ë˜ëŠ” 26-35ê°œì˜ ì˜ìˆ«ì ì‹ë³„ìì…ë‹ˆë‹¤. ì„œë¡œ ë‹¤ë¥¸ ì‚¬ì–‘ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì—¬ëŸ¬ ì£¼ì†Œ í˜•ì‹ì´ ìˆìŠµë‹ˆë‹¤.\n\nì‚¬ìš©ìê°€ ì£¼ì†Œë¥¼ ì…ë ¥í•  ë•Œ ì´ëŸ¬í•œ í˜•ì‹ì—ëŠ” íŠ¹ì • ì ‘ë‘ì‚¬ê°€ ìˆìœ¼ë¯€ë¡œ ì‚¬ìš© ì¤‘ì¸ í˜•ì‹ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\në‹¤ìŒì€ ì˜¤ëŠ˜ë‚  ì‚¬ìš©ë˜ëŠ” ì¼ë°˜ì ì¸ ì£¼ì†Œ í˜•ì‹ì…ë‹ˆë‹¤:\n\n## Taproot address - P2TR\n\nTaproot ë˜ëŠ” Bech32m ì£¼ì†Œë¼ê³ ë„ í•˜ëŠ” P2TR(Pay-to-Taproot)ì€ ê°€ì¥ ìµœì‹ ì˜ ê³ ê¸‰ ë¹„íŠ¸ì½”ì¸ ì£¼ì†Œ í˜•ì‹ì…ë‹ˆë‹¤. TaprootëŠ” ë¹„íŠ¸ì½”ì¸ì— ë”ìš± í–¥ìƒëœ ë³´ì•ˆ, ê°œì¸ ì •ë³´ ë³´í˜¸, ìœ ì—°ì„± ë° í™•ì¥ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. SegWitì™€ ë§ˆì°¬ê°€ì§€ë¡œ Taproot ì£¼ì†ŒëŠ” ì˜µíŠ¸ì¸ì´ë©° í˜„ì¬ ë„ë¦¬ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\nTaprootì˜ ì¥ì ì€ ìŠˆë…¸ë¥´ ì„œëª…(Schnorr Signatures)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥, ë” ë‚˜ì€ ë³´ì•ˆ, ë” ë‚®ì€ ìˆ˜ìˆ˜ë£Œ, ë” ìœ ì—°í•œ ë‹¤ì¤‘ í‚¤ íŠ¸ëœì­ì…˜ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. P2TRì„ ì‚¬ìš©í•˜ëŠ” ë‹¤ì¤‘ í‚¤ ì£¼ì†ŒëŠ” ë‹¨ì¼ í‚¤ ì£¼ì†Œì™€ ë™ì¼í•˜ê²Œ ë³´ì—¬ ë‹¤ì¤‘ í‚¤ ì‚¬ìš©ìì—ê²Œ í–¥ìƒëœ ê°œì¸ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. TaprootëŠ” ë˜í•œ ë” ê³ ê¸‰ ìŠ¤í¬ë¦½íŒ…ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ ë¹„íŠ¸ì½”ì¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë” ë³µì¡í•œ ìŠ¤ë§ˆíŠ¸ ê³„ì•½ì„ êµ¬ì¶•í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\n\nTaproot ì£¼ì†ŒëŠ” bc1pë¡œ ì‹œì‘í•˜ë©° ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\nExample: bc1pmzfrwwndsqmk5yh69yjr5lfgfg4ev8c0tsc06e\n\n## SegWit address - P2WPKH\n\në„¤ì´í‹°ë¸Œ SegWit ë˜ëŠ” Bech32 ì£¼ì†Œë¡œë„ ì•Œë ¤ì§„ P2WPKH(pay-to-witness-public-key-hash)ëŠ” í˜„ëŒ€ì ì´ê³  ë³´ë‹¤ íš¨ìœ¨ì ì¸ ì£¼ì†Œ í˜•ì‹ì…ë‹ˆë‹¤. SegWit ì£¼ì†ŒëŠ” ì˜µíŠ¸ì¸ì´ë¯€ë¡œ ëª¨ë“  ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì´ë¥¼ ì§€ì›í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆì§€ë§Œ ì˜¤ëŠ˜ë‚  ëŒ€ë¶€ë¶„ì´ ì§€ì›í•˜ê³  ì§€ì›í•´ì•¼ í•©ë‹ˆë‹¤. SegWit ì±„íƒ ì—¬ë¶€ëŠ” ì—¬ê¸°ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nSegWit ì£¼ì†Œì˜ ì´ì ì€ ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„í•˜ì§€ ì•Šê³  ì˜¤ë¥˜ ìˆ˜ì • ì½”ë“œë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì…ë ¥ ì˜¤ë¥˜ì— ëŒ€í•œ ì €í•­ì„±ì´ í–¥ìƒë˜ê³  íŠ¸ëœì­ì…˜ ë¹„ìš©ì´ ì ˆê°ëœë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ìˆ˜ìˆ˜ë£Œ ì ˆê°ì€ ê±°ë˜ ìœ í˜•ì— ë”°ë¼ ë‹¤ë¥´ê² ì§€ë§Œ ì¼ë°˜ì ì¸ ìê¸ˆ ì´ì²´ì˜ ê²½ìš° 30-40%ê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nSegWit ì£¼ì†ŒëŠ” bc1që¡œ ì‹œì‘í•˜ë©° ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\nExample: bc1qar0srrr7xfkvy5l643lydnw9re59gtzzwf5mdq\n\n## Script address - P2SH\n\nP2SH(Pay-to-Script-Hash) ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ ì£¼ì†ŒëŠ” ì£¼ì†Œì— ì¶”ê°€ ê·œì¹™ ë° ê¸°ëŠ¥ì„ ì²¨ë¶€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ ì£¼ì†ŒëŠ” ì¼ë°˜ì ìœ¼ë¡œ íŠ¸ëœì­ì…˜ì„ ì¸ì¦í•˜ëŠ” ë° ì—¬ëŸ¬ í‚¤ì˜ ì„œëª…ì´ í•„ìš”í•¨ì„ ì§€ì •í•  ìˆ˜ ìˆëŠ” ë‹¤ì¤‘ ì„œëª… ì£¼ì†Œì— ì‚¬ìš©ë©ë‹ˆë‹¤.\n\nìŠ¤í¬ë¦½íŠ¸ ì£¼ì†ŒëŠ” ìˆ«ì 3ìœ¼ë¡œ ì‹œì‘í•˜ê³  ëŒ€ë¬¸ìì™€ ì†Œë¬¸ìë¥¼ í¬í•¨í•  ìˆ˜ ìˆìœ¼ë©° ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„í•©ë‹ˆë‹¤.\n\nExample: 3J98t1WpEZ73CNmQviecrnyiWrnqRhWNLy\n\n## Legacy address - P2PKH\n\nP2PKH(pay-to-pubkey-hash) ë˜ëŠ” ë ˆê±°ì‹œ ì£¼ì†ŒëŠ” ê°€ì¥ ì˜¤ë˜ë˜ê³  ì›ë˜ì˜ ë¹„íŠ¸ì½”ì¸ ì£¼ì†Œ í˜•ì‹ì…ë‹ˆë‹¤. ì´ ì£¼ì†Œ í˜•ì‹ì€ ì´ í˜•ì‹ì„ ì‚¬ìš©í•˜ë©´ íŠ¸ëœì­ì…˜ ë¹„ìš©ì´ ë” ë†’ê¸° ë•Œë¬¸ì— ì˜¤ëŠ˜ë‚  ë„ë¦¬ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì—¬ì „íˆ ì¼ë¶€ ìƒˆë¡œìš´ ì£¼ì†Œì²´ê³„ë¥¼ ì ìš©í•˜ì§€ ì•Šì€ ë ˆê±°ì‹œ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì¡´ì¬í•˜ë¯€ë¡œ, ì´ë“¤ê³¼ì˜ í˜¸í™˜ì„±ì„ ì§€í‚¤ëŠ” ê²ƒì´ ê¶Œì¥ë©ë‹ˆë‹¤.\n\nLegacy addressëŠ” ìˆ«ì 1ë¡œ ì‹œì‘í•˜ê³  ëŒ€ë¬¸ìì™€ ì†Œë¬¸ìë¥¼ í¬í•¨í•  ìˆ˜ ìˆìœ¼ë©° ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„í•©ë‹ˆë‹¤.\n\nExample: 1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN2\n\n## Address í˜¸í™˜ì„±\n\në ˆê±°ì‹œ ì£¼ì†Œê°€ ì—¬ì „íˆ ì‚¬ìš©ë˜ê³  ìˆê¸° ë•Œë¬¸ì—, ì¼ë¶€ ì˜¤ë˜ëœ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ì—¬ì „íˆ ì—…ê·¸ë ˆì´ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ ì£¼ì†Œë¥¼ ì‚¬ìš©í•˜ë©´ ì†¡ì‹ ìì˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ìˆ˜ì‹ ìì— ì˜í•´ ì œê³µë°›ì€ ì„¸ê·¸ìœ— ì£¼ì†Œë¥¼ ìœ íš¨í•˜ê²Œ ì¸ì‹í•˜ì§€ ëª»í•˜ëŠ” í˜¸í™˜ì„± ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nê·¸ëŸ¬ë¯€ë¡œ ìˆ˜ì‹ ìëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì„¸ê·¸ìœ— ì£¼ì†Œë¥¼ ì‚¬ìš©í•œë‹¤ê³  í•˜ë”ë¼ë„ ì†¡ì‹ ìì˜ í˜¸í™˜ì„±ì„ ë§ì¶°ì£¼ê¸° ìœ„í•´ì„œ ìŠ¤í¬ë¦½íŠ¸ ë˜ëŠ” íƒ­ë£¨íŠ¸ ì£¼ì†Œë¡œ ì „í™˜í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 2,
    "wordCount": 386,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "upgrade-move-code",
    "slug": "upgrade-move-code",
    "path": "blockchain/aptos",
    "fullPath": "blockchain/aptos/upgrade-move-code",
    "title": "Upgrade Move code",
    "excerpt": "Upgrade Move code Move code (e.g., Move modules) on the Aptos blockchain can be upgraded. This allows code owners and module developers to update a...",
    "content": "# Upgrade Move code\n\nMove code (e.g., Move modules) on the Aptos blockchain can be upgraded. This\nallows code owners and module developers to update and evolve their contracts\nunder a single, stable, well-known account address that doesn't change. If a\nmodule upgrade happens, all consumers of that module will automatically receive\nthe latest version of the code (e.g., the next time they interact with it).\n\nThe Aptos blockchain natively supports different _upgrade policies_, which allow\nmove developers to explicitly define the constraints around how their move code\ncan be upgraded. The default policy is _(backwards) compatible_. This means that\ncode upgrades are accepted only if they guarantee that no existing resource storage\nor public APIs are broken by the upgrade (including public functions).\nThis compatibility checking is possible because of Move's strongly typed bytecode\nsemantics.\n\nWe note, however, that even compatible upgrades can have hazardous effects on\napplications and dependent Move code (for example, if the semantics of the underlying\nmodule are modified). As a result, developers should be careful when depending on\nthird-party Move code that can be upgraded on-chain. See\n[Security considerations for dependencies](#security-considerations-for-dependencies)\nfor more details.\n\n## How it works\n\nMove code upgrades on the Aptos blockchain happen at the [Move package](https://move-language.github.io/move/packages.html)\ngranularity. A package specifies an upgrade policy in the `Move.toml` manifest:\n\n```toml\n[package]\nname = \"MyApp\"\nversion = \"0.0.1\"\nupgrade_policy = \"compatible\"\n...\n```\n\n:::tip Compatibility check\nAptos checks compatibility at the time a [Move package](https://move-language.github.io/move/packages.html) is published via an Aptos transaction. This transaction will abort if deemed incompatible.\n:::\n\n## How to upgrade\n\nTo upgrade already published Move code, simply attempt to republish the code at\nthe same address that it was previously published. This can be done by following the\ninstructions for code compilation and publishing using the\n[Aptos CLI](../../cli-tools/aptos-cli-tool/use-aptos-cli.md). For an example,\nsee the [Your First Move Module](../../tutorials/first-move-module.md) tutorial.\n\n## Upgrade policies\n\nThere are two different upgrade policies currently supported by Aptos:\n\n- `compatible`: these upgrades must be backwards compatible, specifically:\n  - For storage, all old struct declarations must be the same in\n    the new code. This ensures that the existing state of storage is\n    correctly interpreted by the new code. However, new struct declarations\n    can be added.\n  - For APIs, all existing public functions must have the same signature as\n    before. New functions, including public and entry functions, can be added.\n- `immutable`: the code is not upgradeable and is guaranteed to stay the same\n  forever.\n\nThose policies are ordered regarding strength such that `compatible < immutable`,\ni.e., compatible is weaker than immutable. The policy of a package on-chain can\nonly get stronger, not weaker. Moreover, the policy of all dependencies of a\npackage must be stronger or equal to the policy of the given package. For example,\nan `immutable` package cannot refer directly or indirectly to a `compatible` package.\nThis gives users the guarantee that no unexpected updates can happen under the hood.\n\nNote that there is one exception to the above rule: framework packages\ninstalled at addresses `0x1` to `0xa` are exempted from the dependency check.\nThis is necessary so one can define an `immutable` package based on the standard\nlibraries, which have the `compatible` policy to allow critical upgrades and fixes.\n\n## Compatibility rules\n\nWhen using `compatible` upgrade policy, a module package can be upgraded. However, updates to existing modules already\npublished previously need to be compatible and follow the rules below:\n\n- All existing structs' fields cannot be updated. This means no new fields can be added and existing fields cannot be\n  modified. Struct abilities also cannot be changed (no new ones added or existing removed).\n- All public and entry functions cannot change their signature (argument types, type argument, return types). However,\n  argument names can change.\n- Public(friend) functions are treated as private and thus their signature can arbitrarily change. This is safe as\n  only modules in the same package can call friend functions anyway and they need to be updated if the signature changes.\n\nWhen updating your modules, if you see an incompatible error, make sure to check the above rules and fix any violations.\n\n## Security considerations for dependencies\n\nAs mentioned above, even compatible upgrades can have disastrous effects for\napplications that depend on the upgraded code. These effects can come from bugs,\nbut they can also be the result of malicious upgrades. For example,\nan upgraded dependency can suddenly make all functions abort, breaking the\noperation of your Move code. Alternatively, an upgraded dependency can make\nall functions suddenly cost much more gas to execute then before the upgrade.\nAs result, dependencies to upgradeable packages need to be handled with care:\n\n- The safest dependency is, of course, an `immutable` package. This guarantees\n  that the dependency will never change, including its transitive dependencies.\n  In order to update an immutable package, the owner would have to introduce a\n  new major version, which is practically like deploying a new, separate\n  and independent package. This is because major versioning can be expressed\n  only by name (e.g. `module feature_v1` and `module feature_v2`). However,\n  not all package owners like to publish their code as `immutable`, because this\n  takes away the ability to fix bugs and update the code in place.\n- If you have a dependency to a `compatible` package, it is highly\n  recommended you know and understand the entity publishing the package.\n  The highest level of assurance is when the package is governed by a\n  Decentralized Autonomous Organization (DAO) where no single user can initiate\n  an upgrade; a vote or similar has to be taken. This is the case for the Aptos\n  framework.\n\n## Programmatic upgrade\n\nIn general, Aptos offers, via the Move module `aptos_framework::code`,\nways to publish code from anywhere in your smart contracts. However,\nnotice that code published in the current transaction can be executed\nonly after that transaction ends.\n\nThe Aptos framework itself, including all the on-chain administration logic, is\nan example for programmatic upgrade. The framework is marked as `compatible`.\nUpgrades happen via specific generated governance scripts. For more details,\nsee [Aptos Governance](../../concepts/governance.md).",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 6,
    "wordCount": 1002,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "move-on-aptos",
    "slug": "move-on-aptos",
    "path": "blockchain/aptos",
    "fullPath": "blockchain/aptos/move-on-aptos",
    "title": "Move on Aptos",
    "excerpt": "Move on Aptos ì•±í† ìŠ¤ ë¸”ë¡ì²´ì¸ì€ í•©ì˜ í”„ë¡œí† ì½œì„ ì‹¤í–‰í•˜ëŠ” ë°¸ë¦¬ë°ì´í„° ë…¸ë“œë“¤ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. í•©ì˜ í”„ë¡œí† ì½œì€ Move Virtual Machine()ì—ì„œ ì‹¤í–‰ë  ë•Œ íŠ¸ëœì­ì…˜ë“¤ì˜ ìˆœì„œì™€ ì¶œë ¥()ì— ë™ì˜í•©ë‹ˆë‹¤. ê° ë°¸ë¦¬ë°ì´í„° ë…¸ë“œëŠ”...",
    "content": "# Move on Aptos\n\nì•±í† ìŠ¤ ë¸”ë¡ì²´ì¸ì€ í•©ì˜ í”„ë¡œí† ì½œì„ ì‹¤í–‰í•˜ëŠ” ë°¸ë¦¬ë°ì´í„° ë…¸ë“œë“¤ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. í•©ì˜ í”„ë¡œí† ì½œì€ Move Virtual Machine(`MoveVM`)ì—ì„œ ì‹¤í–‰ë  ë•Œ íŠ¸ëœì­ì…˜ë“¤ì˜ ìˆœì„œì™€ ì¶œë ¥(`output`)ì— ë™ì˜í•©ë‹ˆë‹¤. ê° ë°¸ë¦¬ë°ì´í„° ë…¸ë“œëŠ” í˜„ì¬ ë¸”ë¡ì²´ì¸ ì›ì¥ ìƒíƒœì™€ í•¨ê»˜ íŠ¸ëœì­ì…˜ì„ VMì— ëŒ€í•œ ì…ë ¥ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. `MoveVMì€` ì´ ì…ë ¥ì„ ì²˜ë¦¬í•˜ì—¬ ë³€ë³€ê²½ì‚¬í•­(`changeset`) ë˜ëŠ” ìŠ¤í† ë¦¬ì§€ ë¸íƒ€ë¥¼ ì¶œë ¥ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤. í•©ì˜ í”„ë¡œí† ì½œì´ ë™ì˜í•˜ê³  ì¶œë ¥ì´ ì»¤ë°‹ë˜ë©´ ì´ëŠ” ê³µê°œì ìœ¼ë¡œ ë³´ì´ëŠ” ìƒíƒœê°€ ë©ë‹ˆë‹¤.\n\n## What is Move?\n\n`Move`ëŠ” í¬ì†Œì„±ê³¼ ì ‘ê·¼ ì œì–´ë¥¼ ê°•ì¡°í•˜ëŠ”, Web3ë¥¼ ìœ„í•œ ì•ˆì „í•˜ê³  ë³´ì•ˆì´ ë›°ì–´ë‚œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. `Move`ì˜ ëª¨ë“  ìì‚°ì€ ë¦¬ì†ŒìŠ¤ë¡œ ë˜ëŠ” ë¦¬ì†ŒìŠ¤ì— ì €ì¥ë˜ì–´ í‘œí˜„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í¬ì†Œì„±ì€ êµ¬ì¡°ì²´(`struct`)ë¥¼ ë³µì œí•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ê¸°ë³¸ì ìœ¼ë¡œ ê°•ì œë©ë‹ˆë‹¤. ì˜¤ì§ ë°”ì´íŠ¸ì½”ë“œ ê³„ì¸µì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ë³µì‚¬ë³¸(`copy`)ìœ¼ë¡œ ì •ì˜ëœ ê²½ìš°ì—ë§Œ ë³µì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì ‘ê·¼ì œì–´(`Access Control`)ëŠ” ëª¨ë“ˆ(`module`) ì ‘ê·¼ ê¶Œí•œê³¼ ê³„ì •(`accounts`)ì˜ ê°œë…ì—ì„œ ë¹„ë¡¯ë©ë‹ˆë‹¤. `Move`ì˜ ëª¨ë“ˆì€ ìì‚°ì„ ìƒì„±(`create`), ì €ì¥(`store`), ì „ì†¡(`transfer`)í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë˜ëŠ” í”„ë¡œê·¸ë¨ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. `Move`ëŠ” ì˜¤ì§ ê³µê°œ(`public`)ëª¨ë“ˆ ê¸°ëŠ¥(`functions`)ë“¤ë§Œ ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆìŒì„ ë³´ì¦í•©ë‹ˆë‹¤. êµ¬ì¡°ì²´ê°€ `public`  ìƒì„±ì(`constructor`)ë¥¼ ê°€ì§„ ê²ƒì´ ì•„ë‹ˆë¼ë©´, ì˜¤ì§ í•´ë‹¹ êµ¬ì¡°ì²´ë¥¼ ì •ì˜í•˜ê³  ìˆëŠ” ëª¨ë“ˆì—ì„œë§Œ ìƒì„±ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ, êµ¬ì¡°ì²´ ë‚´ì˜ í•„ë“œëŠ” í•´ë‹¹ ëª¨ë“ˆ ë‚´ì—ì„œ ë˜ëŠ” `public` `getter`ë˜ëŠ” `setter`ë¥¼ í†µí•´ì„œë§Œ ì•¡ì„¸ìŠ¤í•˜ê³  ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n`Move`ì—ì„œ íŠ¸ëœì­ì…˜ì˜ ì „ì†¡ìëŠ” íŠ¹ì • `account`ì˜ í™•ì¸ëœ ì†Œìœ ìì¸ `signer`ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.  `signer`ëŠ” `Move`ì—ì„œ ê°€ì¥ ë†’ì€ ê¶Œí•œ ë ˆë²¨ì„ ê°€ì§€ê³  ìˆê³  `account`ì— `resources`ë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆëŠ” ìœ ì¼í•œ ì—”í‹°í‹°ì…ë‹ˆë‹¤. ê²Œë‹¤ê°€, ëª¨ë“ˆ ê°œë°œìëŠ” `resource`ì— ì ‘ê·¼í•˜ê±°ë‚˜ ê³„ì •ì— ì €ì¥ëœ ìì‚°ì„ ìˆ˜ì •í•˜ê¸° ìœ„í•´  `signer`ê°€ ìˆì–´ì•¼í•œë‹¤ê³  ìš”êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## Comparison to other VMs\n\n| | Aptos/Move | Solana/SeaLevel | EVM |\n|--|------|------|------|\n| Data Storage | ì†Œìœ ìì˜ ê³„ì •ì— ì €ì¥ë¨ Stored within the owner's account | í”„ë¡œê·¸ë¨ê³¼ ê´€ë ¨ëœ ì†Œìœ ì ê³„ì • ë‚´ì— ì €ì¥ Stored within the owner's account associated with a program | ì»¨íŠ¸ë™íŠ¸ì™€ ê´€ë ¨ëœ ê³„ì •ë‚´ì— ì €ì¥ Stored within the account associated with a smart contract |\n| Parallelization | ì•±í† ìŠ¤ ë‚´ì—ì„œ ëŸ°íƒ€ì„ì— ë³‘ë ¬í™”ë¥¼ ì¶”ë¡ í•  ìˆ˜ ìˆìŒ Capable of inferring parallelization at runtime within Aptos | íŠ¸ëœì­ì…˜ ë‚´ì—ì„œ ì•¡ì„¸ìŠ¤í•˜ëŠ” ëª¨ë“  ê³„ì • ë° í”„ë¡œê·¸ë¨ì„ ì§€ì •í•´ì•¼ í•¨ Requires specifying within the transaction all accounts and programs accessed | í˜„ì¬ë¡œì„  ì§ë ¬í™” í•˜ì§€ ì•ŠìŒ Currently serial nothing in production |\n| Transaction safety | Sequence number | íŠ¸ëœì­ì…˜ ê³ ìœ ì„± + íŠ¸ëœì­ì…˜ ê¸°ì–µ Transaction uniqueness + remembering transactions | nonces, similar to sequence numbers |\n| Type safety | Module structs and generics| Program structs | Contract types |\n| Function calling | ì œë„¤ë¦­ì´ ì•„ë‹Œ ì •ì  ë””ìŠ¤íŒ¨ì¹˜ Static dispatch not on generics | ì •ì  ë””ìŠ¤íŒ¨ì¹˜ Static dispatch | ë™ì  ë””ìŠ¤íŒ¨ì¹˜ Dynamic dispatch |\n\n## Aptos Move features\n\n`MoveVM`ì˜ ê° ë°°í¬ëŠ” ì–´ëŒ‘í„° ë ˆì´ì–´ë¥¼ í†µí•´ ì½”ì–´ `MoveVM`ì„ ì¶”ê°€ ê¸°ëŠ¥ìœ¼ë¡œ í™•ì¥í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì´ ìˆìŠµë‹ˆë‹¤. ë˜í•œ `MoveVM`ì—ëŠ” ì»´í“¨í„°ê°€ `OS`ë¥¼ ê°€ì§„ ê²ƒ ì²˜ëŸ¼ í‘œì¤€ì‘ì—…ì„ ì§€ì›í•˜ëŠ” í”„ë ˆì„ì›Œí¬ê°€ ìˆìŠµë‹ˆë‹¤.\n\n`Aptos Move Adapter`ì—ëŠ” ì•„ë˜ ê¸°ëŠ¥ë“¤ì´ ìˆìŠµë‹ˆë‹¤.\n- ê³„ì •ì— ì €ì¥ëœ ë°ì´í„° ì–‘ì„ ë¶„ë¦¬í•˜ì—¬ ê³„ì •ê³¼ ê´€ë ¨ëœ íŠ¸ëœì­ì…˜ì˜ ê°€ìŠ¤ ìš”ê¸ˆì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì„¸ë¶„í™”ëœ ìŠ¤í† ë¦¬ì§€ (`Fine grained storage`) \n- í¬ê¸°ì— ë§ê²Œ ê³„ì • ë‚´ì— í‚¤, ê°’ ë°ì´í„°ë¥¼ ì €ì¥í•  ìˆ˜ ìˆëŠ” `Tables`\n- ì‚¬ìš©ìì˜ ì…ë ¥ ì—†ì´ íŠ¸ëœì­ì…˜ì„ ë™ì‹œì— ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” `Block-STM`ì„ í†µí•œ ë³‘ë ¬ì„±\n\n`Aptos framework`ëŠ” ì—¬ëŸ¬ ìœ ìš©í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.\n- ìŠ¤ë§ˆíŠ¸ ê³„ì•½ì„ ê²Œì‹œí•˜ì§€ ì•Šê³ ë„ NFT ë° ê¸°íƒ€ í’ë¶€í•œ í† í°ì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” `Token standard`\n- ê°€ë²¼ìš´ ëª¨ë“ˆì„ ê²Œì‹œí•˜ì—¬ ì•ˆì „í•œ ìœ í˜•ì˜ ì½”ì¸ ìƒì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” `Coin standard`\n- ìŠ¤í…Œì´í‚¹ ë° ìœ„ì„ í”„ë ˆì„ì›Œí¬\n- ì£¼ì–´ì§„ ìœ í˜•ì˜ ì£¼ì†Œ, ëª¨ë“ˆ ë° êµ¬ì¡°ì²´ ì´ë¦„ì„ ëŸ°íƒ€ì„ì— ì‹ë³„í•˜ëŠ” `type_of` ì„œë¹„ìŠ¤\n- ì—¬ëŸ¬ `signer` ì—”í„°í‹°ë¥¼ í—ˆìš©í•˜ëŠ” `Multi-Signer` í”„ë ˆì„ì›Œí¬\n- ì‹¤ì œ í˜„ì¬ ìœ ë‹‰ìŠ¤íƒ€ì„ì— ë§¤í•‘ë˜ëŠ” ë‹¨ì¡°ë¡­ê²Œ ì¦ê°€í•˜ëŠ” ì‹œê³„ë¥¼ ì œê³µí•˜ëŠ” `timestamp` ì„œë¹„ìŠ¤\n\n## Key Concepts in Aptos Move\n\n- ë°ì´í„°ëŠ” ëª¨ë“ˆì„ ê²Œì‹œí•œ ê³„ì •ì´ ì•„ë‹Œ ë°ì´í„°ë¥¼ ì†Œìœ í•œ ê³„ì • ë‚´ì— ì €ì¥ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n- ë°ì´í„° íë¦„ì—ëŠ” ìƒíƒœê³„ ìœ ìš©ì„±ì— ì¤‘ì ì„ ë‘” ìµœì†Œí•œì˜ ì œì•½ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n- ì œë„¤ë¦­ì„ í†µí•œ ëŸ°íƒ€ì„ ì•ˆì „ë³´ë‹¤ ì •ì  ìœ í˜• ì•ˆì „ì„ ì„ í˜¸í•©ë‹ˆë‹¤.\n- ëª…ì‹œì ìœ¼ë¡œ ëª…í™•í•˜ì§€ ì•Šì€ ê²½ìš° `signer`ëŠ” ìì‚°ì„ ê³„ì •ì— ì¶”ê°€í•˜ê±°ë‚˜ ì œê±°í•  ìˆ˜ ìˆëŠ” ì•¡ì„¸ìŠ¤ë¥¼ ì œí•œí•´ì•¼ í•©ë‹ˆë‹¤.\n\n### Data ownership\n\në°ì´í„°ëŠ” ëª¨ë“ˆì„ ê²Œì‹œí•œ ê³„ì •ì´ ì•„ë‹ˆë¼ ë°ì´í„°ë¥¼ ì†Œìœ í•œ ê³„ì •ì— ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.  \n\n`Solidity`ì—ì„œ ë°ì´í„°ëŠ” ê³„ì•½ì„ ìƒì„±í•œ ê³„ì •ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë‚´ì— ì €ì¥ë©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì´ê²ƒì€ ê°’ì— ëŒ€í•œ ì£¼ì†Œì˜ ë§µ ë˜ëŠ” ì†Œìœ ìì˜ ì£¼ì†Œì— ëŒ€í•œ ì¸ìŠ¤í„´ìŠ¤ IDì˜ ë§µìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.  \n\n`Solana`ì—ì„œ ë°ì´í„°ëŠ” ê³„ì•½ê³¼ ê´€ë ¨ëœ ë³„ê°œì˜ ê³„ì •ì— ì €ì¥ë©ë‹ˆë‹¤.  \n\n`Move`ì—ì„œ ë°ì´í„°ëŠ” ëª¨ë“ˆ ì†Œìœ ìì˜ ê³„ì •ì— ì €ì¥ë  ìˆ˜ ìˆì§€ë§Œ, ì´ëŠ” ì†Œìœ ê¶Œ ëª¨í˜¸ì„± ë¬¸ì œë¥¼ ì•¼ê¸°í•˜ë©° ë‹¤ìŒ ë‘ ê°€ì§€ ë¬¸ì œë¥¼ ì•”ì‹œí•œë‹¤:  \n1. ìì‚°ì— ì†Œìœ ìì™€ ì—°ê²°ëœ ë¦¬ì†ŒìŠ¤ê°€ ì—†ê¸° ë•Œë¬¸ì— ì†Œìœ ê¶Œì´ ëª¨í˜¸í•´ì§‘ë‹ˆë‹¤  \n2. ëª¨ë“ˆ ì‘ì„±ìëŠ” ì„ëŒ€, íšŒìˆ˜ ë“±ê³¼ ê°™ì€ ë¦¬ì†ŒìŠ¤ì˜ ìˆ˜ëª…ì— ëŒ€í•œ ì±…ì„ì„ ì§‘ë‹ˆë‹¤  \n\nì²« ë²ˆì§¸ë¡œ, ìì‚°ì„ ê³„ì • ë‚´ì˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë¦¬ì†ŒìŠ¤ì— ë°°ì¹˜í•¨ìœ¼ë¡œì¨ ì†Œìœ ìëŠ” ì•…ì˜ì ìœ¼ë¡œ í”„ë¡œê·¸ë˜ë°ëœ ëª¨ë“ˆì¡°ì°¨ë„ í•´ë‹¹ ìì‚°ì„ ìˆ˜ì •í•  ìˆ˜ ì—†ë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `Move`ì—ì„œëŠ” í‘œì¤€ ì£¼ë¬¸ì„œ êµ¬ì¡°ì™€ ì¸í„°í˜ì´ìŠ¤ë¥¼ í”„ë¡œê·¸ë˜ë°í•˜ì—¬ ìƒë‹¨ì— êµ¬ì¶•ëœ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ê³„ì •ì´ë‚˜ ì£¼ë¬¸ì„œ í•­ëª©ì— ë°±ë„ì–´ ì•¡ì„¸ìŠ¤ë¥¼ í•  ìˆ˜ ì—†ë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n  \në‹¤ìŒ ë‘ ê°€ì§€ `Coin` ë³´ê´€ ì „ëµì„ ë¹„êµí•´ ë³´ì‹­ì‹œì˜¤:  \n  \në‹¤ìŒì€ ì¸ë±ìŠ¤ë¡œ í‘œì‹œëœ ì†Œìœ ê¶Œì„ ê°€ì§„ ë‹¨ì¼ `Account`ë¡œ ì½”ì¸ì„ ë°°ì¹˜í•©ë‹ˆë‹¤.\n\n```move\nstruct CoinStore has key {\n\tcoins: table<address, Coin>,\n}\n```\n\nëŒ€ì‹ ì—  `Account` ì— `Coin`ì„ ì €ì¥í•˜ëŠ” ì ‘ê·¼ë²•ì—ì„œëŠ” ì†Œìœ ê¶Œì„ ëª…ì‹œì ìœ¼ë¡œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```move\nstruct CoinStore has key {\n    coin: Coin,\n}\n```\n\n### Data flow\n\në°ì´í„° íë¦„ì€ ìƒíƒœê³„ ìœ ìš©ì„±ì— ì¤‘ì ì„ ë‘ê³  ìµœì†Œí•œì˜ ì œì•½ ì¡°ê±´ì„ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.\n\nMoveì—ì„œ ì–´ë–¤ ì¸í„°í˜ì´ìŠ¤ë„ í•´ë‹¹ êµ¬ì¡°ì²´(`struct`)ë¥¼ ê°’ í˜•íƒœë¡œ ë‚˜íƒ€ë‚´ì§€ ì•Šë„ë¡ í•˜ê³ , ëŒ€ì‹  ëª¨ë“ˆë‚´ì— ì •ì˜ëœ ë°ì´í„°ë¥¼ ì¡°ì‘í•˜ê¸° ìœ„í•œ ê¸°ëŠ¥ë§Œì„ ì œê³µí•˜ë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ìº¡ìŠí™”, `encapsulation`). ì´ë¥¼ í†µí•´ ìì‚°ì´ ì˜¤ì§ ëª¨ë“ˆ ë‚´ì—ì„œë§Œ ì ‘ê·¼ í•  ìˆ˜ ìˆë„ë¡ í”„ë¡œê·¸ë˜ë° í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ ì§ì ‘ì ì¸ ì½ê¸°ì™€ ì“°ê¸° ì ‘ê·¼ì„ ì œí•œí•˜ì—¬ ë‹¤ë¥¸ ëª¨ë“ˆê³¼ì˜ ìƒí˜¸ìš´ìš©ì„±ì´ ì°¨ë‹¨ë©ë‹ˆë‹¤. \n\n`Coin<T>`ë¥¼ `input`ìœ¼ë¡œ í•˜ê³  `Ticket`ì„ `return`í•˜ëŠ” ì»¨íŠ¸ë™íŠ¸ê°€ ìˆë‹¤ê³  ê°€ì •í•´ë³´ê² ìŠµë‹ˆë‹¤. ë§Œì•½ `Coin<T>`ë¥¼ ëª¨ë“ˆ ë‚´ì—ì„œë§Œ ì •ì˜í–ˆê³  ì™¸ë¶€ë¡œ ë‚´ë³´ë‚¼ ìˆ˜ ì—†ëŠ” ê²½ìš°, í•´ë‹¹ `Coin<T>`ì— ëŒ€í•œ ìš”ì²­ë“¤ì€ ëª¨ë“ˆì´ ì •ì˜í•œ í•­ëª©ìœ¼ë¡œë§Œ ì œí•œë©ë‹ˆë‹¤.\n  \nì…ê¸ˆ ë° ì¸ì¶œì„ ì‚¬ìš©í•˜ì—¬ ì½”ì¸ ì „ì†¡ì„ êµ¬í˜„í•˜ëŠ” ë‹¤ìŒì˜ ë‘ ê°€ì§€ ê¸°ëŠ¥ì„ ë¹„êµí•´ ë³´ì‹­ì‹œì˜¤:\n\n```move\npublic fun transfer<T>(sender: &signer, recipient: address, amount: u64) {\n    let coin = withdraw(&sender, amount);\n    deposit(recipient, coin);\n}\n```\n\nëª¨ë“ˆ ì™¸ë¶€ì—ì„œ ì½”ì¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë²”ìœ„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n```move\nfun withdraw<T>(account: &signer, amount: u64): Coin<T>\nfun deposit<T>(account: address, coin: Coin<T>)\n```\n\nì¸ì¶œ ë° ì…ê¸ˆì— `public accessors`ë¥¼ ì¶”ê°€í•˜ë©´ ì½”ì¸ì„ ëª¨ë“ˆ ì™¸ë¶€ë¡œ ê°€ì ¸ì™€ ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ ì‚¬ìš©í•˜ê³  ëª¨ë“ˆë¡œ ë°˜í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```move\npublic fun withdraw<T>(account: &signer, amount: u64): Coin<T>\npublic fun deposit<T>(account: address, coin: Coin<T>)\n```\n\n### Type-safety\n\n`Move`ì—ì„œ `A`ë¼ê³  í•˜ëŠ” íŠ¹ì • êµ¬ì¡°ì²´ê°€ ì£¼ì–´ì§€ë©´ ì„œë¡œ ë‹¤ë¥¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n- GUIDì™€ ê°™ì€ ë‚´ë¶€ ì‹ë³„ì\n- `T`ê°€ ë˜ ë‹¤ë¥¸ êµ¬ì¡°ì²´ì¸ `A<T>`ì™€ ê°™ì€ ì œë„¤ë¦­\n\në‚´ë¶€ ì‹ë³„ìëŠ” ë‹¨ìˆœí•˜ê³  í”„ë¡œê·¸ë˜ë°ì´ ìš©ì´í•˜ê¸° ë•Œë¬¸ì— í¸ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì œë„¤ë¦­ì€ ëª…ì‹œì  ì»´íŒŒì¼ ë˜ëŠ” ìœ íš¨ì„± ê²€ì‚¬ ì‹œê°„ì„ í¬í•¨í•œ í›¨ì”¬ ë” ë†’ì€ ë³´ì¥ì„ ì œê³µí•˜ì§€ë§Œ ì¼ë¶€ ë¹„ìš©ì´ ë°œìƒí•©ë‹ˆë‹¤.\n\nì œë„¤ë¦­ì€ ì´ëŸ¬í•œ ìœ í˜•ì„ ì˜ˆìƒí•˜ëŠ” ì™„ì „íˆ ë‹¤ë¥¸ ìœ í˜•ê³¼ ë¦¬ì†ŒìŠ¤ ë° ì¸í„°í˜ì´ìŠ¤ë¥¼ í—ˆìš©í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì£¼ë¬¸ì„œì—ëŠ” ëª¨ë“  ì£¼ë¬¸ì— ëŒ€í•´ ë‘ ê°œì˜ í†µí™”ê°€ ì˜ˆìƒë˜ì§€ë§Œ ê·¸ ì¤‘ í•˜ë‚˜ëŠ” ìˆ˜ì •ë˜ì–´ì•¼ í•œë‹¤ê³  ëª…ì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, `buy<T>(Coin: Coin<APT>): Coin<T>`. ì´ê²ƒì€ ì‚¬ìš©ìê°€ ì–´ë– í•œ `Coin<T>`ë„ êµ¬ë§¤í•  ìˆ˜ ìˆì§€ë§Œ `Coin<APT>`ë¡œ ê²°ì œí•´ì•¼ í•œë‹¤ëŠ” ê²ƒì„ ëª…ì‹œí•©ë‹ˆë‹¤.\n\nì œë„¤ë¦­ì˜ ë³µì¡ì„±ì€ ë°ì´í„°ë¥¼ `T`ì— ì €ì¥í•˜ëŠ” ê²ƒì´ ë°”ëŒì§í•  ë•Œ ë°œìƒí•©ë‹ˆë‹¤. `Move`ëŠ” ì œë„¤ë¦­ì—ì„œ ì •ì  ë””ìŠ¤íŒ¨ì¹˜ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, `create<T>(...) : Coin<T>`ì™€ ê°™ì€ í•¨ìˆ˜ì—ì„œ TëŠ” íŒ¬í…€ ìœ í˜•ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ì¦‰, Coinì—ì„œ ìœ í˜• ë§¤ê°œ ë³€ìˆ˜ë¡œë§Œ ì‚¬ìš©ë˜ê±°ë‚˜ ìƒì„±í•  ì…ë ¥ìœ¼ë¡œ ì§€ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ëª¨ë“  `T`ê°€ í•´ë‹¹ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ë”ë¼ë„ `T:: function`ê³¼ ê°™ì´ ì‚¬ìš©ë  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nëŒ€ëŸ‰ìœ¼ë¡œ ìƒì„±ë  ìˆ˜ ìˆëŠ” êµ¬ì¡°ì²´ì˜ ê²½ìš° ì œë„¤ë¦­ì€ ë°ì´í„° ì¶”ì  ë° ì´ë²¤íŠ¸ ë°©ì¶œê³¼ ê´€ë ¨ëœ ë§ì€ ìƒˆ ì €ì¥ì†Œ ë° ë¦¬ì†ŒìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ê²°ê³¼ë¥¼ ë‚³ìŠµë‹ˆë‹¤.\n\nì´ ë•Œë¬¸ì—, ìš°ë¦¬ëŠ” ë‘ ê°€ì§€ \"í† í°\" í‘œì¤€ì„ ë§Œë“œëŠ” ì–´ë ¤ìš´ ì„ íƒì„ í–ˆìŠµë‹ˆë‹¤. í•˜ë‚˜ëŠ” `Coin`ì´ë¼ëŠ” í†µí™”ì™€ ê´€ë ¨ëœ í† í°ì— ëŒ€í•œ ê²ƒì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ìì‚°ê³¼ ë˜ëŠ” NFTë¡œ ë¶ˆë¦¬ëŠ” `Token`ì— ëŒ€í•œ ê²ƒì…ë‹ˆë‹¤. `Coin`ì€ ì œë„¤ë¦­ì„ í†µí•´ ì •ì  ìœ í˜•ì˜ ì•ˆì „ì„±ì„ í™œìš©í•˜ì§€ë§Œ í›¨ì”¬ ë‹¨ìˆœí•œ ê³„ì•½ì…ë‹ˆë‹¤. `Token`ì€ ìì²´ ë²”ìš© ì‹ë³„ìë¥¼ í†µí•´ ë™ì  ìœ í˜• ì•ˆì „ì„ í™œìš©í•˜ê³  ì‚¬ìš©ì˜ ì¸ì²´ê³µí•™ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ë³µì¡ì„± ë•Œë¬¸ì— ì¼ë°˜ë¡ ì„ íšŒí”¼í•©ë‹ˆë‹¤\n\n### Data access\n\n- `signer`ëŠ” ëª…ì‹œì ìœ¼ë¡œ ëª…í™•í•˜ì§€ ì•Šì€ í•œ ê³„ì •ì— ìì‚°ì„ ì¶”ê°€í•˜ê±°ë‚˜ ì œê±°í•˜ëŠ” ì•¡ì„¸ìŠ¤ë¥¼ ì œí•œí•´ì•¼ í•©ë‹ˆë‹¤.\n\n`Move`ì—ì„œ ëª¨ë“ˆì€ ê³„ì • ì†Œìœ ì ì„œëª…ìì˜ ì¡´ì¬ì— ê´€ê³„ì—†ì´ ë¦¬ì†ŒìŠ¤ì— ì•¡ì„¸ìŠ¤í•˜ëŠ” ë°©ë²•ê³¼ ë¦¬ì†ŒìŠ¤ ë‚´ìš©ì„ ìˆ˜ì •í•˜ëŠ” ë°©ë²•ì„ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¦‰, í”„ë¡œê·¸ë˜ë¨¸ê°€ ì‹¤ìˆ˜ë¡œ ë‹¤ë¥¸ ì‚¬ìš©ìì˜ ê³„ì •ì—ì„œ ì„ì˜ë¡œ ìì‚°ì„ ìƒì„±í•˜ê±°ë‚˜ ì œê±°í•  ìˆ˜ ìˆëŠ” ë¦¬ì†ŒìŠ¤ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nAptos Core Frameworkë¥¼ ê°œë°œí•˜ë©´ì„œ ì ‘ê·¼ ê¶Œí•œì„ í—ˆìš© ë˜ëŠ” ë°©ì§€í•œ ìœ„ì¹˜ì— ëŒ€í•œ ëª‡ ê°€ì§€ ì˜ˆê°€ ìˆìŠµë‹ˆë‹¤:\n\n- Aìœ ì €ê°€ ì´ë¯¸  `U Token`ì„ ë³´ìœ í•œ ê²ƒì´ ì•„ë‹ˆë¼ë©´, `U Token`ì€ Aìœ ì €ì˜ `account`ì— ì§ì ‘ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n- ì ‘ê·¼ ì œì–´ ëª©ë¡ì„ íš¨ê³¼ì ìœ¼ë¡œ ì‚¬ìš©í•´ì„œ  `TokenTransfers`ê°€ ìœ ì €ê°€ ëª…ì‹œì ìœ¼ë¡œ ë‹¤ë¥¸ ìœ ì €ì˜ ë¦¬ì†ŒìŠ¤ì˜ í† í°ì„ ìš”êµ¬í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.\n- Aìœ ì €ê°€ `Coin<T>`ì„ ì €ì¥í•  `CoinStore<Coin<T>>`ë¦¬ì†ŒìŠ¤ë¥¼ ê°€ì§€ê³  ìˆëŠ” í•œ, ì–´ë–¤ ìœ ì €ë“  Aìœ ì €ì—ê²Œ `Coin<T>`ì„ ì§ì ‘ ì „ì†¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n`Token`ì— ëŒ€í•œ ì—„ê²©í•œ ë…¸ë ¥ì´ ë¶€ì¡±í•˜ë©´ ì‚¬ìš©ìê°€ ë‹¤ë¥¸ ì‚¬ìš©ì ê³„ì •ìœ¼ë¡œ ì§ì ‘ `Token`ì„ ì—ì–´ë“œë¡­í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ì‚¬ìš©ì ê³„ì •ì— ì¶”ê°€ ìŠ¤í† ë¦¬ì§€ë¥¼ ì¶”ê°€í•˜ê³  ì²˜ìŒ ìŠ¹ì¸í•˜ì§€ ì•Šì€ ì½˜í…ì¸ ì˜ ì†Œìœ ìê°€ ë©ë‹ˆë‹¤.\n\nêµ¬ì²´ì ì¸ ì˜ˆë¡œ, ì¸ì¶œ ê¸°ëŠ¥ì´ ìˆëŠ” ì´ì „ `Coin` ì¼€ì´ìŠ¤ë¡œ ëŒì•„ê°€ì‹­ì‹œì˜¤. ëŒ€ì‹  `withdraw` í•¨ìˆ˜ê°€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœ ê²½ìš°(`signer` ì¸ìˆ˜ê°€ ì—†ìŒì— ìœ ì˜í•˜ì‹­ì‹œì˜¤):\n\n```move\npublic fun withdraw<T>(account: address, amount: u64): Coin<T>\n```\n\nëˆ„êµ¬ë‚˜ ê³„ì •ì—ì„œ ì½”ì¸ì„ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n### Resource accounts\n\n`Move` ëª¨ë¸ì€ ì¢…ì¢… íŠ¸ëœì­ì…˜ì˜ `signer`ë¥¼ ì•Œì•„ì•¼ í•˜ê¸° ë•Œë¬¸ì—, AptosëŠ” `signer` ê¸°ëŠ¥ì„ í• ë‹¹í•˜ê¸° ìœ„í•œ ë¦¬ì†ŒìŠ¤ ê³„ì •ì„ ì œê³µí•©ë‹ˆë‹¤. ë¦¬ì†ŒìŠ¤ ê³„ì •ì„ ìƒì„±í•˜ë©´ `signer` ê¸°ëŠ¥ì— ì•¡ì„¸ìŠ¤í•˜ì—¬ ìë™ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `signer` ê¸°ëŠ¥ì€ ë¦¬ì†ŒìŠ¤ ê³„ì •ì˜ `signer`ê°€ ë¦¬ì†ŒìŠ¤ ê³„ì •ì„ ìƒì„±í•˜ê±°ë‚˜ ëª¨ë“ˆì˜ ë¡œì»¬ ì €ì¥ì†Œì— ë°°ì¹˜ëœ ì›ë³¸ ê³„ì •ì˜ ì£¼ì†Œì™€ í•¨ê»˜ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `create_nft_with_resource_account.move`ì—ì„œ `resource_signer_cap` ì°¸ì¡°ë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.\n\në¦¬ì†ŒìŠ¤ ê³„ì •ì„ ìƒì„±í•  ë•Œ í•´ë‹¹ ê³„ì •ì— `signer` ê¸°ëŠ¥ë„ ë¶€ì—¬í•©ë‹ˆë‹¤. `signer` ê¸°ëŠ¥ ë‚´ì˜ ìœ ì¼í•œ í•„ë“œëŠ” `signer`ì˜ ì£¼ì†Œì…ë‹ˆë‹¤. `signer` ê¸°ëŠ¥ì—ì„œ `signer`ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ë³´ë ¤ë©´ `create_nft_with_resource_account.move`ì˜ `let resource_signer` í•¨ìˆ˜ë¥¼ ê²€í† í•˜ì‹­ì‹œì˜¤.  \n  \në³´ì•ˆ ì¹¨í•´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ëª¨ë“ˆê³¼ ë¦¬ì†ŒìŠ¤ ê³„ì •ë§Œ ì„œëª…ì ê¸°ëŠ¥ì„ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì„œëª…ì ê¸°ëŠ¥ì—ì„œ ì„œëª…ìë¥¼ ì—­ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ì—†ìœ¼ë©°, ëŒ€ì‹  ìƒˆ ë¦¬ì†ŒìŠ¤ ê³„ì •ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ê°œì¸ í‚¤ì—ì„œ ì„œëª…ì ê¸°ëŠ¥ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n`signer` ì·¨ì•½ì„±ì„ ì¶”ê°€ë¡œ ë°©ì§€í•˜ë ¤ë©´ ì§€ê°‘ ì´ë²¤íŠ¸ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³  ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•˜ì‹­ì‹œì˜¤:  \n  \n- ê³µì œë˜ëŠ” ê¸ˆì•¡ì´ ë§ìŠµë‹ˆë‹¤.  \n- NFT ìƒì„± ì´ë²¤íŠ¸ê°€ ìˆìŠµë‹ˆë‹¤.  \n- NFT ì¸ì¶œ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\n\nSeeÂ [resource accounts](https://aptos.dev/guides/resource-accounts)Â to learn more.\n\n### Coins\n\nAPT(Aptos Token)ëŠ” ì„ì˜ì˜ ì£¼ì†Œë¡œ ì „ì†¡í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì£¼ì†Œì— ëŒ€í•œ ê³„ì •ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, USDCë¥¼ êµ¬ì…í–ˆìœ¼ë©° ì´ë¥¼ APTë¡œ ë³€í™˜í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ì‚¬ìš©ìë¥¼ ë³´í˜¸í•˜ë ¤ë©´ í•´ë‹¹ í† í°ì„ ìˆ˜ë½í•´ì•¼ í•©ë‹ˆë‹¤.\n\n### Wrapping\n\n`Coin` ëŒ€ì‹  ë°¸ëŸ°ìŠ¤ë¥¼ ì§ì ‘ ë³´ê´€í•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ? ìš°ë¦¬ëŠ” ë‹¹ì‹ ì´ `wrapper` ê¸°ëŠ¥ì„ ì¶”ê°€í•  ìˆ˜ ìˆë„ë¡ ê°„ì ‘ì ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.\n\nì˜ˆë¥¼ ë“¤ì–´, `Coin`ì—ì„œ ì¶œê¸ˆ ë° ì…ê¸ˆ ì´ë²¤íŠ¸ë¥¼ ì „ì†¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\ní•˜ì§€ë§Œ ì—ìŠ¤í¬ë¡œì˜ ê²½ìš° `Coin`ì„ ë³´ìœ í•˜ê¸° ìœ„í•œ ì´ë²¤íŠ¸ë„ ë°©ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nëª¨ë“ˆ ë‚´ì—ì„œ ë‹¤ë¥¸ êµ¬ì¡°ì²´ë“¤ì„(`structure`)ë¥¼ `destructure` í•˜ì—¬ ê°„ì ‘ì ìœ¼ë¡œ ì”ì•¡ ëŒ€ì‹  ì§ì ‘ `Coin`ì—ì„œ ë™ì‘ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nê·¸ê²ƒì€ ê°œë³„ êµ¬í˜„ì— ë‹¬ë ¤ ìˆìŠµë‹ˆë‹¤. ë™ì¼í•œ ëª¨ë“ˆì—ì„œ `Coin`ê³¼ ë°¸ëŸ°ìŠ¤ë¥¼ ëª¨ë‘ ì •ì˜í•˜ëŠ” ê²½ìš° `destructure`ë¥¼ í†µí•´ ë‚´ë¶€ì˜ `Coin`ì— ëŒ€í•œ ì°¸ì¡°ë¥¼ ì–»ì„ ìˆ˜ ìˆìœ¼ë©° êµ¬ì¡° ìì²´ì— ëŒ€í•œ ë³€ê²½ ê°€ëŠ¥í•œ ì°¸ì¡°ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëŒ€ì‹  `Coin` ëª¨ë“ˆì— ì˜ì¡´í•˜ëŠ” ê²½ìš°, ì‚¬ìš©ìì—ê²Œ ì…ê¸ˆí•˜ê¸° ìœ„í•´ `Balance` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ `BalanceWithdraw` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ `Coin`ì„ ì–»ì–´ì•¼ í•©ë‹ˆë‹¤. í•¨ê»˜ ì¶”ê°€í•˜ë ¤ë©´ `CoinMergeë¥¼` ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.\n\n### Generics\n\nì‚¬ìš©ì ì§€ì • í† í°ê³¼ Aptos í† í° ëª¨ë‘ì— ì œë„¤ë¦­ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Aptosê°€ ì œê³µí•˜ëŠ” ìœ ì¼í•œ ë§ˆë²•ì€ Aptosê°€ ì• ê·¸ë¦¬ê²Œì´í„°ë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì½”ì¸ ìœ í˜•ì—ëŠ” ì•„ì§ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n### Visibility\n\n`Functions`ëŠ” `private`ì´ ê¸°ë³¸ ê°’ì´ë¯€ë¡œ ê°™ì€ íŒŒì¼ì—ì„œë§Œ í˜¸ì¶œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Â [`public`,Â `public(entry)`, etc.]Â ë¥¼ ì‚¬ìš©í•´ì„œ íŒŒì¼ ë°”ê¹¥ì—ì„œ í˜¸ì¶œ í•˜ë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n-   `entry`Â - í•¨ìˆ˜ í˜¸ì¶œì„ ì‹¤ì œ entry í•¨ìˆ˜ë¡œ ë§Œë“¤ì–´ ë¶„ë¦¬í•©ë‹ˆë‹¤. ì¬ì¤‘ë³µì„±ì„ ë°©ì§€í•©ë‹ˆë‹¤(ì»´íŒŒì¼ëŸ¬ ì˜¤ë¥˜ ë°œìƒ)\n-   `public`Â - ëˆ„êµ¬ë‚˜ ì–´ë””ì„œë‚˜ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n-   `public(entry)`Â - ê´€ë ¨ íŠ¸ëœì­ì…˜ì— ì •ì˜ëœ ë©”ì„œë“œë§Œ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n-   `public(friend)`Â - í˜„ì¬ ëª¨ë“ˆì´ ì‹ ë¢°í•˜ëŠ” ëª¨ë“ˆì„ ì„ ì–¸í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n-   `public(script)`Â - Aptos ë„¤íŠ¸ì›Œí¬ì—ì„œ ì„ì˜ ì´ë™ ì½”ë“œë¥¼ ì œì¶œ, ì»´íŒŒì¼ ë° ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n\nê°€ëŠ¥í•œ `public(entry)`ì´ ì•„ë‹Œ `entry`ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•˜ëŠ”ë°, ì½”ë“œê°€ ì¶”ê°€ ê°œì²´(`object`)ë¡œ ê°ì‹¸ì´ì§€ ì•Šë„ë¡ í•´ì£¼ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n\n`Move`ëŠ” ë‘ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ì¬ì§„ì…ì„ ë°©ì§€í•©ë‹ˆë‹¤\n\n1.  ë™ì  ë””ìŠ¤íŒ¨ì¹˜ê°€ ì—†ëŠ” ê²½ìš° ëª¨ë“ˆ ë‚´ì˜ ë‹¤ë¥¸ ëª¨ë“ˆì„ í˜¸ì¶œí•˜ë ¤ë©´ í•´ë‹¹ ëª¨ë“ˆì— ëª…ì‹œì ìœ¼ë¡œ ì˜ì¡´í•´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¤ë¥¸ ëª¨ë“ˆì€ ì‚¬ìš©ìì—ê²Œ ì˜ì¡´í•´ì•¼ í•©ë‹ˆë‹¤.\n2.  ìˆœí™˜ ì¢…ì†ì„±ì€ í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ Aê°€ Bë¥¼ í˜¸ì¶œí•˜ê³  Bê°€ Aì— ìƒí˜¸ ì˜ì¡´í•˜ë©´ ëª¨ë“ˆ Bë¥¼ ë°°í¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nFind out more about the Move programming language among theÂ [Move Guides](https://aptos.dev/).",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 9,
    "wordCount": 1763,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "interact-with-the-move-vm",
    "slug": "interact-with-the-move-vm",
    "path": "blockchain/aptos",
    "fullPath": "blockchain/aptos/interact-with-the-move-vm",
    "title": "Interact with the Move VM",
    "excerpt": "Interact with the Move VM The Aptos blockchain uses the Move virtual machine (VM) for executing operations...",
    "content": "# Interact with the Move VM\n\nThe Aptos blockchain uses the [Move](https://github.com/move-language/move) virtual machine (VM) for executing operations. While many blockchains implement a set of\nnative operations, Aptos delegates all operations to Move, including: account creation, fund transfer and publishing Move modules.\nTo support these operations, blockchains built on top of Move must provide a framework (akin to\nan operating system for a computer or a minimal viable set of functions) for interacting with the blockchain. In this section, we discuss\nthese functions, exposed via the Aptos Framework's `script` functions.\n\nThis guide (in concert with the [Move module tutorial](../tutorials/first-move-module.md) ) will unlock the minimal amount of information required to start building rich applications on top of the Aptos blockchain. Note: the Aptos Framework is under heavy development and this document may not\nbe up to date. The most recent framework can be found in the [source code](https://github.com/aptos-labs/aptos-core/tree/main/aptos-move/framework).\n\nThe core functions provided to users within the Aptos Framework include:\n\n- Sending and receiving the network coin `Coin<AptosCoin>`\n- Creating a new account\n- Publishing a new Move module\n\nNote: this document assumes readers are already familiar with submitting transactions, as described in the [Your first transaction tutorial](../tutorials/first-transaction.md).\n\n## Sending and Receiving the network coin `Coin<AptosCoin>`\n\n`Coin<AptosCoin>` is required for paying gas fees when submitting and executing transactions. `Coin<AptosCoin>` can be obtained by calling the Devnet Faucet. See the [Your first transaction](../tutorials/first-transaction.md) tutorial for an example.\n\nThe payload for instructing the blockchain to perform a transfer is:\n\n```\n{\n  \"type\": \"entry_function_payload\",\n  \"function\": \"0x1::coin::transfer\",\n  \"type_arguments\": [\"0x1::aptos_coin::AptosCoin\"],\n  \"arguments\": [\n    \"0x737b36c96926043794ed3a0b3eaaceaf\",\n    \"1000\",\n  ]\n}\n```\n\nThis instructs the VM to execute the `script` `0x1::coin::transfer` with a type argument of 0x1::aptos_coin::AptosCoin. Type is required here as Coin is our standard module that can be used to create many types of Coins. See the [Your first coin tutorial](../tutorials/first-coin.md) for an example of creating a custom Coin. The first argument is the recipient address, `0x737b36c96926043794ed3a0b3eaaceaf`, and the second is the amount to transfer, `1000`. The sender address is the account\naddress that sent the transaction querying this `script`.\n\n## Creating a new account\n\nThe payload for instructing the blockchain to create a new account is:\n\n```\n{\n  \"type\": \"entry_function_payload\",\n  \"function\": \"0x1::aptos_account::create_account\",\n  \"type_arguments\": [],\n  \"arguments\": [\n    \"0x0c7e09cd9185a27104fa218a0b26ea88\",\n    \"0xaacf87ae9d8a5e523c7f1107c668cb28dec005933c4a3bf0465ffd8a9800a2d900\",\n  ]\n}\n```\n\nThis instructs the Move virtual machine to execute the `script` `0x1::aptos_account::create_account`. The first argument is the address of the account to create and the second is the authentication key pre-image (which is mentioned in [Accounts](../concepts/accounts.md). For single signer authentication, this is the public key concatenated with the `0` byte (or `pubkey_A | 0x00`). This is required to prevent account address land grabbing. The execution of this instruction verifies that the 32-bytes of the authentication key are the same as the 32-byte account address. We are actively working on improving this API to support taking in a 32-byte account address that would eliminate concerns around land grabbing or account manipulation.\n\n## Publishing a new Move module\n\nThe payload for publishing a new module is:\n\n```\n\"type\": \"module_bundle_payload\",\n\"modules\": [\n    {\"bytecode\": \"0x...\"},\n],\n```\n\nThis instructs the VM to publish the module bytecode under the sender's account. For a full-length tutorial, see [Your first move module](../tutorials/first-move-module.md).\n\nIt is important to note that the Move bytecode must specify the same address as the sender's account, otherwise the transaction will be rejected. For example, assuming account address `0xe110`, the Move module would need to be updated as such `module 0xe110::Message`, `module 0xbar::Message` would be rejected. Alternatively an aliased address could be used, such as `module HelloBlockchain::Message` but the `HelloBlockchain` alias would need to updated to `0xe110` in the `Move.toml` file. We are working with the Move team and planning on incorporating a compiler into our REST interface to mitigate this issue.\n\n[accounts]: /concepts/accounts\n[your-first-coin]: /tutorials/your-first-coin\n[your-first-move-module]: /tutorials/first-move-module\n[your-first-transaction]: /tutorials/your-first-transaction\n[move_url]: https://diem.github.io/move/\n[aptos_framework]: https://github.com/aptos-labs/aptos-core/tree/main/aptos-move/framework/aptos-framework/sources",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 4,
    "wordCount": 632,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "aptos-whitepaper",
    "slug": "aptos-whitepaper",
    "path": "blockchain/aptos",
    "fullPath": "blockchain/aptos/aptos-whitepaper",
    "title": "Aptos Whitepaper",
    "excerpt": "Aptos Whitepaper https://aptos.dev/aptos-white-paper/aptos-white-paper-in-korean...",
    "content": "# Aptos Whitepaper\n\nhttps://aptos.dev/aptos-white-paper/aptos-white-paper-in-korean",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 5,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "aptos-move-structure",
    "slug": "aptos-move-structure",
    "path": "blockchain/aptos",
    "fullPath": "blockchain/aptos/aptos-move-structure",
    "title": "Aptos Move Structure",
    "excerpt": "Aptos Move Structure ì—¬ëŸ¬ë¶„ì˜ Move ì½”ë“œë¥¼ ì–´ë–»ê²Œ êµ¬ì„±í•˜ëŠ”ê²Œ ì¢‹ì€ì§€ ì´í•´í•˜ëŠ” ì‹œê°„ì„ ê°€ì ¸ë´…ì‹œë‹¤ Moveì˜ êµ¬ì¡°ì²´ëŠ” í•¨ìˆ˜ í´ë˜ìŠ¤ ì—­í• ì„ í•˜ëŠ” Rustì™€ ê°™ì€ ë‹¤ë¥¸ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì˜ êµ¬ì¡°ì²´ì™€ ë¹„ìŠ·í•©ë‹ˆë‹¤. ì›í•˜ëŠ” ë§Œí¼ êµ¬ì¡°ì²´ì— ë§ì€ í•„ë“œë¥¼ ê°€ì§ˆ ìˆ˜...",
    "content": "# Aptos Move Structure\n\nì—¬ëŸ¬ë¶„ì˜ Move ì½”ë“œë¥¼ ì–´ë–»ê²Œ êµ¬ì„±í•˜ëŠ”ê²Œ ì¢‹ì€ì§€ ì´í•´í•˜ëŠ” ì‹œê°„ì„ ê°€ì ¸ë´…ì‹œë‹¤\n\nMoveì˜ êµ¬ì¡°ì²´ëŠ” í•¨ìˆ˜ í´ë˜ìŠ¤ ì—­í• ì„ í•˜ëŠ” Rustì™€ ê°™ì€ ë‹¤ë¥¸ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì˜ êµ¬ì¡°ì²´ì™€ ë¹„ìŠ·í•©ë‹ˆë‹¤. ì›í•˜ëŠ” ë§Œí¼ êµ¬ì¡°ì²´ì— ë§ì€ í•„ë“œë¥¼ ê°€ì§ˆ ìˆ˜ ìˆì§€ë§Œ ê°œì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë°ì—ì„œì™€ ê°™ì´ êµ¬ì¡°ì²´ì— ë©”ì„œë“œë¥¼ ê°€ì§ˆ ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ Moveì—ëŠ” ìƒì†ì´ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì‹  êµ¬ì¡°ì²´ë¥¼ ë‹¤ì‹œ ìƒì„±í•˜ë ¤ë©´ êµ¬ì¡°ì²´ë¥¼ ë³µì œí•´ì•¼ í•©ë‹ˆë‹¤.\n\nì¼ë‹¨ ê²Œì‹œë˜ë©´ Moveì˜ êµ¬ì¡°ì²´ ì •ì˜ëŠ” ë³€ê²½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. êµ¬ì¡°ì²´ ìì²´ëŠ” ì—…ê·¸ë ˆì´ë“œí•  ìˆ˜ ì—†ì§€ë§Œ í•´ë‹¹ í•„ë“œì˜ ê°’ì€ ë³€ê²½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Moveì˜ ë³´ì•ˆì„ ìœ„í•´ êµ¬ì¡°ì²´ê°€ ì •ì˜ëœ ëª¨ë“ˆë§Œ êµ¬ì¡°ì²´ë¥¼ ë¶„í•´í•˜ê±°ë‚˜ í•´ë‹¹ ì†ì„±ì— ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## Abilities\n\nMoveì˜ êµ¬ì¡°ì²´(Structures)ì—ëŠ” í•´ë‹¹ ìœ í˜•ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ë‹¤ì–‘í•œ ê¸°ëŠ¥ì´ ë¶€ì—¬ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ë„¤ ê°€ì§€ ê¸°ëŠ¥ì´ ìˆìŠµë‹ˆë‹¤.\n\n- copy: ë³µì‚¬í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥. ì§€ë¦¬ì  IDê°€ ì¢‹ì€ í™œìš© ì‚¬ë¡€ê°€ ë  ê²ƒì…ë‹ˆë‹¤. NFTëŠ” ì´ ê¸°ëŠ¥ì„ ê°€ì§€ê³  ìˆì–´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.  values of types with this ability to be copied. A geographic ID would be a good use case. NFTs should not have this ability.\n- drop: íŒ/ë“œë¡­í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥.\n- store: ê¸€ë¡œë²Œ ìŠ¤í† ë¦¬ì§€ì˜ êµ¬ì¡°ì²´ ë‚´ë¶€ì— ì €ì¥ ë˜ëŠ” ì €ì¥í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥.\n- key: ê¸€ë¡œë²Œ ìŠ¤í† ë¦¬ì§€ ì‘ì—…ì˜ í‚¤ ì—­í• ì„ í•˜ëŠ” ìœ í˜•ì…ë‹ˆë‹¤.Â ì´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë©´ ê°’ì„ ê³„ì • ë‚´ ìµœìƒìœ„ í•­ëª©ìœ¼ë¡œ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## Global storage\n\nMoveì—ì„œ ê° ê³„ì •ì€ ì£¼ì–´ì§„ ìœ í˜•ì˜ ë¦¬ì†ŒìŠ¤ë¥¼ í•˜ë‚˜ë§Œ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.Â `Coin`ì˜ˆë¥¼ ë“¤ì–´ Moveì˜ ê³„ì •ì€ í•˜ë‚˜ì˜ ìœ í˜•Â ë§Œ ì¡´ì¬í•˜ëŠ” í•´ì‹œë§µê³¼ ìœ ì‚¬í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤ .Â í•´ì‹œë§µì€ ë¦¬ì†ŒìŠ¤ ìœ í˜• ë˜ëŠ” ëª¨ë“ˆ ì´ë¦„ì„ ë¦¬ì†ŒìŠ¤ ê°’ì— ë§¤í•‘í•œ ê²ƒì…ë‹ˆë‹¤.Â ì´ê²ƒì´ Aptosê°€Â ì—¬ëŸ¬ ì½”ì¸ê³¼ í† í°ì„ ë³´ìœ í•˜ê¸° ìœ„í•œ ì¶”ìƒí™”ë¥¼ ì œê³µí•˜ê¸° ìœ„í•´Â `CoinStore`ë° ì˜ í™€ë” íŒ¨í„´ì„ ì œê³µí•˜ëŠ” ì´ìœ ì…ë‹ˆë‹¤.Â `TokenStore`ì´ëŸ¬í•œ í™€ë”ëŠ” í…Œì´ë¸”ì„ í¬í•¨í•˜ê±°ë‚˜ ì €ì¥ì„ ìœ„í•´ ì œë„¤ë¦­ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\nAptosÂ ëŠ” íš¨ìœ¨ì ì¸ ìƒíƒœ ë™ê¸°í™” ë° ì¸ì¦ëœ ìŠ¤í† ë¦¬ì§€ ì½ê¸°ë¥¼ ìœ„í•´Â [Merkle íŠ¸ë¦¬ ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.](https://aptos.dev/reference/glossary/#merkle-trees)\n\n## Signers\n\nAptosì—ì„œ ì„œëª…ìëŠ” ì—„ì²­ë‚˜ê²Œ ê°•ë ¥í•©ë‹ˆë‹¤.Â êµ¬ì¡°ì²´ëŠ” ì„œëª…ì ì£¼ì†Œë¡œ ê²Œì‹œë©ë‹ˆë‹¤.Â ì„œëª…ìëŠ” íŠ¸ëœì­ì…˜ì— ì„œëª…í•˜ê³  ì œì¶œí•  ë•Œ ìƒì„±ë©ë‹ˆë‹¤.Â íŠ¸ëœì­ì…˜ì„ ì œì¶œí•  ë•Œ ì„œëª…ìëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì²« ë²ˆì§¸ ë§¤ê°œë³€ìˆ˜ì…ë‹ˆë‹¤.Â ì„œëª…ìëŠ” ìì‹ ì˜ êµ¬ì¡°ì²´ë¥¼ ì²´ì¸ì— í¬í•¨í•˜ëŠ” ë° ë™ì˜í–ˆìŠµë‹ˆë‹¤.Â ì„œëª…ìì—ê²ŒëŠ” ì €ì¥ ë˜ëŠ” í‚¤ ê¸°ëŠ¥ì´ ì—†ê³  ë³µì‚¬ ê¸°ëŠ¥ë§Œ ìˆìŠµë‹ˆë‹¤.\n\n## key\n\në‹¤ë¥¸ ì‚¬ìš©ìê°€ ì„œëª…ìë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ê¸° ìœ„í•´ ì„œëª…ìëŠ” ë¦¬ì†ŒìŠ¤ì— ì €ì¥ë©ë‹ˆë‹¤. `key` ê¸°ëŠ¥ì„ í†µí•´ `type`ì€ `store` ê¸°ëŠ¥ì´ ìˆëŠ” `Coin`ê³¼ ê°™ì€ ê¸€ë¡œë²Œ ìŠ¤í† ë¦¬ì§€ ì‘ì—…ì˜ `key` ì—­í• ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `Balance`ì—ëŠ” `key`ê¸°ëŠ¥ì´ ìˆìœ¼ë¯€ë¡œ ê³„ì • ë‚´ ìµœìƒìœ„ í•­ëª©ìœ¼ë¡œ `store`í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nAptosëŠ” ì„œëª…ìë¥¼ ì €ì¥í•˜ì§€ ì•Šê³  ì„œëª…ì ê¸°ëŠ¥ì„ ì €ì¥í•©ë‹ˆë‹¤. ì œí•œëœ `native` ê¸°ëŠ¥ë§Œ ì„œëª…ì ê¸°ëŠ¥ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. NFTë¥¼ `minting`í•˜ë ¤ë©´ ì»¬ë ‰ì…˜ì„ ìƒì„±í•œ ì„œëª…ìì— ëŒ€í•œ ì•¡ì„¸ìŠ¤ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ê²ƒì´ ë™ì  ë°œí–‰ì„ ìˆ˜í–‰í•  ë•Œ ë§ì€ ì‚¬ëŒë“¤ì´ NFTë¥¼ ë¯¸ë¦¬ ë°œí–‰(`pre-mint`)í•˜ëŠ” ì´ìœ ì…ë‹ˆë‹¤. AptosëŠ” íŠ¸ëœì­ì…˜ì— ììœ¨ì ìœ¼ë¡œ ì„œëª…í•  ìˆ˜ ìˆëŠ” ë¦¬ì†ŒìŠ¤ ê³„ì •ì„ ì œê³µí•©ë‹ˆë‹¤.\n\n## acquires\n\nìœ ì €ê°€ êµ¬ì¡°ì²´ì™€ ê°™ì€ ê¸€ë¡œë²Œ ë¦¬ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í• ë•Œë§ˆë‹¤ í•­ìƒ ë¨¼ì € ì´ê²ƒì„ íšë“(`acquire`)í•´ì•¼í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´,  NFT ì…ì¶œê¸ˆì‹œ `TokenStore`ë¥¼ íšë“í•´ì•¼ í•©ë‹ˆë‹¤. ë¦¬ì†ŒìŠ¤ë¥¼ íšë“í•˜ëŠ” ëª¨ë“ˆ ë‚´ë¶€ì˜ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” ë‹¤ë¥¸ ëª¨ë“ˆì˜ í•¨ìˆ˜ê°€ ìˆëŠ” ê²½ìš°, ì²« ë²ˆì§¸ í•¨ìˆ˜ì— `acquires()`ë¡œ ë ˆì´ë¸”ì„ ì§€ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.\n\në¦¬ì†ŒìŠ¤ê°€ ê³„ì • ë‚´ë¶€ì— ì €ì¥ë˜ë¯€ë¡œ ì†Œìœ ê¶Œì´ ëª…í™•í•´ì§‘ë‹ˆë‹¤. ê³„ì •ì€ ë¦¬ì†ŒìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•´ë‹¹ ë¦¬ì†ŒìŠ¤ë¥¼ ì •ì˜í•˜ëŠ” ëª¨ë“ˆì€ í•´ë‹¹ êµ¬ì¡°ì²´ë¥¼ ì½ê³  ìˆ˜ì •í•˜ëŠ” ê¶Œí•œì„ ê°€ì§‘ë‹ˆë‹¤. ë”°ë¼ì„œ í•´ë‹¹ ëª¨ë“ˆ ë‚´ë¶€ì˜ ì½”ë“œëŠ” í•´ë‹¹ êµ¬ì¡°ì²´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ íšë“í•´ì•¼ í•©ë‹ˆë‹¤.\n\nê·¸ë˜ë„ Moveì—ì„œ ë¹Œë¦¬ê±°ë‚˜ ì´ë™í•˜ëŠ” ëª¨ë“  ìœ„ì¹˜ì—ì„œ ìë™ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ë¥¼ íšë“í•˜ê²Œ ë©ë‹ˆë‹¤. ëª…í™•ì„±ì„ ìœ„í•´ ëª…ì‹œì  í¬í•¨ì„ ìœ„í•´ ì·¨ë“ì„ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤. ë§ˆì°¬ê°€ì§€ë¡œ `exists()` í•¨ìˆ˜ì—ëŠ” `acquires()` í•¨ìˆ˜ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\nì°¸ê³ : ìì‹ ì˜ ëª¨ë“ˆì— ì •ì˜ëœ êµ¬ì¡°ì²´ì—ì„œ ëª¨ë“  ê³„ì •ì˜ ëª¨ë“ˆ ë‚´ ì „ì—­(`global`)ì„ ë¹Œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“ˆ ì™¸ë¶€ì—ì„œ ì „ì—­ì„ ë¹Œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n## move_to\n\nê·¸ëŸ° ë‹¤ìŒ `move_to` í•¨ìˆ˜ë¥¼ ì„œëª…ì ë° ê³„ì •ì— ëŒ€í•œ ì°¸ì¡°ì™€ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°ì²´ë¥¼ ê³„ì •ìœ¼ë¡œ ì´ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ ê³¼ì •ì—ì„œ ìš°ë¦¬ëŠ” ê°€ì¹˜ê°€ ìˆëŠ” ì½”ì¸ì˜ ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\nYou may then use the `move_to` function along with a reference to signer and account to move the struct into an account. In the process, we create a new instance of coin with value.\n\n## Initialization\n\n`init_module`ì€ ëª¨ë“ˆì´ ë°°í¬ë  ë•Œ ìë™ìœ¼ë¡œ í˜¸ì¶œë˜ê³  ì‹¤í–‰ë©ë‹ˆë‹¤.\nThe `init_module` automatically gets called and run when the module is published:\n\n```shell\n    fun init_module(resource_account: &signer) {\n        let resource_signer_cap = resource_account::retrieve_resource_account_cap(resource_account, @source_addr);\n        let resource_signer = account::create_signer_with_capability(&resource_signer_cap);\n```\n\n`mint_nft_ticket()` í•¨ìˆ˜ëŠ” ì½œë ‰ì…˜ì„ ê°€ì ¸ì˜¤ê³  í† í°ì„ ìƒì„±í•©ë‹ˆë‹¤.\nThe `mint_nft_ticket()` function gets a collection and creates a token.\n\nTokenData IDì˜ ê²°ê³¼ë¡œ, í•¨ìˆ˜ëŠ” ëª¨ë“ˆì˜ ë¦¬ì†ŒìŠ¤ ì„œëª…ìë¥¼ ì‚¬ìš©í•˜ì—¬ í† í°ì„ NFT ìˆ˜ì‹ ìì— `mint`í•©ë‹ˆë‹¤.\n\nFor example:\n\n```shell\n    public entry fun mint_nft(receiver: &signer) acquires ModuleData {\n        let receiver_addr = signer::address_of(receiver);\n```\n\n## Signing\n\nëª¨ë“  `entry fun`ì€ `&signer` ìœ í˜•ì„ ì²« ë²ˆì§¸ ë§¤ê°œë³€ìˆ˜ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. Moveì™€ Aptos ëª¨ë‘ íŠ¸ëœì­ì…˜ì„ ì œì¶œí•  ë•Œë§ˆë‹¤ íŠ¸ëœì­ì…˜ì— ì„œëª…í•˜ëŠ” ê°œì¸ í‚¤ê°€ ìë™ìœ¼ë¡œ ì—°ê²°ëœ ê³„ì •ì„ ì„œëª…ìì˜ ì²« ë²ˆì§¸ ë§¤ê°œë³€ìˆ˜ë¡œ ë§Œë“­ë‹ˆë‹¤.\n\nì„œëª…ìì—ì„œ ì£¼ì†Œë¡œ ì´ë™í•  ìˆ˜ ìˆì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ê·¸ ë°˜ëŒ€ëŠ” ì•„ë‹™ë‹ˆë‹¤. ë”°ë¼ì„œ NFTë¥¼ ì²­êµ¬í•  ë•Œ ì•„ë˜ ì§€ì¹¨ê³¼ ê°™ì´ ìƒì„±ìì™€ ìˆ˜ì‹ ìì˜ ê°œì¸ í‚¤ê°€ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤.\n\n`init_module`ì—ì„œ ì„œëª…ìëŠ” í•­ìƒ ê³„ì•½ì„ ì—…ë¡œë“œí•˜ëŠ” ê³„ì •ì…ë‹ˆë‹¤. ì´ê²ƒì€ ë‹¤ìŒê³¼ ê²°í•©ë©ë‹ˆë‹¤.\n\n```move\n        token::create_collection(&resource_signer, collection, description, collection_uri, maximum_supply, mutate_setting);\n\n```\n\nThen:\n\n```move\n        signer_cap: account::SignerCapability,\n```\n\nì„œëª…ì ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë©´ ëª¨ë“ˆì´ ììœ¨ì ìœ¼ë¡œ ì„œëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì› ê³„ì •ì€ ëˆ„êµ¬ë„ ê°œì¸ í‚¤ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•˜ë„ë¡ í•˜ë©° ì „ì ìœ¼ë¡œ ê³„ì•½ì— ì˜í•´ ì œì–´ë©ë‹ˆë‹¤.\n\n## Module data\n\nê·¸ëŸ° ë‹¤ìŒ 'ModuleData'ê°€ ì´ˆê¸°í™”ë˜ê³  ì„œëª…ì ê¸°ëŠ¥ì´ ìˆëŠ” ë¦¬ì†ŒìŠ¤ ê³„ì •ìœ¼ë¡œ `_moved_`ë©ë‹ˆë‹¤.\n\n```shell\n        move_to(resource_account, ModuleData {\n```\n\n`mint_nft_ticket()` í•¨ìˆ˜ì—ì„œ ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” `ModuleData` êµ¬ì¡°ì²´ë¥¼ ì°¨ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n\n```shell\n        let module_data = borrow_global_mut<ModuleData>(@mint_nft);\n```\n\nê·¸ëŸ° ë‹¤ìŒ `ModuleData` êµ¬ì¡°ì²´ì˜ ì„œëª…ì ê¸°ëŠ¥ì— ëŒ€í•œ ì°¸ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ `resource_signer`ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n\n```shell\n        let resource_signer = account::create_signer_with_capability(&module_data.signer_cap);\n```\n\nì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ëª¨ë“ˆì— ì´ë¯¸ ì €ì¥ëœ ì„œëª…ì ê¸°ëŠ¥ì„ ë‚˜ì¤‘ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“ˆê³¼ í•´ë‹¹ êµ¬ì¡°ë¥¼ ê³„ì •ìœ¼ë¡œ ì´ë™í•˜ë©´ í•´ë‹¹ ê³„ì •ê³¼ ì—°ê²°ëœ Aptos Explorerì— í‘œì‹œë©ë‹ˆë‹¤.\n\n## Accounts\n\nì˜ˆë¥¼ ë“¤ì–´ NFTë¥¼ ë°œí–‰í•  ë•Œ NFTëŠ” ê³„ì • ì£¼ì†Œì— ì €ì¥ë©ë‹ˆë‹¤. íŠ¸ëœì­ì…˜ì„ ì œì¶œí•˜ë©´ íŠ¸ëœì­ì…˜ì— ì„œëª…í•©ë‹ˆë‹¤. `aptos init`ë¥¼ ì‹¤í–‰í•˜ëŠ” ìœ„ì¹˜(ì•„ë˜)ì™€ ê´€ë ¨ëœ `.aptos/config.yaml`ì—ì„œ ê³„ì • êµ¬ì„± ì •ë³´ë¥¼ ì°¾ìœ¼ì‹­ì‹œì˜¤.\n\në¦¬ì†ŒìŠ¤ ê³„ì •ì„ ì‚¬ìš©í•˜ë©´ íŠ¸ëœì­ì…˜ ì„œëª…ì„ ìœ„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¦¬ì†ŒìŠ¤ ê³„ì •ì„ ìƒì„±í•˜ì—¬ ë™ì¼í•œ ê³„ì •ì˜ ìƒˆ ë¦¬ì†ŒìŠ¤ì— ì €ì¥í•  ìˆ˜ ìˆê³  ììœ¨ì ìœ¼ë¡œ íŠ¸ëœì­ì…˜ì— ì„œëª…í•  ìˆ˜ ìˆëŠ” ì„œëª…ì ê¸°ëŠ¥ì„ ë¶€ì—¬í•©ë‹ˆë‹¤. ì•„ë¬´ë„ ìì› ê³„ì •ì˜ ê°œì¸ í‚¤ì— ì•¡ì„¸ìŠ¤í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì„œëª…ì ê¸°ëŠ¥ì´ ë³´í˜¸ë©ë‹ˆë‹¤.",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 5,
    "wordCount": 872,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "aptos-integration-guide",
    "slug": "aptos-integration-guide",
    "path": "blockchain/aptos",
    "fullPath": "blockchain/aptos/aptos-integration-guide",
    "title": "Aptos Integration Guide",
    "excerpt": "Aptos Integration Guide https://aptos.dev/guides/system-integrators-guide...",
    "content": "# Aptos Integration Guide\n\nhttps://aptos.dev/guides/system-integrators-guide",
    "docType": "original",
    "category": "Blockchain",
    "tags": [
      "Blockchain"
    ],
    "readingTime": 1,
    "wordCount": 6,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "í™•ì¥ ê°€ëŠ¥í•œ ì‹¤ì‹œê°„ WebSocket ì„œë²„ ì•„í‚¤í…ì²˜ ì„¤ê³„",
    "slug": "hwagjang-ganeunghan-silsigan-websocket-seobeo-akitegceo-seolgye",
    "path": "backend/websocket",
    "fullPath": "backend/websocket/hwagjang-ganeunghan-silsigan-websocket-seobeo-akitegceo-seolgye",
    "title": "í™•ì¥ ê°€ëŠ¥í•œ ì‹¤ì‹œê°„ WebSocket ì„œë²„ ì•„í‚¤í…ì²˜ ì„¤ê³„",
    "excerpt": "í™•ì¥ ê°€ëŠ¥í•œ ì‹¤ì‹œê°„ WebSocket ì„œë²„ ì•„í‚¤í…ì²˜ ì„¤ê³„",
    "content": "# í™•ì¥ ê°€ëŠ¥í•œ ì‹¤ì‹œê°„ WebSocket ì„œë²„ ì•„í‚¤í…ì²˜ ì„¤ê³„\n\nì‹¤ì‹œê°„ í†µì‹  í™˜ê²½ì—ì„œ ì„œë²„ë¥¼ ìˆ˜í‰ í™•ì¥(Horizontal Scaling)í•  ë•Œ ê°€ì¥ í° ê¸°ìˆ ì  ê³¼ì œëŠ” **'ìƒíƒœ(State)ì˜ ì¤‘ì•™ ì§‘ì¤‘í™”'**ì…ë‹ˆë‹¤. íŠ¹ì • í´ë¼ì´ì–¸íŠ¸ê°€ ì–´ëŠ ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ì— ì ìœ ë˜ì–´ ìˆëŠ”ì§€ë¥¼ ê´€ë¦¬í•˜ê³ , ì„œë²„ ê°„ ë©”ì‹œì§€ë¥¼ ì •í™•íˆ ë¼ìš°íŒ…í•˜ëŠ” ì„¤ê³„ê°€ í•µì‹¬ì…ë‹ˆë‹¤.\n\n---\n\n### 1. ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜\n\nì—¬ëŸ¬ ëŒ€ì˜ WebSocket ì„œë²„ê°€ ë¡œë“œë°¸ëŸ°ì„œ í•˜ë‹¨ì— ë°°ì¹˜ë˜ë©°, Redisë¥¼ ê³µìœ  ì €ì¥ì†Œ ë° ë©”ì‹œì§€ ë¸Œë¡œì»¤ë¡œ í™œìš©í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.\n\n\n```mermaid\ngraph TD\n    C1[Client A] --> LB(\"Load Balancer\")\n    C2[Client B] --> LB\n    LB --> WS1[\"WS Server 1<br/>(ID: SVR_1)\"]\n    LB --> WS2[\"WS Server 2<br/>(ID: SVR_2)\"]\n    \n    subgraph Redis_Layer [Redis Instance / Cluster]\n        R_HASH[(\"Redis Hash<br/>(Session Store)\")]\n        R_PUBSUB((\"(Redis Pub/Sub<br/>(Message Broker)\"))\n        R_STREAM[\"Redis Streams<br/>(Event Log)\"]\n    end\n\n    WS1 <--> R_HASH\n    WS2 <--> R_HASH\n    WS1 <--> R_PUBSUB\n    WS2 <--> R_PUBSUB\n    WS1 -.-> R_STREAM\n    WS2 -.-> R_STREAM\n\n    style LB fill:#f5f5f5,stroke:#333\n    style Redis_Layer fill:#fff5f5,stroke:#ff0000,stroke-dasharray: 5 5\n```\n\n---\n\n### 2. Redis ì»´í¬ë„ŒíŠ¸ë³„ ì—­í•  ì •ì˜\n\n|**ì»´í¬ë„ŒíŠ¸**|**í•µì‹¬ ê¸°ëŠ¥**|**ë¹„ê³ **|\n|---|---|---|\n|**Hash**|**ì „ì—­ ì„¸ì…˜ ê´€ë¦¬:** `{UserId: ServerId}` ë§¤í•‘ ì •ë³´ ì €ì¥|ì„¸ì…˜ ìœ„ì¹˜ í™•ì¸ìš©|\n|**Pub/Sub**|**ì‹¤ì‹œê°„ ë¼ìš°íŒ…:** ì„œë²„ ê°„ ì´ë²¤íŠ¸ ì „íŒŒ (Fire-and-forget)|ë‚®ì€ ì§€ì—° ì‹œê°„ ìš°ì„ |\n|**Streams**|**ì‹ ë¢°ì„± ë³´ì¥:** ë©”ì‹œì§€ ì˜ì†í™” ë° ì¬ì—°ê²° ì‹œ ëˆ„ë½ ë°ì´í„° ë³µêµ¬|At-least-once ì „ë‹¬ ë³´ì¥|\n\n---\n\n### 3. í•µì‹¬ ìš´ì˜ ì›Œí¬í”Œë¡œìš°\n\n#### â‘  ì„¸ì…˜ ë“±ë¡ í”„ë¡œì„¸ìŠ¤\n\ní´ë¼ì´ì–¸íŠ¸ê°€ í•¸ë“œì…°ì´í¬ë¥¼ ì™„ë£Œí•˜ë©´ Redis Hashì— ì„¸ì…˜ ì •ë³´ë¥¼ ê¸°ë¡í•˜ì—¬ ì „ì—­ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n\n\n```mermaid\nsequenceDiagram\n    participant C as Client A\n    participant S as WS Server 1 (SVR_1)\n    participant R as Redis Hash\n\n    C->>S: WebSocket Upgrade\n    S->>R: HSET user:session \"User_A\" \"SVR_1\"\n    R-->>S: OK\n    S-->>C: 101 Switching Protocols\n```\n\n#### â‘¡ ì„œë²„ ê°„ ë©”ì‹œì§€ ë¼ìš°íŒ…\n\nìƒëŒ€ë°©ì´ ë‹¤ë¥¸ ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ì— ì ‘ì† ì¤‘ì¼ ê²½ìš°, ì„¸ì…˜ ì •ë³´ë¥¼ ì¡°íšŒí•œ í›„ í•´ë‹¹ ì„œë²„ ì „ìš© ì±„ë„ë¡œ ë©”ì‹œì§€ë¥¼ ë°œí–‰í•©ë‹ˆë‹¤.\n\n\n```mermaid\nsequenceDiagram\n    participant S1 as Server 1 (User A)\n    participant RH as Redis Hash\n    participant RP as Redis Pub/Sub\n    participant S2 as Server 2 (User B)\n\n    S1->>RH: HGET user:session \"User_B\"\n    RH-->>S1: return \"SVR_2\"\n    S1->>RP: PUBLISH \"server:SVR_2\" \"{ payload }\"\n    Note over RP, S2: S2ëŠ” 'server:SVR_2' ì±„ë„ êµ¬ë… ì¤‘\n    RP-->>S2: Message Received\n    S2->>S2: Local Session ë§¤í•‘ í›„ í´ë¼ì´ì–¸íŠ¸ ì „ì†¡\n```\n\n#### â‘¢ Redis Streamsë¥¼ ì´ìš©í•œ ë°ì´í„° ë³µêµ¬\n\në„¤íŠ¸ì›Œí¬ ìˆœë‹¨ ì‹œ í´ë¼ì´ì–¸íŠ¸ê°€ `Last-Event-ID`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëˆ„ë½ëœ ë©”ì‹œì§€ë¥¼ ìš”ì²­í•˜ëŠ” íë¦„ì…ë‹ˆë‹¤.\n\n\n```mermaid\nflowchart TD\n    Start([í´ë¼ì´ì–¸íŠ¸ ì¬ì—°ê²°]) --> Request[Last-Event-ID í¬í•¨ ìš”ì²­ ì „ì†¡]\n    Request --> Check{ID ìœ íš¨ì„± ê²€ì‚¬}\n    \n    Check -- \"ìœ íš¨\" --> Read[XREAD: Last-ID ì´í›„ ë°ì´í„° ì¸ì¶œ]\n    Check -- \"ë§Œë£Œ/ì—†ìŒ\" --> Default[XREAD: ìµœì‹  ë°ì´í„° $ ë¶€í„° êµ¬ë…]\n    \n    Read --> Push[WebSocketìœ¼ë¡œ ëˆ„ë½ë¶„ Push]\n    Push --> RealTime[ì‹¤ì‹œê°„ ëª¨ë“œ ì „í™˜]\n    Default --> RealTime\n\n    style Start fill:#f9f,stroke:#333\n    style RealTime fill:#ccf,stroke:#333\n```\n\n---\n\n### 4. ì„¤ê³„ ë° ìš´ì˜ ê°€ì´ë“œë¼ì¸\n\n- **ì¢€ë¹„ ì„¸ì…˜(Zombie Session) ë°©ì§€:** ì„œë²„ ë¹„ì •ìƒ ì¢…ë£Œ ì‹œ ì„¸ì…˜ ì •ë³´ê°€ ë‚¨ì§€ ì•Šë„ë¡ ì„¸ì…˜ í‚¤ì— **TTL**ì„ ì„¤ì •í•˜ê³ , **Heartbeat(Ping/Pong)** ì‹œì ì— ìˆ˜ì‹œë¡œ ê°±ì‹ í•´ì•¼ í•©ë‹ˆë‹¤.\n    \n- **Graceful Shutdown:** ë°°í¬ ì‹œ ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ê°€ ì¢…ë£Œ ì‹ í˜¸(`SIGTERM`)ë¥¼ ë°›ìœ¼ë©´, ê´€ë¦¬ ì¤‘ì¸ ëª¨ë“  ì„¸ì…˜ì„ Redis Hashì—ì„œ ì‚­ì œ(`HDEL`)í•œ í›„ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•´ì•¼ í•©ë‹ˆë‹¤.\n    \n- **Backpressure ì œì–´:** í´ë¼ì´ì–¸íŠ¸ì˜ ìˆ˜ì‹  ì†ë„ê°€ ëŠë¦´ ê²½ìš° ì„œë²„ ë©”ëª¨ë¦¬ ë¶€í•˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Redis Streamsì˜ **Consumer Group** ê¸°ëŠ¥ì„ í™œìš©í•´ ì²˜ë¦¬ëŸ‰ì„ ì¡°ì ˆí•˜ëŠ” ì„¤ê³„ë¥¼ ê²€í† í•˜ì‹­ì‹œì˜¤.",
    "docType": "original",
    "category": "Backend_Websocket",
    "tags": [],
    "readingTime": 3,
    "wordCount": 453,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ë°ì½”ë ˆì´í„°-decorator",
    "slug": "dekoreiteo-decorator",
    "path": "backend/patterns",
    "fullPath": "backend/patterns/dekoreiteo-decorator",
    "title": "ë°ì½”ë ˆì´í„° (Decorator)",
    "excerpt": "ë°ì½”ë ˆì´í„° (Decorator) ì •ì˜ ë°ì½”ë ˆì´í„°ëŠ” í´ë˜ìŠ¤ ì„ ì–¸, ë©”ì„œë“œ, ì ‘ê·¼ì, í”„ë¡œí¼í‹°, ë˜ëŠ” ë§¤ê°œë³€ìˆ˜ì— ì²¨ë¶€í•  ìˆ˜ ìˆëŠ” íŠ¹ìˆ˜í•œ ì¢…ë¥˜ì˜ ì„ ì–¸ì…ë‹ˆë‹¤. ì‚¬ìš© í˜•ì‹ì€ ê³¼ ê°™ìœ¼ë©°, ì—¬ê¸°ì„œ ì€ ë°ì½”ë ˆì´íŒ… ëœ ì„ ì–¸...",
    "content": "# ë°ì½”ë ˆì´í„° (Decorator)\n\n## ì •ì˜\n\në°ì½”ë ˆì´í„°ëŠ” í´ë˜ìŠ¤ ì„ ì–¸, ë©”ì„œë“œ, ì ‘ê·¼ì, í”„ë¡œí¼í‹°, ë˜ëŠ” ë§¤ê°œë³€ìˆ˜ì— ì²¨ë¶€í•  ìˆ˜ ìˆëŠ” íŠ¹ìˆ˜í•œ ì¢…ë¥˜ì˜ ì„ ì–¸ì…ë‹ˆë‹¤.  ì‚¬ìš© í˜•ì‹ì€ `@expression`ê³¼ ê°™ìœ¼ë©°, ì—¬ê¸°ì„œ `@expression`ì€ ë°ì½”ë ˆì´íŒ… ëœ ì„ ì–¸ì— ëŒ€í•œ ì •ë³´ì™€ í•¨ê»˜ ëŸ°íƒ€ì„ì— í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤. ë°ì½”ë ˆì´í„°ëŠ” í´ë˜ìŠ¤ì™€ í´ë˜ìŠ¤ ë©¤ë²„ì— ì–´ë…¸í…Œì´ì…˜ê³¼ ë©”íƒ€-í”„ë¡œê·¸ë˜ë° êµ¬ë¬¸ì„ ì¶”ê°€í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.\n\n## ë°ì½”ë ˆì´í„° íŒ©í† ë¦¬ (Decorator Factories)\n\n```typescript\nfunction color(value: string) { // ë°ì½”ë ˆì´í„° íŒ©í† ë¦¬\n\treturn function (target) { // ë°ì½”ë ˆì´í„°\n\t\t// 'target'ê³¼ 'value' ë³€ìˆ˜ë¥¼ ê°€ì§€ê³  ë¬´ì–¸ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤\n\t\t...\n\t}\n}\n\n# Reference\n- https://www.typescriptlang.org/docs/handbook/decorators.html#handbook-content\n```\n\n## ë°ì½”ë ˆì´í„° í•©ì„± (Decorator Composition)\n\nTypeScriptì—ì„œ ë‹¨ì¼ ì„ ì–¸ì—ì„œ ì—¬ëŸ¬ ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•  ë•Œ ë‹¤ìŒ ë‹¨ê³„ê°€ ìˆ˜í–‰ë©ë‹ˆë‹¤.\n\n1.  ê° ë°ì½”ë ˆì´í„°ì˜ í‘œí˜„ì€ ìœ„ì—ì„œ ì•„ë˜ë¡œ í‰ê°€ë©ë‹ˆë‹¤.\n2.  ê·¸ëŸ° ë‹¤ìŒ ê²°ê³¼ëŠ” ì•„ë˜ì—ì„œ ìœ„ë¡œ í•¨ìˆ˜ë¡œ í˜¸ì¶œë©ë‹ˆë‹¤.\n\nì—¬ëŸ¬ ë°ì½”ë ˆì´í„°ê°€ ë‹¨ì¼ ì„ ì–¸ì— ì ìš©ë˜ëŠ” ê²½ìš° ì˜ˆì‹œë¥¼ ë³´ì—¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\n```typescript\nfunction first() {  \n  console.log('first(): factory evaluated');  \n  return (target: any, propertyKey: string, descriptor: PropertyDescriptor) => {  \n    console.log('first(): called');  \n  };  \n}  \n  \nfunction second() {  \n  console.log('second(): factory evaluated');  \n  return (target: any, propertyKey: string, descriptor: PropertyDescriptor) => {  \n    console.log('second(): called');  \n  };  \n}  \n  \nclass ExampleClass {  \n  @first()  \n  @second()  \n  method() {}  \n}\n\n# Reference\n- https://www.typescriptlang.org/docs/handbook/decorators.html#handbook-content\n```\nìœ„ `ExampleClass`ì˜  `method` ë¥¼ í˜¸ì¶œí•˜ë©´ ì•„ë˜ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ê²Œ ë©ë‹ˆë‹¤\n```\nfirst(): factory evaluated\nsecond(): factory evaluated\nsecond(): called\nfirst(): called\n```\n\n## ë°ì½”ë ˆì´í„° í‰ê°€ (Decorator Evaluation)\n\ní´ë˜ìŠ¤ì—ì„œ ë‹¤ì–‘í•œ ì„ ì–¸ì— ë°ì½”ë ˆì´í„°ë¥¼ ì ìš©í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ì´ ì˜ ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n1. _Parameter Decorators_, followed byÂ _Method_,Â _Accessor_, orÂ _Property Decorators_Â are applied for each instance member.\n2.  _Parameter Decorators_, followed byÂ _Method_,Â _Accessor_, orÂ _Property Decorators_Â are applied for each static member.\n3.  _Parameter Decorators_Â are applied for the constructor.\n4.  _Class Decorators_Â are applied for the class.\n\n## í´ë˜ìŠ¤ ë°ì½”ë ˆì´í„° (Class Decorators)\n\ní´ë˜ìŠ¤ ë°ì½”ë ˆì´í„°ëŠ” í´ë˜ìŠ¤ ì„ ì–¸ ì§ì „ì— ì„ ì–¸ë˜ë©°, í•´ë‹¹ í´ë˜ìŠ¤ì˜ ìƒì„±ìì— ì ìš©ë©ë‹ˆë‹¤.\n```typescript\n@sealed\nclass BugReport {\n  type = \"report\";\n  title: string;\n  \n  constructor(t: string) {\n    this.title = t;\n  }\n}\n\nfunction sealed(constructor: Function) {\n  Object.seal(constructor);\n  Object.seal(constructor.prototype);\n}\n```\n\n```typescript\nfunction reportableClassDecorator<T extends { new (...args: any[]): {} }>(  \n  constructor: T,  \n) {  \n  return class extends constructor {  \n    reportingURL = 'http://www...';  \n  };  \n}  \n  \n@reportableClassDecorator  \nclass BugReport {  \n  type = 'report';  \n  title: string;  \n  \n  constructor(t: string) {  \n    this.title = t;  \n  }  \n}  \n  \nconst bug = new BugReport('Needs dark mode');  \nconsole.log(bug.title); // Prints \"Needs dark mode\"  \nconsole.log(bug.type); // Prints \"report\"  \n  \n// Note that the decorator *does not* change the TypeScript type  \n// and so the new property `reportingURL` is not known  \n// to the type system:  \nbug.reportingURL;\n\nProperty 'reportingURL' does not exist on type 'BugReport'.\n```\n\n## ë©”ì†Œë“œ ë°ì½”ë ˆì´í„° (Method Decorators)\n\në©”ì„œë“œ ë°ì½”ë ˆì´í„°ëŠ” ë©”ì„œë“œ ì„ ì–¸ ì§ì „ì— ì„ ì–¸ë©ë‹ˆë‹¤. ë°ì½”ë ˆì´í„°ëŠ” ë©”ì„œë“œì˜Â í”„ë¡œí¼í‹° ì„¤ëª…ì(Property Descriptor)Â ì— ì ìš©ë˜ë©° ë©”ì„œë“œ ì •ì˜ë¥¼ ê´€ì°°, ìˆ˜ì • ë˜ëŠ” ëŒ€ì²´í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë©”ì„œë“œ ë°ì½”ë ˆì´í„°ëŠ” ì„ ì–¸ íŒŒì¼, ì˜¤ë²„ë¡œë“œ ë˜ëŠ” ê¸°íƒ€ ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸(ì˜ˆ:Â `ì„ ì–¸`Â í´ë˜ìŠ¤)ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\në©”ì„œë“œ ë°ì½”ë ˆì´í„°ì˜ í‘œí˜„ì‹ì€ ëŸ°íƒ€ì„ì— ë‹¤ìŒ ì„¸ ê°œì˜ ì¸ìˆ˜ì™€ í•¨ê»˜ í•¨ìˆ˜ë¡œ í˜¸ì¶œë©ë‹ˆë‹¤:\n\n1.  ì •ì  ë©¤ë²„ì— ëŒ€í•œ í´ë˜ìŠ¤ì˜ ìƒì„±ì í•¨ìˆ˜ ë˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ ë©¤ë²„ì— ëŒ€í•œ í´ë˜ìŠ¤ì˜ í”„ë¡œí† íƒ€ì…ì…ë‹ˆë‹¤.\n2.  ë©¤ë²„ì˜ ì´ë¦„\n3.  ë©¤ë²„ì˜Â _í”„ë¡œí¼í‹° ì„¤ëª…ì_\n\n> ì°¸ê³ â€ƒ ìŠ¤í¬ë¦½íŠ¸ ëŒ€ìƒì´ â€˜ES5â€™ë³´ë‹¤ ë‚®ì€ ê²½ìš°Â _í”„ë¡œí¼í‹° ì„¤ëª…ì_Â ëŠ” â€˜undefinedâ€™ì´ ë©ë‹ˆë‹¤.\n\në©”ì„œë“œ ë°ì½”ë ˆì´í„°ê°€ ê°’ì„ ë°˜í™˜í•˜ë©´, ë©”ì„œë“œì˜Â _í”„ë¡œí¼í‹° ì„¤ëª…ì_Â ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n\n> ì°¸ê³ â€ƒ ìŠ¤í¬ë¦½íŠ¸ ëŒ€ìƒì´ â€˜ES5â€™ë³´ë‹¤ ë‚®ì€ ê²½ìš° ë°˜í™˜ ê°’ì€ ë¬´ì‹œë©ë‹ˆë‹¤.\n\n```typescript\nfunction printClassName(enabled: boolean) {  \n  return function (  \n    target: any,  \n    propertyKey: string,  \n    descriptor: PropertyDescriptor,  \n  ) {  \n    if (enabled) {  \n      console.log(target.constructor.name);  \n    }  \n  };  \n}  \n  \nclass Person {  \n  msg: string;  \n  constructor(msg: string) {  \n    this.msg = msg;  \n  }  \n  @printClassName(true)  \n  sayMessage() {  \n    return this.msg;  \n  }  \n}\n```\n\n## ì ‘ê·¼ì ë°ì½”ë ˆì´í„° (Accessor Decorators)\n\nì ‘ê·¼ì ë°ì½”ë ˆì´í„°ëŠ” ì ‘ê·¼ì ì„ ì–¸ ë°”ë¡œ ì „ì— ì„ ì–¸ë©ë‹ˆë‹¤. ì ‘ê·¼ì ë°ì½”ë ˆì´í„°ëŠ” ì ‘ê·¼ìì˜Â _í”„ë¡œí¼í‹° ì„¤ëª…ì_ì— ì ìš©ë˜ë©° ì ‘ê·¼ìì˜ ì •ì˜ë¥¼ ê´€ì°°, ìˆ˜ì • ë˜ëŠ” êµì²´í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ‘ê·¼ì ë°ì½”ë ˆì´í„°ëŠ” ì„ ì–¸ íŒŒì¼ì´ë‚˜ ë‹¤ë¥¸ ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸(ì˜ˆ:Â `ì„ ì–¸`Â í´ë˜ìŠ¤)ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n> ì°¸ê³ â€ƒ TypeScriptëŠ” ë‹¨ì¼ ë©¤ë²„ì— ëŒ€í•´Â `get`Â ë°Â `set`Â ì ‘ê·¼ìë¥¼ ë°ì½”ë ˆì´íŒ… í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì‹  ë©¤ë²„ì˜ ëª¨ë“  ë°ì½”ë ˆì´í„°ë¥¼ ë¬¸ì„œ ìˆœì„œëŒ€ë¡œ ì§€ì •ëœ ì²« ë²ˆì§¸ ì ‘ê·¼ìì— ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ì™œëƒí•˜ë©´, ë°ì½”ë ˆì´í„°ëŠ” ê°ê°ì˜ ì„ ì–¸ì´ ì•„ë‹ŒÂ `get`ê³¼Â `set`Â ì ‘ê·¼ìë¥¼ ê²°í•©í•œÂ í”„ë¡œí¼í‹° ì„¤ëª…ìì— ì ìš©ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n\nì ‘ê·¼ì ë°ì½”ë ˆì´í„°ì˜ í‘œí˜„ ì‹ì€ ëŸ°íƒ€ì„ì— ë‹¤ìŒ ì„¸ ê°€ì§€ ì¸ìˆ˜ì™€ í•¨ê»˜ í•¨ìˆ˜ë¡œ í˜¸ì¶œë©ë‹ˆë‹¤:\n\n1.  ì •ì  ë©¤ë²„ì— ëŒ€í•œ í´ë˜ìŠ¤ì˜ ìƒì„±ì í•¨ìˆ˜ ë˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ ë©¤ë²„ì— ëŒ€í•œ í´ë˜ìŠ¤ì˜ í”„ë¡œí† íƒ€ì…\n2.  ë©¤ë²„ì˜ ì´ë¦„\n3.  ë©¤ë²„ì˜Â _í”„ë¡œí¼í‹° ì„¤ëª…ì_\n\n> ì°¸ê³ â€ƒ ìŠ¤í¬ë¦½íŠ¸ ëŒ€ìƒì´ â€˜ES5â€™ë³´ë‹¤ ë‚®ì€ ê²½ìš°Â í”„ë¡œí¼í‹° ì„¤ëª…ìëŠ”Â `undefined`ê°€ ë©ë‹ˆë‹¤.\n\nì ‘ê·¼ì ë°ì½”ë ˆì´í„°ê°€ ê°’ì„ ë°˜í™˜í•˜ë©´ ë©¤ë²„ì˜Â í”„ë¡œí¼í‹° ì„¤ëª…ìë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n\n> ì°¸ê³ â€ƒ ìŠ¤í¬ë¦½íŠ¸ ëŒ€ìƒì´ â€˜ES5â€™ë³´ë‹¤ ë‚®ì€ ê²½ìš° ë°˜í™˜ ê°’ì€ ë¬´ì‹œë©ë‹ˆë‹¤.\n\n## í”„ë¡œí¼í‹° ë°ì½”ë ˆì´í„° (Property Decorators)\n\ní”„ë¡œí¼í‹° ë°ì½”ë ˆì´í„°ëŠ” í”„ë¡œí¼í‹° ì„ ì–¸ ë°”ë¡œ ì „ì— ì„ ì–¸ë©ë‹ˆë‹¤. í”„ë¡œí¼í‹° ë°ì½”ë ˆì´í„°ëŠ” ì„ ì–¸ íŒŒì¼ì´ë‚˜ ë‹¤ë¥¸ ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸(ì˜ˆ:Â `ì„ ì–¸`Â í´ë˜ìŠ¤)ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\ní”„ë¡œí¼í‹° ë°ì½”ë ˆì´í„°ì˜ í‘œí˜„ ì‹ì€ ëŸ°íƒ€ì„ì— ë‹¤ìŒ ë‘ ê°œì˜ ì¸ìˆ˜ì™€ í•¨ê»˜ í•¨ìˆ˜ë¡œ í˜¸ì¶œë©ë‹ˆë‹¤:\n\n1.  ì •ì  ë©¤ë²„ì— ëŒ€í•œ í´ë˜ìŠ¤ì˜ ìƒì„±ì í•¨ìˆ˜ ë˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ ë©¤ë²„ì— ëŒ€í•œ í´ë˜ìŠ¤ì˜ í”„ë¡œí† íƒ€ì…\n2.  ë©¤ë²„ì˜ ì´ë¦„\n\n> ì°¸ê³ â€ƒ TypeScriptì—ì„œÂ `í”„ë¡œí¼í‹° ë°ì½”ë ˆì´í„°`ê°€ ì´ˆê¸°í™”ë˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì¸í•´Â í”„ë¡œí¼í‹° ì„¤ëª…ìê°€ í”„ë¡œí¼í‹° ë°ì½”ë ˆì´í„°ì— ëŒ€í•œ ì¸ìˆ˜ë¡œ ì œê³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í˜„ì¬ í”„ë¡œí† íƒ€ì…ì˜ ë©¤ë²„ë¥¼ ì •ì˜í•  ë•Œ ì¸ìŠ¤í„´ìŠ¤ í”„ë¡œí¼í‹°ë¥¼ ì„¤ëª…í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì´ ì—†ê³  í”„ë¡œí¼í‹°ì˜ ì´ë‹ˆì…œë¼ì´ì €ë¥¼ ê´€ì°°í•˜ê±°ë‚˜ ìˆ˜ì •í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ì—†ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë°˜í™˜ ê°’ë„ ë¬´ì‹œë©ë‹ˆë‹¤. ë”°ë¼ì„œ í”„ë¡œí¼í‹° ë°ì½”ë ˆì´í„°ëŠ” íŠ¹ì • ì´ë¦„ì˜ í”„ë¡œí¼í‹°ê°€ í´ë˜ìŠ¤ì— ì„ ì–¸ë˜ì—ˆìŒì„ ê´€ì°°í•˜ëŠ” ë°ë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì´ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ì˜ˆì™€ ê°™ì´ í”„ë¡œí¼í‹°ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n```typescript\nclass Greeter {  \n  @format(\"Hello, %s\")  \n  greeting: string;  \n  \n  constructor(message: string) {  \n    this.greeting = message;  \n  }  \n  \n  greet() {  \n    let formatString = getFormat(this, \"greeting\");  \n    return formatString.replace(\"%s\", this.greeting);  \n  }  \n}\n```\n\në‹¤ìŒ í•¨ìˆ˜ ì„ ì–¸ì„ ì‚¬ìš©í•˜ì—¬Â `@format`Â ë°ì½”ë ˆì´í„°ì™€Â `getFormat`Â í•¨ìˆ˜ë¥¼ ì •ì˜ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n```typescript\nimport \"reflect-metadata\";  \n\nconst formatMetadataKey = Symbol(\"format\");  \n\nfunction format(formatString: string) {  \n  return Reflect.metadata(formatMetadataKey, formatString);  \n}  \n\nfunction getFormat(target: any, propertyKey: string) {  \n  return Reflect.getMetadata(formatMetadataKey, target, propertyKey);  \n}\n```\n\n`@format(\"Hello, %s\")`Â ë°ì½”ë ˆì´í„°ëŠ”Â [ë°ì½”ë ˆì´í„° íŒ©í† ë¦¬](https://www.typescriptlang.org/ko/docs/handbook/decorators.html#%EB%8D%B0%EC%BD%94%EB%A0%88%EC%9D%B4%ED%84%B0-%ED%8C%A9%ED%86%A0%EB%A6%AC-Decorator-Factories)ì…ë‹ˆë‹¤.Â `@format(\"Hello, %s\")`ê°€ í˜¸ì¶œë˜ë©´Â `reflect-metadata`Â ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜Â `Reflect.metadata`Â í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡œí¼í‹°ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„° í•­ëª©ì„ ì¶”ê°€í•©ë‹ˆë‹¤.Â `getFormat`ì´ í˜¸ì¶œë˜ë©´ í˜•ì‹ì˜ ë©”íƒ€ë°ì´í„° ê°’ì„ ì½ìŠµë‹ˆë‹¤.\n\n> ì°¸ê³ â€ƒ ì´ ì˜ˆì œì—ëŠ”Â `reflect-metadata`Â ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.Â `reflect-metadata`Â ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€Â [ë©”íƒ€ë°ì´í„°](https://www.typescriptlang.org/ko/docs/handbook/decorators.html#%EB%A9%94%ED%83%80%EB%8D%B0%EC%9D%B4%ED%84%B0-metadata)ë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.\n\n\n## ë©”íƒ€ë°ì´í„° (Metadata)\n\nì¼ë¶€ ì˜ˆì œëŠ”Â [ì‹¤í—˜ì  ë©”íƒ€ë°ì´í„° API](https://github.com/rbuckton/ReflectDecorators)ì— ëŒ€í•œ í´ë¦¬í•„(polyfill)ì„ ì¶”ê°€í•˜ëŠ”Â `reflect-metadata`Â ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì•„ì§ ECMAScript (JavaScript) í‘œì¤€ì˜ ì¼ë¶€ê°€ ì•„ë‹™ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë°ì½”ë ˆì´í„°ê°€ ê³µì‹ì ìœ¼ë¡œ ECMAScript í‘œì¤€ì˜ ì¼ë¶€ë¡œ ì±„íƒë˜ë©´ ì´ëŸ¬í•œ í™•ì¥ì„ ì±„íƒí•˜ê²Œ ë  ê²ƒì…ë‹ˆë‹¤.\n\nnpmì„ í†µí•´ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n`   npm i reflect-metadata --save   `\n\nTypeScriptì—ëŠ” ë°ì½”ë ˆì´í„°ê°€ ìˆëŠ” ì„ ì–¸ì— ëŒ€í•´ íŠ¹ì • íƒ€ì…ì˜ ë©”íƒ€ ë°ì´í„°ë¥¼ ë‚´ë³´ë‚´ëŠ” ì‹¤í—˜ì ì¸ ì§€ì›ì„ í¬í•¨í•©ë‹ˆë‹¤. ì´ ì‹¤í—˜ì ì¸ ì§€ì›ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë ¤ë©´, ëª…ë ¹í–‰ ë˜ëŠ”`tsconfig.json`ì—ì„œÂ `emitDecoratorMetadata`Â ì»´íŒŒì¼ëŸ¬ ì˜µì…˜ì„ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n\n**ëª…ë ¹ì¤„**:\n\n`tsc --target ES5 --experimentalDecorators --emitDecoratorMetadata`\n\n**tsconfig.json**:\n\n```typescript\n{  \n  \"compilerOptions\": {  \n    \"target\": \"ES5\",  \n    \"experimentalDecorators\": true,  \n    \"emitDecoratorMetadata\": true  \n  }  \n}\n```\n\ní™œì„±í™”ë˜ë©´Â `reflect-metadata`ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê°€ì ¸ì˜¤ê¸°ë§Œ í•˜ë©´ ì¶”ê°€ ë””ìì¸-íƒ€ì„ íƒ€ì… ì •ë³´ê°€ ëŸ°íƒ€ì„ì— ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\në‹¤ìŒ ì˜ˆì œì—ì„œ ì´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```typescript\nimport 'reflect-metadata';  \n  \nclass Point {  \n  constructor(public x: number, public y: number) {}  \n}  \n  \nclass Line {  \n  private _start: Point;  \n  private _end: Point;  \n  \n  @validate  \n  set start(value: Point) {  \n    this._start = value;  \n  }  \n  \n  get start() {  \n    return this._start;  \n  }  \n  \n  @validate  \n  set end(value: Point) {  \n    this._end = value;  \n  }  \n  \n  get end() {  \n    return this._end;  \n  }  \n}  \n  \nfunction validate<T>(  \n  target: any,  \n  propertyKey: string,  \n  descriptor: TypedPropertyDescriptor<T>,  \n) {  \n  let set = descriptor.set!;  \n  \n  descriptor.set = function (value: T) {  \n    let type = Reflect.getMetadata('design:type', target, propertyKey);  \n  \n    if (!(value instanceof type)) {  \n      throw new TypeError(  \n        `Invalid type, got ${typeof value} not ${type.name}.`,  \n      );  \n    }  \n  \n    set.call(this, value);  \n  };  \n}  \n  \nconst line = new Line();  \nline.start = new Point(0, 0);  \n  \n// @ts-ignore  \n// line.end = {}  \n  \n// Fails at runtime with:  \n// > Invalid type, got object not Point\n```\n\nTypeScript ì»´íŒŒì¼ëŸ¬ëŠ”Â `@Reflect.metadata`Â ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë””ìì¸-íƒ€ì„ íƒ€ì… ì •ë³´ë¥¼ ì£¼ì…í•©ë‹ˆë‹¤. ë‹¤ìŒ TypeScriptì™€ ë™ì¼í•˜ë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```typescript\nclass Line {  \n  private _start: Point;  \n  private _end: Point;  \n  @validate  \n  @Reflect.metadata('design:type', Point)  \n  set start(value: Point) {  \n    this._start = value;  \n  }  \n  get start() {  \n    return this._start;  \n  }  \n  @validate  \n  @Reflect.metadata('design:type', Point)  \n  set end(value: Point) {  \n    this._end = value;  \n  }  \n  get end() {  \n    return this._end;  \n  }  \n}\n```",
    "docType": "original",
    "category": "Research",
    "tags": [],
    "readingTime": 6,
    "wordCount": 1199,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ê°œì²´-ì§€í–¥-í”„ë¡œê·¸ë˜ë°-object-oriented-programming",
    "slug": "gaece-jihyang-peurogeuraeming-object-oriented-programming",
    "path": "backend/patterns",
    "fullPath": "backend/patterns/gaece-jihyang-peurogeuraeming-object-oriented-programming",
    "title": "ê°œì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë° (Object Oriented Programming)",
    "excerpt": "ê°œì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë° (Object Oriented Programming) ê°œì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì˜ ë„¤ê°€ì§€ ê¸°ë³¸ ì›ì¹™ ì¶”ìƒí™” - ê´€ë ¨ íŠ¹ì„± ë° ì—”í„°í‹°ì˜ ìƒí˜¸ ì‘ìš©ì„ í´ë˜ìŠ¤ë¡œ ëª¨ë¸ë§í•˜ì—¬ ì‹œìŠ¤í…œì˜ ì¶”ìƒì  í‘œí˜„ì„ ì •ì˜í•©ë‹ˆë‹¤. ìº¡ìŠí™” - ê°œì²´ì˜ ë‚´ë¶€...",
    "content": "# ê°œì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë° (Object Oriented Programming)\n\n## ê°œì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì˜ ë„¤ê°€ì§€ ê¸°ë³¸ ì›ì¹™\n\n1. ì¶”ìƒí™” - ê´€ë ¨ íŠ¹ì„± ë° ì—”í„°í‹°ì˜ ìƒí˜¸ ì‘ìš©ì„ í´ë˜ìŠ¤ë¡œ ëª¨ë¸ë§í•˜ì—¬ ì‹œìŠ¤í…œì˜ ì¶”ìƒì  í‘œí˜„ì„ ì •ì˜í•©ë‹ˆë‹¤.\n2. ìº¡ìŠí™” - ê°œì²´ì˜ ë‚´ë¶€ ìƒíƒœì™€ ê¸°ëŠ¥ì„ ìˆ¨ê¸°ê³  public í•¨ìˆ˜ ì„¸íŠ¸ë¥¼ í†µí•´ì„œë§Œ ê°œì²´ì— ì—‘ì„¸ìŠ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n3. ìƒì† - ê¸°ì¡´ ì¶”ìƒí™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒˆ ì¶”ìƒí™”ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n4. ë‹¤í˜•ì„± - ì—¬ëŸ¬ ì¶”ìƒí™” ì—ì„œ ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ìƒì†ëœ ì†ì„± ë˜ëŠ” ë©”ì„œë“œë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## C#ì˜ í´ë˜ìŠ¤, êµ¬ì¡°ì²´ ë° ë ˆì½”ë“œ ê°œìš”\n\n### ìº¡ìŠí™”\n\nìº¡ìŠí™”ëŠ” ê²½ìš°ì— ë”°ë¼ ê°œì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë°ì˜ ì²« ë²ˆì§¸ ì›ë˜ë¡œ ì¸ì‹ë©ë‹ˆë‹¤. í´ë˜ìŠ¤ ë˜ëŠ” êµ¬ì¡°ì²´ëŠ” ê·¸ê²ƒì˜ ì™¸ë¶€ì˜ ì½”ë“œì— ê° ë©¤ë²„ê°€ ì—‘ì„¸ìŠ¤í•˜ëŠ” ë°©ë²•ì„ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í´ë˜ìŠ¤ ë˜ëŠ” ì–´ì…ˆë¸”ë¦¬ ì™¸ë¶€ì—ì„œ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë ¤ëŠ” ë©”ì„œë“œ ë° ë³€ìˆ˜ë¥¼ ìˆ¨ê²¨ ì½”ë”© ì˜¤ë¥˜ ë˜ëŠ” ì•…ì˜ì ì¸ ì•…ìš© ê°€ëŠ¥ì„±ì„ ì œí•œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n### ë©¤ë²„\n\në©¤ë²„ëŠ” ëª¨ë“  ë©”ì„œë“œ, í•„ë“œ, ìƒìˆ˜, ì†ì„±, ì´ë²¤íŠ¸ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì–¸ì–´ì—ëŠ” ìˆì§€ë§Œ C#ì—ëŠ” ì „ì—­ ë³€ìˆ˜ ë˜ëŠ” ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì˜ ì§„ì…ì ì¸ `Main`ë©”ì„œë“œê¹Œì§€ë„ í´ë˜ìŠ¤ë‚˜ êµ¬ì¡°ì²´ ë‚´ì— ì„ ì–¸ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n\n## ì€í–‰ ê³„ì¢Œ ì˜ˆì œë¥¼ í†µí•œ ê°œì²´ì§€í–¥ í”„ë¡œê·¸ë˜ë° ì‚´í´ë³´ê¸°\n\n### ì€í–‰ ê³„ì¢Œì˜ ê¸°ëŠ¥\n\n1. ì€í–‰ê³„ì¢Œë¥¼ ê³ ìœ í•˜ê²Œ ì‹ë³„í•˜ëŠ” 10ìë¦¬ ìˆ«ìê°€ ìˆìŠµë‹ˆë‹¤.\n2. ì†Œìœ ìì˜ ì´ë¦„ì„ ì €ì¥í•˜ëŠ” ë¬¸ìì—´ì´ ìˆìŠµë‹ˆë‹¤.\n3. ì”ì•¡ì„ ê²€ì‚¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n4. ì˜ˆê¸ˆì„ í—ˆìš©í•©ë‹ˆë‹¤.\n5. ì¸ì¶œì„ í—ˆìš©í•©ë‹ˆë‹¤.\n6. ì´ˆê¸° ì”ì•¡ì€ ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.\n7. ì¸ì¶œì€ ë§ˆì´ë„ˆìŠ¤ ì”ì•¡ì„ ì´ˆë˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n### ì€í–‰ ê³„ì¢Œ í˜•ì‹ ì •ë¦¬\n\n```cs\nnamespace Classes;\n\npublic class BankAccount {\n  public string Number { get; }\n  public string Owner { get; set; }\n  public decimal Balance { get; }\n\n  public void MakeDeposit(decimal amount, DateTime date, string note)\n  {\n  }\n\n  public vode MakeWithdraw(decimal amount, DateTime date, string note)\n  {\n  }\n}\n```\n\n`BankAccount` í´ë˜ìŠ¤ëŠ” ë‹¤ì„¯ ê°œì˜ ë©¤ë²„ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì•ì˜ ì„¸ê°€ì§€ëŠ” ì†ì„±ì¸ë°, ì†ì„±ì€ ë°ì´í„° ìš”ì†Œì´ë©° ìœ íš¨ì„± ê²€ì‚¬ ë˜ëŠ” ê¸°íƒ€ ê·œì¹™ì„ ì ìš©í•˜ëŠ” ì½”ë“œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‚˜ë¨¸ì§€ ë‘ ê°€ì§€ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤. ë©”ì„œë“œëŠ” ë‹¨ì¼ í•¨ìˆ˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œ ë¸”ë¡ì…ë‹ˆë‹¤. í´ë˜ìŠ¤ì˜ ê° ë©¤ë²„ëŠ” ê·¸ê²ƒì˜ ì´ë¦„ì„ ì½ìœ¼ë©´ íƒ€ì¸ì´ í•´ë‹¹ í´ë˜ìŠ¤ê°€ ìˆ˜í–‰í•˜ëŠ” ì‘ì—…ì„ ì´í•´í•˜ê¸°ì— ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.\n\n### ìƒˆ ê³„ì¢Œ ê°œì„¤ (ìƒì„±ì)\n\n`BankAccount` í˜•ì‹ì˜ ìƒˆ ê°œì²´ë¥¼ ë§Œë“œëŠ” ê²ƒì€ í•´ë‹¹ ê°’ì„ í• ë‹¹í•˜ëŠ” ìƒì„±ìë¥¼ ì •ì˜í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. C#ì—ì„œ ìƒì„±ìëŠ” í´ë˜ìŠ¤ì™€ ì´ë¦„ì´ ê°™ì€ ë©¤ë²„ì…ë‹ˆë‹¤.\n\n```cs\nnamespace Classes;\n\npublic class BankAccount {\n  private static int accountNumberSeed = 1234567890;\n  public string Number { get; }\n  public string Owner { get; set; }\n  public decimal Balance { get; }\n\n  public BankAccount(string, name, decimal initialBalance)\n  {\n    // C#ëŠ” this í•œì •ìê°€ ì„ íƒì‚¬í•­ì´ë©° ì¼ë°˜ì ìœ¼ë¡œ ìƒëµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n    // this.Owner = name;\n    // this.Balance = initialBalance;\n    Owner = name;\n    Balance = initialBalance;\n    // ê³„ì¢Œ ë²ˆí˜¸\n    Number = accountNumberSeed.ToString();\n    accountNumberSeed++;\n  }\n\n  public void MakeDeposit(decimal amount, DateTime date, string note)\n  {\n  }\n\n  public vode MakeWithdraw(decimal amount, DateTime date, string note)\n  {\n  }\n}\n```\n\nìƒì„±ìëŠ” newë¥¼ ì‚¬ìš©í•˜ì—¬ ê°œì²´ë¥¼ ë§Œë“¤ ë•Œ í˜¸ì¶œë©ë‹ˆë‹¤.\n\n```cs\nusing Classes;\n\nvar account = new BankAccount(\"<name>\", 1000);\nConsole.WriteLine($\"Account {account.Number} was created for {account.Owner} with {account.Balance} initial balance.\");\n```\n\n### ì˜ˆê¸ˆ ë° ì¸ì¶œ ë§Œë“¤ê¸°\n\nì€í–‰ ê³„ì¢Œ í´ë˜ìŠ¤ì˜ ì˜ˆê¸ˆê³¼ ì¸ì¶œì„ êµ¬í˜„í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ë¨¼ì € íŠ¸ëœì­ì…˜ì„ ë‚˜íƒ€ë‚´ëŠ” ìƒˆ í´ë˜ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\nê·¸ë¦¬ê³  Transaction ê°œì²´ì˜ `List<T>`ë¥¼ `BankAccount`í´ë˜ìŠ¤ì— ì¶”ê°€í•˜ê³  Balance ì†ì„±ì„ ì—…ë°ì´íŠ¸ í•©ë‹ˆë‹¤.\në‹¤ìŒìœ¼ë¡œ `MakeDeposit`, `MakeWithdrawal`ë©”ì„œë“œë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\nê·¸ ë’¤ì—ëŠ” ìƒì„±ìì—ì„œ ìµœì´ˆ ì…ê¸ˆ ê±´ì„ ì¶”ê°€í•´ì¤„ ìˆ˜ ìˆë„ë¡ `MakeDeposit`ì„ í˜¸ì¶œí•˜ê²Œ í•´ë³´ê² ìŠµë‹ˆë‹¤.\n\n```cs\nnamespace Classses\n\npublic class Transaction\n{\n  public decimal Amount { get; }\n  public DateTime Date { get; }\n  public string Notes { get; }\n\n  public Transaction(decimal amount, DateTime date, string note)\n  {\n    Amount = amount;\n    Date = date;\n    Notes = note;\n  }\n}\n\npublic class BankAccount {\n  private static int accountNumberSeed = 1234567890;\n  private List<Transaction> allTransactions = new List<Transaction>();\n\n  public string Number { get; }\n  public string Owner { get; set; }\n  public decimal Balance\n  {\n     get\n     {\n        decimal balance = 0;\n        foreach (var item in allTransactions)\n        {\n          balanace += item.Amount;\n        }\n        return balance;\n     }\n  }\n\n  public BankAccount(string, name, decimal initialBalance)\n  {\n    // C#ëŠ” this í•œì •ìê°€ ì„ íƒì‚¬í•­ì´ë©° ì¼ë°˜ì ìœ¼ë¡œ ìƒëµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n    // this.Owner = name;\n    // this.Balance = initialBalance;\n    Owner = name;\n    MakeDeposit(initialBalance, DateTime.Now, \"Initial balance\");\n    // ê³„ì¢Œ ë²ˆí˜¸\n    Number = accountNumberSeed.ToString();\n    accountNumberSeed++;\n  }\n\n  public void MakeDeposit(decimal amount, DateTime date, string note)\n  {\n    if (amount <= 0)\n    {\n      throw new ArgumentOutOfRangeException(nameof(amount), \"Amount of deposit must be positive\");\n    }\n    var deposit = new Transaction(amount, date, note);\n    allTransactions.add(deposit);\n  }\n\n  public vode MakeWithdraw(decimal amount, DateTime date, string note)\n  {\n    if (amount <= 0)\n    {\n      throw new ArgumentOutOfRangeException(nameof(amount), \"Amount of withdrawal must be positive\");\n    }\n    if (Balance - amount < 0)\n    {\n      throw new InvalidOperationException(\"Not sufficient funds for this withdrawal\");\n    }\n    var deposit = new Transaction(-amount, date, note);\n    allTransactions.add(deposit);\n  }\n}\n```\n\n## Ref\n\n- [í´ë˜ìŠ¤ ë° ê°œì²´ë¥¼ ì‚¬ìš©í•œ ê°œì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë° ì‚´í´ë³´ê¸°](https://learn.microsoft.com/ko-kr/dotnet/csharp/fundamentals/tutorials/classes)\n- [ê°œì²´ ì§€í–¥ í”„ë¡œê·¸ë˜ë°(C#)](https://learn.microsoft.com/ko-kr/dotnet/csharp/fundamentals/tutorials/oop)\n- [C#ì˜ í´ë˜ìŠ¤, êµ¬ì¡°ì²´ ë° ë ˆì½”ë“œ ê°œìš”](https://learn.microsoft.com/ko-kr/dotnet/csharp/fundamentals/object-oriented/)",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [],
    "readingTime": 4,
    "wordCount": 726,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ê°œì²´-vs-ê°ì²´",
    "slug": "gaece-vs-gaegce",
    "path": "backend/patterns",
    "fullPath": "backend/patterns/gaece-vs-gaegce",
    "title": "ê°œì²´ vs ê°ì²´",
    "excerpt": "ê°œì²´ vs ê°ì²´ ë˜ëŠ” ë¼ëŠ” ìš©ì–´ëŠ” ê°œë°œì, íŠ¹íˆ ë°±ì—”ë“œ ê°œë°œì ì‚¬ì´ì—ì„œ ë¹¼ë†“ì„ ìˆ˜ ì—†ë‹¤. ë§¤ì²´ì— ë”°ë¼ì„œ ê°œì²´ ë˜ëŠ” ê°ì²´ë¼ê³  í‘œí˜„í•˜ëŠ” ëŠ” ë¬´ì—‡ì¼ê¹Œ? ê°ì²´ ì² í•™ ì˜ì‚¬ë‚˜ í–‰ìœ„ê°€ ë¯¸ì¹˜ëŠ” ëŒ€ìƒ. ì–¸ì–´ ë¬¸ì¥ ë‚´ì—ì„œ ë™ì‚¬ì˜ í–‰...",
    "content": "# ê°œì²´ vs ê°ì²´\n\n`ê°œì²´` ë˜ëŠ” `ê°ì²´`ë¼ëŠ” ìš©ì–´ëŠ” ê°œë°œì, íŠ¹íˆ ë°±ì—”ë“œ ê°œë°œì ì‚¬ì´ì—ì„œ ë¹¼ë†“ì„ ìˆ˜ ì—†ë‹¤. ë§¤ì²´ì— ë”°ë¼ì„œ ê°œì²´ ë˜ëŠ” ê°ì²´ë¼ê³  í‘œí˜„í•˜ëŠ” `Object`ëŠ” ë¬´ì—‡ì¼ê¹Œ?\n\n## ê°ì²´\n\n1. ì² í•™ ì˜ì‚¬ë‚˜ í–‰ìœ„ê°€ ë¯¸ì¹˜ëŠ” ëŒ€ìƒ.\n2. ì–¸ì–´ ë¬¸ì¥ ë‚´ì—ì„œ ë™ì‚¬ì˜ í–‰ìœ„ê°€ ë¯¸ì¹˜ëŠ” ëŒ€ìƒ.\n3. ì² í•™ ì‘ìš©ì˜ ëŒ€ìƒì´ ë˜ëŠ” ìª½.\n\n## ê°œì²´\n\n1. ì „ì²´ë‚˜ ì§‘ë‹¨ì— ìƒëŒ€í•˜ì—¬ í•˜ë‚˜í•˜ë‚˜ì˜ ë‚±ê°œë¥¼ ì´ë¥´ëŠ” ë§.\n2. ìƒëª… í•˜ë‚˜ì˜ ë…ë¦½ëœ ìƒë¬¼ì²´. ì‚´ì•„ê°€ëŠ” ë°ì— í•„ìš”í•œ ë…ë¦½ì ì¸ ê¸°ëŠ¥ì„ ê°–ê³  ìˆë‹¤.\n3. ì² í•™ ë‹¨ì¼í•˜ê³  ë…ë¦½ì ì¸ í†µì¼ì  ì¡´ì¬. ì² í•™ ì‚¬ìƒì˜ ë°œì „ ê³¼ì •ì—ì„œ ì´ í†µì¼ì„±ì€ ë¬¼ì§ˆì ã†ì–‘ì  ì¸¡ë©´, ë˜ëŠ” ì •ì‹ ì ã†ì§ˆì  ì¸¡ë©´ ë”°ìœ„ì˜ ì—¬ëŸ¬ ê´€ì ì—ì„œ ê³ ì°°ë˜ì—ˆë‹¤.\n\n## ë‚˜ë§Œì˜ ì •ë¦¬\n\nê°œì²´ê°€ ë§ëŠ” ê²ƒ ê°™ë‹¤. ê°ì²´ëŠ” ê°œì²´ë³´ë‹¤ ìˆ˜ë™ì ì¸ ê°œë…ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ”ë°, ì‹¤ì œ í´ë˜ìŠ¤ë“¤ì€ ì„œë¡œ ë‹¤ë¥¸ í´ë˜ìŠ¤ë“¤ê³¼ ìƒí˜¸ì‘ìš©ì„ í•˜ê¸° ë•Œë¬¸ì— ì˜í–¥ì„ ë°›ëŠ” ê²ƒ ë¿ë§Œ ì•„ë‹ˆë¼ ì˜í–¥ì„ ì£¼ê¸°ë„ í•˜ê¸° ë•Œë¬¸ì´ë‹¤.\n\n## Ref\n\n- [ê°œì²´ì§€í–¥í”„ë¡œê·¸ë˜ë°(C#)](https://learn.microsoft.com/ko-kr/dotnet/csharp/fundamentals/tutorials/oop)\n- [ê°ì²´ ì •ì˜](https://ko.dict.naver.com/#/entry/koko/9ef345f0a0884eca89d5c3155653fe94)\n- [ê°œì²´ ì •ì˜](https://ko.dict.naver.com/#/entry/koko/1f8d5f43afa2434f8d56f9ced1231c60)",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [],
    "readingTime": 1,
    "wordCount": 131,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "solid-ì›ì¹™",
    "slug": "solid-weoncig",
    "path": "backend/patterns",
    "fullPath": "backend/patterns/solid-weoncig",
    "title": "SOLID ì›ì¹™",
    "excerpt": "SOLID ì›ì¹™ Ref https://ko.wikipedia.org/wiki/SOLID_(%EA%B0%9D%EC%B2%B4_%EC%A7%80%ED%96%A5_%EC%84%A4%EA%B3%84)...",
    "content": "# SOLID ì›ì¹™\n\n## Ref\n\nhttps://ko.wikipedia.org/wiki/SOLID_(%EA%B0%9D%EC%B2%B4_%EC%A7%80%ED%96%A5_%EC%84%A4%EA%B3%84)",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [],
    "readingTime": 1,
    "wordCount": 7,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "ë°ì½”ë ˆì´í„°ë¥¼-ì´ìš©í•œ-nest-jsì—ì„œì˜-aop-ì ìš©",
    "slug": "dekoreiteoreul-iyonghan-nest-jseseoyi-aop-jeogyong",
    "path": "backend/nestjs",
    "fullPath": "backend/nestjs/dekoreiteoreul-iyonghan-nest-jseseoyi-aop-jeogyong",
    "title": "ë°ì½”ë ˆì´í„°ë¥¼ ì´ìš©í•œ Nest.jsì—ì„œì˜ AOP ì ìš©",
    "excerpt": "ë°ì½”ë ˆì´í„°ë¥¼ ì´ìš©í•œ Nest.jsì—ì„œì˜ AOP ì ìš© ë°ì½”ë ˆì´í„°? [ë°ì½”ë ˆì´í„°](<obsidian://open?vault=seogyugim.coinone&file=Typescript%2F%EB%8D%B0%EC%BD%94%EB%A0%88%EC%9D%B4%ED%...",
    "content": "# ë°ì½”ë ˆì´í„°ë¥¼ ì´ìš©í•œ Nest.jsì—ì„œì˜ AOP ì ìš©\n\n## ë°ì½”ë ˆì´í„°?\n\n[ë°ì½”ë ˆì´í„°](<obsidian://open?vault=seogyugim.coinone&file=Typescript%2F%EB%8D%B0%EC%BD%94%EB%A0%88%EC%9D%B4%ED%84%B0%20(Decorator)>) ë¬¸ì„œë¥¼ ì°¸ì¡°í•´ì£¼ì„¸ìš”!",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [
      "Backend",
      "NestJS"
    ],
    "readingTime": 1,
    "wordCount": 12,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "nestjs-dynamic-module-ì£¼ì˜ì ",
    "slug": "nestjs-dynamic-module-juyijeom",
    "path": "backend/nestjs",
    "fullPath": "backend/nestjs/nestjs-dynamic-module-juyijeom",
    "title": "NestJS Dynamic Module ì£¼ì˜ì ",
    "excerpt": "NestJS Dynamic Module ì£¼ì˜ì  ```typescript export const databaseProviders = [ { provide: MysqlDatasourceKey, inject: [ConfigService, MysqlConfigS...",
    "content": "# NestJS Dynamic Module ì£¼ì˜ì \n\n```typescript\nexport const databaseProviders = [\n\t{\n\t\tprovide: MysqlDatasourceKey,\n\t\tinject: [ConfigService, MysqlConfigService],\n\t\tuseFactory: async (\n\t\t\tconfigService: ConfigService,\n\t\t\tdatabaseConfigService: MysqlConfigService,\n\t\t) => {\n\t\t\tconst dataSource = new DataSource(\n\t\t\t\tdatabaseConfigService.getTypeormConfig(),\n\t\t\t)\n\t\t\treturn dataSource.initialize()\n\t\t},\n\t},\n]\n```\n\n> ìœ„ì™€ ê°™ì€ ìƒí™©ì—ì„œ useFactoryì— ì¸ìì—ëŠ” ë¬´ì¡°ê±´ **`inject`ë°°ì—´ì˜ ìˆœì„œ**ëŒ€ë¡œ ì¸ìŠ¤í„´ìŠ¤ê°€ ë“¤ì–´ì˜¨ë‹¤\n\n- ì°¸ê³ \n  - `MySqlConfigService` ëŠ” `ConfigService`ì— ì˜ì¡´ì„±ì´ ìˆë‹¤.",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [
      "Backend",
      "NestJS"
    ],
    "readingTime": 1,
    "wordCount": 60,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "nestjs-cache-manager-v5-ì‚¬ìš©ì‹œ-ë¬¸ì œ-í•´ê²°ë²•",
    "slug": "nestjs-cache-manager-v5-sayongsi-munje-haegyeolbeob",
    "path": "backend/nestjs",
    "fullPath": "backend/nestjs/nestjs-cache-manager-v5-sayongsi-munje-haegyeolbeob",
    "title": "NestJS, Cache-Manager v5 ì‚¬ìš©ì‹œ ë¬¸ì œ í•´ê²°ë²•",
    "excerpt": "NestJS, Cache-Manager v5 ì‚¬ìš©ì‹œ ë¬¸ì œ í•´ê²°ë²• ë¬¸ì œ ê³µì‹ë¬¸ì„œì— ë‚˜ì˜¨ëŒ€ë¡œ í•˜ë©´ ë°œìƒ ì›ì¸ ê°€ ë¡œ ì˜¬ë¼ê°€ë©´ì„œ ì‚¬ìš©ë²•ì´ ì•„ì˜ˆ...",
    "content": "# NestJS, Cache-Manager v5 ì‚¬ìš©ì‹œ ë¬¸ì œ í•´ê²°ë²•\n\n## ë¬¸ì œ\n\nê³µì‹ë¬¸ì„œì— ë‚˜ì˜¨ëŒ€ë¡œ í•˜ë©´ `this.cacheManager.get is not a function` ë°œìƒ\n\n## ì›ì¸\n\n`cache-manager` ê°€ `v5`ë¡œ ì˜¬ë¼ê°€ë©´ì„œ ì‚¬ìš©ë²•ì´ ì•„ì˜ˆ ë°”ë€œ..\n\n## í•´ê²°\n\n```typescript\nimport { registerAs } from '@nestjs/config'\nimport { CACHE_CONFIG_KEY } from '../constants/index.js'\nimport { redisStore } from 'cache-manager-redis-store'\nimport { CacheConfigSchema } from './config.zod.js'\n\nexport const cacheConfig = registerAs(CACHE_CONFIG_KEY, async () =>\n\tCacheConfigSchema.parseAsync({\n\t\thost: process.env.CACHE_HOST,\n\t\tport: Number(process.env.CACHE_PORT),\n\t\tttl: Number(process.env.CACHE_TTL),\n\t\tstore: redisStore,\n\t}),\n)\n\n@Module({\n  providers: [\n    {\n      provide: CACHE_MANAGER,\n      inject: [cacheConfig.KEY],\n      useFactory: ({ store, ...config }: ConfigType<typeof cacheConfig>) =>\n        caching(store, config),\n    },\n  ],\n})\nexport class AppCacheModule\n```",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [
      "Backend",
      "NestJS"
    ],
    "readingTime": 1,
    "wordCount": 99,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "http-ì¿ í‚¤-ë³´ì•ˆ-ì„¤ì •",
    "slug": "http-kuki-boan-seoljeong",
    "path": "backend/http",
    "fullPath": "backend/http/http-kuki-boan-seoljeong",
    "title": "HTTP ì¿ í‚¤ ë³´ì•ˆ ì„¤ì •",
    "excerpt": "HTTP ì¿ í‚¤ ë³´ì•ˆ ì„¤ì • [ì„¸ì…˜ í•˜ì´ì¬í‚¹ê³¼ XSS](https://developer.mozilla.org/ko/docs/Web/HTTP/Cookies%EC%84%B8%EC%85%98_%ED%95%98%EC%9D%B4%EC%9E%AC%ED%82%B9%EA%B...",
    "content": "# HTTP ì¿ í‚¤ ë³´ì•ˆ ì„¤ì •\n\n1. [ì„¸ì…˜ í•˜ì´ì¬í‚¹ê³¼ XSS](https://developer.mozilla.org/ko/docs/Web/HTTP/Cookies#%EC%84%B8%EC%85%98_%ED%95%98%EC%9D%B4%EC%9E%AC%ED%82%B9%EA%B3%BC_xss)\n2. [CSRF](https://developer.mozilla.org/ko/docs/Web/HTTP/Cookies#cross-site_%EC%9A%94%EC%B2%AD_%EC%9C%84%EC%A1%B0_csrf)\n3. [SameSite](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-Cookie/SameSite)\n\n\n## ì°¸ê³  ë¬¸ì„œ\n\n- [HTTP cookies ë³´ì•ˆ](https://developer.mozilla.org/ko/docs/Web/HTTP/Cookies#%EB%B3%B4%EC%95%88)",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [
      "Backend",
      "HTTP"
    ],
    "readingTime": 1,
    "wordCount": 21,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "http-ìš”ì²­-ìµœì†Œí™”-ê¸°ë²•",
    "slug": "http-yoceong-coesohwa-gibeob",
    "path": "backend/http",
    "fullPath": "backend/http/http-yoceong-coesohwa-gibeob",
    "title": "Http ìš”ì²­ ìµœì†Œí™” ê¸°ë²•",
    "excerpt": "Http ìš”ì²­ ìµœì†Œí™” ê¸°ë²• ë­ê°€ ìˆì„ê¹Œ? ë‹¤ë¥¸ ê³µê¸‰ìì— ëŒ€í•´ HTTP ìš”êµ¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì€ ë§ì€ í¬ë¡¤ í”„ë¡œê·¸ë¨ì´ë‚˜ ë°°ì¹˜ í”„ë¡œê·¸ë¨ì—ì„œ í•„ìš”í•  ìˆ˜ ìˆì§€ë§Œ, ìš”ì²­ ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³  ì˜¤ë¥˜ë‚˜ íƒ€ì„ì•„ì›ƒ ìœ„í—˜ì„ ì¤„ì´ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. HTTP ìš”ì²­ì„...",
    "content": "# Http ìš”ì²­ ìµœì†Œí™” ê¸°ë²•\n\n## ë­ê°€ ìˆì„ê¹Œ?\n\në‹¤ë¥¸ ê³µê¸‰ìì— ëŒ€í•´ HTTP ìš”êµ¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì€ ë§ì€ í¬ë¡¤ í”„ë¡œê·¸ë¨ì´ë‚˜ ë°°ì¹˜ í”„ë¡œê·¸ë¨ì—ì„œ í•„ìš”í•  ìˆ˜ ìˆì§€ë§Œ, ìš”ì²­ ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³  ì˜¤ë¥˜ë‚˜ íƒ€ì„ì•„ì›ƒ ìœ„í—˜ì„ ì¤„ì´ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. HTTP ìš”ì²­ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•œ ì¼ë°˜ì ì¸ ëª¨ë²” ì‚¬ë¡€ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n- ë°ì´í„° ë¡œì»¬ ìºì‹œí•˜ê¸°: ë™ì¼í•œ ë°ì´í„°ì— ëŒ€í•´ ë°˜ë³µì ìœ¼ë¡œ ìš”êµ¬í•˜ëŠ” ê²½ìš° ë¶ˆí•„ìš”í•œ HTTP ìš”ì²­ì„ ìˆ˜í–‰í•˜ì§€ ì•Šë„ë¡ ë°ì´í„° ë¡œì»¬ ìºì‹œë¥¼ ê²€í† í•˜ì‹­ì‹œì˜¤. Redisë‚˜ Memcached ë“±ì˜ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìì£¼ ì•¡ì„¸ìŠ¤ë˜ëŠ” ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ê±°ë‚˜ ë¡œì»¬ íŒŒì¼ ìºì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n- ë°°ì¹˜ ìš”ì²­: ë°ì´í„°ë§ˆë‹¤ ê°œë³„ ìš”ì²­ì„ ì‘ì„±í•˜ëŠ” ëŒ€ì‹  ìš”ì²­ì„ ë°°ì¹˜ ì²˜ë¦¬í•˜ì—¬ HTTP ìš”ì²­ ìˆ˜ë¥¼ ì¤„ì´ëŠ” ê²ƒì„ ê²€í† í•˜ì‹­ì‹œì˜¤. ì˜ˆë¥¼ ë“¤ì–´, í•˜ë‚˜ì˜ API í˜¸ì¶œì„ ì‚¬ìš©í•˜ì—¬ í•œ ë²ˆì— ì—¬ëŸ¬ ë ˆì½”ë“œë¥¼ í˜ì¹˜í•˜ê±°ë‚˜ ì—¬ëŸ¬ ìš”ì²­ì„ í•˜ë‚˜ì˜ ë°°ì¹˜ ìš”ì²­ì— ê²°í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n- í˜ì´ì§€ë„¤ì´ì…˜ ì‚¬ìš©: ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ ìš”êµ¬í•˜ëŠ” ê²½ìš° í˜ì´ì§€ë„¤ì´ì…˜ì„ ì‚¬ìš©í•˜ì—¬ í•œ ë²ˆì— ì‘ì€ ë°°ì¹˜ì˜ ë°ì´í„°ë¥¼ í˜ì¹˜í•˜ëŠ” ê²ƒì„ ê²€í† í•˜ì‹­ì‹œì˜¤. ì´ë¥¼ í†µí•´ ìš”ì²­ì˜ ì „ì²´ ìˆ˜ë¥¼ ì¤„ì´ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n- HTTP ìºì‹œ ì‚¬ìš©: ìš”êµ¬í•˜ëŠ” ë°ì´í„°ê°€ ìì£¼ ë³€ê²½ë  ê°€ëŠ¥ì„±ì´ ë‚®ì€ ê²½ìš° HTTP ìºì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ ìºì‹œì— ì‘ë‹µì„ ì €ì¥í•˜ëŠ” ê²ƒì„ ê²€í† í•˜ì‹­ì‹œì˜¤. ì´ë¥¼ í†µí•´ ì„œë²„ì— ëŒ€í•œ ìš”ì²­ ìˆ˜ë¥¼ ì¤„ì´ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n- ìš”ì²­ì— ìš°ì„  ìˆœìœ„ë¥¼ ë§¤ê¹€: ì—¬ëŸ¬ ë¦¬ì†ŒìŠ¤ì— ëŒ€í•´ ìš”ì²­ì„ í•  ê²½ìš° ê°€ì¥ ì¤‘ìš”í•œ ë°ì´í„°ê°€ ë¨¼ì € í˜ì¹˜ë˜ë„ë¡ ìš”ì²­ì— ìš°ì„  ìˆœìœ„ë¥¼ ë§¤ê¸°ëŠ” ê²ƒì„ ê²€í† í•˜ì‹­ì‹œì˜¤. ì´ë¥¼ í†µí•´ ìš”ì²­ì˜ ì „ì²´ ìˆ˜ë¥¼ ì¤„ì´ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì´ëŸ¬í•œ ëª¨ë²” ì‚¬ë¡€ë¥¼ ë”°ë¦„ìœ¼ë¡œì¨ í¬ë¡¤ í”„ë¡œê·¸ë¨ ë˜ëŠ” ë°°ì¹˜ í”„ë¡œê·¸ë¨ì— ì˜í•´ ìˆ˜í–‰ë˜ëŠ” HTTP ìš”ì²­ì˜ ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³  ì˜¤ë¥˜ ë° íƒ€ì„ì•„ì›ƒ ìœ„í—˜ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [
      "Backend",
      "HTTP"
    ],
    "readingTime": 2,
    "wordCount": 232,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "http-transfer-í¬ê¸°ë¥¼-ì¤„ì´ëŠ”-ë²•",
    "slug": "http-transfer-keugireul-julineun-beob",
    "path": "backend/http",
    "fullPath": "backend/http/http-transfer-keugireul-julineun-beob",
    "title": "HTTP Transfer í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ë²•",
    "excerpt": "HTTP Transfer í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ë²• ì¢…ë‹¨ ê°„ ì••ì¶• !ì¢…ë‹¨ ê°„ ì••ì¶• Accept-...",
    "content": "# HTTP Transfer í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ë²•\n\n1. ì¢…ë‹¨ ê°„ ì••ì¶•\n   ![ì¢…ë‹¨ ê°„ ì••ì¶•](https://developer.mozilla.org/en-US/docs/Web/HTTP/Compression/httpcompression1.png)\n    1. Accept-Encoding, Content-Encoding í´ë¼ì´ì–¸íŠ¸ì™€ ì„œë²„ì—ì„œ í—¤ë”ë¥¼ í†µí•´ ì •ë³´ë¥¼ ì£¼ê³  ë°›ëŠ”ë‹¤\n    2. ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ë‚˜ ë¹„ë””ì˜¤ ë“±ì˜ ë¯¸ë””ì–´ íŒŒì¼ì€ ì´ë¯¸ ì••ì¶•ë˜ì–´ ìˆì„ í™•ë¥ ì´ ë†’ë‹¤.\n    3. CPUê°€ ìƒëŒ€ì ìœ¼ë¡œ ë§ì´ ë“ ë‹¤.\n    4. gzipì´ ê°€ì¥ ë³´í¸ì ì´ë©°, êµ¬ê¸€ì—ì„œ ê°œë°œí•œ brì´ í›„ë°œì£¼ìë¡œ ë– ì˜¤ë¥¸ë‹¤.\n2. Hop-By-Hop ì••ì¶•\n   ![Hop-By-Hop ì••ì¶•](https://developer.mozilla.org/en-US/docs/Web/HTTP/Compression/httpcomp2.png)\n    1. í´ë¼ì´ì–¸íŠ¸ì™€ ì„œë²„ ì‚¬ì´ì˜ ê²½ë¡œì— ìˆëŠ” ë…¸ë“œë“¤ì—ì„œ ì••ì¶•ì´ ë°œìƒí•œë‹¤.\n    2. Transfer-Encoding í—¤ë”ë¥¼ í†µí•´ì„œ ì••ì¶• ë°©ë²•ì„ ì£¼ê³  ë°›ëŠ”ë‹¤.\n    3. ë³´í†µ í”„ë¡ì‹œ ê³„ì¸µì—ì„œ ì ìš©ëœë‹¤\n    4. `Transfer-Encoding: chunked`\n       í—¤ë”ê°€ ì‚¬ìš©ë  ë•ŒëŠ” ìš”ì²­ì´ ì™„ì „íˆ ì²˜ë¦¬ë˜ê¸° ì „ê¹Œì§€ ì‘ë‹µì˜ ì „ì²´ í¬ê¸°ë¥¼ ì•Œ ìˆ˜ ì—†ë‹¤. ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ì˜ ê²°ê³¼ê°€ ë  í° HTML í…Œì´ë¸”ì„ ìƒì„±í•˜ëŠ” ê²½ìš°ë‚˜, í° ì´ë¯¸ì§€ë¥¼ ì „ì†¡í•˜ëŠ” ê²½ìš°ê°€ ì˜ˆì‹œê°€ ëœë‹¤.\n\n## ì°¸ê³  ë¬¸ì„œ\n\n- [Web HTTP Compression](https://developer.mozilla.org/ko/docs/Web/HTTP/Compression)\n- [HTTP Headers Transfer-Encoding](https://developer.mozilla.org/ko/docs/Web/HTTP/Headers/Transfer-Encoding)\n- [ExpressJS Compression Middleware Library](https://github.com/expressjs/compression)",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [
      "Backend",
      "HTTP"
    ],
    "readingTime": 1,
    "wordCount": 124,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "eaddrnotavail",
    "slug": "eaddrnotavail",
    "path": "backend/http",
    "fullPath": "backend/http/eaddrnotavail",
    "title": "EADDRNOTAVAIL",
    "excerpt": "EADDRNOTAVAIL ë¬¸ì œ ìƒí™© node.js ì„œë²„ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë¦¬ëˆ…ìŠ¤ í™˜ê²½ì—ì„œ ì‚¬ìš©ì¤‘ì´ë‹¤. ì•„ë˜ì™€ ê°™ì€ ì—ëŸ¬ê°€ ë°œìƒí–ˆë‹¤ ```json { \"message\": \"request to aa failed, reason: connect EAD...",
    "content": "# EADDRNOTAVAIL\n\n## ë¬¸ì œ ìƒí™©\n\n1. node.js ì„œë²„ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë¦¬ëˆ…ìŠ¤ í™˜ê²½ì—ì„œ ì‚¬ìš©ì¤‘ì´ë‹¤.\n2. ì•„ë˜ì™€ ê°™ì€ ì—ëŸ¬ê°€ ë°œìƒí–ˆë‹¤\n\n```json\n{\n  \"message\": \"request to aa failed, reason: connect EADDRNOTAVAIL aa:30000 - Local (aa:0)\",\n  \"type\": \"system\",\n  \"errno\": \"EADDRNOTAVAIL\",\n  \"code\": \"EADDRNOTAVAIL\",\n  \"isFetchError\": true,\n  \"url\": \"aa\"\n}\n```\n\n## í•´ê²° ë°©ë²• ì¡°ì‚¬\n\n`etc/sysctl.conf`ì˜ `net.ipv4.tcp_tw_recycle`ë¥¼ í™œì„±í™” í•˜ëŠ” ê²ƒì€ ì¢‹ì€ ìƒê°ì´ ì•„ë‹ˆë‹¤.\n\nìœ„ ì˜µì…˜ ë³€ê²½ìœ¼ë¡œ íŠœë‹ í•˜ë¼ëŠ” ê°€ì´ë“œ ë¬¸ì„œê°€ ë§ì´ ë³´ì´ì§€ë§Œ, tcp(7) ë§¤ë‰´ì–¼ í˜ì´ì§€ì— ì˜í•˜ë©´ NATë¥¼ ê°€ì§€ê³  ë™ì‘í•  ë•Œ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆì–´ì„œ ê¶Œì¥í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  í•œë‹¤.\n\nTIME_WAIT ìƒíƒœì˜ ì ‘ì†ì€ ì ‘ì† í…Œì´ë¸”ì—ì„œ 1ë¶„ë™ì•ˆ ìœ ì§€ëœë‹¤. ì¦‰ ê°™ì€ ë„¤ ìŒë‘¥ì´(ì†ŒìŠ¤ì£¼ì†Œ, ì†ŒìŠ¤í¬íŠ¸, ëª©ì ì§€ì£¼ì†Œ, ëª©ì ì§€í¬íŠ¸)ì— ë‹¤ë¥¸ ì—°ê²°ì€ ì¡´ì¬í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì„ ëœ»í•œë‹¤.\nì›¹ ì„œë²„ë¡œ ì˜ˆë¥¼ ë“¤ë©´ ëª©ì ì§€ ì£¼ì†Œì™€ ëª©ì ì§€ í¬íŠ¸ëŠ” ìƒìˆ˜ì¼ ê²ƒì´ë‹¤. ì›¹ ì„œë²„ê°€ L7 ë¡œë“œë°¸ëŸ°ì„œ ë’¤ì— ìˆë‹¤ë©´ ì†ŒìŠ¤ ì£¼ì†Œ ë„í•œ ìƒìˆ˜ì¼ ìˆ˜ ìˆë‹¤. ë¦¬ëˆ…ìŠ¤ì—ì„œ í´ë¼ì´ì–¸íŠ¸ í¬íŠ¸ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 30,000ê°œì˜ í¬íŠ¸ ë²”ìœ„ì—ì„œ í• ë‹¹\nëœë‹¤(`net.ipv4.ip_local_port_range`ë¥¼ íŠœë‹í•˜ì—¬ ë³€ê²½ê°€ëŠ¥). ì¦‰, ì›¹ì„œë²„ì™€ ë¡œë“œë°¸ëŸ°ì„œ ì‚¬ì´ì— ì—°ê²°ì´ ë§¤ ë¶„ë‹¹ ì´ 30,000ê°œì´ë©° ë§¤ì´ˆ í‰ê·  500ê°œì˜ ì—°ê²°ì´ ë§ºì–´ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„\nì˜ë¯¸í•˜ê² ë‹¤.\nì´ê²ƒì„ ì´ˆê³¼í•˜ë©´ EADDRNOTAVAILì„ ë¦¬í„´í•œë‹¤. í•´ê²°ë°©ë²•ì€ ë„¤ ìŒë‘¥ì´ë¥¼ ë” ë§ì´ ê°€ì§ˆ ìˆ˜ ìˆê²Œ í•˜ëŠ” ê²ƒ ë°–ì— ì—†ë‹¤.\në‚œì´ë„ê°€ ì–´ë ¤ìš´ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n- `net.ipv4.ip_local_port_range`ë¥¼ ì¢€ ë” ë„“ê²Œ ì„¸íŒ…í•´ì„œ ë” ë§ì€ í´ë¼ì´ì–¸íŠ¸ í¬íŠ¸ë¥¼ ì‚¬ìš©í•œë‹¤\n- ë¦¬ìŠ¤ë‹í•˜ëŠ” ì›¹ì„œë²„ì— ì¶”ê°€ì ì¸ í¬íŠ¸(81,82,83,...)ë¥¼ í• ë‹¹í•¨ìœ¼ë¡œì¨ ì¢€ ë” ë§ì€ ì„œë²„ í¬íŠ¸ë¥¼ ì‚¬ìš©í•œë‹¤\n- ë¡œë“œ ë°¸ëŸ°ì„œì— ì•„ì´í”¼ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ë¼ìš´ë“œ ë¡œë¹ˆ ê¸°ëŠ¥ì„ ì ìš©í•´ì„œ ì¢€ ë” ë§ì€ í´ë¼ì´ì–¸íŠ¸ ì•„ì´í”¼ë¥¼ ì‚¬ìš©í•œë‹¤.\n- ì›¹ ì„œë²„ì— ì•„ì´í”¼ë¥¼ ì¶”ê°€í•˜ì—¬ ì¢€ ë” ë§ì€ ì•„ì´í”¼ë¥¼ ì‚¬ìš©í•œë‹¤.\n\nNodeJSì—ì„œì˜ ìš°íšŒì ì¸ ë°©ë²•ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ì´ ìˆë‹¤.\n\n```js\nvar http = require( \"http\"),\n    agent = new http.Agent( {maxSockets: 1} );\n\n\nfunction httpRequest( callback ) {\n    var options = {\n            host: 'localhost',\n            port: 80,\n            path: '',\n\n            agent: agent\n        },\n...\n```\n\n```js\nvar http = require(\"http\");\n\nfunction httpRequest(callback) {\n    var options = {\n        ...,\n        agent: false\n    };\n...\n```\n\në˜í•œ ì•„ë˜ë¥¼ `/etc/sysctl.conf`ì— ì¶”ê°€í•´ì£¼ì—ˆë‹¤.\n\n`net.ipv4.ip_local_port_range = 1024 65535`\n\n## ì°¸ê³  ë¬¸ì„œ\n\n- [ë¦¬ëˆ…ìŠ¤ man-pages](https://man7.org/linux/man-pages/index.html)\n- [[ë²ˆì—­] ë°”ìœ ë¦¬ëˆ…ìŠ¤ ì„œë²„ì—ì„œ TCP TIME-WAIT ìƒíƒœ ëŒ€ì²˜í•˜ê¸°](https://linux.systemv.pe.kr/%EB%B2%88%EC%97%AD-%EB%B0%94%EC%81%9C-%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%84%9C%EB%B2%84%EC%97%90%EC%84%9C-tcp-time-wait-%EC%83%81%ED%83%9C-%EB%8C%80%EC%B2%98%ED%95%98%EA%B8%B0/)\n- [2014ë…„ë„ êµ¬ê¸€ ê·¸ë£¹ì—ì„œì˜ í•œ ë¬¸ë‹µ..](https://groups.google.com/g/nodejs/c/68SYd6_ksns?pli=1)\n- [[Linux] ì»¤ë„ íŒŒë¼ë¯¸í„° ìˆ˜ì • - TCP ì„±ëŠ¥í–¥ìƒ](https://bangu4.tistory.com/135)",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [
      "Backend",
      "HTTP"
    ],
    "readingTime": 2,
    "wordCount": 317,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  },
  {
    "id": "trunk-based-development",
    "slug": "trunk-based-development",
    "path": "backend/devops",
    "fullPath": "backend/devops/trunk-based-development",
    "title": "Trunk-Based Development",
    "excerpt": "Trunk-Based Development íŠ¸ë í¬ ê¸°ë°˜ ê°œë°œì€ ê°œë°œìë“¤ì´ ë¼ê³  ë¶€ë¥´ëŠ” ë‹¨ì¼ ë¸Œëœì¹˜ì—ì„œ ê³µë™ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì†ŒìŠ¤ ì œì–´ ë¸Œëœì¹­ ëª¨ë¸ì´ë©°, ìˆ˜ëª…ì´ ê¸´ ë‹¤ë¥¸ ê°œë°œ ë¸Œëœì¹˜ë¥¼ ìƒì„±í•˜ì§€ ì•Šê¸° ìœ„í•œ ë°©ë²•ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì´ ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ ë³‘í•©ì§€ì˜¥...",
    "content": "# Trunk-Based Development\n\níŠ¸ë í¬ ê¸°ë°˜ ê°œë°œì€ ê°œë°œìë“¤ì´ `trunk`ë¼ê³  ë¶€ë¥´ëŠ” ë‹¨ì¼ ë¸Œëœì¹˜ì—ì„œ ê³µë™ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì†ŒìŠ¤ ì œì–´ ë¸Œëœì¹­ ëª¨ë¸ì´ë©°, ìˆ˜ëª…ì´ ê¸´ ë‹¤ë¥¸ ê°œë°œ ë¸Œëœì¹˜ë¥¼ ìƒì„±í•˜ì§€ ì•Šê¸° ìœ„í•œ ë°©ë²•ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì´ ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ ë³‘í•©ì§€ì˜¥ì„ í”¼í•˜ê³  ë¹Œë“œë¥¼ ê¹¨ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## íŠ¸ë í¬ ê¸°ë°˜ ê°œë°œì˜ Good or Bad Practice\n\n### Shared branches off mainline/main/trunk are bad at any release cadence\n\n![trunk-1a](https://trunkbaseddevelopment.com/trunk1a.png)\n\n### Trunk-Based Development For Smaller Teams\n\n![trunk-1b](https://trunkbaseddevelopment.com/trunk1b.png)\n\n### Scaled Trunk-Based Development\n\n![trunk-1c](https://trunkbaseddevelopment.com/trunk1c.png)\n\n## ì •ë¦¬\n\nê°œë°œìë“¤ì€ í”¼ì³ë¸Œëœì¹˜ì—ì„œ ì‘ì—…í•œ ë’¤ Trunk (or main) ë¸Œëœì¹˜ë¡œ ë³‘í•©í•˜ê³ , ë¦´ë¦¬ì¦ˆê°€ ì§„í–‰ë ë–„ ë¦´ë¦¬ì¦ˆ ë²„ì „ì„ ëª…ì‹œí•œ ë¸Œëœì¹˜ë¥¼ ë¶„ê¸°í•œë‹¤. ì´ë•Œ ë¶„ê¸°ëœ ë¸Œëœì¹˜ì—ëŠ” ì¶”ê°€ì ì¸ ì»¤ë°‹ì„ í•˜ì§€ ì•Šìœ¼ë©°, ë³€ê²½ì‚¬í•­ì€ ë‹¤ìŒ ë¦´ë¦¬ì¦ˆì— ë°˜ì˜í•œë‹¤. ì´ëŸ° ì‘ì—… ë°©ì‹ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ë²„ì „ì—ì„œ ë°˜ì˜ë˜ì–´ì•¼í•  ì‚¬í•­ì´ main ë¸Œëœì¹˜ì— ê³µì¡´í•˜ëŠ” ìƒí™©ì´ ìˆì„ ìˆ˜ ìˆëŠ”ë°, ì´ë¥¼ í†µì œí•˜ê¸° ìœ„í•´ ê¸°ëŠ¥ ë³„ í”Œë˜ê·¸ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. ì´ ë‚´ìš©ì„ ê³µì‹ ì‚¬ì´íŠ¸ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì†Œê°œí•˜ê³  ìˆë‹¤. [ë§í¬](https://trunkbaseddevelopment.com/feature-flags/)\n\n## NodeJSì—ì„œ í”Œë˜ê·¸ ì²˜ë¦¬ë¥¼ ì–´ë–»ê²Œ í•˜ëŠ”ê²Œ ì¢‹ì„ê¹Œ?\n\npackage.jsonì˜ ë²„ì „ì„ ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ”ê²Œ ì–´ë–¨ê¹Œ ì‹¶ë‹¤. semverë¥¼ ì‚¬ìš©í•˜ê³  ìˆë‹¤ë©´, release ë²„ì „ì„ ë¶€ë²„ì „ê¹Œì§€ëŠ” ê³ ì •ì‹œì¼œ ë†“ê³  ìˆ˜ë²„ì „ì„ ì˜¬ë¦¬ë©´ì„œ íŒ¨ì¹˜ë¥¼ í•˜ê²Œ ë í…ë°, í•œ ë¦´ë¦¬ì¦ˆì—ì„œì˜ ëª¨ë“  ê¸°ëŠ¥ì„ ë³´ì¥í•œë‹¤ë©´ í”Œë˜ê·¸ ì²˜ë¦¬ë¥¼ ë¶€ë²„ì „ê¹Œì§€ë¥¼ ê²€ì‚¬í•˜ëŠ” ê²ƒìœ¼ë¡œ í•  ìˆ˜ ìˆë‹¤.\n\nì•„ë˜ëŠ” v2.5.x ë¦´ë¦¬ì¦ˆë¥¼ í•˜ëŠ” ê²½ìš°ì˜ ì˜ˆì‹œì´ë‹¤\n\n```ts\nconst version = process.env.npm_package_version || '100'\n\nif (version.startsWith('2.5')) {\n   someCode();\n}\n\nif (Number(version) > 2.5) {\n   runAnotherCode();\n}\n...\n\n```\n\n## NodeJSì—ì„œ ë²„ì „ì„ ê°€ì ¸ì˜¤ëŠ” ë°©ë²•\n\n1. package.jsonì˜ versionì„ ê°€ì ¸ì˜¤ëŠ” ê²ƒ\n   ë‹¤ë§Œ ì°¸ì¡°í•œ ìŠ¤íƒì˜¤ë²„í”Œë¡œìš° ë‹µë³€ì—ë„ ì í˜€ìˆë“¯ì´, ë¸Œë¼ìš°ì €ì—ì„œ ì‚¬ìš©í•  ì‹œì—” package.jsonì˜ ë‚´ìš©ì„ í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì°¸ì¡°í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ NodeJSë¥¼ í™œìš©í•œ ì„œë²„ì—ì„œë§Œ í™œìš©í•˜ëŠ”ê²Œ ì¢‹ì„ ë“¯ í•˜ë‹¤.\n\n   ```ts\n   // You need to set resolveJsonModule true in tsconfig.json\n   import { version } from './package.json'\n   console.log(`version = ${version}`)\n   console.log(`Is flag enabled? ${version.startsWith('2.5')}`)\n   // version = 1.0.0\n   // Is flag enabled? false\n   ```\n\n2. process í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ëŠ” ê²ƒ\n   ë§Œì•½ NodeJS ì• í”Œë¦¬ì¼€ì´ì…˜ì´ npm ë“±ì˜ íŒ¨í‚¤ì§€ ë§¤ë‹ˆì €ë¡œ ì‹œì‘ë˜ì—ˆë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤.\n\n   ```ts\n   // run npm start\n   const version = process.env.npm_package_version\n   console.log(`version = ${version}`)\n   console.log(\n   \t`Is flag enabled? ${version?.startsWith('2.5') ?? 'version is undefined'}`,\n   )\n   // version = 1.0.0\n   // Is flag enabled? false\n   ```\n\në¬¼ë¡  ë²„ì „ì„ í†µí•œ í”Œë˜ê·¸ ì²˜ë¦¬ë¥¼ í•˜ê¸° ìœ„í•´ì„œëŠ” ì² ì €í•œ package.json ê´€ë¦¬ê°€ í•„ìš”í•˜ë‹¤.\n\n## Ref\n\n- [íŠ¸ë í¬ê¸°ë°˜ê°œë°œ](https://trunkbaseddevelopment.com/)\n- [semver](https://semver.org/lang/ko/)\n- [is-there-a-way-to-get-version-from-package-json-in-nodejs-code](https://stackoverflow.com/questions/9153571/is-there-a-way-to-get-version-from-package-json-in-nodejs-code)",
    "docType": "original",
    "category": "Backend_DevOps",
    "tags": [
      "Backend",
      "DevOps"
    ],
    "readingTime": 2,
    "wordCount": 332,
    "isFeatured": false,
    "isPublic": true,
    "date": "2025-12-27"
  }
]