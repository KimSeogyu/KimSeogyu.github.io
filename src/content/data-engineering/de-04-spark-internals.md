---
public: true
title: "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #4: Spark ë‚´ë¶€ ë™ì‘ ì›ë¦¬ - Job, Stage, Task"
date: '2026-01-02'
category: Data Engineering
series: data-engineering
tags: [Data Engineering, Partitioning, Performance, Spark]
excerpt: "Sparkì˜ ì‹¤í–‰ ëª¨ë¸ì„ ì´í•´í•©ë‹ˆë‹¤. Job, Stage, Task ê³„ì¸µ, Shuffleì˜ ë¹„ìš©, íŒŒí‹°ì…”ë‹ ì „ëµ, ê·¸ë¦¬ê³  Spark UIë¥¼ ì½ëŠ” ë²•ê¹Œì§€."
---

# ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #4: Spark ë‚´ë¶€ ë™ì‘ ì›ë¦¬ - Job, Stage, Task

> **ëŒ€ìƒ ë…ì**: ì¶©ë¶„í•œ ê²½í—˜ì„ ê°€ì§„ ë°±ì—”ë“œ/í’€ìŠ¤íƒ ì—”ì§€ë‹ˆì–´ë¡œ, Sparkì˜ ê¸°ë³¸ ê°œë…ì„ ì•Œê³  ì„±ëŠ¥ íŠœë‹ì— ê´€ì‹¬ ìˆëŠ” ë¶„

## ì´ í¸ì—ì„œ ë‹¤ë£¨ëŠ” ê²ƒ

Spark ì½”ë“œë¥¼ ì‘ì„±í•  ë•Œ **ì™œ ì–´ë–¤ ì½”ë“œëŠ” ëŠë¦¬ê³  ì–´ë–¤ ì½”ë“œëŠ” ë¹ ë¥¸ì§€** ì´í•´í•˜ë ¤ë©´, ë‚´ë¶€ ì‹¤í–‰ ëª¨ë¸ì„ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤.

---

## ì‹¤í–‰ ê³„ì¸µ êµ¬ì¡°: Application â†’ Job â†’ Stage â†’ Task

### ì „ì²´ êµ¬ì¡°

```mermaid
flowchart TB
    subgraph App ["Application (ì•± ì „ì²´)"]
        subgraph Job1 ["Job 1 (Action 1)"]
            subgraph Stage1 ["Stage 1"]
                T1["Task 1"]
                T2["Task 2"]
                T3["Task 3"]
            end
            subgraph Stage2 ["Stage 2"]
                T4["Task 4"]
                T5["Task 5"]
            end
        end
        
        subgraph Job2 ["Job 2 (Action 2)"]
            subgraph Stage3 ["Stage 3"]
                T6["Task 6"]
                T7["Task 7"]
            end
        end
    end
    
    Stage1 -->|"Shuffle"| Stage2
```

### ê° ê³„ì¸µì˜ ì—­í• 

| ê³„ì¸µ | ë¬´ì—‡ | ì–¸ì œ ìƒì„± | ë³‘ë ¬ì„± |
|------|------|---------|--------|
| **Application** | ì „ì²´ Spark í”„ë¡œê·¸ë¨ | spark-submit ì‹œ | 1ê°œ |
| **Job** | í•˜ë‚˜ì˜ Action ì‹¤í–‰ ë‹¨ìœ„ | count(), save() í˜¸ì¶œ | ìˆœì°¨ |
| **Stage** | Shuffle ê¸°ì¤€ ë¶„ë¦¬ | ìë™ ë¶„ë¦¬ | ìˆœì°¨ |
| **Task** | íŒŒí‹°ì…˜ë‹¹ ì‹¤í–‰ ë‹¨ìœ„ | íŒŒí‹°ì…˜ ìˆ˜ë§Œí¼ | **ë³‘ë ¬** |

### ì½”ë“œ ì˜ˆì‹œì™€ ì‹¤í–‰ íë¦„

```python
# ì´ ì½”ë“œê°€ ì–´ë–»ê²Œ ì‹¤í–‰ë ê¹Œ?
df = spark.read.parquet("data/")  # Transformation
filtered = df.filter(df.age > 20)  # Transformation
grouped = filtered.groupBy("city").count()  # Transformation
grouped.show()  # Action â†’ Job ìƒì„±!
```

```mermaid
flowchart LR
    subgraph Job ["Job (show í˜¸ì¶œ)"]
        subgraph Stage1 ["Stage 1: Wide ì „ê¹Œì§€"]
            Read["read.parquet"]
            Filter["filter(age > 20)"]
            Read --> Filter
        end
        
        Shuffle["âš¡ Shuffle<br/>(city ê¸°ì¤€ ì¬ë°°ì¹˜)"]
        
        subgraph Stage2 ["Stage 2: ì§‘ê³„"]
            GroupBy["groupBy + count"]
            Show["show()"]
            GroupBy --> Show
        end
        
        Stage1 --> Shuffle --> Stage2
    end
```

---

## Narrow vs Wide Transformations

### ì´ê²ƒì´ ì„±ëŠ¥ì˜ í•µì‹¬

```mermaid
flowchart TB
    subgraph Narrow ["Narrow Transformations"]
        direction TB
        N1["map"]
        N2["filter"]
        N3["flatMap"]
        N4["select"]
        
        NP1["Partition 1"] --> NP1R["ê²°ê³¼ 1"]
        NP2["Partition 2"] --> NP2R["ê²°ê³¼ 2"]
        NP3["Partition 3"] --> NP3R["ê²°ê³¼ 3"]
        
        Desc1["âœ… íŒŒí‹°ì…˜ ë…ë¦½ ì²˜ë¦¬<br/>âœ… ë„¤íŠ¸ì›Œí¬ í†µì‹  ì—†ìŒ<br/>âœ… ë§¤ìš° ë¹ ë¦„"]
    end
    
    subgraph Wide ["Wide Transformations"]
        direction TB
        W1["groupBy"]
        W2["join"]
        W3["orderBy"]
        W4["repartition"]
        
        WP1["Partition 1"] --> WS["Shuffle<br/>ğŸ”€"] --> WR1["ê²°ê³¼ 1"]
        WP2["Partition 2"] --> WS --> WR2["ê²°ê³¼ 2"]
        WP3["Partition 3"] --> WS --> WR3["ê²°ê³¼ 3"]
        
        Desc2["âš ï¸ ë°ì´í„° ì¬ë°°ì¹˜<br/>âš ï¸ ë„¤íŠ¸ì›Œí¬ I/O ë°œìƒ<br/>âš ï¸ ëŠë¦¼"]
    end
```

### Shuffleì´ ë¹„ì‹¼ ì´ìœ 

```mermaid
flowchart TB
    subgraph Before ["Shuffle ì „"]
        P1["Executor 1<br/>í‚¤: A, B, C"]
        P2["Executor 2<br/>í‚¤: A, D, E"]
        P3["Executor 3<br/>í‚¤: B, C, F"]
    end
    
    subgraph Network ["ë„¤íŠ¸ì›Œí¬ ì „ì†¡"]
        direction TB
        N1["A ë°ì´í„° â†’ Reducer 1ë¡œ"]
        N2["B ë°ì´í„° â†’ Reducer 2ë¡œ"]
        N3["C ë°ì´í„° â†’ Reducer 3ìœ¼ë¡œ"]
        N4["..."]
    end
    
    subgraph After ["Shuffle í›„"]
        R1["Reducer 1<br/>í‚¤ Aë§Œ"]
        R2["Reducer 2<br/>í‚¤ Bë§Œ"]
        R3["Reducer 3<br/>í‚¤ Cë§Œ"]
    end
    
    Before --> Network --> After
    
    Cost["ğŸ’¸ ë¹„ìš© ë°œìƒ<br/>â€¢ ë””ìŠ¤í¬ ì“°ê¸°<br/>â€¢ ë„¤íŠ¸ì›Œí¬ ì „ì†¡<br/>â€¢ ë””ìŠ¤í¬ ì½ê¸°<br/>â€¢ ì •ë ¬"]
```

**Shuffleì´ ë°œìƒí•˜ë©´**:

1. ê° Executorê°€ ê²°ê³¼ë¥¼ **ë””ìŠ¤í¬ì— ì €ì¥**
2. í‚¤ ê¸°ì¤€ìœ¼ë¡œ **ë„¤íŠ¸ì›Œí¬ë¡œ ì „ì†¡**
3. ë°›ëŠ” ìª½ì—ì„œ **ë””ìŠ¤í¬ì— ì €ì¥**
4. í‚¤ ê¸°ì¤€ **ì •ë ¬**
5. ë©”ëª¨ë¦¬ë¡œ **ì½ì–´ì„œ ì²˜ë¦¬**

---

## íŒŒí‹°ì…”ë‹ ì „ëµ

### íŒŒí‹°ì…˜ì´ë€?

```mermaid
flowchart TB
    subgraph Data ["ì›ë³¸ ë°ì´í„°"]
        BigData["1TB ë°ì´í„°"]
    end
    
    subgraph Partitions ["íŒŒí‹°ì…˜ ë¶„í• "]
        P1["Partition 1<br/>100GB"]
        P2["Partition 2<br/>100GB"]
        P3["Partition 3<br/>100GB"]
        PN["...<br/>100GB"]
    end
    
    subgraph Tasks ["ë³‘ë ¬ ì²˜ë¦¬"]
        T1["Task 1<br/>â†’ Core 1"]
        T2["Task 2<br/>â†’ Core 2"]
        T3["Task 3<br/>â†’ Core 3"]
        TN["Task N<br/>â†’ Core N"]
    end
    
    BigData --> Partitions --> Tasks
```

### íŒŒí‹°ì…˜ ìˆ˜ì™€ ë³‘ë ¬ì„±

```mermaid
flowchart TB
    subgraph TooFew ["íŒŒí‹°ì…˜ì´ ë„ˆë¬´ ì ìŒ"]
        F1["4 íŒŒí‹°ì…˜"]
        F2["100 ì½”ì–´ í´ëŸ¬ìŠ¤í„°"]
        F3["âŒ 96 ì½”ì–´ ë†€ê³  ìˆìŒ"]
    end
    
    subgraph TooMany ["íŒŒí‹°ì…˜ì´ ë„ˆë¬´ ë§ìŒ"]
        M1["10000 íŒŒí‹°ì…˜"]
        M2["100 ì½”ì–´ í´ëŸ¬ìŠ¤í„°"]
        M3["âŒ ìŠ¤ì¼€ì¤„ë§ ì˜¤ë²„í—¤ë“œ"]
    end
    
    subgraph JustRight ["ì ì ˆí•œ íŒŒí‹°ì…˜"]
        R1["200~400 íŒŒí‹°ì…˜"]
        R2["100 ì½”ì–´ í´ëŸ¬ìŠ¤í„°"]
        R3["âœ… ì½”ì–´ë‹¹ 2~4 Task"]
    end
```

**ê²½í—˜ì¹™**:

- íŒŒí‹°ì…˜ ìˆ˜ = ì½”ì–´ ìˆ˜ Ã— 2~4
- íŒŒí‹°ì…˜ë‹¹ í¬ê¸° = 100MB ~ 1GB

### ë°ì´í„° ìŠ¤í(Skew) ë¬¸ì œ

```mermaid
flowchart TB
    subgraph Skewed ["ìŠ¤í ë°œìƒ"]
        S1["Partition 1<br/>10MB"]
        S2["Partition 2<br/>10MB"]
        S3["Partition 3<br/>10GB !!"]
        
        ST1["Task 1<br/>1ì´ˆ"]
        ST2["Task 2<br/>1ì´ˆ"]
        ST3["Task 3<br/>100ì´ˆ ğŸ˜±"]
        
        S1 --> ST1
        S2 --> ST2
        S3 --> ST3
    end
    
    Result["ì „ì²´ ì‹œê°„ = 100ì´ˆ<br/>(ê°€ì¥ ëŠë¦° Task ê¸°ì¤€)"]
    
    Skewed --> Result
```

**í•´ê²°ì±…**:

1. **Salting**: í•« í‚¤ì— ëœë¤ ì ‘ë‘ì‚¬ ì¶”ê°€
2. **Broadcast Join**: ì‘ì€ í…Œì´ë¸”ì€ ì „ì²´ ë³µì‚¬
3. **Adaptive Query Execution (AQE)**: Spark 3.0+ ìë™ ìµœì í™”

---

## ë©”ëª¨ë¦¬ ê´€ë¦¬

### Executor ë©”ëª¨ë¦¬ êµ¬ì¡°

```mermaid
flowchart TB
    subgraph Executor ["Executor ë©”ëª¨ë¦¬"]
        subgraph Reserved ["Reserved (300MB)"]
            R["ì‹œìŠ¤í…œìš©"]
        end
        
        subgraph Unified ["Unified Memory (60%)"]
            Storage["Storage<br/>(ìºì‹œ)"]
            Execution["Execution<br/>(Shuffle, ì •ë ¬)"]
            Storage <-->|"ë™ì  ê³µìœ "| Execution
        end
        
        subgraph User ["User Memory (40%)"]
            UDF["UDF ê°ì²´"]
            Meta["ë©”íƒ€ë°ì´í„°"]
        end
    end
```

### ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ: Spill to Disk

```mermaid
flowchart LR
    subgraph Normal ["ì •ìƒ ìƒíƒœ"]
        Mem1["ë©”ëª¨ë¦¬ ì‚¬ìš©<br/>3GB"]
        Limit1["í• ë‹¹ëŸ‰<br/>4GB"]
    end
    
    subgraph Spill ["Spill ë°œìƒ"]
        Mem2["ë©”ëª¨ë¦¬ ì‚¬ìš©<br/>4GB+"]
        Disk["ë””ìŠ¤í¬ë¡œ<br/>ë‚´ë³´ë‚´ê¸° ğŸ’¾"]
        Slow["ğŸ¢ ëŠë ¤ì§"]
        
        Mem2 --> Disk --> Slow
    end
    
    Normal -->|"ë°ì´í„° ì¦ê°€"| Spill
```

**Spill ê°ì§€ ë°©ë²•**: Spark UIì—ì„œ "Spill (Memory)" / "Spill (Disk)" í™•ì¸

---

## Spark UI ì½ëŠ” ë²•

### í•µì‹¬ ì§€í‘œë“¤

```mermaid
flowchart TB
    subgraph SparkUI ["Spark UI"]
        subgraph JobsTab ["Jobs íƒ­"]
            J1["Job ì„±ê³µ/ì‹¤íŒ¨"]
            J2["ì „ì²´ ì†Œìš” ì‹œê°„"]
        end
        
        subgraph StagesTab ["Stages íƒ­ â­"]
            S1["Stageë³„ ì‹œê°„"]
            S2["Shuffle Read/Write"]
            S3["Task ë¶„í¬"]
        end
        
        subgraph SQLTab ["SQL íƒ­"]
            SQL1["ì‹¤í–‰ ê³„íš"]
            SQL2["ë¬¼ë¦¬ ê³„íš"]
        end
    end
```

### Stages íƒ­ í•´ì„

```mermaid
flowchart TB
    subgraph StageMetrics ["Stage ì§€í‘œ"]
        subgraph Good ["âœ… ì •ìƒ"]
            G1["Task Duration ê· ì¼"]
            G2["Shuffle Write ì ìŒ"]
            G3["Spill ì—†ìŒ"]
        end
        
        subgraph Bad ["âš ï¸ ë¬¸ì œ"]
            B1["Task Duration í¸ì°¨ í¼<br/>â†’ ë°ì´í„° ìŠ¤í"]
            B2["Shuffle í¬ê¸° ê±°ëŒ€<br/>â†’ ì¡°ì¸/ê·¸ë£¹ ìµœì í™” í•„ìš”"]
            B3["Spill ë°œìƒ<br/>â†’ ë©”ëª¨ë¦¬ ë¶€ì¡±"]
        end
    end
```

### ì‹¤ì „ ë””ë²„ê¹… í”Œë¡œìš°

```mermaid
flowchart TB
    Start["Jobì´ ëŠë¦¼"]
    
    Q1{"Shuffleì´<br/>í°ê°€?"}
    Q2{"Task Duration<br/>í¸ì°¨ê°€ í°ê°€?"}
    Q3{"Spillì´<br/>ë°œìƒí•˜ëŠ”ê°€?"}
    Q4{"GC ì‹œê°„ì´<br/>ê¸´ê°€?"}
    
    A1["ì¡°ì¸/ê·¸ë£¹ ìµœì í™”<br/>Broadcast Join ê³ ë ¤"]
    A2["ë°ì´í„° ìŠ¤í í•´ê²°<br/>Salting, AQE"]
    A3["ë©”ëª¨ë¦¬ ì¦ê°€<br/>íŒŒí‹°ì…˜ ìˆ˜ ì¡°ì •"]
    A4["Executor ë©”ëª¨ë¦¬ ì¦ê°€<br/>GC íŠœë‹"]
    
    Start --> Q1
    Q1 -->|"ì˜ˆ"| A1
    Q1 -->|"ì•„ë‹ˆì˜¤"| Q2
    Q2 -->|"ì˜ˆ"| A2
    Q2 -->|"ì•„ë‹ˆì˜¤"| Q3
    Q3 -->|"ì˜ˆ"| A3
    Q3 -->|"ì•„ë‹ˆì˜¤"| Q4
    Q4 -->|"ì˜ˆ"| A4
    Q4 -->|"ì•„ë‹ˆì˜¤"| Other["ë‹¤ë¥¸ ì›ì¸ ì¡°ì‚¬"]
```

---

## ì‹¤ì „ ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì½”ë“œ ë ˆë²¨

| í•­ëª© | ì¢‹ì€ ì˜ˆ | ë‚˜ìœ ì˜ˆ |
|------|--------|--------|
| **ì¡°ì¸** | Broadcast Join (ì‘ì€ í…Œì´ë¸”) | ì–‘ìª½ ë‹¤ í° Shuffle Join |
| **í•„í„°** | ì¡°ì¸ ì „ì— filter | ì¡°ì¸ í›„ì— filter |
| **ì»¬ëŸ¼ ì„ íƒ** | í•„ìš”í•œ ì»¬ëŸ¼ë§Œ select | SELECT * |
| **UDF** | Built-in í•¨ìˆ˜ ì‚¬ìš© | Python UDF ë‚¨ìš© |
| **collect** | ì§‘ê³„ í›„ collect | í° ë°ì´í„° collect |

### ì„¤ì • ë ˆë²¨

```python
# ê¶Œì¥ ì„¤ì •
spark.conf.set("spark.sql.adaptive.enabled", "true")  # AQE
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
spark.conf.set("spark.sql.shuffle.partitions", "200")  # ê¸°ë³¸ 200
```

---

## ì •ë¦¬

```mermaid
mindmap
  root((Spark<br/>ë‚´ë¶€ ë™ì‘))
    ê³„ì¸µ êµ¬ì¡°
      Application
      Job (Actionë§ˆë‹¤)
      Stage (Shuffleë§ˆë‹¤)
      Task (íŒŒí‹°ì…˜ë§ˆë‹¤)
    Transformation
      Narrow
        map, filter
        ë¹ ë¦„
      Wide
        groupBy, join
        Shuffle ë°œìƒ
        ëŠë¦¼
    Shuffle
      ë„¤íŠ¸ì›Œí¬ ì „ì†¡
      ë””ìŠ¤í¬ I/O
      ì„±ëŠ¥ ë³‘ëª©
    íŒŒí‹°ì…”ë‹
      ì ì • ìˆ˜: ì½”ì–´ x 2~4
      ìŠ¤í ì£¼ì˜
      AQE í™œìš©
    ë©”ëª¨ë¦¬
      Storage + Execution
      Spill ë°œìƒ ì‹œ ëŠë ¤ì§
    Spark UI
      Stage íƒ­ í™•ì¸
      Shuffle í¬ê¸°
      Task ë¶„í¬
```

---

## ë‹¤ìŒ í¸ ì˜ˆê³ 

**5í¸: PySpark ì‹¤ì „**ì—ì„œëŠ” ì‹¤ë¬´ íŒ¨í„´ì„ ë‹¤ë£¹ë‹ˆë‹¤:

- ìì£¼ ì“°ëŠ” DataFrame ì—°ì‚°
- UDF vs Built-in Functions
- ì¡°ì¸ ìµœì í™” ê¸°ë²•
- ìºì‹±ê³¼ ì²´í¬í¬ì¸íŒ…
- ì•ˆí‹°íŒ¨í„´ í”¼í•˜ê¸°

---

## ì°¸ê³  ìë£Œ

- [Spark Web UI](https://spark.apache.org/docs/latest/web-ui.html)
- [Tuning Spark](https://spark.apache.org/docs/latest/tuning.html)
- [Adaptive Query Execution](https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution)
- Jacek Laskowski, "The Internals of Apache Spark"
