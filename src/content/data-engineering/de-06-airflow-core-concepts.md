---
public: true
title: "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #6: Airflow í•µì‹¬ ê°œë… - DAG, Operator, Task"
date: '2026-01-02'
category: Data Engineering
series: data-engineering
tags: [Airflow, DAG, Workflow, Orchestration, TaskFlow, ETL]
excerpt: "ì™œ cronìœ¼ë¡œëŠ” ë¶€ì¡±í• ê¹Œìš”? Airflowì˜ í•µì‹¬ ê°œë…ì¸ DAG, Operator, Taskë¥¼ ì´í•´í•˜ê³  TaskFlow APIë¡œ í˜„ëŒ€ì ì¸ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‘ì„±í•˜ëŠ” ë²•ì„ ë°°ì›ë‹ˆë‹¤."
---

# ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #6: Airflow í•µì‹¬ ê°œë… - DAG, Operator, Task

> **ëŒ€ìƒ ë…ì**: ì¶©ë¶„í•œ ê°€ì§„ ë°±ì—”ë“œ/í’€ìŠ¤íƒ ì—”ì§€ë‹ˆì–´ë¡œ, CI/CD íŒŒì´í”„ë¼ì¸ì´ë‚˜ cron jobì— ìµìˆ™í•˜ì§€ë§Œ AirflowëŠ” ì²˜ìŒì¸ ë¶„

## ì´ í¸ì—ì„œ ë‹¤ë£¨ëŠ” ê²ƒ

GitHub Actionsë‚˜ cronìœ¼ë¡œ ë°°ì¹˜ ì‘ì—…ì„ ëŒë ¤ë³¸ ê²½í—˜ì´ ìˆë‹¤ë©´, **ì™œ ë°ì´í„° íŒ€ì€ Airflowë¥¼ ì“°ëŠ”ì§€** ê¶ê¸ˆí–ˆì„ ê²ë‹ˆë‹¤. ê·¸ ì´ìœ ì™€ í•µì‹¬ ê°œë…ì„ ë°°ì›ë‹ˆë‹¤.

---

## ì™œ cron jobìœ¼ë¡œëŠ” ë¶€ì¡±í•œê°€?

### cronì˜ í•œê³„

```mermaid
flowchart TB
    subgraph Cron ["cron ë°©ì‹ì˜ ë¬¸ì œ"]
        C1["0 1 * * * extract.sh"]
        C2["0 2 * * * transform.sh"]
        C3["0 3 * * * load.sh"]
        
        Problem1["â“ extractê°€ ëŠ¦ì–´ì§€ë©´?"]
        Problem2["â“ ì¤‘ê°„ì— ì‹¤íŒ¨í•˜ë©´?"]
        Problem3["â“ ì–´ì œ ë°ì´í„°ë¥¼ ì¬ì²˜ë¦¬í•˜ë ¤ë©´?"]
        Problem4["â“ ì‹¤í–‰ ìƒíƒœë¥¼ ì–´ë–»ê²Œ í™•ì¸?"]
    end
```

| ë¬¸ì œ | cron | Airflow |
|------|------|---------|
| **ì˜ì¡´ì„± ê´€ë¦¬** | ì‹œê°„ìœ¼ë¡œë§Œ (ë¶ˆí™•ì‹¤) | ëª…ì‹œì  ì˜ì¡´ì„± âœ… |
| **ì‹¤íŒ¨ ì²˜ë¦¬** | ìˆ˜ë™ í™•ì¸/ì¬ì‹¤í–‰ | ìë™ ì¬ì‹œë„ âœ… |
| **ë°±í•„** | ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ë™ ìˆ˜ì • | ë‚ ì§œ ì§€ì • ì¬ì‹¤í–‰ âœ… |
| **ëª¨ë‹ˆí„°ë§** | ë¡œê·¸ íŒŒì¼ ë’¤ì§€ê¸° | ì›¹ UI âœ… |
| **ì•Œë¦¼** | ì§ì ‘ êµ¬í˜„ | Slack/Email ì—°ë™ âœ… |

### ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤

```mermaid
flowchart LR
    subgraph Reality ["í˜„ì‹¤ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì¼"]
        A["Extract<br/>(01:00 ì˜ˆì •)"]
        B["Transform<br/>(02:00 ì˜ˆì •)"]
        C["Load<br/>(03:00 ì˜ˆì •)"]
        
        A -->|"01:30ì— ëë‚¨"| Delay
        Delay["âš ï¸ Transformì´<br/>ë¶ˆì™„ì „í•œ ë°ì´í„°ë¡œ ì‹œì‘"]
        Delay --> Bad["âŒ ì˜ëª»ëœ ê²°ê³¼"]
    end
```

**Airflowì˜ í•´ê²°ì±…**: Task ê°„ **ì˜ì¡´ì„±**ì„ ì •ì˜í•˜ì—¬ ì´ì „ Taskê°€ ì™„ë£Œë˜ì–´ì•¼ ë‹¤ìŒì´ ì‹œì‘

---

## Airflow ì•„í‚¤í…ì²˜

### êµ¬ì„± ìš”ì†Œ

```mermaid
flowchart TB
    subgraph Airflow ["Airflow ì‹œìŠ¤í…œ"]
        Web["Webserver<br/>ğŸ“Š UI ì œê³µ"]
        Sched["Scheduler<br/>â° DAG íŒŒì‹±/ìŠ¤ì¼€ì¤„ë§"]
        Worker["Worker(s)<br/>âš™ï¸ Task ì‹¤í–‰"]
        DB[(Metadata DB<br/>ğŸ“ ìƒíƒœ ì €ì¥)]
        
        Web <--> DB
        Sched <--> DB
        Worker <--> DB
        Sched -->|"Task í• ë‹¹"| Worker
    end
    
    subgraph DAGs ["DAG íŒŒì¼"]
        D1["dag1.py"]
        D2["dag2.py"]
        D3["dag3.py"]
    end
    
    DAGs -->|"íŒŒì‹±"| Sched
```

### Executor ì¢…ë¥˜

| Executor | íŠ¹ì§• | ì í•©í•œ í™˜ê²½ |
|----------|------|------------|
| **LocalExecutor** | ë‹¨ì¼ ë¨¸ì‹ , ë©€í‹° í”„ë¡œì„¸ìŠ¤ | ê°œë°œ, ì†Œê·œëª¨ |
| **CeleryExecutor** | ë¶„ì‚° ì›Œì»¤ (Redis/RabbitMQ) | ì¤‘ê·œëª¨ í”„ë¡œë•ì…˜ |
| **KubernetesExecutor** | ê° Taskë¥¼ Podë¡œ | ëŒ€ê·œëª¨, í´ë¼ìš°ë“œ |

---

## DAG (Directed Acyclic Graph)

### DAGë€?

```mermaid
flowchart LR
    subgraph DAG ["DAG: Directed Acyclic Graph"]
        A["Task A"]
        B["Task B"]
        C["Task C"]
        D["Task D"]
        E["Task E"]
        
        A --> B
        A --> C
        B --> D
        C --> D
        D --> E
    end
    
    subgraph Rules ["ê·œì¹™"]
        R1["âœ… Directed: ë°©í–¥ì´ ìˆìŒ"]
        R2["âœ… Acyclic: ìˆœí™˜ ì—†ìŒ"]
        R3["âŒ A â†’ B â†’ A (ë¶ˆê°€)"]
    end
```

**ì™œ ê·¸ë˜í”„ì¸ê°€?**

- ìˆœì°¨ ì‹¤í–‰ë§Œ ìˆëŠ” ê²Œ ì•„ë‹˜
- ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥ (Bì™€ C ë™ì‹œ ì‹¤í–‰)
- ì˜ì¡´ì„± ëª…í™•íˆ í‘œí˜„

### DAG ì •ì˜ ì˜ˆì‹œ

```python
from airflow import DAG
from datetime import datetime

# DAG ì •ì˜
dag = DAG(
    dag_id="my_etl_pipeline",
    start_date=datetime(2024, 1, 1),
    schedule="@daily",  # ë§¤ì¼ ì‹¤í–‰
    catchup=False,
    tags=["etl", "production"]
)
```

---

## Operatorì™€ Task

### Operator: ë¬´ì—‡ì„ í•  ê²ƒì¸ê°€?

```mermaid
flowchart TB
    subgraph Operators ["ì£¼ìš” Operator ì¢…ë¥˜"]
        subgraph Basic ["ê¸°ë³¸"]
            O1["BashOperator<br/>ì‰˜ ëª…ë ¹ ì‹¤í–‰"]
            O2["PythonOperator<br/>Python í•¨ìˆ˜ ì‹¤í–‰"]
            O3["EmptyOperator<br/>ì•„ë¬´ê²ƒë„ ì•ˆ í•¨"]
        end
        
        subgraph Transfer ["ë°ì´í„° ì „ì†¡"]
            O4["S3ToRedshiftOperator"]
            O5["GCSToGCSOperator"]
        end
        
        subgraph External ["ì™¸ë¶€ ì‹œìŠ¤í…œ"]
            O6["SparkSubmitOperator<br/>Spark ì‘ì—… ì œì¶œ"]
            O7["DockerOperator<br/>ì»¨í…Œì´ë„ˆ ì‹¤í–‰"]
            O8["PostgresOperator<br/>SQL ì‹¤í–‰"]
        end
        
        subgraph Sensors ["ì„¼ì„œ (ëŒ€ê¸°)"]
            O9["FileSensor<br/>íŒŒì¼ ì¡´ì¬ ëŒ€ê¸°"]
            O10["HttpSensor<br/>API ì‘ë‹µ ëŒ€ê¸°"]
        end
    end
```

### Task: Operatorì˜ ì¸ìŠ¤í„´ìŠ¤

```mermaid
flowchart LR
    subgraph Definition ["ì •ì˜"]
        Operator["PythonOperator<br/>(í´ë˜ìŠ¤)"]
    end
    
    subgraph Instance ["ì¸ìŠ¤í„´ìŠ¤"]
        Task1["extract_task<br/>(Task)"]
        Task2["transform_task<br/>(Task)"]
        Task3["load_task<br/>(Task)"]
    end
    
    Operator --> Task1
    Operator --> Task2
    Operator --> Task3
```

```python
from airflow.operators.python import PythonOperator

def extract_data():
    # ë°ì´í„° ì¶”ì¶œ ë¡œì§
    return {"records": 1000}

def transform_data(**context):
    # ì´ì „ Task ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    data = context["ti"].xcom_pull(task_ids="extract")
    # ë³€í™˜ ë¡œì§
    return {"processed": data["records"]}

# Task ì •ì˜
extract_task = PythonOperator(
    task_id="extract",
    python_callable=extract_data,
    dag=dag
)

transform_task = PythonOperator(
    task_id="transform",
    python_callable=transform_data,
    dag=dag
)

# ì˜ì¡´ì„± ì •ì˜
extract_task >> transform_task
```

---

## TaskFlow API (Airflow 2.0+)

### ì „í†µì  ë°©ì‹ vs TaskFlow

```mermaid
flowchart TB
    subgraph Traditional ["ì „í†µì  ë°©ì‹"]
        T1["Operator ì •ì˜"]
        T2["XComìœ¼ë¡œ ë°ì´í„° ì „ë‹¬"]
        T3["ì˜ì¡´ì„± ë³„ë„ ì •ì˜"]
        
        T1 --> T2 --> T3
        Note1["ì¥í™©í•œ ì½”ë“œ ğŸ˜“"]
    end
    
    subgraph TaskFlow ["TaskFlow API"]
        TF1["@task ë°ì½”ë ˆì´í„°"]
        TF2["returnìœ¼ë¡œ ì „ë‹¬"]
        TF3["í•¨ìˆ˜ í˜¸ì¶œë¡œ ì˜ì¡´ì„±"]
        
        TF1 --> TF2 --> TF3
        Note2["ê¹”ë”í•œ ì½”ë“œ âœ¨"]
    end
```

### TaskFlow ì˜ˆì‹œ

```python
from airflow.decorators import dag, task
from datetime import datetime

@dag(
    dag_id="taskflow_etl",
    start_date=datetime(2024, 1, 1),
    schedule="@daily",
    catchup=False
)
def my_etl_pipeline():
    """TaskFlow APIë¥¼ ì‚¬ìš©í•œ ETL íŒŒì´í”„ë¼ì¸"""
    
    @task
    def extract():
        """ë°ì´í„° ì¶”ì¶œ"""
        return {"data": [1, 2, 3, 4, 5]}
    
    @task
    def transform(raw_data: dict):
        """ë°ì´í„° ë³€í™˜"""
        return {
            "data": [x * 2 for x in raw_data["data"]],
            "count": len(raw_data["data"])
        }
    
    @task
    def load(processed_data: dict):
        """ë°ì´í„° ì ì¬"""
        print(f"Loaded {processed_data['count']} records")
    
    # ì˜ì¡´ì„±ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì •ì˜ë¨
    raw = extract()
    processed = transform(raw)
    load(processed)

# DAG ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
my_etl_pipeline()
```

### XCom ìë™ ì²˜ë¦¬

```mermaid
flowchart LR
    subgraph Traditional ["ì „í†µì  XCom"]
        E1["extract"]
        X1["xcom_push()"]
        X2["xcom_pull()"]
        T1["transform"]
        
        E1 --> X1 --> X2 --> T1
        Note1["ëª…ì‹œì  push/pull í•„ìš”"]
    end
    
    subgraph TaskFlow ["TaskFlow XCom"]
        E2["@task<br/>return data"]
        T2["@task<br/>def fn(data):"]
        
        E2 -->|"ìë™!"| T2
        Note2["return/íŒŒë¼ë¯¸í„°ë¡œ ìë™ ì „ë‹¬"]
    end
```

---

## ìŠ¤ì¼€ì¤„ë§ê³¼ Data Interval

### schedule í‘œí˜„ì‹

| í‘œí˜„ì‹ | ì˜ë¯¸ | cron í‘œí˜„ |
|--------|------|----------|
| `@once` | í•œ ë²ˆë§Œ | - |
| `@hourly` | ë§¤ì‹œ | `0 * * * *` |
| `@daily` | ë§¤ì¼ | `0 0 * * *` |
| `@weekly` | ë§¤ì£¼ | `0 0 * * 0` |
| `@monthly` | ë§¤ì›” | `0 0 1 * *` |
| `0 6 * * *` | ë§¤ì¼ 6ì‹œ | - |
| `None` | ìˆ˜ë™ íŠ¸ë¦¬ê±°ë§Œ | - |

### Data Interval ê°œë… (ì¤‘ìš”!)

```mermaid
flowchart TB
    subgraph Timeline ["ì‹œê°„ì„ "]
        T1["2024-01-01<br/>00:00"]
        T2["2024-01-02<br/>00:00"]
        T3["2024-01-03<br/>00:00"]
    end
    
    subgraph DAGRun ["DAG ì‹¤í–‰"]
        D1["DAG Run 1<br/>data_interval: 01-01 ~ 01-02"]
        D2["DAG Run 2<br/>data_interval: 01-02 ~ 01-03"]
    end
    
    T2 -->|"ì‹¤í–‰ ì‹œì "| D1
    T3 -->|"ì‹¤í–‰ ì‹œì "| D2
    
    Note["âš ï¸ 1ì›” 2ì¼ì— 1ì›” 1ì¼ ë°ì´í„°ë¥¼ ì²˜ë¦¬!"]
```

```python
@task
def process_data(**context):
    # ì²˜ë¦¬í•  ë°ì´í„°ì˜ ë‚ ì§œ ë²”ìœ„
    data_interval_start = context["data_interval_start"]
    data_interval_end = context["data_interval_end"]
    
    # ì˜ˆ: 2024-01-01 00:00 ~ 2024-01-02 00:00
    print(f"Processing data from {data_interval_start} to {data_interval_end}")
```

### Catchupê³¼ Backfill

```mermaid
flowchart TB
    subgraph Catchup ["catchup=True"]
        C1["DAG ìƒì„±: 2024-01-05"]
        C2["start_date: 2024-01-01"]
        C3["ëˆ„ë½ëœ 4ì¼ì¹˜ ìë™ ì‹¤í–‰"]
        
        C1 --> C2 --> C3
    end
    
    subgraph NoCatchup ["catchup=False"]
        N1["DAG ìƒì„±: 2024-01-05"]
        N2["start_date: 2024-01-01"]
        N3["ì˜¤ëŠ˜(01-05)ë¶€í„°ë§Œ ì‹¤í–‰"]
        
        N1 --> N2 --> N3
    end
```

```bash
# ìˆ˜ë™ Backfill
airflow dags backfill \
    --start-date 2024-01-01 \
    --end-date 2024-01-10 \
    my_etl_pipeline
```

---

## Task ì˜ì¡´ì„± íŒ¨í„´

### ê¸°ë³¸ íŒ¨í„´

```mermaid
flowchart LR
    subgraph Sequential ["ìˆœì°¨"]
        S1["A"] --> S2["B"] --> S3["C"]
    end
    
    subgraph Parallel ["ë³‘ë ¬"]
        P1["A"] --> P2["B"]
        P1 --> P3["C"]
        P2 --> P4["D"]
        P3 --> P4
    end
    
    subgraph FanOut ["Fan-out"]
        F1["A"] --> F2["B1"]
        F1 --> F3["B2"]
        F1 --> F4["B3"]
    end
```

### ì½”ë“œì—ì„œ ì˜ì¡´ì„± ì •ì˜

```python
# ë°©ë²• 1: >> ì—°ì‚°ì
task_a >> task_b >> task_c

# ë°©ë²• 2: << ì—°ì‚°ì (ì—­ë°©í–¥)
task_c << task_b << task_a

# ë°©ë²• 3: ë¦¬ìŠ¤íŠ¸ë¡œ ë³‘ë ¬
task_a >> [task_b, task_c] >> task_d

# ë°©ë²• 4: set_downstream/set_upstream
task_a.set_downstream(task_b)
task_b.set_upstream(task_a)
```

### TaskFlowì—ì„œëŠ” ë” ìì—°ìŠ¤ëŸ½ê²Œ

```python
@dag(...)
def pipeline():
    @task
    def start(): pass
    
    @task
    def process_a(data): pass
    
    @task
    def process_b(data): pass
    
    @task
    def end(a, b): pass
    
    data = start()
    result_a = process_a(data)
    result_b = process_b(data)
    end(result_a, result_b)  # ìë™ìœ¼ë¡œ ì˜ì¡´ì„± ìƒì„±
```

---

## ì‹¤ì „ ì˜ˆì œ: ë°ì´í„° íŒŒì´í”„ë¼ì¸

```python
from airflow.decorators import dag, task
from airflow.providers.postgres.operators.postgres import PostgresOperator
from datetime import datetime, timedelta

default_args = {
    "owner": "data-team",
    "retries": 3,
    "retry_delay": timedelta(minutes=5),
}

@dag(
    dag_id="daily_user_analytics",
    start_date=datetime(2024, 1, 1),
    schedule="@daily",
    catchup=False,
    default_args=default_args,
    tags=["analytics", "production"]
)
def daily_user_analytics():
    """ì¼ì¼ ì‚¬ìš©ì ë¶„ì„ íŒŒì´í”„ë¼ì¸"""
    
    @task
    def extract_users(**context):
        """PostgreSQLì—ì„œ ì‚¬ìš©ì ë°ì´í„° ì¶”ì¶œ"""
        from airflow.providers.postgres.hooks.postgres import PostgresHook
        
        date = context["data_interval_start"].strftime("%Y-%m-%d")
        hook = PostgresHook(postgres_conn_id="production_db")
        
        sql = f"""
            SELECT user_id, event_type, created_at
            FROM user_events
            WHERE DATE(created_at) = '{date}'
        """
        
        df = hook.get_pandas_df(sql)
        return df.to_dict("records")
    
    @task
    def calculate_metrics(events: list):
        """ì‚¬ìš©ì ë©”íŠ¸ë¦­ ê³„ì‚°"""
        from collections import Counter
        
        user_events = Counter(e["user_id"] for e in events)
        
        return {
            "total_events": len(events),
            "unique_users": len(user_events),
            "events_per_user": len(events) / len(user_events) if user_events else 0
        }
    
    @task
    def save_to_warehouse(metrics: dict, **context):
        """ê²°ê³¼ë¥¼ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì— ì €ì¥"""
        from airflow.providers.postgres.hooks.postgres import PostgresHook
        
        date = context["data_interval_start"].strftime("%Y-%m-%d")
        hook = PostgresHook(postgres_conn_id="analytics_db")
        
        hook.run(f"""
            INSERT INTO daily_metrics (date, total_events, unique_users, events_per_user)
            VALUES ('{date}', {metrics['total_events']}, {metrics['unique_users']}, {metrics['events_per_user']})
            ON CONFLICT (date) DO UPDATE SET
                total_events = EXCLUDED.total_events,
                unique_users = EXCLUDED.unique_users,
                events_per_user = EXCLUDED.events_per_user
        """)
    
    @task
    def notify_slack(metrics: dict):
        """Slack ì•Œë¦¼ ì „ì†¡"""
        from airflow.providers.slack.hooks.slack import SlackHook
        
        hook = SlackHook(slack_conn_id="slack")
        hook.send(
            channel="#data-alerts",
            text=f"ğŸ“Š Daily Metrics: {metrics['unique_users']} users, {metrics['total_events']} events"
        )
    
    # ì˜ì¡´ì„± ì •ì˜
    events = extract_users()
    metrics = calculate_metrics(events)
    save_to_warehouse(metrics)
    notify_slack(metrics)

daily_user_analytics()
```

---

## ì •ë¦¬

```mermaid
mindmap
  root((Airflow<br/>í•µì‹¬ ê°œë…))
    ì™œ Airflow?
      ì˜ì¡´ì„± ê´€ë¦¬
      ì‹¤íŒ¨ ì²˜ë¦¬
      ë°±í•„
      ëª¨ë‹ˆí„°ë§
    ì•„í‚¤í…ì²˜
      Webserver
      Scheduler
      Worker
      Metadata DB
    DAG
      ë°©í–¥ì„± ê·¸ë˜í”„
      ìˆœí™˜ ì—†ìŒ
      Taskë“¤ì˜ ëª¨ìŒ
    Operator/Task
      Operator: ë¬´ì—‡ì„
      Task: ì¸ìŠ¤í„´ìŠ¤
      ì˜ì¡´ì„± ì •ì˜
    TaskFlow API
      @task ë°ì½”ë ˆì´í„°
      ìë™ XCom
      ê¹”ë”í•œ ì½”ë“œ
    ìŠ¤ì¼€ì¤„ë§
      Data Interval
      Catchup
      Backfill
```

---

## ë‹¤ìŒ í¸ ì˜ˆê³ 

**7í¸: Airflow ì‹¤ì „**ì—ì„œëŠ” í”„ë¡œë•ì…˜ ìš´ì˜ì„ ë‹¤ë£¹ë‹ˆë‹¤:

- DAG ëª¨ë“ˆí™” ì „ëµ
- ë™ì  Task ìƒì„±
- í…ŒìŠ¤íŠ¸ ë°©ë²•
- ì—ëŸ¬ ì²˜ë¦¬ì™€ ì•Œë¦¼
- ëª¨ë‹ˆí„°ë§

---

## ì°¸ê³  ìë£Œ

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [TaskFlow API Tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html)
- Astronomer, "Airflow Best Practices"
- "Data Pipelines with Apache Airflow" (Manning)
