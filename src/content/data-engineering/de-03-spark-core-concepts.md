---
public: true
title: "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #3: Spark í•µì‹¬ ê°œë… - RDDì—ì„œ DataFrameê¹Œì§€"
date: '2026-01-02'
category: Data Engineering
tags: [Spark, PySpark, RDD, DataFrame, Distributed Computing, Big Data]
excerpt: "ë¶„ì‚° ì²˜ë¦¬ì˜ í•µì‹¬ ê°œë…ê³¼ Sparkì˜ ì¶”ìƒí™” ê³„ì¸µì„ ì´í•´í•©ë‹ˆë‹¤. Goroutine, ThreadPoolExecutorì™€ ë¹„êµí•˜ë©° Sparkê°€ í•´ê²°í•˜ëŠ” ë¬¸ì œë¥¼ íŒŒì•…í•©ë‹ˆë‹¤."
---

# ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #3: Spark í•µì‹¬ ê°œë… - RDDì—ì„œ DataFrameê¹Œì§€

> **ëŒ€ìƒ ë…ì**: ì¶©ë¶„í•œ ê²½í—˜ì„ ê°€ì§„ ë°±ì—”ë“œ/í’€ìŠ¤íƒ ì—”ì§€ë‹ˆì–´ë¡œ, Goì˜ Goroutineì´ë‚˜ Pythonì˜ ThreadPoolExecutorì— ìµìˆ™í•˜ì§€ë§Œ SparkëŠ” ì²˜ìŒì¸ ë¶„

## ì´ í¸ì—ì„œ ë‹¤ë£¨ëŠ” ê²ƒ

"Sparkê°€ ë¹ ë¥´ë‹¤"ëŠ” ë§ì€ ë§ì´ ë“¤ì–´ë´¤ì„ ê²ë‹ˆë‹¤. í•˜ì§€ë§Œ **ì™œ ë¹ ë¥¸ì§€**, **ê¸°ì¡´ ë³‘ë ¬ ì²˜ë¦¬ì™€ ë¬´ì—‡ì´ ë‹¤ë¥¸ì§€** ì´í•´í•˜ëŠ” ê²ƒì´ ë¨¼ì €ì…ë‹ˆë‹¤.

---

## ì™œ ë¶„ì‚° ì²˜ë¦¬ê°€ í•„ìš”í•œê°€?

### ë‹¨ì¼ ì„œë²„ì˜ í•œê³„

ë°±ì—”ë“œ ê°œë°œì—ì„œ ì„±ëŠ¥ì„ ë†’ì´ë ¤ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?

```mermaid
flowchart TB
    subgraph SingleServer ["ë‹¨ì¼ ì„œë²„ ìµœì í™”"]
        direction TB
        S1["1. ì•Œê³ ë¦¬ì¦˜ ê°œì„ "]
        S2["2. ì¸ë±ìŠ¤ ì¶”ê°€"]
        S3["3. ìºì‹±"]
        S4["4. ë³‘ë ¬ ì²˜ë¦¬<br/>(Goroutine, ThreadPool)"]
        S5["5. ìˆ˜ì§ í™•ì¥<br/>(ë” ì¢‹ì€ ì„œë²„)"]
    end
    
    Limit["ê·¸ë˜ë„ ì•ˆ ë˜ë©´?"]
    
    SingleServer --> Limit
    Limit --> Distribute["ìˆ˜í‰ í™•ì¥<br/>(ì—¬ëŸ¬ ì„œë²„ë¡œ ë¶„ì‚°)"]
```

ì–¸ì  ê°€ëŠ” **ë‹¨ì¼ ì„œë²„ë¡œëŠ” ë¶ˆê°€ëŠ¥í•œ ìˆœê°„**ì´ ì˜µë‹ˆë‹¤:

| ìƒí™© | ì˜ˆì‹œ |
|------|------|
| **ë°ì´í„°ê°€ ë©”ëª¨ë¦¬ì— ì•ˆ ë“¤ì–´ê°** | 1TB ë°ì´í„°ë¥¼ 32GB ì„œë²„ì—ì„œ ì²˜ë¦¬ |
| **ì²˜ë¦¬ ì‹œê°„ì´ ë„ˆë¬´ ê¹€** | ë‹¨ì¼ ì½”ì–´ë¡œ 10ì–µ ê±´ ì²˜ë¦¬ì— 10ì‹œê°„ |
| **ë””ìŠ¤í¬ I/O ë³‘ëª©** | ì´ˆë‹¹ ì½ê¸° í•œê³„ ë„ë‹¬ |

### ë¶„ì‚° ì²˜ë¦¬ì˜ í•µì‹¬ ì•„ì´ë””ì–´

```mermaid
flowchart LR
    subgraph Before ["ë‹¨ì¼ ì„œë²„"]
        Server1["ì„œë²„ 1ëŒ€<br/>ğŸ–¥ï¸"]
        Data1["1TB ë°ì´í„°"]
        Time1["â±ï¸ 10ì‹œê°„"]
        Data1 --> Server1 --> Time1
    end
    
    subgraph After ["ë¶„ì‚° ì²˜ë¦¬"]
        Data2["1TB ë°ì´í„°"]
        
        subgraph Cluster ["10ëŒ€ í´ëŸ¬ìŠ¤í„°"]
            C1["ğŸ–¥ï¸ 100GB"]
            C2["ğŸ–¥ï¸ 100GB"]
            C3["ğŸ–¥ï¸ 100GB"]
            CN["ğŸ–¥ï¸ ..."]
        end
        
        Time2["â±ï¸ 1ì‹œê°„"]
        
        Data2 --> Cluster --> Time2
    end
    
    Before -.->|"10ë°° ë¹ ë¥´ê²Œ"| After
```

**í•µì‹¬**: ë°ì´í„°ì™€ ì—°ì‚°ì„ ì—¬ëŸ¬ ì„œë²„ì— **ë‚˜ëˆ ì„œ** ë™ì‹œì— ì²˜ë¦¬

---

## Goroutine/ThreadPoolExecutorì™€ Sparkì˜ ì°¨ì´

### ê¸°ì¡´ ë³‘ë ¬ ì²˜ë¦¬: ë‹¨ì¼ ì„œë²„ ë‚´

Goì™€ Pythonì—ì„œì˜ ë³‘ë ¬ ì²˜ë¦¬ëŠ” **ë‹¨ì¼ ì„œë²„ì˜ CPU ì½”ì–´ë¥¼ í™œìš©**í•©ë‹ˆë‹¤.

```mermaid
flowchart TB
    subgraph Go ["Go: Goroutines"]
        GoRuntime["Go Runtime<br/>(ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤)"]
        G1["goroutine"]
        G2["goroutine"]
        G3["goroutine"]
        G4["goroutine"]
        
        GoRuntime --> G1 & G2 & G3 & G4
        
        subgraph GoCPU ["CPU ì½”ì–´"]
            GC1["Core 1"]
            GC2["Core 2"]
            GC3["Core 3"]
            GC4["Core 4"]
        end
        
        G1 -.-> GC1
        G2 -.-> GC2
        G3 -.-> GC3
        G4 -.-> GC4
    end
    
    subgraph Python ["Python: ThreadPoolExecutor"]
        Pool["ThreadPoolExecutor"]
        T1["Thread 1"]
        T2["Thread 2"]
        T3["Thread 3"]
        T4["Thread 4"]
        
        Pool --> T1 & T2 & T3 & T4
        
        GIL["âš ï¸ GIL ì œì•½"]
    end
```

**í•œê³„**:

- **ë©”ëª¨ë¦¬ í•œê³„**: ì„œë²„ RAM í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ëŠ” ë°ì´í„° ì²˜ë¦¬ ë¶ˆê°€
- **CPU í•œê³„**: ì½”ì–´ ìˆ˜ ì´ìƒì˜ ë³‘ë ¬ì„± ë¶ˆê°€
- **GIL (Python)**: CPU-bound ì‘ì—… ì‹œ ì§„ì •í•œ ë³‘ë ¬ì„± ì–´ë ¤ì›€

### Spark: ì—¬ëŸ¬ ì„œë²„ì— ë¶„ì‚°

```mermaid
flowchart TB
    subgraph Driver ["Driver (ë§ˆìŠ¤í„°)"]
        App["Spark Application"]
    end
    
    subgraph Cluster ["í´ëŸ¬ìŠ¤í„° (ì›Œì»¤ë“¤)"]
        subgraph Worker1 ["Worker 1 (ì„œë²„ A)"]
            E1["Executor"]
            E1T1["Task"]
            E1T2["Task"]
            E1 --> E1T1 & E1T2
        end
        
        subgraph Worker2 ["Worker 2 (ì„œë²„ B)"]
            E2["Executor"]
            E2T1["Task"]
            E2T2["Task"]
            E2 --> E2T1 & E2T2
        end
        
        subgraph Worker3 ["Worker 3 (ì„œë²„ C)"]
            E3["Executor"]
            E3T1["Task"]
            E3T2["Task"]
            E3 --> E3T1 & E3T2
        end
    end
    
    App -->|"ì‘ì—… ë¶„ë°°"| Worker1 & Worker2 & Worker3
```

**Sparkì˜ í•´ê²°ì±…**:

- **ë©”ëª¨ë¦¬ ë¶„ì‚°**: ê° ì„œë²„ê°€ ë°ì´í„° ì¼ë¶€ë§Œ ì²˜ë¦¬
- **CPU ë¶„ì‚°**: ì´ CPU = ì„œë²„ ìˆ˜ Ã— ì„œë²„ë‹¹ ì½”ì–´
- **ì¥ì•  ë³µêµ¬**: í•œ ì„œë²„ê°€ ì£½ì–´ë„ ë‹¤ë¥¸ ì„œë²„ê°€ ì¬ì²˜ë¦¬

### ë¹„êµ ì •ë¦¬

| íŠ¹ì„± | Goroutine / ThreadPool | Spark |
|------|----------------------|-------|
| **ë²”ìœ„** | ë‹¨ì¼ ì„œë²„ | ì—¬ëŸ¬ ì„œë²„ í´ëŸ¬ìŠ¤í„° |
| **ìŠ¤ì¼€ì¼ë§** | ìˆ˜ì§ (ë” ì¢‹ì€ ì„œë²„) | ìˆ˜í‰ (ì„œë²„ ì¶”ê°€) |
| **ë©”ëª¨ë¦¬** | ì„œë²„ RAM í•œê³„ | í´ëŸ¬ìŠ¤í„° í•©ì‚° RAM |
| **ì¥ì•  ì²˜ë¦¬** | í”„ë¡œì„¸ìŠ¤ ì¬ì‹œì‘ | ë‹¤ë¥¸ ë…¸ë“œê°€ ì¬ì²˜ë¦¬ |
| **ë°ì´í„° ê³µìœ ** | ë©”ëª¨ë¦¬ ì§ì ‘ ê³µìœ  | ë„¤íŠ¸ì›Œí¬ í†µì‹  |
| **ì í•©í•œ ë°ì´í„°** | GB ì´í•˜ | TB ~ PB |

---

## MapReduce íŒ¨ëŸ¬ë‹¤ì„

Sparkë¥¼ ì´í•´í•˜ë ¤ë©´ ë¨¼ì € **MapReduce**ë¥¼ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤.

### í´ë˜ì‹ ì˜ˆì œ: Word Count

"Hello World Hello" ë¼ëŠ” í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ë³„ ê°œìˆ˜ë¥¼ ì„¸ëŠ” ë¬¸ì œì…ë‹ˆë‹¤.

```mermaid
flowchart LR
    subgraph Input ["ì…ë ¥"]
        I1["Hello World Hello"]
    end
    
    subgraph Map ["Map (ë³€í™˜)"]
        M1["(Hello, 1)"]
        M2["(World, 1)"]
        M3["(Hello, 1)"]
    end
    
    subgraph Shuffle ["Shuffle (ì¬ë°°ì¹˜)"]
        S1["Hello â†’ [(Hello, 1), (Hello, 1)]"]
        S2["World â†’ [(World, 1)]"]
    end
    
    subgraph Reduce ["Reduce (ì§‘ê³„)"]
        R1["(Hello, 2)"]
        R2["(World, 1)"]
    end
    
    Input --> Map --> Shuffle --> Reduce
```

### ë¶„ì‚° í™˜ê²½ì—ì„œì˜ MapReduce

```mermaid
flowchart TB
    subgraph Data ["ë¶„ì‚°ëœ ë°ì´í„°"]
        D1["Partition 1<br/>'Hello World'"]
        D2["Partition 2<br/>'Hello Spark'"]
        D3["Partition 3<br/>'World Spark'"]
    end
    
    subgraph MapPhase ["Map Phase (ë³‘ë ¬)"]
        M1["Worker 1<br/>(Hello,1) (World,1)"]
        M2["Worker 2<br/>(Hello,1) (Spark,1)"]
        M3["Worker 3<br/>(World,1) (Spark,1)"]
    end
    
    subgraph ShufflePhase ["Shuffle Phase"]
        direction LR
        SH["í‚¤ ê¸°ì¤€ìœ¼ë¡œ ì¬ë°°ì¹˜<br/>(ë„¤íŠ¸ì›Œí¬ ì „ì†¡ ë°œìƒ)"]
    end
    
    subgraph ReducePhase ["Reduce Phase (ë³‘ë ¬)"]
        R1["Reducer 1<br/>Hello â†’ 2"]
        R2["Reducer 2<br/>World â†’ 2"]
        R3["Reducer 3<br/>Spark â†’ 2"]
    end
    
    D1 --> M1
    D2 --> M2
    D3 --> M3
    
    M1 & M2 & M3 --> ShufflePhase --> R1 & R2 & R3
```

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸**:

- **Map**: ê° ì„œë²„ê°€ ìê¸° íŒŒí‹°ì…˜ë§Œ ì²˜ë¦¬ (ë³‘ë ¬, ë¹ ë¦„)
- **Shuffle**: í‚¤ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ì¬ë°°ì¹˜ (ë„¤íŠ¸ì›Œí¬ í†µì‹ , ëŠë¦¼ âš ï¸)
- **Reduce**: ê°™ì€ í‚¤ë¼ë¦¬ ëª¨ì—¬ì„œ ì§‘ê³„ (ë³‘ë ¬)

---

## RDD (Resilient Distributed Dataset)

Sparkì˜ í•µì‹¬ ì¶”ìƒí™”ì…ë‹ˆë‹¤.

### RDDë€?

```mermaid
flowchart TB
    subgraph RDD ["RDD: Resilient Distributed Dataset"]
        R["ë¶ˆë³€(Immutable)"]
        D["ë¶„ì‚°(Distributed)"]
        F["ì¥ì•  ë³µêµ¬(Fault-tolerant)"]
    end
    
    subgraph Partitions ["íŒŒí‹°ì…˜ìœ¼ë¡œ ë¶„ì‚°"]
        P1["Partition 1<br/>ì„œë²„ A"]
        P2["Partition 2<br/>ì„œë²„ B"]
        P3["Partition 3<br/>ì„œë²„ C"]
    end
    
    RDD --> Partitions
```

**í•µì‹¬ íŠ¹ì„±**:

| íŠ¹ì„± | ì˜ë¯¸ | ì™œ ì¤‘ìš”í•œê°€? |
|------|------|------------|
| **Resilient** | ì¥ì•  ë³µêµ¬ ê°€ëŠ¥ | ë…¸ë“œê°€ ì£½ì–´ë„ ë°ì´í„° ë³µêµ¬ |
| **Distributed** | í´ëŸ¬ìŠ¤í„°ì— ë¶„ì‚° | ì—¬ëŸ¬ ì„œë²„ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬ |
| **Immutable** | ë³€ê²½ ë¶ˆê°€ | ì—°ì‚° ê²°ê³¼ëŠ” ìƒˆ RDD ìƒì„± |

### Transformations vs Actions

RDD ì—°ì‚°ì€ ë‘ ì¢…ë¥˜ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.

```mermaid
flowchart LR
    subgraph Transformations ["Transformations (ë³€í™˜)"]
        T1["map"]
        T2["filter"]
        T3["flatMap"]
        T4["groupBy"]
        T5["join"]
        
        Lazy["â¸ï¸ Lazy: ë°”ë¡œ ì‹¤í–‰ ì•ˆ í•¨"]
    end
    
    subgraph Actions ["Actions (ì‹¤í–‰)"]
        A1["count"]
        A2["collect"]
        A3["save"]
        A4["reduce"]
        
        Execute["â–¶ï¸ ì‹¤í–‰: ì´ë•Œ ê³„ì‚° ì‹œì‘"]
    end
    
    RDD1["RDD"] --> Transformations --> RDD2["ìƒˆ RDD"]
    RDD2 --> Actions --> Result["ê²°ê³¼"]
```

### Lazy Evaluationì˜ í˜

```mermaid
flowchart TB
    subgraph Eager ["Eager Evaluation (ì¼ë°˜ì ì¸ ë°©ì‹)"]
        EE1["data = load()"] -->|"ì‹¤í–‰"| EE2["filtered = filter()"]
        EE2 -->|"ì‹¤í–‰"| EE3["mapped = map()"]
        EE3 -->|"ì‹¤í–‰"| EE4["result = count()"]
    end
    
    subgraph Lazy ["Lazy Evaluation (Spark)"]
        LE1["rdd = load()"] -->|"ê¸°ë¡ë§Œ"| LE2["filtered = filter()"]
        LE2 -->|"ê¸°ë¡ë§Œ"| LE3["mapped = map()"]
        LE3 -->|"ê¸°ë¡ë§Œ"| LE4["count()"]
        LE4 -->|"ìµœì í™” í›„ ì‹¤í–‰!"| Result["ê²°ê³¼"]
    end
    
    Lazy -->|"ì¥ì "| Optimize["âœ… ì‹¤í–‰ ê³„íš ìµœì í™”<br/>âœ… ë¶ˆí•„ìš”í•œ ì—°ì‚° ì œê±°<br/>âœ… íŒŒì´í”„ë¼ì´ë‹"]
```

**ì‹¤ì œ ì˜ˆì‹œ**:

```python
# SparkëŠ” ì´ ì‹œì ì— ì•„ë¬´ê²ƒë„ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ
rdd = spark.read.text("huge_file.txt")  # ê¸°ë¡ë§Œ
filtered = rdd.filter(lambda x: "error" in x)  # ê¸°ë¡ë§Œ
mapped = filtered.map(lambda x: (x, 1))  # ê¸°ë¡ë§Œ

# ì´ ì‹œì ì— ìµœì í™”ëœ ê³„íšìœ¼ë¡œ í•œ ë²ˆì— ì‹¤í–‰
count = mapped.count()  # ì‹¤í–‰!
```

---

## DataFrame: RDDì˜ ì§„í™”

### RDDì˜ í•œê³„

```mermaid
flowchart TB
    subgraph Problem ["RDDì˜ ë¬¸ì œ"]
        P1["íƒ€ì… ì •ë³´ ì—†ìŒ<br/>(Python ê°ì²´)"]
        P2["ìµœì í™” ì–´ë ¤ì›€<br/>(ë¸”ë™ë°•ìŠ¤)"]
        P3["ì§ë ¬í™” ì˜¤ë²„í—¤ë“œ<br/>(Python â†” JVM)"]
    end
    
    Problem --> Solution["DataFrame ë“±ì¥"]
```

### DataFrameì´ë€?

```mermaid
flowchart TB
    subgraph DataFrame ["DataFrame"]
        direction TB
        Schema["ìŠ¤í‚¤ë§ˆ (ì»¬ëŸ¼ëª…, íƒ€ì…)"]
        Rows["Row ë°ì´í„°"]
        Catalyst["Catalyst Optimizer"]
    end
    
    subgraph Analogy ["ìµìˆ™í•œ ë¹„ìœ "]
        SQL["SQL í…Œì´ë¸”"]
        Pandas["Pandas DataFrame"]
        Excel["ì—‘ì…€ ì‹œíŠ¸"]
    end
    
    DataFrame --> Analogy
```

**DataFrame vs RDD**:

| íŠ¹ì„± | RDD | DataFrame |
|------|-----|-----------|
| **ìŠ¤í‚¤ë§ˆ** | ì—†ìŒ (Python ê°ì²´) | ìˆìŒ (ì»¬ëŸ¼ëª…, íƒ€ì…) |
| **ìµœì í™”** | ìˆ˜ë™ (ê°œë°œìê°€) | ìë™ (Catalyst) |
| **API** | map, filter (í•¨ìˆ˜í˜•) | select, where (SQLí˜•) |
| **ì„±ëŠ¥** | ëŠë¦¼ (ì§ë ¬í™”) | ë¹ ë¦„ (ìµœì í™”) |
| **ì–¸ì–´** | ì–¸ì–´ë³„ ì°¨ì´ í¼ | ì–¸ì–´ë³„ ì°¨ì´ ì ìŒ |

### ì™œ DataFrameì´ ë” ë¹ ë¥¸ê°€?

```mermaid
flowchart TB
    subgraph RDDPath ["RDD ê²½ë¡œ"]
        R1["Python í•¨ìˆ˜"] --> R2["ì§ë ¬í™”<br/>(pickle)"]
        R2 --> R3["JVM ì „ì†¡"]
        R3 --> R4["ì—­ì§ë ¬í™”"]
        R4 --> R5["ì‹¤í–‰"]
    end
    
    subgraph DFPath ["DataFrame ê²½ë¡œ"]
        D1["DataFrame API"] --> D2["Catalyst<br/>ìµœì í™”"]
        D2 --> D3["JVM ì½”ë“œ<br/>ì§ì ‘ ì‹¤í–‰"]
    end
    
    RDDPath -->|"ğŸ¢"| Slow["ëŠë¦¼"]
    DFPath -->|"ğŸš€"| Fast["ë¹ ë¦„"]
```

---

## Spark Connect (4.0+)

Spark 4.0ì˜ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.

### ê¸°ì¡´ ë°©ì‹ vs Spark Connect

```mermaid
flowchart TB
    subgraph Before ["ê¸°ì¡´ ë°©ì‹"]
        Client1["Python Driver"] -->|"ê°™ì€ ì„œë²„"| Cluster1["Spark Cluster"]
    end
    
    subgraph After ["Spark Connect"]
        Client2["Thin Client<br/>(ì–´ë””ì„œë“ )"] -->|"gRPC"| Server["Spark Connect<br/>Server"]
        Server --> Cluster2["Spark Cluster"]
    end
    
    After -->|"ì¥ì "| Benefits["âœ… í´ë¼ì´ì–¸íŠ¸ ê°€ë²¼ì›€<br/>âœ… ì›ê²© ì—°ê²° ê°€ëŠ¥<br/>âœ… ë‹¤ì–‘í•œ ì–¸ì–´ ì§€ì›"]
```

```python
# Spark Connect ì‚¬ìš© ì˜ˆ
from pyspark.sql import SparkSession

# ì›ê²© í´ëŸ¬ìŠ¤í„°ì— ì—°ê²°
spark = SparkSession.builder \
    .remote("sc://spark-server:15002") \
    .getOrCreate()

# ë‚˜ë¨¸ì§€ëŠ” ë™ì¼í•˜ê²Œ ì‚¬ìš©
df = spark.range(1000000)
result = df.groupBy((df.id % 10).alias("group")).count()
result.show()
```

---

## ì‹¤ì „ ì½”ë“œ: Word Count ë¹„êµ

### Python (ThreadPoolExecutor)

```python
from concurrent.futures import ThreadPoolExecutor
from collections import Counter

def count_words_in_chunk(text_chunk):
    words = text_chunk.lower().split()
    return Counter(words)

def word_count_threaded(text, num_workers=4):
    # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
    chunks = [text[i::num_workers] for i in range(num_workers)]
    
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        results = list(executor.map(count_words_in_chunk, chunks))
    
    # ê²°ê³¼ í•©ì¹˜ê¸°
    total = Counter()
    for result in results:
        total.update(result)
    
    return total

# í•œê³„: ë©”ëª¨ë¦¬ì— ì „ì²´ í…ìŠ¤íŠ¸ê°€ ì˜¬ë¼ì™€ì•¼ í•¨
```

### Go (Goroutines)

```go
func wordCount(texts []string) map[string]int {
    results := make(chan map[string]int, len(texts))
    
    // ê° ì²­í¬ë¥¼ goroutineìœ¼ë¡œ ì²˜ë¦¬
    for _, text := range texts {
        go func(t string) {
            counts := make(map[string]int)
            for _, word := range strings.Fields(strings.ToLower(t)) {
                counts[word]++
            }
            results <- counts
        }(text)
    }
    
    // ê²°ê³¼ í•©ì¹˜ê¸°
    total := make(map[string]int)
    for i := 0; i < len(texts); i++ {
        for word, count := range <-results {
            total[word] += count
        }
    }
    
    return total
}

// í•œê³„: ë‹¨ì¼ ì„œë²„ ë©”ëª¨ë¦¬ í•œê³„
```

### PySpark (ë¶„ì‚° ì²˜ë¦¬)

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import explode, split, lower, col

spark = SparkSession.builder.appName("WordCount").getOrCreate()

# TB ë‹¨ìœ„ íŒŒì¼ë„ ì²˜ë¦¬ ê°€ëŠ¥
df = spark.read.text("hdfs://path/to/huge_files/*.txt")

word_counts = df \
    .select(explode(split(lower(col("value")), "\\s+")).alias("word")) \
    .groupBy("word") \
    .count() \
    .orderBy(col("count").desc())

word_counts.show(20)

# ì¥ì : ìë™ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° ì „ì²´ì— ë¶„ì‚° ì²˜ë¦¬
```

---

## ì •ë¦¬

```mermaid
mindmap
  root((Spark<br/>í•µì‹¬ ê°œë…))
    ì™œ ë¶„ì‚° ì²˜ë¦¬?
      ë‹¨ì¼ ì„œë²„ í•œê³„
      ë°ì´í„° > ë©”ëª¨ë¦¬
      ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•
    vs ê¸°ì¡´ ë³‘ë ¬ì²˜ë¦¬
      Goroutine: ë‹¨ì¼ ì„œë²„
      Spark: ì—¬ëŸ¬ ì„œë²„
      ìŠ¤ì¼€ì¼ ì°¨ì´
    MapReduce
      Map: ë³€í™˜
      Shuffle: ì¬ë°°ì¹˜
      Reduce: ì§‘ê³„
    RDD
      Immutable
      Distributed
      Fault-tolerant
    Lazy Evaluation
      ê¸°ë¡ë§Œ í•˜ë‹¤ê°€
      Actionì—ì„œ ì‹¤í–‰
      ìµœì í™” ê°€ëŠ¥
    DataFrame
      ìŠ¤í‚¤ë§ˆ ìˆìŒ
      Catalyst ìµœì í™”
      ë¹ ë¦„
```

---

## ë‹¤ìŒ í¸ ì˜ˆê³ 

**4í¸: Spark ë‚´ë¶€ ë™ì‘ ì›ë¦¬**ì—ì„œëŠ” ë” ê¹Šì´ ë“¤ì–´ê°‘ë‹ˆë‹¤:

- Job â†’ Stage â†’ Task ê³„ì¸µ
- Shuffleì´ ëŠë¦° ì´ìœ 
- íŒŒí‹°ì…”ë‹ ì „ëµ
- ë©”ëª¨ë¦¬ ê´€ë¦¬ì™€ Spill
- Spark UI ì½ëŠ” ë²•

---

## ì°¸ê³  ìë£Œ

- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [Spark: The Definitive Guide](https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/) (O'Reilly)
- [Spark Connect Overview](https://spark.apache.org/docs/latest/spark-connect-overview.html)
- [Learning Spark, 2nd Edition](https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/)
