---
public: true
title: "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #9: Spark Structured Streaming - ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬"
date: '2026-01-02'
category: Data Engineering
tags: [Spark, Streaming, Kafka, Watermark, Window, Real-time]
excerpt: "Spark Structured Streamingìœ¼ë¡œ ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. Kafka ì—°ë™, Watermark, Window ì—°ì‚°, ì²´í¬í¬ì¸íŒ…ê¹Œì§€."
---

# ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #9: Spark Structured Streaming - ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬

> **ëŒ€ìƒ ë…ì**: 6ë…„ ì´ìƒì˜ ê²½í—˜ì„ ê°€ì§„ ë°±ì—”ë“œ/í’€ìŠ¤íƒ ì—”ì§€ë‹ˆì–´ë¡œ, Sparkê³¼ Kafka ê¸°ë³¸ ê°œë…ì„ ìµíˆê³  ì‹¤ì‹œê°„ ì²˜ë¦¬ë¥¼ ë°°ìš°ë ¤ëŠ” ë¶„

## ì´ í¸ì—ì„œ ë‹¤ë£¨ëŠ” ê²ƒ

ë°°ì¹˜ ì²˜ë¦¬ì™€ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ë¥¼ **ê°™ì€ APIë¡œ** ë‹¤ë£¨ëŠ” Spark Structured Streamingì˜ í•µì‹¬ì„ ë°°ì›ë‹ˆë‹¤.

---

## ë°°ì¹˜ì™€ ìŠ¤íŠ¸ë¦¬ë°ì˜ í†µí•©

### Structured Streamingì˜ ì² í•™

```mermaid
flowchart TB
    subgraph Traditional ["ì „í†µì  ì ‘ê·¼"]
        Batch["ë°°ì¹˜ ì½”ë“œ<br/>(Spark SQL)"]
        Stream["ìŠ¤íŠ¸ë¦¬ë° ì½”ë“œ<br/>(DStream)"]
        Two["ì„œë¡œ ë‹¤ë¥¸ API ğŸ˜“"]
    end
    
    subgraph Unified ["Structured Streaming"]
        Single["ë™ì¼í•œ DataFrame API"]
        Batch2["ë°°ì¹˜ ì²˜ë¦¬"]
        Stream2["ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬"]
        Single --> Batch2
        Single --> Stream2
    end
```

### ë¬´í•œ í…Œì´ë¸” ê°œë…

```mermaid
flowchart LR
    subgraph Input ["ì…ë ¥ ìŠ¤íŠ¸ë¦¼"]
        T1["t=1: row 1, 2"]
        T2["t=2: row 3"]
        T3["t=3: row 4, 5, 6"]
        
        T1 --> T2 --> T3
    end
    
    subgraph Table ["ë¬´í•œ í…Œì´ë¸”"]
        Row1["row 1"]
        Row2["row 2"]
        Row3["row 3"]
        Row4["row 4"]
        Row5["row 5"]
        Row6["row 6"]
        Dots["..."]
        
        Row1 --> Row2 --> Row3 --> Row4 --> Row5 --> Row6 --> Dots
    end
    
    subgraph Output ["ê²°ê³¼"]
        Q["ë™ì¼í•œ ì¿¼ë¦¬ ì ìš©"]
    end
    
    Input --> Table --> Output
```

**í•µì‹¬ ì•„ì´ë””ì–´**: ìŠ¤íŠ¸ë¦¼ì„ "ê³„ì† ì¶”ê°€ë˜ëŠ” í…Œì´ë¸”"ë¡œ ìƒê°

---

## Sourceì™€ Sink

### ì§€ì›ë˜ëŠ” Source

```mermaid
flowchart TB
    subgraph Sources ["Input Sources"]
        Kafka["Kafka<br/>âœ… í”„ë¡œë•ì…˜"]
        File["File Source<br/>(JSON, Parquet, CSV)"]
        Socket["Socket<br/>(í…ŒìŠ¤íŠ¸ìš©)"]
        Rate["Rate Source<br/>(í…ŒìŠ¤íŠ¸ìš©)"]
    end
```

### ì§€ì›ë˜ëŠ” Sink

```mermaid
flowchart TB
    subgraph Sinks ["Output Sinks"]
        Kafka2["Kafka"]
        File2["File<br/>(Parquet, JSON)"]
        Console["Console<br/>(ë””ë²„ê¹…)"]
        Memory["Memory<br/>(í…ŒìŠ¤íŠ¸)"]
        ForeachBatch["foreachBatch<br/>(ì»¤ìŠ¤í…€ ë¡œì§)"]
    end
```

---

## Kafka â†’ Spark Streaming ì—°ë™

### ê¸°ë³¸ êµ¬ì¡°

```mermaid
flowchart LR
    subgraph Kafka ["Kafka"]
        Topic["Topic: events"]
    end
    
    subgraph Spark ["Spark Streaming"]
        Read["readStream"]
        Transform["ë³€í™˜ ë¡œì§"]
        Write["writeStream"]
    end
    
    subgraph Output ["ì¶œë ¥"]
        DeltaLake["Delta Lake"]
    end
    
    Kafka --> Read --> Transform --> Write --> Output
```

### ì½”ë“œ ì˜ˆì‹œ

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StringType, TimestampType, DoubleType

spark = SparkSession.builder \
    .appName("StreamingApp") \
    .getOrCreate()

# ìŠ¤í‚¤ë§ˆ ì •ì˜
event_schema = StructType() \
    .add("user_id", StringType()) \
    .add("event_type", StringType()) \
    .add("timestamp", TimestampType()) \
    .add("amount", DoubleType())

# Kafkaì—ì„œ ì½ê¸°
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "user_events") \
    .option("startingOffsets", "latest") \
    .load()

# value íŒŒì‹± (Kafka ë©”ì‹œì§€ëŠ” binary)
parsed = df.select(
    from_json(col("value").cast("string"), event_schema).alias("data")
).select("data.*")

# ë³€í™˜ ë¡œì§ (ë°°ì¹˜ì™€ ë™ì¼!)
result = parsed.filter(col("amount") > 0)

# ì¶œë ¥
query = result.writeStream \
    .format("delta") \
    .option("checkpointLocation", "/checkpoints/events") \
    .outputMode("append") \
    .start("/delta/events")

query.awaitTermination()
```

---

## Output Modes

### ì„¸ ê°€ì§€ ëª¨ë“œ

```mermaid
flowchart TB
    subgraph Append ["Append Mode"]
        A1["ìƒˆë¡œ ì¶”ê°€ëœ í–‰ë§Œ ì¶œë ¥"]
        A2["ì§‘ê³„ ì—†ëŠ” ì¿¼ë¦¬ì— ì í•©"]
        A3["ì˜ˆ: í•„í„°ë§, ë§µí•‘"]
    end
    
    subgraph Complete ["Complete Mode"]
        C1["ì „ì²´ ê²°ê³¼ ë§¤ë²ˆ ì¶œë ¥"]
        C2["ì§‘ê³„ ì¿¼ë¦¬ì— ì í•©"]
        C3["ì˜ˆ: groupBy().count()"]
    end
    
    subgraph Update ["Update Mode"]
        U1["ë³€ê²½ëœ í–‰ë§Œ ì¶œë ¥"]
        U2["ì§‘ê³„ ì¿¼ë¦¬ì— íš¨ìœ¨ì "]
        U3["ì˜ˆ: ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸"]
    end
```

### ì–¸ì œ ì–´ë–¤ ëª¨ë“œ?

| ì¿¼ë¦¬ ìœ í˜• | Append | Complete | Update |
|----------|--------|----------|--------|
| **SELECT, WHERE** | âœ… | âŒ | âœ… |
| **ì§‘ê³„ (groupBy)** | âŒ* | âœ… | âœ… |
| **ì›Œí„°ë§ˆí¬ + ì§‘ê³„** | âœ… | âœ… | âœ… |

*ì›Œí„°ë§ˆí¬ ì—†ëŠ” ì§‘ê³„ëŠ” Append ë¶ˆê°€

---

## Event Time vs Processing Time

### ë‘ ì‹œê°„ì˜ ì°¨ì´

```mermaid
flowchart LR
    subgraph EventTime ["Event Time"]
        ET["ì´ë²¤íŠ¸ê°€ ì‹¤ì œë¡œ ë°œìƒí•œ ì‹œê°„<br/>(ë°ì´í„°ì— í¬í•¨ëœ timestamp)"]
    end
    
    subgraph ProcessingTime ["Processing Time"]
        PT["Sparkì´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì‹œê°„<br/>(ì‹œìŠ¤í…œ ì‹œê³„)"]
    end
    
    subgraph Problem ["ë¬¸ì œ ìƒí™©"]
        P1["Event: 10:00:00"]
        P2["ë„¤íŠ¸ì›Œí¬ ì§€ì—°"]
        P3["Processing: 10:05:00"]
        P1 --> P2 --> P3
        
        Q["ì–´ë–¤ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ìœˆë„ìš°?"]
    end
```

### Event Time ì²˜ë¦¬

```python
# timestamp ì»¬ëŸ¼ì„ Event Timeìœ¼ë¡œ ì‚¬ìš©
parsed = df.select(
    from_json(col("value").cast("string"), event_schema).alias("data")
).select("data.*")

# Event Time ê¸°ì¤€ ìœˆë„ìš° ì§‘ê³„
result = parsed \
    .groupBy(
        window(col("timestamp"), "5 minutes"),
        col("event_type")
    ) \
    .count()
```

---

## Watermarkì™€ Late Data

### ì™œ Watermarkê°€ í•„ìš”í•œê°€?

```mermaid
flowchart TB
    subgraph Problem ["ë¬¸ì œ: ì§€ì—° ë°ì´í„°"]
        W1["10:00 ìœˆë„ìš° ì²˜ë¦¬ ì¤‘"]
        W2["10:05ì— ë„ì°©í•œ ë°ì´í„°"]
        W3["ê·¼ë° event_timeì€ 09:55!"]
        W4["ì–´ë–»ê²Œ ì²˜ë¦¬?"]
        
        W1 --> W2 --> W3 --> W4
    end
    
    subgraph Solution ["í•´ê²°: Watermark"]
        S1["í—ˆìš© ì§€ì—° ì‹œê°„ ì„¤ì •<br/>(ì˜ˆ: 10ë¶„)"]
        S2["Watermark = max(event_time) - 10ë¶„"]
        S3["Watermark ì´ì „ ìœˆë„ìš°ëŠ” ë‹«í˜"]
        
        S1 --> S2 --> S3
    end
```

### Watermark ë™ì‘ ë°©ì‹

```mermaid
flowchart LR
    subgraph Timeline ["ì‹œê°„ íë¦„"]
        T1["Event: 10:05"]
        T2["Event: 10:10"]
        T3["Event: 10:08"]
        T4["Event: 10:15"]
        T5["Late: 09:55"]
    end
    
    subgraph Watermark ["Watermark (ì§€ì—° 10ë¶„)"]
        W1["max=10:05<br/>WM=09:55"]
        W2["max=10:10<br/>WM=10:00"]
        W3["max=10:10<br/>WM=10:00"]
        W4["max=10:15<br/>WM=10:05"]
        W5["âŒ 09:55 < 10:05<br/>â†’ ë²„ë ¤ì§"]
    end
    
    T1 --> W1
    T2 --> W2
    T3 --> W3
    T4 --> W4
    T5 --> W5
```

### ì½”ë“œ ì˜ˆì‹œ

```python
from pyspark.sql.functions import window, col

# Watermark ì„¤ì •: 10ë¶„ ì§€ì—° í—ˆìš©
result = parsed \
    .withWatermark("timestamp", "10 minutes") \
    .groupBy(
        window(col("timestamp"), "5 minutes"),
        col("page")
    ) \
    .agg(count("*").alias("views"))

# Watermark ë•ë¶„ì— Append ëª¨ë“œ ê°€ëŠ¥
query = result.writeStream \
    .format("delta") \
    .outputMode("append") \
    .option("checkpointLocation", "/checkpoints/views") \
    .start("/delta/page_views")
```

---

## Window ì—°ì‚°

### Window ì¢…ë¥˜

```mermaid
flowchart TB
    subgraph Tumbling ["Tumbling Window"]
        T1["0-5ë¶„"]
        T2["5-10ë¶„"]
        T3["10-15ë¶„"]
        T1 --> T2 --> T3
        TNote["ê²¹ì¹˜ì§€ ì•ŠìŒ"]
    end
    
    subgraph Sliding ["Sliding Window"]
        S1["0-5ë¶„"]
        S2["2.5-7.5ë¶„"]
        S3["5-10ë¶„"]
        SNote["ê²¹ì¹¨ (slide < window)"]
    end
    
    subgraph Session ["Session Window"]
        SE1["í™œë™ ê¸°ê°„ A"]
        Gap["ë¹„í™œë™ gap"]
        SE2["í™œë™ ê¸°ê°„ B"]
        SE1 --> Gap --> SE2
        SENote["gap ê¸°ì¤€ ë¶„ë¦¬"]
    end
```

### Window í•¨ìˆ˜ ì‚¬ìš©

```python
from pyspark.sql.functions import window, sum, avg

# Tumbling Window: 5ë¶„ ìœˆë„ìš°
tumbling = parsed \
    .groupBy(window("timestamp", "5 minutes")) \
    .agg(sum("amount").alias("total"))

# Sliding Window: 10ë¶„ ìœˆë„ìš°, 5ë¶„ ìŠ¬ë¼ì´ë“œ
sliding = parsed \
    .groupBy(window("timestamp", "10 minutes", "5 minutes")) \
    .agg(avg("amount").alias("avg_amount"))

# Session Window (Spark 3.2+)
session = parsed \
    .groupBy(
        session_window("timestamp", "10 minutes"),
        col("user_id")
    ) \
    .agg(count("*").alias("session_events"))
```

---

## ì²´í¬í¬ì¸íŒ…ê³¼ ì¥ì•  ë³µêµ¬

### ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡°

```mermaid
flowchart TB
    subgraph Checkpoint ["ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬"]
        Offsets["offsets/<br/>Kafka offset ì •ë³´"]
        Commits["commits/<br/>ì²˜ë¦¬ ì™„ë£Œ ë°°ì¹˜"]
        State["state/<br/>ì§‘ê³„ ìƒíƒœ"]
        Metadata["metadata/<br/>ì¿¼ë¦¬ ë©”íƒ€ë°ì´í„°"]
    end
    
    subgraph Recovery ["ì¥ì•  ë³µêµ¬"]
        R1["ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"]
        R2["ë¯¸ì²˜ë¦¬ offsetë¶€í„° ì¬ì‹œì‘"]
        R3["ìƒíƒœ ë³µì›"]
        R1 --> R2 --> R3
    end
```

### Exactly-Once ë³´ì¥

```python
# ì²´í¬í¬ì¸íŠ¸ í•„ìˆ˜ ì„¤ì •
query = result.writeStream \
    .format("delta") \
    .outputMode("append") \
    .option("checkpointLocation", "hdfs://path/checkpoints/my_query") \
    .trigger(processingTime="1 minute") \
    .start("/delta/output")
```

**ì²´í¬í¬ì¸íŠ¸ê°€ ë³´ì¥í•˜ëŠ” ê²ƒ**:

- Kafka offset ì¶”ì  â†’ ì¤‘ë³µ ì½ê¸° ë°©ì§€
- ìƒíƒœ ì €ì¥ â†’ ì§‘ê³„ ê²°ê³¼ ìœ ì§€
- Atomic ì»¤ë°‹ â†’ Exactly-Once

---

## ì‹¤ì „ ì˜ˆì œ: ì‹¤ì‹œê°„ í´ë¦­ìŠ¤íŠ¸ë¦¼ ë¶„ì„

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    from_json, col, window, count, sum, avg,
    current_timestamp, expr
)
from pyspark.sql.types import StructType, StringType, TimestampType

spark = SparkSession.builder \
    .appName("ClickstreamAnalysis") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

# ìŠ¤í‚¤ë§ˆ
click_schema = StructType() \
    .add("user_id", StringType()) \
    .add("page", StringType()) \
    .add("action", StringType()) \
    .add("timestamp", TimestampType())

# Kafkaì—ì„œ ì½ê¸°
clicks = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "clickstream") \
    .option("startingOffsets", "latest") \
    .load() \
    .select(
        from_json(col("value").cast("string"), click_schema).alias("click")
    ).select("click.*")

# 5ë¶„ ìœˆë„ìš°ë¡œ í˜ì´ì§€ë³„ í†µê³„
page_stats = clicks \
    .withWatermark("timestamp", "10 minutes") \
    .groupBy(
        window(col("timestamp"), "5 minutes"),
        col("page")
    ) \
    .agg(
        count("*").alias("view_count"),
        count("user_id").alias("unique_users")
    )

# Delta Lakeì— ì €ì¥
query = page_stats.writeStream \
    .format("delta") \
    .outputMode("append") \
    .option("checkpointLocation", "/checkpoints/clickstream") \
    .trigger(processingTime="1 minute") \
    .start("/delta/page_stats")

# ì½˜ì†”ì—ë„ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
debug_query = page_stats.writeStream \
    .format("console") \
    .outputMode("update") \
    .trigger(processingTime="30 seconds") \
    .start()

query.awaitTermination()
```

---

## ëª¨ë‹ˆí„°ë§

### Streaming Query ìƒíƒœ í™•ì¸

```python
# ì¿¼ë¦¬ ì§„í–‰ ìƒí™©
print(query.status)
# {'message': 'Processing new data', 'isActive': True, ...}

# ìµœê·¼ ì§„í–‰ ìƒí™©
for progress in query.recentProgress:
    print(f"Batch {progress['batchId']}")
    print(f"  Input rows: {progress['numInputRows']}")
    print(f"  Processing time: {progress['batchDuration']} ms")
```

### Spark UIì—ì„œ í™•ì¸

```mermaid
flowchart TB
    subgraph SparkUI ["Structured Streaming UI"]
        Tab["Streaming íƒ­"]
        Metrics["â€¢ Input Rate<br/>â€¢ Processing Rate<br/>â€¢ Batch Duration<br/>â€¢ State Rows"]
    end
```

---

## ì •ë¦¬

```mermaid
mindmap
  root((Spark<br/>Structured<br/>Streaming))
    í•µì‹¬ ê°œë…
      ë¬´í•œ í…Œì´ë¸”
      ë™ì¼í•œ API
      ë°°ì¹˜ & ìŠ¤íŠ¸ë¦¬ë° í†µí•©
    Source/Sink
      Kafka
      File
      Delta Lake
    Output Mode
      Append
      Complete
      Update
    ì‹œê°„ ì²˜ë¦¬
      Event Time
      Processing Time
      Watermark
    Window
      Tumbling
      Sliding
      Session
    ì•ˆì •ì„±
      Checkpoint
      Exactly-Once
      ì¥ì•  ë³µêµ¬
```

---

## ë‹¤ìŒ í¸ ì˜ˆê³ 

**10í¸: ë ˆì´í¬í•˜ìš°ìŠ¤ ì•„í‚¤í…ì²˜**ì—ì„œëŠ” ë°ì´í„° ì €ì¥ì†Œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤:

- Data Lake vs Data Warehouse
- Delta Lake ì‹¬ì¸µ ë¶„ì„
- ACID, Time Travel, Schema Evolution
- Apache Iceberg ë¹„êµ

---

## ì°¸ê³  ìë£Œ

- [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Kafka Integration](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)
- Databricks, "Real-time Streaming with Spark 3.0"
