---
public: true
title: "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #7: Airflow ì‹¤ì „ - í”„ë¡œë•ì…˜ê¸‰ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•"
date: '2026-01-02'
category: Data Engineering
tags: [Airflow, DAG, Production, Testing, Monitoring, Best Practices]
excerpt: "í”„ë¡œë•ì…˜ì—ì„œ Airflowë¥¼ ìš´ì˜í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤. DAG ëª¨ë“ˆí™”, ë™ì  Task ìƒì„±, í…ŒìŠ¤íŠ¸, ì—ëŸ¬ ì²˜ë¦¬, ëª¨ë‹ˆí„°ë§ê¹Œì§€."
---

# ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #7: Airflow ì‹¤ì „ - í”„ë¡œë•ì…˜ê¸‰ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

> **ëŒ€ìƒ ë…ì**: 6ë…„ ì´ìƒì˜ ê²½í—˜ì„ ê°€ì§„ ë°±ì—”ë“œ/í’€ìŠ¤íƒ ì—”ì§€ë‹ˆì–´ë¡œ, Airflow ê¸°ë³¸ ê°œë…ì„ ìµíˆê³  í”„ë¡œë•ì…˜ ìš´ì˜ì— ê´€ì‹¬ ìˆëŠ” ë¶„

## ì´ í¸ì—ì„œ ë‹¤ë£¨ëŠ” ê²ƒ

6í¸ì—ì„œ Airflow ê°œë…ì„ ë°°ì› ë‹¤ë©´, ì´ì œ **ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œ ì–´ë–»ê²Œ ìš´ì˜í•˜ëŠ”ì§€** ì‹¤ì „ íŒ¨í„´ì„ ë°°ì›ë‹ˆë‹¤.

---

## DAG ëª¨ë“ˆí™” ì „ëµ

### ì™œ ëª¨ë“ˆí™”ê°€ í•„ìš”í•œê°€?

```mermaid
flowchart TB
    subgraph Bad ["âŒ ëª¨ë“  ê²ƒì´ í•œ íŒŒì¼ì—"]
        B1["dag_everything.py<br/>â€¢ DB ì—°ê²°<br/>â€¢ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§<br/>â€¢ ì„¤ì •<br/>â€¢ í—¬í¼ í•¨ìˆ˜<br/>...<br/>2000ì¤„ ğŸ˜±"]
    end
    
    subgraph Good ["âœ… ëª¨ë“ˆí™”ëœ êµ¬ì¡°"]
        G1["dags/daily_etl.py"]
        G2["plugins/operators/"]
        G3["plugins/hooks/"]
        G4["config/"]
        G5["utils/"]
    end
    
    Bad -->|"ë¦¬íŒ©í† ë§"| Good
```

### ê¶Œì¥ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
airflow/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ daily_etl.py
â”‚   â”œâ”€â”€ hourly_metrics.py
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ daily_etl_config.py
â”‚       â””â”€â”€ tables.py
â”‚
â”œâ”€â”€ plugins/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ operators/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ custom_operators.py
â”‚   â””â”€â”€ hooks/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ custom_hooks.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ test_daily_etl.py
â”‚   â””â”€â”€ plugins/
â”‚       â””â”€â”€ test_operators.py
â”‚
â””â”€â”€ requirements.txt
```

### ê³µí†µ ì„¤ì • ì¶”ì¶œ

```python
# config/daily_etl_config.py
from dataclasses import dataclass
from datetime import timedelta

@dataclass
class ETLConfig:
    source_conn_id: str = "production_db"
    target_conn_id: str = "warehouse_db"
    retries: int = 3
    retry_delay: timedelta = timedelta(minutes=5)
    
    @property
    def default_args(self):
        return {
            "owner": "data-team",
            "retries": self.retries,
            "retry_delay": self.retry_delay,
        }

# í…Œì´ë¸”ë³„ ì„¤ì •
TABLES = {
    "users": {"schedule": "@daily", "partition_key": "created_at"},
    "orders": {"schedule": "@hourly", "partition_key": "order_date"},
    "events": {"schedule": "*/15 * * * *", "partition_key": "event_time"},
}
```

---

## DAG Factory íŒ¨í„´

### ë™ì  DAG ìƒì„±

```mermaid
flowchart TB
    subgraph Factory ["DAG Factory"]
        Config["ì„¤ì • íŒŒì¼<br/>(tables.py)"]
        Template["DAG í…œí”Œë¦¿<br/>(create_etl_dag)"]
    end
    
    subgraph Output ["ìƒì„±ëœ DAGë“¤"]
        D1["etl_users"]
        D2["etl_orders"]
        D3["etl_events"]
    end
    
    Config --> Template --> Output
```

```python
# dags/etl_factory.py
from airflow.decorators import dag, task
from datetime import datetime
from config.daily_etl_config import ETLConfig, TABLES

def create_etl_dag(table_name: str, table_config: dict):
    """í…Œì´ë¸”ë³„ ETL DAGë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±"""
    
    config = ETLConfig()
    
    @dag(
        dag_id=f"etl_{table_name}",
        schedule=table_config["schedule"],
        start_date=datetime(2024, 1, 1),
        catchup=False,
        default_args=config.default_args,
        tags=["etl", "generated"]
    )
    def etl_pipeline():
        
        @task
        def extract(**context):
            from airflow.providers.postgres.hooks.postgres import PostgresHook
            
            hook = PostgresHook(postgres_conn_id=config.source_conn_id)
            date = context["data_interval_start"].strftime("%Y-%m-%d")
            partition_key = table_config["partition_key"]
            
            sql = f"""
                SELECT * FROM {table_name}
                WHERE DATE({partition_key}) = '{date}'
            """
            return hook.get_records(sql)
        
        @task
        def transform(raw_data):
            # ë³€í™˜ ë¡œì§
            return raw_data
        
        @task
        def load(data, **context):
            from airflow.providers.postgres.hooks.postgres import PostgresHook
            
            hook = PostgresHook(postgres_conn_id=config.target_conn_id)
            # ë¡œë“œ ë¡œì§
            print(f"Loaded {len(data)} records to warehouse.{table_name}")
        
        raw = extract()
        transformed = transform(raw)
        load(transformed)
    
    return etl_pipeline()

# ëª¨ë“  í…Œì´ë¸”ì— ëŒ€í•´ DAG ìƒì„±
for table_name, table_config in TABLES.items():
    globals()[f"etl_{table_name}"] = create_etl_dag(table_name, table_config)
```

---

## Dynamic Task Mapping (Airflow 2.3+)

### ëŸ°íƒ€ì„ì— Task ê°œìˆ˜ ê²°ì •

```mermaid
flowchart LR
    subgraph Before ["ì •ì  ë°©ì‹"]
        B1["process_user_1"]
        B2["process_user_2"]
        B3["process_user_3"]
        Note1["ë¯¸ë¦¬ ì •í•´ì§„ ìˆ˜"]
    end
    
    subgraph After ["Dynamic Mapping"]
        List["get_users()<br/>â†’ [u1, u2, ... uN]"]
        Expand["process.expand(user=users)"]
        Tasks["Nê°œì˜ Task ìƒì„±"]
        
        List --> Expand --> Tasks
        Note2["ëŸ°íƒ€ì„ì— ê²°ì •"]
    end
```

```python
from airflow.decorators import dag, task
from datetime import datetime

@dag(
    dag_id="dynamic_processing",
    schedule="@daily",
    start_date=datetime(2024, 1, 1),
    catchup=False
)
def dynamic_processing():
    
    @task
    def get_partitions(**context):
        """ì²˜ë¦¬í•  íŒŒí‹°ì…˜ ëª©ë¡ ë™ì  ë°˜í™˜"""
        date = context["data_interval_start"]
        # ì˜ˆ: ë‚ ì§œì— ë”°ë¼ ë‹¤ë¥¸ ê°œìˆ˜
        return [f"partition_{i}" for i in range(10)]  # 10ê°œ íŒŒí‹°ì…˜
    
    @task
    def process_partition(partition: str):
        """ê° íŒŒí‹°ì…˜ ë³‘ë ¬ ì²˜ë¦¬"""
        print(f"Processing {partition}")
        return {"partition": partition, "count": 1000}
    
    @task
    def aggregate(results: list):
        """ëª¨ë“  ê²°ê³¼ ì§‘ê³„"""
        total = sum(r["count"] for r in results)
        print(f"Total: {total} records from {len(results)} partitions")
    
    # Dynamic Task Mapping
    partitions = get_partitions()
    results = process_partition.expand(partition=partitions)  # Nê°œ Task ìƒì„±
    aggregate(results)

dynamic_processing()
```

---

## í…ŒìŠ¤íŠ¸ ì „ëµ

### í…ŒìŠ¤íŠ¸ í”¼ë¼ë¯¸ë“œ

```mermaid
flowchart TB
    subgraph Pyramid ["í…ŒìŠ¤íŠ¸ í”¼ë¼ë¯¸ë“œ"]
        E2E["E2E í…ŒìŠ¤íŠ¸<br/>(ì‹¤ì œ í™˜ê²½)"]
        Integration["í†µí•© í…ŒìŠ¤íŠ¸<br/>(DAG ìœ íš¨ì„±)"]
        Unit["ë‹¨ìœ„ í…ŒìŠ¤íŠ¸<br/>(ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§)"]
    end
    
    Unit -->|"ê°€ì¥ ë§ì´"| Integration -->|"ì ë‹¹íˆ"| E2E
```

### DAG ìœ íš¨ì„± í…ŒìŠ¤íŠ¸

```python
# tests/dags/test_dag_validity.py
import pytest
from airflow.models import DagBag

class TestDAGValidity:
    """ëª¨ë“  DAGì˜ ê¸°ë³¸ ìœ íš¨ì„± ê²€ì‚¬"""
    
    @pytest.fixture
    def dagbag(self):
        return DagBag(include_examples=False)
    
    def test_no_import_errors(self, dagbag):
        """DAG import ì˜¤ë¥˜ ì—†ìŒ"""
        assert dagbag.import_errors == {}, f"Import errors: {dagbag.import_errors}"
    
    def test_all_dags_have_tags(self, dagbag):
        """ëª¨ë“  DAGì— íƒœê·¸ ìˆìŒ"""
        for dag_id, dag in dagbag.dags.items():
            assert dag.tags, f"DAG {dag_id} has no tags"
    
    def test_no_cycles(self, dagbag):
        """ìˆœí™˜ ì˜ì¡´ì„± ì—†ìŒ"""
        for dag_id, dag in dagbag.dags.items():
            # Airflowê°€ ìë™ìœ¼ë¡œ ê²€ì‚¬í•˜ì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ
            assert not dag.test_cycle(), f"DAG {dag_id} has a cycle"
    
    def test_default_args(self, dagbag):
        """í•„ìˆ˜ default_args ì¡´ì¬"""
        required_keys = ["owner", "retries"]
        for dag_id, dag in dagbag.dags.items():
            for key in required_keys:
                assert key in dag.default_args, f"DAG {dag_id} missing {key}"
```

### ê°œë³„ Task í…ŒìŠ¤íŠ¸

```python
# tests/dags/test_daily_etl.py
import pytest
from unittest.mock import patch, MagicMock
from dags.daily_etl import calculate_metrics

class TestDailyETL:
    
    def test_calculate_metrics(self):
        """ë©”íŠ¸ë¦­ ê³„ì‚° ë¡œì§ í…ŒìŠ¤íŠ¸"""
        events = [
            {"user_id": 1, "event": "click"},
            {"user_id": 1, "event": "view"},
            {"user_id": 2, "event": "click"},
        ]
        
        result = calculate_metrics.function(events)
        
        assert result["total_events"] == 3
        assert result["unique_users"] == 2
        assert result["events_per_user"] == 1.5
    
    @patch("dags.daily_etl.PostgresHook")
    def test_extract_users(self, mock_hook):
        """ì¶”ì¶œ Task í…ŒìŠ¤íŠ¸ (Mock ì‚¬ìš©)"""
        mock_hook.return_value.get_pandas_df.return_value = pd.DataFrame({
            "user_id": [1, 2],
            "event_type": ["click", "view"]
        })
        
        # Task ì‹¤í–‰
        result = extract_users.function(data_interval_start=datetime(2024, 1, 1))
        
        assert len(result) == 2
        mock_hook.assert_called_once()
```

---

## ì—ëŸ¬ ì²˜ë¦¬ì™€ ì•Œë¦¼

### ì½œë°± í•¨ìˆ˜

```mermaid
flowchart TB
    subgraph Callbacks ["ì½œë°± ì¢…ë¥˜"]
        C1["on_success_callback<br/>ì„±ê³µ ì‹œ"]
        C2["on_failure_callback<br/>ì‹¤íŒ¨ ì‹œ"]
        C3["on_retry_callback<br/>ì¬ì‹œë„ ì‹œ"]
        C4["sla_miss_callback<br/>SLA ì´ˆê³¼ ì‹œ"]
    end
    
    subgraph Actions ["ê°€ëŠ¥í•œ ì•¡ì…˜"]
        A1["Slack ì•Œë¦¼"]
        A2["PagerDuty í˜¸ì¶œ"]
        A3["ì´ë©”ì¼ ì „ì†¡"]
        A4["ë©”íŠ¸ë¦­ ê¸°ë¡"]
    end
    
    Callbacks --> Actions
```

```python
from airflow.decorators import dag, task
from datetime import datetime, timedelta

def send_slack_alert(context):
    """ì‹¤íŒ¨ ì‹œ Slack ì•Œë¦¼"""
    from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook
    
    task_instance = context["task_instance"]
    dag_id = context["dag"].dag_id
    task_id = task_instance.task_id
    execution_date = context["execution_date"]
    log_url = task_instance.log_url
    
    message = f"""
    ğŸš¨ *Task Failed*
    â€¢ DAG: `{dag_id}`
    â€¢ Task: `{task_id}`
    â€¢ Execution: {execution_date}
    â€¢ <{log_url}|View Logs>
    """
    
    hook = SlackWebhookHook(slack_webhook_conn_id="slack_webhook")
    hook.send(text=message)

def send_success_notification(context):
    """ì„±ê³µ ì‹œ ì•Œë¦¼ (ì„ íƒì )"""
    # ì¤‘ìš”í•œ DAGë§Œ ì„±ê³µ ì•Œë¦¼
    pass

@dag(
    dag_id="monitored_pipeline",
    schedule="@daily",
    start_date=datetime(2024, 1, 1),
    catchup=False,
    default_args={
        "retries": 3,
        "retry_delay": timedelta(minutes=5),
        "on_failure_callback": send_slack_alert,
    },
    on_success_callback=send_success_notification
)
def monitored_pipeline():
    
    @task(
        retries=5,  # Task ê°œë³„ ì„¤ì •ë„ ê°€ëŠ¥
        retry_delay=timedelta(minutes=2)
    )
    def critical_task():
        # ì¤‘ìš” ë¡œì§
        pass
    
    critical_task()

monitored_pipeline()
```

### SLA (Service Level Agreement)

```python
from airflow.decorators import dag, task
from datetime import datetime, timedelta

@dag(
    dag_id="sla_monitored",
    schedule="@hourly",
    start_date=datetime(2024, 1, 1),
    sla_miss_callback=send_sla_alert
)
def sla_monitored():
    
    @task(
        sla=timedelta(minutes=30)  # 30ë¶„ ë‚´ ì™„ë£Œë˜ì–´ì•¼ í•¨
    )
    def time_sensitive_task():
        # SLAë¥¼ ì´ˆê³¼í•˜ë©´ sla_miss_callback í˜¸ì¶œ
        pass
```

---

## ëª¨ë‹ˆí„°ë§ê³¼ ê´€ì¸¡ì„±

### Airflow ë©”íŠ¸ë¦­

```mermaid
flowchart TB
    subgraph Metrics ["ì£¼ìš” ëª¨ë‹ˆí„°ë§ ì§€í‘œ"]
        subgraph DAG ["DAG ë ˆë²¨"]
            D1["DAG Run ì„±ê³µë¥ "]
            D2["í‰ê·  ì‹¤í–‰ ì‹œê°„"]
            D3["ì§€ì—°(Lag)"]
        end
        
        subgraph Task ["Task ë ˆë²¨"]
            T1["Task ì„±ê³µ/ì‹¤íŒ¨ìœ¨"]
            T2["Task Duration"]
            T3["Queue ëŒ€ê¸° ì‹œê°„"]
        end
        
        subgraph System ["ì‹œìŠ¤í…œ"]
            S1["Scheduler Heartbeat"]
            S2["Worker ìƒíƒœ"]
            S3["DB Connection Pool"]
        end
    end
```

### StatsD + Grafana ì—°ë™

```python
# airflow.cfg
[metrics]
statsd_on = True
statsd_host = statsd-exporter
statsd_port = 9125
statsd_prefix = airflow
```

### ìœ ìš©í•œ ëŒ€ì‹œë³´ë“œ ì¿¼ë¦¬

```python
# ì‹¤íŒ¨í•œ DAG Run ì¡°íšŒ
from airflow.models import DagRun

failed_runs = DagRun.find(
    state="failed",
    execution_start_date=datetime.now() - timedelta(days=1)
)

for run in failed_runs:
    print(f"{run.dag_id}: {run.execution_date}")
```

---

## ë©±ë“±ì„± ë³´ì¥

### ì™œ ë©±ë“±ì„±ì´ ì¤‘ìš”í•œê°€?

```mermaid
flowchart TB
    subgraph Problem ["ë©±ë“±í•˜ì§€ ì•Šì€ ê²½ìš°"]
        Run1["ì²« ì‹¤í–‰: 100ê±´ ì‚½ì…"]
        Run2["ì¬ì‹¤í–‰: 100ê±´ ì¶”ê°€ ì‚½ì…"]
        Result1["ê²°ê³¼: 200ê±´ (ì¤‘ë³µ!)"]
    end
    
    subgraph Solution ["ë©±ë“±í•œ ê²½ìš°"]
        Run3["ì²« ì‹¤í–‰: 100ê±´ ì‚½ì…"]
        Run4["ì¬ì‹¤í–‰: 100ê±´ ë®ì–´ì“°ê¸°"]
        Result2["ê²°ê³¼: 100ê±´ (ì •í™•!)"]
    end
```

### ë©±ë“±ì„± í™•ë³´ íŒ¨í„´

```python
@task
def load_idempotent(data, **context):
    """ë©±ë“±í•œ ë¡œë“œ"""
    from airflow.providers.postgres.hooks.postgres import PostgresHook
    
    hook = PostgresHook(postgres_conn_id="warehouse")
    date = context["data_interval_start"].strftime("%Y-%m-%d")
    table = "daily_metrics"
    
    # íŒ¨í„´ 1: DELETE + INSERT
    hook.run(f"DELETE FROM {table} WHERE date = '{date}'")
    hook.insert_rows(table, data)
    
    # íŒ¨í„´ 2: UPSERT (PostgreSQL)
    hook.run(f"""
        INSERT INTO {table} (date, value)
        VALUES ('{date}', {data['value']})
        ON CONFLICT (date) DO UPDATE SET
            value = EXCLUDED.value,
            updated_at = NOW()
    """)
    
    # íŒ¨í„´ 3: Partition êµì²´ (S3/GCS)
    # s3://bucket/table/date=2024-01-01/ ì „ì²´ êµì²´
```

---

## ì •ë¦¬

```mermaid
mindmap
  root((Airflow<br/>ì‹¤ì „))
    ëª¨ë“ˆí™”
      ë””ë ‰í† ë¦¬ êµ¬ì¡°
      ì„¤ì • ë¶„ë¦¬
      ì¬ì‚¬ìš© ê°€ëŠ¥
    DAG Factory
      ë™ì  ìƒì„±
      ì„¤ì • ê¸°ë°˜
      ìœ ì§€ë³´ìˆ˜ ìš©ì´
    Dynamic Mapping
      ëŸ°íƒ€ì„ ê²°ì •
      expand ì‚¬ìš©
      ë³‘ë ¬ ì²˜ë¦¬
    í…ŒìŠ¤íŠ¸
      DAG ìœ íš¨ì„±
      Task ë‹¨ìœ„
      Mock í™œìš©
    ì—ëŸ¬ ì²˜ë¦¬
      on_failure_callback
      SLA ì„¤ì •
      Slack ì—°ë™
    ëª¨ë‹ˆí„°ë§
      ë©”íŠ¸ë¦­ ìˆ˜ì§‘
      Grafana
      ë¡œê·¸ ì§‘ê³„
    ë©±ë“±ì„±
      DELETE + INSERT
      UPSERT
      íŒŒí‹°ì…˜ êµì²´
```

---

## ë‹¤ìŒ í¸ ì˜ˆê³ 

**8í¸: Kafka í•µì‹¬**ì—ì„œëŠ” ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°ì„ ë‹¤ë£¹ë‹ˆë‹¤:

- Redis Streamsì™€ì˜ ë¹„êµ
- Topic, Partition, Consumer Group
- Exactly-Once Semantics
- KRaft ëª¨ë“œ

---

## ì°¸ê³  ìë£Œ

- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)
- [Testing Airflow DAGs](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html#testing-a-dag)
- [Dynamic Task Mapping](https://airflow.apache.org/docs/apache-airflow/stable/concepts/dynamic-task-mapping.html)
- Astronomer, "Airflow in Production"
