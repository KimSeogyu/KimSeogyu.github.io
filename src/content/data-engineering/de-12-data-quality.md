---
public: true
title: "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #12: ë°ì´í„° í’ˆì§ˆ - í…ŒìŠ¤íŠ¸, ëª¨ë‹ˆí„°ë§, ê´€ì¸¡ì„±"
date: '2026-01-02'
category: Data Engineering
series: data-engineering
tags: [Data Engineering, Data Quality, Great Expectations, Lineage, Observability, Testing, dbt]
excerpt: "ë°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ í’ˆì§ˆì„ ë³´ì¥í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤. dbt í…ŒìŠ¤íŠ¸, Great Expectations, ë°ì´í„° ê³„ë³´, ê´€ì¸¡ì„±ê¹Œì§€."
---

# ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #12: ë°ì´í„° í’ˆì§ˆ - í…ŒìŠ¤íŠ¸, ëª¨ë‹ˆí„°ë§, ê´€ì¸¡ì„±

> **ëŒ€ìƒ ë…ì**: ì¶©ë¶„í•œ ê²½í—˜ì„ ê°€ì§„ ë°±ì—”ë“œ/í’€ìŠ¤íƒ ì—”ì§€ë‹ˆì–´ë¡œ, ì†Œí”„íŠ¸ì›¨ì–´ í…ŒìŠ¤íŠ¸ì— ìµìˆ™í•˜ì§€ë§Œ ë°ì´í„° í…ŒìŠ¤íŠ¸ëŠ” ì²˜ìŒì¸ ë¶„

## ì´ í¸ì—ì„œ ë‹¤ë£¨ëŠ” ê²ƒ

"ëŒ€ì‹œë³´ë“œ ìˆ«ìê°€ ì™œ ì–´ì œì™€ ë‹¬ë¼ìš”?" ì´ëŸ° ì§ˆë¬¸ì— ì²´ê³„ì ìœ¼ë¡œ ë‹µí•  ìˆ˜ ìˆëŠ” **ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì²´ê³„**ë¥¼ ë°°ì›ë‹ˆë‹¤.

---

## ë°ì´í„° í’ˆì§ˆì´ë€?

### í’ˆì§ˆì˜ ë‹¤ì„¯ ê°€ì§€ ì°¨ì›

```mermaid
flowchart TB
    subgraph Dimensions ["ë°ì´í„° í’ˆì§ˆ ì°¨ì›"]
        C["ì™„ì „ì„±<br/>(Completeness)<br/>NULLì´ ì—†ëŠ”ê°€?"]
        A["ì •í™•ì„±<br/>(Accuracy)<br/>ê°’ì´ ì˜¬ë°”ë¥¸ê°€?"]
        Con["ì¼ê´€ì„±<br/>(Consistency)<br/>ê·œì¹™ì— ë§ëŠ”ê°€?"]
        T["ì ì‹œì„±<br/>(Timeliness)<br/>ìµœì‹ ì¸ê°€?"]
        V["ìœ íš¨ì„±<br/>(Validity)<br/>í˜•ì‹ì´ ë§ëŠ”ê°€?"]
    end
```

### ì†Œí”„íŠ¸ì›¨ì–´ í…ŒìŠ¤íŠ¸ì™€ì˜ ë¹„êµ

| íŠ¹ì„± | ì†Œí”„íŠ¸ì›¨ì–´ í…ŒìŠ¤íŠ¸ | ë°ì´í„° í…ŒìŠ¤íŠ¸ |
|------|------------------|--------------|
| **ëŒ€ìƒ** | ì½”ë“œ | ë°ì´í„° |
| **ì‹œì ** | ë°°í¬ ì „ | íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘/í›„ |
| **ì…ë ¥** | ê³ ì • (mock) | ë³€ë™ (ì‹¤ì œ ë°ì´í„°) |
| **ì‹¤íŒ¨ ëŒ€ì‘** | ë°°í¬ ì¤‘ë‹¨ | ì•Œë¦¼/ì¬ì²˜ë¦¬/ê²©ë¦¬ |
| **ë„êµ¬** | JUnit, Jest | dbt, Great Expectations |

---

## dbt: ë³€í™˜ê³¼ í…ŒìŠ¤íŠ¸ì˜ í†µí•©

### dbtë€?

```mermaid
flowchart LR
    subgraph dbt ["dbt (data build tool)"]
        Models["SQL ëª¨ë¸"]
        Tests["í…ŒìŠ¤íŠ¸"]
        Docs["ë¬¸ì„œí™”"]
        
        Models --> Tests --> Docs
    end
    
    subgraph Workflow ["ì›Œí¬í”Œë¡œìš°"]
        Source["ì›ë³¸ ë°ì´í„°"]
        Transform["ë³€í™˜"]
        Target["ê²°ê³¼ í…Œì´ë¸”"]
        
        Source --> Transform --> Target
    end
    
    dbt --> Workflow
```

### í•µì‹¬ ì² í•™

1. **SQL ê¸°ë°˜**: ë³µì¡í•œ ì½”ë“œ ì—†ì´ SQLë§Œìœ¼ë¡œ ë³€í™˜
2. **ë²„ì „ ê´€ë¦¬**: Gitìœ¼ë¡œ ëª¨ë¸ ê´€ë¦¬
3. **í…ŒìŠ¤íŠ¸ ë‚´ì¥**: ìŠ¤í‚¤ë§ˆì— í…ŒìŠ¤íŠ¸ ì •ì˜
4. **ë¬¸ì„œ ìë™í™”**: ëª¨ë¸ ì •ë³´ ìë™ ìƒì„±
5. **ì˜ì¡´ì„± ê´€ë¦¬**: ref() í•¨ìˆ˜ë¡œ ëª¨ë¸ ê°„ ì˜ì¡´ì„±

### í”„ë¡œì íŠ¸ êµ¬ì¡°

```
my_dbt_project/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ stg_orders.sql
â”‚   â”‚   â””â”€â”€ schema.yml
â”‚   â”œâ”€â”€ marts/
â”‚   â”‚   â”œâ”€â”€ fct_orders.sql
â”‚   â”‚   â””â”€â”€ dim_customers.sql
â”‚   â””â”€â”€ schema.yml
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ custom_tests.sql
â”œâ”€â”€ macros/
â”œâ”€â”€ dbt_project.yml
â””â”€â”€ profiles.yml
```

---

## dbt í…ŒìŠ¤íŠ¸

### í…ŒìŠ¤íŠ¸ ì¢…ë¥˜

```mermaid
flowchart TB
    subgraph Tests ["dbt í…ŒìŠ¤íŠ¸ ìœ í˜•"]
        subgraph Schema ["ìŠ¤í‚¤ë§ˆ í…ŒìŠ¤íŠ¸"]
            S1["unique<br/>ì¤‘ë³µ ì—†ìŒ"]
            S2["not_null<br/>NULL ì—†ìŒ"]
            S3["accepted_values<br/>í—ˆìš©ê°’ ëª©ë¡"]
            S4["relationships<br/>ì°¸ì¡° ë¬´ê²°ì„±"]
        end
        
        subgraph Custom ["ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸"]
            C1["SQL ê¸°ë°˜"]
            C2["ë³µì¡í•œ ë¡œì§"]
            C3["ë§¤í¬ë¡œ ì¬ì‚¬ìš©"]
        end
    end
```

### schema.yml ì‘ì„±

```yaml
# models/marts/schema.yml
version: 2

models:
  - name: fct_orders
    description: "ì£¼ë¬¸ Fact í…Œì´ë¸”"
    columns:
      - name: order_id
        description: "ì£¼ë¬¸ ê³ ìœ  ID"
        data_tests:
          - unique
          - not_null
      
      - name: customer_id
        description: "ê³ ê° ID"
        data_tests:
          - not_null
          - relationships:
              to: ref('dim_customers')
              field: customer_id
      
      - name: order_status
        description: "ì£¼ë¬¸ ìƒíƒœ"
        data_tests:
          - accepted_values:
              values: ['pending', 'shipped', 'delivered', 'cancelled']
      
      - name: total_amount
        description: "ì£¼ë¬¸ ì´ì•¡"
        data_tests:
          - not_null
          # dbt_utils íŒ¨í‚¤ì§€ ì‚¬ìš©
          - dbt_utils.expression_is_true:
              expression: ">= 0"
```

### ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸

```sql
-- tests/assert_positive_revenue.sql
-- ì´ ë§¤ì¶œì´ ì–‘ìˆ˜ì¸ì§€ í™•ì¸

SELECT 
    order_date,
    SUM(total_amount) as daily_revenue
FROM {{ ref('fct_orders') }}
GROUP BY order_date
HAVING SUM(total_amount) < 0
```

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
dbt test

# íŠ¹ì • ëª¨ë¸ í…ŒìŠ¤íŠ¸
dbt test --select fct_orders

# ì‹¤íŒ¨ ì‹œ ìƒì„¸ ì •ë³´
dbt test --store-failures
```

---

## ë°ì´í„° Freshness

### Source Freshness

```yaml
# models/staging/sources.yml
version: 2

sources:
  - name: raw
    database: production
    schema: public
    freshness:
      warn_after: {count: 12, period: hour}
      error_after: {count: 24, period: hour}
    
    tables:
      - name: orders
        loaded_at_field: _etl_loaded_at
        
      - name: customers
        loaded_at_field: updated_at
```

```bash
# Freshness ì²´í¬
dbt source freshness
```

### Freshness ê²°ê³¼

```mermaid
flowchart TB
    subgraph Status ["Freshness ìƒíƒœ"]
        Pass["âœ… Pass<br/>12ì‹œê°„ ì´ë‚´"]
        Warn["âš ï¸ Warn<br/>12~24ì‹œê°„"]
        Error["âŒ Error<br/>24ì‹œê°„ ì´ˆê³¼"]
    end
```

---

## Great Expectations

### dbtì™€ì˜ ë¹„êµ

| íŠ¹ì„± | dbt | Great Expectations |
|------|-----|-------------------|
| **ì–¸ì–´** | SQL | Python |
| **ì í•©í•œ ê²½ìš°** | SQL ë³€í™˜ í›„ í…ŒìŠ¤íŠ¸ | ì›ë³¸ ë°ì´í„° ê²€ì¦ |
| **í•™ìŠµ ê³¡ì„ ** | ë‚®ìŒ | ì¤‘ê°„ |
| **ìœ ì—°ì„±** | ì œí•œì  | ë†’ìŒ |
| **ë¬¸ì„œí™”** | ìë™ | ìë™ (Data Docs) |

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
import great_expectations as gx

# Context ìƒì„±
context = gx.get_context()

# ë°ì´í„° ì†ŒìŠ¤ ì—°ê²°
datasource = context.sources.add_pandas("my_datasource")
data_asset = datasource.add_dataframe_asset("orders")

# Expectation Suite ì •ì˜
suite = context.add_expectation_suite("orders_suite")

# Expectations ì¶”ê°€
suite.add_expectation(
    gx.expectations.ExpectColumnValuesToBeUnique(column="order_id")
)
suite.add_expectation(
    gx.expectations.ExpectColumnValuesToNotBeNull(column="customer_id")
)
suite.add_expectation(
    gx.expectations.ExpectColumnValuesToBeBetween(
        column="total_amount",
        min_value=0,
        max_value=1000000
    )
)
suite.add_expectation(
    gx.expectations.ExpectColumnValuesToMatchRegex(
        column="email",
        regex=r"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$"
    )
)

# Validation ì‹¤í–‰
batch = data_asset.build_batch_request()
results = context.run_checkpoint(
    checkpoint_name="orders_checkpoint",
    batch_request=batch,
    expectation_suite_name="orders_suite"
)

# ê²°ê³¼ í™•ì¸
print(f"Success: {results.success}")
```

### Airflow ì—°ë™

```python
from airflow.decorators import dag, task
from datetime import datetime

@dag(
    dag_id="data_quality_pipeline",
    schedule="@daily",
    start_date=datetime(2024, 1, 1)
)
def quality_pipeline():
    
    @task
    def run_great_expectations(**context):
        import great_expectations as gx
        
        gx_context = gx.get_context()
        results = gx_context.run_checkpoint(
            checkpoint_name="orders_checkpoint"
        )
        
        if not results.success:
            raise ValueError("Data quality check failed!")
        
        return {"success": True, "statistics": results.statistics}
    
    @task
    def run_dbt_tests():
        import subprocess
        result = subprocess.run(["dbt", "test"], capture_output=True)
        
        if result.returncode != 0:
            raise ValueError(f"dbt tests failed: {result.stderr}")
    
    @task
    def load_to_warehouse(quality_result):
        # í’ˆì§ˆ ê²€ì¦ í†µê³¼ í›„ì—ë§Œ ë¡œë“œ
        print("Loading data to warehouse...")
    
    quality = run_great_expectations()
    dbt = run_dbt_tests()
    load_to_warehouse(quality)
    
    # dbtë„ í†µê³¼í•´ì•¼ í•¨
    dbt >> load_to_warehouse

quality_pipeline()
```

---

## ë°ì´í„° ê³„ë³´ (Lineage)

### ì™œ ê³„ë³´ê°€ ì¤‘ìš”í•œê°€?

```mermaid
flowchart TB
    subgraph Problem ["ë¬¸ì œ ìƒí™©"]
        P1["ëŒ€ì‹œë³´ë“œ ìˆ«ìê°€ ì´ìƒí•´ìš”"]
        P2["ì–´ë””ì„œ ì˜ëª»ëœ ê±°ì£ ?"]
        P3["ì–´ë–¤ í…Œì´ë¸”ì„ ë´ì•¼ í•˜ì£ ?"]
    end
    
    subgraph Lineage ["Lineageë¡œ í•´ê²°"]
        L1["ë°ì´í„° íë¦„ ì¶”ì "]
        L2["ì˜í–¥ ë²”ìœ„ íŒŒì•…"]
        L3["ì›ì¸ ë¶„ì„"]
    end
    
    Problem --> Lineage
```

### dbtì˜ ìë™ Lineage

```mermaid
flowchart LR
    subgraph Sources ["Sources"]
        S1[("raw.orders")]
        S2[("raw.customers")]
    end
    
    subgraph Staging ["Staging"]
        ST1["stg_orders"]
        ST2["stg_customers"]
    end
    
    subgraph Marts ["Marts"]
        M1["fct_orders"]
        M2["dim_customers"]
    end
    
    S1 --> ST1 --> M1
    S2 --> ST2 --> M2
    ST2 --> M1
```

```bash
# Lineage ë¬¸ì„œ ìƒì„±
dbt docs generate
dbt docs serve
```

### OpenLineage í‘œì¤€

```mermaid
flowchart TB
    subgraph Tools ["ë‹¤ì–‘í•œ ë„êµ¬"]
        Airflow["Airflow"]
        Spark["Spark"]
        dbt["dbt"]
        Flink["Flink"]
    end
    
    subgraph Standard ["OpenLineage"]
        OL["í†µí•© í¬ë§·"]
    end
    
    subgraph Catalog ["ë°ì´í„° ì¹´íƒˆë¡œê·¸"]
        Marquez["Marquez"]
        DataHub["DataHub"]
        Atlan["Atlan"]
    end
    
    Tools --> Standard --> Catalog
```

---

## ëª¨ë‹ˆí„°ë§ê³¼ ê´€ì¸¡ì„±

### í•µì‹¬ ì§€í‘œ

```mermaid
flowchart TB
    subgraph Metrics ["ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì§€í‘œ"]
        subgraph Availability ["ê°€ìš©ì„±"]
            A1["íŒŒì´í”„ë¼ì¸ ì„±ê³µë¥ "]
            A2["SLA ì¤€ìˆ˜ìœ¨"]
        end
        
        subgraph Quality ["í’ˆì§ˆ"]
            Q1["í…ŒìŠ¤íŠ¸ í†µê³¼ìœ¨"]
            Q2["ì´ìƒì¹˜ ë¹„ìœ¨"]
        end
        
        subgraph Freshness ["ì‹ ì„ ë„"]
            F1["ë°ì´í„° ì§€ì—°"]
            F2["ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸"]
        end
        
        subgraph Volume ["ë³¼ë¥¨"]
            V1["í–‰ ìˆ˜ ë³€í™”"]
            V2["íŒŒì¼ í¬ê¸°"]
        end
    end
```

### ì´ìƒ íƒì§€

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import count, avg, stddev

spark = SparkSession.builder.getOrCreate()

# ì˜¤ëŠ˜ ë°ì´í„°
today = spark.read.parquet("/data/today")
today_stats = today.agg(
    count("*").alias("row_count"),
    avg("amount").alias("avg_amount")
).collect()[0]

# íˆìŠ¤í† ë¦¬ (ìµœê·¼ 30ì¼ í‰ê· )
history = spark.read.parquet("/data/history_30d")
history_stats = history.agg(
    avg("daily_count").alias("avg_count"),
    stddev("daily_count").alias("std_count"),
    avg("daily_avg_amount").alias("avg_amount")
).collect()[0]

# ì´ìƒ íƒì§€ (3-sigma)
if abs(today_stats["row_count"] - history_stats["avg_count"]) > 3 * history_stats["std_count"]:
    alert("Row count anomaly detected!")
```

### ëŒ€ì‹œë³´ë“œ êµ¬ì„±

```mermaid
flowchart TB
    subgraph Dashboard ["ë°ì´í„° í’ˆì§ˆ ëŒ€ì‹œë³´ë“œ"]
        subgraph Overview ["ê°œìš”"]
            O1["ğŸŸ¢ 95% íŒŒì´í”„ë¼ì¸ ì„±ê³µ"]
            O2["ğŸŸ¡ 2ê°œ SLA ê²½ê³ "]
            O3["ğŸ”´ 1ê°œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨"]
        end
        
        subgraph Details ["ìƒì„¸"]
            D1["íŒŒì´í”„ë¼ì¸ë³„ ìƒíƒœ"]
            D2["í…ŒìŠ¤íŠ¸ ê²°ê³¼"]
            D3["ë°ì´í„° ë³¼ë¥¨ íŠ¸ë Œë“œ"]
        end
        
        subgraph Alerts ["ì•Œë¦¼"]
            AL1["ì‹¤íŒ¨ ì•Œë¦¼"]
            AL2["ì´ìƒ íƒì§€ ì•Œë¦¼"]
        end
    end
```

---

## í”„ë¡œë•ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ë°°í¬ ì „ í™•ì¸

```mermaid
flowchart TB
    subgraph Checklist ["í”„ë¡œë•ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸"]
        C1["âœ… ëª¨ë“  dbt í…ŒìŠ¤íŠ¸ í†µê³¼"]
        C2["âœ… Source freshness í™•ì¸"]
        C3["âœ… Lineage ë¬¸ì„œí™”"]
        C4["âœ… ì•Œë¦¼ ì„¤ì • ì™„ë£Œ"]
        C5["âœ… ë¡¤ë°± ê³„íš ìˆ˜ë¦½"]
        C6["âœ… ë‹´ë‹¹ì ì§€ì •"]
    end
```

### ì¼ì¼ ìš´ì˜

| ì‹œê°„ | ì‘ì—… | ë‹´ë‹¹ |
|------|------|------|
| 09:00 | ì•¼ê°„ ë°°ì¹˜ ê²°ê³¼ í™•ì¸ | ì˜¨ì½œ |
| 09:30 | í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ê²€í†  | ë°ì´í„° íŒ€ |
| 10:00 | ì´ìƒ ì•Œë¦¼ ì²˜ë¦¬ | í•´ë‹¹ ë‹´ë‹¹ì |
| ë§¤ì‹œ | ìë™ freshness ì²´í¬ | ìë™í™” |

---

## ì •ë¦¬

```mermaid
mindmap
  root((ë°ì´í„°<br/>í’ˆì§ˆ))
    í’ˆì§ˆ ì°¨ì›
      ì™„ì „ì„±
      ì •í™•ì„±
      ì¼ê´€ì„±
      ì ì‹œì„±
      ìœ íš¨ì„±
    dbt
      SQL ê¸°ë°˜
      ìŠ¤í‚¤ë§ˆ í…ŒìŠ¤íŠ¸
      Freshness
      Lineage
    Great Expectations
      Python ê¸°ë°˜
      ì»¤ìŠ¤í…€ ê²€ì¦
      Data Docs
    Lineage
      ë°ì´í„° íë¦„
      ì˜í–¥ ë¶„ì„
      OpenLineage
    ëª¨ë‹ˆí„°ë§
      ì„±ê³µë¥ 
      ì§€ì—° ì‹œê°„
      ì´ìƒ íƒì§€
      ëŒ€ì‹œë³´ë“œ
```

---

## ì‹œë¦¬ì¦ˆ ë§ˆë¬´ë¦¬

12í¸ì— ê±¸ì³ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ì˜ í•µì‹¬ì„ ë‹¤ë¤˜ìŠµë‹ˆë‹¤:

| Part | ì£¼ì œ | í•µì‹¬ ê¸°ìˆ  |
|------|------|----------|
| 1-2 | ê°œë… | OLTP/OLAP, ì•„í‚¤í…ì²˜ |
| 3-5 | Spark | RDD, DataFrame, ìµœì í™” |
| 6-7 | Airflow | DAG, TaskFlow, ìš´ì˜ |
| 8-9 | ìŠ¤íŠ¸ë¦¬ë° | Kafka, Spark Streaming |
| 10-11 | ì €ì¥ì†Œ | Lakehouse, ëª¨ë¸ë§ |
| 12 | í’ˆì§ˆ | í…ŒìŠ¤íŠ¸, ëª¨ë‹ˆí„°ë§ |

ì´ì œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ ì „ì²´ ê·¸ë¦¼ì„ ì´í•´í•˜ì…¨ì„ ê²ë‹ˆë‹¤. ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì ìš©í•˜ë©´ì„œ ê¹Šì´ë¥¼ ë”í•´ ê°€ì‹œê¸¸ ë°”ëë‹ˆë‹¤!

---

## ì°¸ê³  ìë£Œ

- [dbt Documentation](https://docs.getdbt.com/)
- [Great Expectations Documentation](https://docs.greatexpectations.io/)
- [OpenLineage](https://openlineage.io/)
- Monte Carlo, "Data Observability Explained"
- "Fundamentals of Data Engineering" (O'Reilly)
