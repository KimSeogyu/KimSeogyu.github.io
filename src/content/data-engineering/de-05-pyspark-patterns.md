---
public: true
title: "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #5: PySpark ì‹¤ì „ - ë°ì´í„° ì²˜ë¦¬ íŒ¨í„´ê³¼ ìµœì í™”"
date: '2026-01-02'
category: Data Engineering
series: data-engineering
tags: [Data Engineering, Performance, Spark, UDF]
excerpt: "ì‹¤ë¬´ì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” PySpark íŒ¨í„´ì„ ë°°ì›ë‹ˆë‹¤. DataFrame ì—°ì‚°, UDF ìµœì í™”, ì¡°ì¸ ì „ëµ, ìºì‹±, ê·¸ë¦¬ê³  í”¼í•´ì•¼ í•  ì•ˆí‹°íŒ¨í„´ê¹Œì§€."
---

# ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ì‹œë¦¬ì¦ˆ #5: PySpark ì‹¤ì „ - ë°ì´í„° ì²˜ë¦¬ íŒ¨í„´ê³¼ ìµœì í™”

> **ëŒ€ìƒ ë…ì**: ì¶©ë¶„í•œ ê²½í—˜ì„ ê°€ì§„ ë°±ì—”ë“œ/í’€ìŠ¤íƒ ì—”ì§€ë‹ˆì–´ë¡œ, Spark ê°œë…ì„ ìµíˆê³  ì‹¤ì „ ì½”ë“œë¥¼ ì‘ì„±í•˜ë ¤ëŠ” ë¶„

## ì´ í¸ì—ì„œ ë‹¤ë£¨ëŠ” ê²ƒ

4í¸ì—ì„œ Spark ë‚´ë¶€ë¥¼ ì´í•´í–ˆë‹¤ë©´, ì´ì œ **ì‹¤ì œë¡œ ì½”ë“œë¥¼ ì–´ë–»ê²Œ ì‘ì„±í•´ì•¼ í•˜ëŠ”ì§€** íŒ¨í„´ê³¼ ìµœì í™” ê¸°ë²•ì„ ë°°ì›ë‹ˆë‹¤.

---

## ìì£¼ ì‚¬ìš©í•˜ëŠ” DataFrame ì—°ì‚°

### ê¸°ë³¸ ì—°ì‚° ë§µ

```mermaid
flowchart TB
    subgraph Selection ["ì„ íƒ/í•„í„°"]
        S1["select()"]
        S2["filter() / where()"]
        S3["drop()"]
    end
    
    subgraph Transform ["ë³€í™˜"]
        T1["withColumn()"]
        T2["withColumnRenamed()"]
        T3["cast()"]
    end
    
    subgraph Aggregate ["ì§‘ê³„"]
        A1["groupBy()"]
        A2["agg()"]
        A3["pivot()"]
    end
    
    subgraph Join ["ì¡°ì¸"]
        J1["join()"]
        J2["crossJoin()"]
        J3["union()"]
    end
    
    subgraph Window ["ìœˆë„ìš°"]
        W1["over()"]
        W2["partitionBy()"]
        W3["orderBy()"]
    end
    
    Selection --> Transform --> Aggregate --> Output["ê²°ê³¼"]
    Join --> Aggregate
    Window --> Aggregate
```

### ì„ íƒê³¼ í•„í„°ë§

```python
from pyspark.sql import functions as F

# ì»¬ëŸ¼ ì„ íƒ - í•„ìš”í•œ ê²ƒë§Œ!
df.select("user_id", "name", "email")

# ì—¬ëŸ¬ ë°©ì‹ì˜ ì»¬ëŸ¼ ì°¸ì¡°
df.select(
    F.col("user_id"),
    df.name,
    df["email"]
)

# í•„í„°ë§
df.filter(F.col("age") > 20)
df.filter((F.col("age") > 20) & (F.col("city") == "Seoul"))

# SQL í‘œí˜„ì‹ë„ ê°€ëŠ¥
df.filter("age > 20 AND city = 'Seoul'")
```

### ì»¬ëŸ¼ ë³€í™˜

```python
# ìƒˆ ì»¬ëŸ¼ ì¶”ê°€
df.withColumn("age_group", 
    F.when(F.col("age") < 20, "teen")
     .when(F.col("age") < 30, "20s")
     .when(F.col("age") < 40, "30s")
     .otherwise("40+")
)

# íƒ€ì… ë³€í™˜
df.withColumn("amount", F.col("amount").cast("double"))

# ë¬¸ìì—´ ì²˜ë¦¬
df.withColumn("email_domain", 
    F.split(F.col("email"), "@").getItem(1)
)

# ë‚ ì§œ ì²˜ë¦¬
df.withColumn("year", F.year("created_at"))
df.withColumn("month", F.month("created_at"))
df.withColumn("date_str", F.date_format("created_at", "yyyy-MM-dd"))
```

### ì§‘ê³„ ì—°ì‚°

```python
# ê¸°ë³¸ ì§‘ê³„
df.groupBy("city").agg(
    F.count("*").alias("user_count"),
    F.avg("age").alias("avg_age"),
    F.sum("purchase_amount").alias("total_purchase"),
    F.max("last_login").alias("last_activity")
)

# ì—¬ëŸ¬ ê·¸ë£¹ ê¸°ì¤€
df.groupBy("city", "gender").count()

# Pivot (í–‰â†’ì—´)
df.groupBy("year").pivot("quarter", ["Q1", "Q2", "Q3", "Q4"]).sum("revenue")
```

### ìœˆë„ìš° í•¨ìˆ˜

```mermaid
flowchart TB
    subgraph WindowConcept ["ìœˆë„ìš° í•¨ìˆ˜ ê°œë…"]
        Data["ë°ì´í„°"]
        Partition["íŒŒí‹°ì…˜ë³„ ê·¸ë£¹í•‘<br/>(groupByì™€ ìœ ì‚¬)"]
        Order["ì •ë ¬"]
        Frame["ìœˆë„ìš° í”„ë ˆì„"]
        Calculate["ê³„ì‚° (rank, sum, etc.)"]
        
        Data --> Partition --> Order --> Frame --> Calculate
    end
    
    Note["âœ… groupByì™€ ë‹¬ë¦¬<br/>ì›ë³¸ í–‰ ìœ ì§€"]
```

```python
from pyspark.sql.window import Window

# ìœˆë„ìš° ì •ì˜
window_spec = Window.partitionBy("user_id").orderBy("timestamp")

# ìˆœìœ„ (íŒŒí‹°ì…˜ ë‚´ ìˆœì„œ)
df.withColumn("row_num", F.row_number().over(window_spec))
df.withColumn("rank", F.rank().over(window_spec))

# ì´ì „/ë‹¤ìŒ ê°’
df.withColumn("prev_value", F.lag("value", 1).over(window_spec))
df.withColumn("next_value", F.lead("value", 1).over(window_spec))

# ëˆ„ì  í•©ê³„
df.withColumn("cumsum", F.sum("amount").over(window_spec))

# íŒŒí‹°ì…˜ ì „ì²´ ê¸°ì¤€ (ì •ë ¬ ì—†ì´)
unbounded = Window.partitionBy("user_id")
df.withColumn("user_total", F.sum("amount").over(unbounded))
```

---

## UDF vs Built-in Functions

### ì™œ Built-inì„ ì¨ì•¼ í•˜ëŠ”ê°€?

```mermaid
flowchart TB
    subgraph UDF ["Python UDF"]
        U1["Python í•¨ìˆ˜ ì •ì˜"]
        U2["ì§ë ¬í™” (pickle)"]
        U3["JVM â†’ Python ì „ì†¡"]
        U4["Pythonì—ì„œ ì‹¤í–‰"]
        U5["ê²°ê³¼ ì§ë ¬í™”"]
        U6["Python â†’ JVM"]
        
        U1 --> U2 --> U3 --> U4 --> U5 --> U6
    end
    
    subgraph BuiltIn ["Built-in Function"]
        B1["API í˜¸ì¶œ"]
        B2["JVMì—ì„œ ì§ì ‘ ì‹¤í–‰"]
        B3["Catalyst ìµœì í™”"]
        
        B1 --> B2 --> B3
    end
    
    UDF -->|"ğŸ¢ 20~100x ëŠë¦¼"| Slow["ì„±ëŠ¥ ì €í•˜"]
    BuiltIn -->|"ğŸš€ ë¹ ë¦„"| Fast["ìµœì  ì„±ëŠ¥"]
```

### ë¹„êµ ì˜ˆì‹œ

```python
# âŒ ë‚˜ìœ ì˜ˆ: UDF ì‚¬ìš©
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

@udf(returnType=StringType())
def extract_domain(email):
    if email:
        return email.split("@")[-1]
    return None

df.withColumn("domain", extract_domain(F.col("email")))

# âœ… ì¢‹ì€ ì˜ˆ: Built-in í•¨ìˆ˜ ì‚¬ìš©
df.withColumn("domain", 
    F.split(F.col("email"), "@").getItem(1)
)
```

### ì •ë§ UDFê°€ í•„ìš”í•œ ê²½ìš°: Pandas UDF

```mermaid
flowchart LR
    subgraph Types ["UDF ìœ í˜•ë³„ ì„±ëŠ¥"]
        T1["Python UDF<br/>ğŸ¢ ëŠë¦¼"]
        T2["Pandas UDF<br/>âš¡ ë¹ ë¦„"]
        T3["Built-in<br/>ğŸš€ ê°€ì¥ ë¹ ë¦„"]
    end
    
    T1 -->|"ë²¡í„°í™”"| T2 -->|"ê°€ëŠ¥í•˜ë©´"| T3
```

```python
import pandas as pd
from pyspark.sql.functions import pandas_udf

# Pandas UDF - Series â†’ Series (ë²¡í„°í™”)
@pandas_udf("double")
def calculate_zscore(values: pd.Series) -> pd.Series:
    return (values - values.mean()) / values.std()

df.withColumn("zscore", calculate_zscore(F.col("value")))

# Pandas UDF - GroupBy Aggregate
@pandas_udf("double")
def median_value(v: pd.Series) -> float:
    return v.median()

df.groupBy("category").agg(median_value(F.col("price")))
```

---

## ì¡°ì¸ ìµœì í™”

### ì¡°ì¸ ì¢…ë¥˜ì™€ ì„ íƒ

```mermaid
flowchart TB
    subgraph JoinTypes ["ì¡°ì¸ ì¢…ë¥˜"]
        direction TB
        Broadcast["Broadcast Join<br/>ì‘ì€ í…Œì´ë¸”ì„ ì „ì²´ ë³µì‚¬"]
        SortMerge["Sort-Merge Join<br/>ì •ë ¬ í›„ ë³‘í•©"]
        Shuffle["Shuffle Hash Join<br/>í•´ì‹œ ê¸°ë°˜ ì¬ë°°ì¹˜"]
    end
    
    Decision{"ì‘ì€ í…Œì´ë¸”ì´<br/>10MB ì´í•˜?"}
    
    Decision -->|"ì˜ˆ"| Broadcast
    Decision -->|"ì•„ë‹ˆì˜¤"| SortMerge
```

### Broadcast Join (í•„ìˆ˜!)

```mermaid
flowchart TB
    subgraph NoBroadcast ["ì¼ë°˜ ì¡°ì¸"]
        L1["Large Table<br/>100GB"]
        S1["Small Table<br/>10MB"]
        Shuffle1["Shuffle ğŸ”€"]
        
        L1 --> Shuffle1
        S1 --> Shuffle1
    end
    
    subgraph WithBroadcast ["Broadcast ì¡°ì¸"]
        L2["Large Table<br/>100GB"]
        S2["Small Table<br/>10MB"]
        
        S2 -->|"ê° Executorë¡œ ë³µì‚¬"| E1["Executor 1"]
        S2 --> E2["Executor 2"]
        S2 --> E3["Executor 3"]
        
        L2 -->|"Shuffle ì—†ìŒ"| E1 & E2 & E3
    end
    
    NoBroadcast -->|"âŒ ëŠë¦¼"| Slow
    WithBroadcast -->|"âœ… ë¹ ë¦„"| Fast
```

```python
from pyspark.sql.functions import broadcast

# ì‘ì€ í…Œì´ë¸”ì— broadcast íŒíŠ¸
result = large_df.join(
    broadcast(small_df), 
    "join_key"
)

# ë˜ëŠ” ì„¤ì •ìœ¼ë¡œ ìë™ ì ìš©
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 10 * 1024 * 1024)  # 10MB
```

### ì¡°ì¸ ìˆœì„œ ìµœì í™”

```python
# âŒ ë‚˜ìœ ì˜ˆ: í•„í„° í›„ì¡°ì¸ì´ ì•„ë‹˜
result = df1.join(df2, "key").filter(df1.status == "active")

# âœ… ì¢‹ì€ ì˜ˆ: ì¡°ì¸ ì „ì— í•„í„°
df1_filtered = df1.filter(df1.status == "active")
result = df1_filtered.join(df2, "key")
```

---

## ìºì‹±ê³¼ ì²´í¬í¬ì¸íŒ…

### ì–¸ì œ ìºì‹œí•˜ëŠ”ê°€?

```mermaid
flowchart TB
    Q1{"ê°™ì€ DataFrameì„<br/>ì—¬ëŸ¬ ë²ˆ ì‚¬ìš©?"}
    Q2{"ê³„ì‚° ë¹„ìš©ì´<br/>ë¹„ì‹¼ê°€?"}
    Q3{"ë©”ëª¨ë¦¬ì—<br/>ë“¤ì–´ê°€ëŠ”ê°€?"}
    
    Q1 -->|"ì˜ˆ"| Q2
    Q1 -->|"ì•„ë‹ˆì˜¤"| NoCache["ìºì‹œ ë¶ˆí•„ìš”"]
    Q2 -->|"ì˜ˆ"| Q3
    Q2 -->|"ì•„ë‹ˆì˜¤"| NoCache
    Q3 -->|"ì˜ˆ"| Cache["âœ… cache() ì‚¬ìš©"]
    Q3 -->|"ì•„ë‹ˆì˜¤"| Persist["persist(DISK) ì‚¬ìš©"]
```

```python
# ê¸°ë³¸ ìºì‹œ (ë©”ëª¨ë¦¬)
expensive_df = df.groupBy("category").agg(...)
expensive_df.cache()

# ì²« ë²ˆì§¸ Actionì—ì„œ ìºì‹œë¨
expensive_df.count()  

# ì´í›„ ì¬ì‚¬ìš© ì‹œ ìºì‹œì—ì„œ ì½ìŒ
expensive_df.filter(...).show()
expensive_df.select(...).write.parquet(...)

# ìºì‹œ í•´ì œ
expensive_df.unpersist()
```

### cache() vs persist()

```python
from pyspark import StorageLevel

# cache() = persist(MEMORY_AND_DISK)
df.cache()

# ëª…ì‹œì  ìŠ¤í† ë¦¬ì§€ ë ˆë²¨
df.persist(StorageLevel.MEMORY_ONLY)
df.persist(StorageLevel.MEMORY_AND_DISK)
df.persist(StorageLevel.DISK_ONLY)
df.persist(StorageLevel.MEMORY_AND_DISK_SER)  # ì§ë ¬í™”í•˜ì—¬ ì €ì¥
```

### ì²´í¬í¬ì¸íŒ…

```python
# checkpointëŠ” ê³„ë³´(lineage)ë¥¼ ëŠìŒ
spark.sparkContext.setCheckpointDir("hdfs://path/checkpoints")

# ë³µì¡í•œ ë³€í™˜ í›„
result = complex_transformations(df)
result.checkpoint()

# ì´í›„ ì¥ì•  ì‹œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µêµ¬
```

---

## ì•ˆí‹°íŒ¨í„´ í”¼í•˜ê¸°

### âŒ collect() ë‚¨ìš©

```mermaid
flowchart LR
    subgraph Cluster ["í´ëŸ¬ìŠ¤í„°"]
        E1["1TB"]
        E2["1TB"]
        E3["1TB"]
    end
    
    subgraph Driver ["Driver"]
        D["ë©”ëª¨ë¦¬: 4GB"]
    end
    
    Cluster -->|"collect()"| Driver
    Driver -->|"ğŸ’¥ OOM"| Crash["OutOfMemory!"]
```

```python
# âŒ ë‚˜ìœ ì˜ˆ
all_data = df.collect()  # ì „ì²´ë¥¼ Driverë¡œ!
for row in all_data:
    process(row)

# âœ… ì¢‹ì€ ì˜ˆ: ì§‘ê³„ í›„ collect
summary = df.groupBy("category").count().collect()

# âœ… ì¢‹ì€ ì˜ˆ: limit ì‚¬ìš©
sample = df.limit(1000).collect()

# âœ… ì¢‹ì€ ì˜ˆ: Iterator ì‚¬ìš©
for row in df.toLocalIterator():
    process(row)  # í•œ ë²ˆì— í•˜ë‚˜ì”©
```

### âŒ ì‘ì€ íŒŒì¼ ë¬¸ì œ

```mermaid
flowchart TB
    subgraph Problem ["ë¬¸ì œ ìƒí™©"]
        P1["10,000ê°œ íŒŒì¼"]
        P2["ê° 1MB"]
        P3["ì´ 10GB"]
        
        P1 --> Overhead["âŒ íŒŒì¼ ì˜¤í”ˆ ì˜¤ë²„í—¤ë“œ<br/>âŒ ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ ë¹„ìš©"]
    end
    
    subgraph Solution ["í•´ê²°ì±…"]
        S1["100ê°œ íŒŒì¼"]
        S2["ê° 100MB"]
        S3["ì´ 10GB"]
        
        S1 --> Efficient["âœ… I/O íš¨ìœ¨ì "]
    end
```

```python
# âŒ ë‚˜ìœ ì˜ˆ: íŒŒí‹°ì…˜ë§ˆë‹¤ íŒŒì¼ ìƒì„±
df.write.parquet("output/")  # íŒŒí‹°ì…˜ ìˆ˜ë§Œí¼ íŒŒì¼

# âœ… ì¢‹ì€ ì˜ˆ: coalesceë¡œ íŒŒì¼ ìˆ˜ ì¡°ì ˆ
df.coalesce(10).write.parquet("output/")  # 10ê°œ íŒŒì¼

# âœ… ì¢‹ì€ ì˜ˆ: ì ì • í¬ê¸°ë¡œ ë¶„í• 
df.repartition(100).write.parquet("output/")  # 100ê°œ íŒŒì¼
```

### âŒ ë¶ˆí•„ìš”í•œ Shuffle

```python
# âŒ ë‚˜ìœ ì˜ˆ: groupBy ë‘ ë²ˆ
result = df.groupBy("a").count() \
           .groupBy("a").agg(F.sum("count"))

# âœ… ì¢‹ì€ ì˜ˆ: í•œ ë²ˆì— ì²˜ë¦¬
result = df.groupBy("a").agg(F.count("*").alias("count"))
```

---

## ì‹¤ì „ ì˜ˆì œ: ë¡œê·¸ ë¶„ì„ íŒŒì´í”„ë¼ì¸

```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window

spark = SparkSession.builder \
    .appName("LogAnalysis") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

# 1. ë°ì´í„° ë¡œë“œ (í•„ìš”í•œ ì»¬ëŸ¼ë§Œ)
logs = spark.read.json("logs/*.json").select(
    "timestamp", "user_id", "event_type", "page", "duration"
)

# 2. ë°ì´í„° ì •ì œ
cleaned = logs \
    .filter(F.col("user_id").isNotNull()) \
    .withColumn("event_date", F.to_date("timestamp")) \
    .withColumn("event_hour", F.hour("timestamp"))

# 3. ì—¬ëŸ¬ ë²ˆ ì‚¬ìš©í•  ê²ƒì´ë¯€ë¡œ ìºì‹œ
cleaned.cache()

# 4. ì¼ë³„ ì§‘ê³„
daily_stats = cleaned.groupBy("event_date").agg(
    F.countDistinct("user_id").alias("dau"),
    F.count("*").alias("total_events"),
    F.avg("duration").alias("avg_duration")
)

# 5. ì‹œê°„ëŒ€ë³„ íŒ¨í„´
hourly_pattern = cleaned.groupBy("event_hour").agg(
    F.count("*").alias("events")
).orderBy("event_hour")

# 6. ìœ ì €ë³„ ì„¸ì…˜ ë¶„ì„ (ìœˆë„ìš° í•¨ìˆ˜)
user_window = Window.partitionBy("user_id").orderBy("timestamp")

sessions = cleaned \
    .withColumn("prev_timestamp", F.lag("timestamp").over(user_window)) \
    .withColumn("time_gap", 
        F.unix_timestamp("timestamp") - F.unix_timestamp("prev_timestamp")) \
    .withColumn("new_session", 
        F.when(F.col("time_gap") > 1800, 1).otherwise(0)) \
    .withColumn("session_id", 
        F.sum("new_session").over(user_window))

# 7. ì €ì¥
daily_stats.write.mode("overwrite").parquet("output/daily_stats")
hourly_pattern.write.mode("overwrite").parquet("output/hourly_pattern")

# 8. ìºì‹œ í•´ì œ
cleaned.unpersist()
```

---

## ì •ë¦¬

```mermaid
mindmap
  root((PySpark<br/>ì‹¤ì „))
    DataFrame ì—°ì‚°
      select, filter
      withColumn
      groupBy, agg
      Window í•¨ìˆ˜
    UDF ìµœì í™”
      Built-in ìš°ì„ 
      Pandas UDF
      Python UDF í”¼í•˜ê¸°
    ì¡°ì¸
      Broadcast Join
      ì¡°ì¸ ì „ í•„í„°
      ì‘ì€ í…Œì´ë¸” íŒíŠ¸
    ìºì‹±
      ì—¬ëŸ¬ ë²ˆ ì‚¬ìš© ì‹œ
      cache vs persist
      unpersist ìŠì§€ ì•Šê¸°
    ì•ˆí‹°íŒ¨í„´
      collect ë‚¨ìš©
      ì‘ì€ íŒŒì¼ ë¬¸ì œ
      ë¶ˆí•„ìš”í•œ Shuffle
```

---

## ë‹¤ìŒ í¸ ì˜ˆê³ 

**6í¸: Airflow í•µì‹¬ ê°œë…**ì—ì„œëŠ” ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ ë‹¤ë£¹ë‹ˆë‹¤:

- ì™œ cronìœ¼ë¡œëŠ” ë¶€ì¡±í•œê°€?
- DAG, Operator, Task ì´í•´
- TaskFlow API (Airflow 2.0+)
- ìŠ¤ì¼€ì¤„ë§ê³¼ Backfill

---

## ì°¸ê³  ìë£Œ

- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)
- [Spark SQL Built-in Functions](https://spark.apache.org/docs/latest/api/sql/)
- Databricks, "PySpark Best Practices"
- [Window Functions Guide](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-window.html)
